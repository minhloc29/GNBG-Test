2025-06-20 18:23:24 INFO Initializing first population
2025-06-20 18:23:24 INFO Initializing population from 4 seed files...
2025-06-20 18:23:24 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:23:29 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1723
2025-06-20 18:23:29 INFO FeHistory: [ 7.41205565e+05  1.13432786e+06  1.16447463e+06 ... -3.82619842e+02
 -3.82619862e+02 -3.82619853e+02]
2025-06-20 18:23:29 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:23:29 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:23:29 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:23:33 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2138
2025-06-20 18:23:33 INFO FeHistory: [310979.71494488 280402.45000304 193422.61770731 ...   -912.85702535
   -912.85716702   -912.85718546]
2025-06-20 18:23:33 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:23:33 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:23:33 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:23:38 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2130
2025-06-20 18:23:38 INFO FeHistory: [190039.41484733 143243.63518498 198857.80012361 ...   -656.78876246
   -656.78881917   -656.78872758]
2025-06-20 18:23:38 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:23:38 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:23:38 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:23:53 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0355
2025-06-20 18:23:53 INFO FeHistory: [177.91810848 220.39683052 171.99872926 ... -67.8404016  -67.84038018
 -67.8403242 ]
2025-06-20 18:23:53 INFO Expected Optimum FE: -100
2025-06-20 18:23:53 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:23:53 INFO Unimodal AOCC mean: 0.1723
2025-06-20 18:23:53 INFO Multimodal (single component) AOCC mean: 0.2134
2025-06-20 18:23:53 INFO Multimodal (multiple components) AOCC mean: 0.0355
2025-06-20 18:23:53 INFO AOCC mean: 0.1587
2025-06-20 18:23:53 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:24:01 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1480
2025-06-20 18:24:01 INFO FeHistory: [ 9.85676638e+05  9.77591891e+05  9.85676638e+05 ...  1.50834346e+02
 -3.82313458e+02  4.41637336e+02]
2025-06-20 18:24:01 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:24:01 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-20 18:24:01 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:24:09 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2351
2025-06-20 18:24:09 INFO FeHistory: [182070.08699487 181109.94325127 182070.08699487 ...   -910.15294437
   -912.8125625    -542.8727263 ]
2025-06-20 18:24:09 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:24:09 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-20 18:24:09 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:24:18 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2185
2025-06-20 18:24:18 INFO FeHistory: [280619.09265994 271550.29276317 280619.09265994 ...   -655.23327695
   -656.70230376   -487.21115257]
2025-06-20 18:24:18 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:24:18 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-20 18:24:18 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:24:48 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0000
2025-06-20 18:24:48 INFO FeHistory: [172.76389019 183.4285142  172.76389019 ... 157.82739776  98.44583808
 106.46870351]
2025-06-20 18:24:48 INFO Expected Optimum FE: -100
2025-06-20 18:24:48 INFO Unimodal AOCC mean: 0.1480
2025-06-20 18:24:48 INFO Multimodal (single component) AOCC mean: 0.2268
2025-06-20 18:24:48 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:24:48 INFO AOCC mean: 0.1504
2025-06-20 18:24:48 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:24:52 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.0455
2025-06-20 18:24:52 INFO FeHistory: [ 9.36287110e+05  1.01957197e+06  1.03235210e+06 ... -3.80484801e+02
 -3.80628708e+02 -3.81072221e+02]
2025-06-20 18:24:52 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:24:52 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 18:24:52 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:24:57 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.0589
2025-06-20 18:24:57 INFO FeHistory: [148771.13377903 173443.54412063 155854.17416791 ...   -911.77159235
   -911.71758311   -911.28635101]
2025-06-20 18:24:57 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:24:57 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 18:24:57 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:25:01 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.0368
2025-06-20 18:25:01 INFO FeHistory: [172956.30348953 160737.03520687 229839.07808575 ...   -650.28467352
   -651.14815769   -652.43885636]
2025-06-20 18:25:01 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:25:01 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 18:25:01 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:25:16 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0278
2025-06-20 18:25:16 INFO FeHistory: [212.15802675 149.597614   166.3618392  ... -73.73089269 -73.53378827
 -73.7880004 ]
2025-06-20 18:25:16 INFO Expected Optimum FE: -100
2025-06-20 18:25:16 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 18:25:16 INFO Unimodal AOCC mean: 0.0455
2025-06-20 18:25:16 INFO Multimodal (single component) AOCC mean: 0.0479
2025-06-20 18:25:16 INFO Multimodal (multiple components) AOCC mean: 0.0278
2025-06-20 18:25:16 INFO AOCC mean: 0.0423
2025-06-20 18:25:16 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:25:21 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.2053
2025-06-20 18:25:21 INFO FeHistory: [ 8.55194846e+05  1.19327145e+06  5.85749362e+05 ... -3.82617409e+02
 -3.82617198e+02 -3.82617353e+02]
2025-06-20 18:25:21 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:25:21 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:25:21 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:25:27 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2613
2025-06-20 18:25:27 INFO FeHistory: [115885.4174318  216776.4007516  210166.61072544 ...   -912.85686353
   -912.85654931   -912.85635979]
2025-06-20 18:25:27 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:25:27 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:25:27 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:25:33 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2552
2025-06-20 18:25:33 INFO FeHistory: [146625.01214202 168855.16572597 241173.8983144  ...   -656.78857113
   -656.78856542   -656.78873004]
2025-06-20 18:25:33 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:25:33 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:25:33 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:25:49 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0256
2025-06-20 18:25:49 INFO FeHistory: [180.88296345 194.07021091 178.65071598 ... -63.80647518 -64.6489998
 -63.25296132]
2025-06-20 18:25:49 INFO Expected Optimum FE: -100
2025-06-20 18:25:49 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:25:49 INFO Unimodal AOCC mean: 0.2053
2025-06-20 18:25:49 INFO Multimodal (single component) AOCC mean: 0.2582
2025-06-20 18:25:49 INFO Multimodal (multiple components) AOCC mean: 0.0256
2025-06-20 18:25:49 INFO AOCC mean: 0.1868
2025-06-20 18:25:49 INFO Started evolutionary loop, best so far: 0.18684680724357763
2025-06-20 18:25:49 INFO --- Performing Long-Term Reflection at Generation 1 ---
2025-06-20 18:25:53 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianMutationDE vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianMutationDE uses adaptive Gaussian mutation to escape local optima and incorporates a decaying mutation scale, leading to better exploration and exploitation. EnhancedArchiveGuidedDE, while using an archive, relies on a simpler DE mutation strategy and lacks mechanisms for adaptive parameter adjustments.  (second best) AdaptiveGaussianSamplingEA vs (second worst) AdaptiveMultimodalOptimizerImproved: AdaptiveGaussianSamplingEA leverages adaptive Gaussian sampling and tournament selection for efficient population updates, resulting in better convergence compared to the AdaptiveMultimodalOptimizerImproved which uses local search and tabu search, which may get stuck in local optima. Comparing (1st) vs (2nd), we see that AdaptiveGaussianMutationDE's differential evolution strategy offers a more robust mechanism for population diversity compared to AdaptiveGaussianSamplingEA's recombination and adaptive mutation which can sometimes lead to premature convergence. (3rd) vs (4th): AdaptiveMultimodalOptimizerImproved's multimodal exploration, via perturbations and adaptive temperature, is more sophisticated than EnhancedArchiveGuidedDE's archive-based approach, though both lack the fine-tuned mutation strategies of top-ranked algorithms. Comparing (second worst) vs (worst), we see that AdaptiveMultimodalOptimizerImproved's incorporation of local search and simulated annealing is more effective at refining solutions than EnhancedArchiveGuidedDE's archive-guided DE, which can struggle with high dimensionality. Overall:  The top-performing algorithms utilize adaptive mechanisms for mutation and exploration, such as Gaussian perturbations and scaling factors, while effectively managing diversity through strategies like differential evolution and tournament selection. The lower-ranked algorithms often rely on less sophisticated methods or lack the adaptive element to escape local optima.  Successful heuristics involve a balanced combination of exploration (diverse sampling) and exploitation (refinement near good solutions), adaptively adjusting parameters according to the search progress.

**Experience:**

Designing effective heuristics requires a careful balance between exploration and exploitation, utilizing adaptive mechanisms to adjust search strategies based on the algorithm's progress.  Parameter tuning and incorporating robust mutation/recombination operators are crucial for high-dimensional optimization problems.

2025-06-20 18:25:53 INFO Generating offspring via Crossover...
2025-06-20 18:26:02 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:26:08 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.2008
2025-06-20 18:26:08 INFO FeHistory: [ 1.17151684e+06  1.12487470e+06  1.44185203e+06 ... -3.82619313e+02
 -3.82619436e+02 -3.82619676e+02]
2025-06-20 18:26:08 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:26:08 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.1 # Percentage of population initialized with Gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Evolution Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation (with probability)
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        population_gaussian = np.random.normal(0, (self.upper_bounds - self.lower_bounds)/4, size=(pop_size_gaussian, self.dim)) #Gaussian sampling around 0
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:26:08 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:26:13 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.3064
2025-06-20 18:26:13 INFO FeHistory: [184225.10507848 157013.22939711 156831.05402729 ...   -912.85724871
   -912.85708606   -912.85724871]
2025-06-20 18:26:13 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:26:13 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.1 # Percentage of population initialized with Gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Evolution Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation (with probability)
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        population_gaussian = np.random.normal(0, (self.upper_bounds - self.lower_bounds)/4, size=(pop_size_gaussian, self.dim)) #Gaussian sampling around 0
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:26:13 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:26:19 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.3208
2025-06-20 18:26:19 INFO FeHistory: [199146.64735329 213300.56348484 272678.12918304 ...   -656.78897232
   -656.78897231   -656.78897231]
2025-06-20 18:26:19 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:26:19 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.1 # Percentage of population initialized with Gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Evolution Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation (with probability)
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        population_gaussian = np.random.normal(0, (self.upper_bounds - self.lower_bounds)/4, size=(pop_size_gaussian, self.dim)) #Gaussian sampling around 0
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:26:19 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:26:35 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0268
2025-06-20 18:26:35 INFO FeHistory: [189.01851109 188.18229636 195.21122641 ... -68.83221282 -68.05620484
 -66.91721697]
2025-06-20 18:26:35 INFO Expected Optimum FE: -100
2025-06-20 18:26:35 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.1 # Percentage of population initialized with Gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Evolution Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation (with probability)
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        population_gaussian = np.random.normal(0, (self.upper_bounds - self.lower_bounds)/4, size=(pop_size_gaussian, self.dim)) #Gaussian sampling around 0
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:26:35 INFO Unimodal AOCC mean: 0.2008
2025-06-20 18:26:35 INFO Multimodal (single component) AOCC mean: 0.3136
2025-06-20 18:26:35 INFO Multimodal (multiple components) AOCC mean: 0.0268
2025-06-20 18:26:35 INFO AOCC mean: 0.2137
2025-06-20 18:26:43 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:26:43 ERROR Can not run the algorithm
2025-06-20 18:26:43 INFO Run function 4 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 18:26:43 INFO FeHistory: []
2025-06-20 18:26:43 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:26:43 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:26:43 ERROR Can not run the algorithm
2025-06-20 18:26:43 INFO Run function 7 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 18:26:43 INFO FeHistory: []
2025-06-20 18:26:43 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:26:43 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:26:43 ERROR Can not run the algorithm
2025-06-20 18:26:43 INFO Run function 8 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 18:26:43 INFO FeHistory: []
2025-06-20 18:26:43 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:26:43 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:26:43 ERROR Can not run the algorithm
2025-06-20 18:26:43 INFO Run function 24 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 18:26:43 INFO FeHistory: []
2025-06-20 18:26:43 INFO Expected Optimum FE: -100
2025-06-20 18:26:43 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:26:43 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:26:43 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:26:43 INFO AOCC mean: 0.0000
2025-06-20 18:26:43 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Worse code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Analyze & experience
- Comparing (best) AdaptiveGaussianMutationDE vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianMutationDE uses adaptive Gaussian mutation to escape local optima and incorporates a decaying mutation scale, leading to better exploration and exploitation. EnhancedArchiveGuidedDE, while using an archive, relies on a simpler DE mutation strategy and lacks mechanisms for adaptive parameter adjustments.  (second best) AdaptiveGaussianSamplingEA vs (second worst) AdaptiveMultimodalOptimizerImproved: AdaptiveGaussianSamplingEA leverages adaptive Gaussian sampling and tournament selection for efficient population updates, resulting in better convergence compared to the AdaptiveMultimodalOptimizerImproved which uses local search and tabu search, which may get stuck in local optima. Comparing (1st) vs (2nd), we see that AdaptiveGaussianMutationDE's differential evolution strategy offers a more robust mechanism for population diversity compared to AdaptiveGaussianSamplingEA's recombination and adaptive mutation which can sometimes lead to premature convergence. (3rd) vs (4th): AdaptiveMultimodalOptimizerImproved's multimodal exploration, via perturbations and adaptive temperature, is more sophisticated than EnhancedArchiveGuidedDE's archive-based approach, though both lack the fine-tuned mutation strategies of top-ranked algorithms. Comparing (second worst) vs (worst), we see that AdaptiveMultimodalOptimizerImproved's incorporation of local search and simulated annealing is more effective at refining solutions than EnhancedArchiveGuidedDE's archive-guided DE, which can struggle with high dimensionality. Overall:  The top-performing algorithms utilize adaptive mechanisms for mutation and exploration, such as Gaussian perturbations and scaling factors, while effectively managing diversity through strategies like differential evolution and tournament selection. The lower-ranked algorithms often rely on less sophisticated methods or lack the adaptive element to escape local optima.  Successful heuristics involve a balanced combination of exploration (diverse sampling) and exploitation (refinement near good solutions), adaptively adjusting parameters according to the search progress.
- Designing effective heuristics requires a careful balance between exploration and exploitation, utilizing adaptive mechanisms to adjust search strategies based on the algorithm's progress.  Parameter tuning and incorporating robust mutation/recombination operators are crucial for high-dimensional optimization problems.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:26:43 INFO Generation 2, best so far: 0.21369870938149668
2025-06-20 18:26:43 INFO --- Performing Long-Term Reflection at Generation 2 ---
2025-06-20 18:26:48 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveDifferentialEvolutionWithGaussianSampling vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveDifferentialEvolutionWithGaussianSampling incorporates adaptive Gaussian sampling alongside differential evolution, leading to better exploration of the search space and handling of multimodal functions.  EnhancedArchiveGuidedDE, while using an archive, lacks the adaptive exploration mechanisms.  (second best) AdaptiveGaussianMutationDE vs (second worst) AdaptiveMultimodalOptimizerImproved shows AdaptiveGaussianMutationDE's direct use of Gaussian mutation for escaping local optima, while AdaptiveMultimodalOptimizerImproved relies on a more indirect simulated annealing approach. Comparing (1st) vs (2nd), we see that the adaptive Gaussian sampling in AdaptiveDifferentialEvolutionWithGaussianSampling provides a more controlled and efficient way to introduce diversity than the solely mutation-based approach of AdaptiveGaussianMutationDE. (3rd) AdaptiveGaussianSamplingEA vs (4th) AdaptiveMultimodalOptimizerImproved reveals that AdaptiveGaussianSamplingEA's structured approach of tournament selection, Gaussian recombination, and adaptive mutation generally leads to better optimization performance than AdaptiveMultimodalOptimizerImproved's more heuristic-driven local search and simulated annealing. Comparing (second worst) AdaptiveMultimodalOptimizerImproved vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveMultimodalOptimizerImproved employs a local search strategy and tabu list, aiming to avoid redundant exploration, although this is less effective than the adaptive sampling and evolutionary mechanisms used in the higher-ranked algorithms. Overall: The top-performing algorithms consistently utilize adaptive mechanisms (adaptive Gaussian sampling, mutation scaling, sigma decay) and efficient evolutionary operators (differential evolution, tournament selection) to explore and exploit the search space more effectively than the lower-ranked algorithms which rely more on simple random sampling and less sophisticated local search strategies.


**Experience:**

Adaptive sampling and mutation strategies alongside robust evolutionary operators are crucial for high-dimensional optimization.  Combining exploration and exploitation methods, along with adaptive parameter adjustments, significantly improves performance in complex search spaces.  Careful consideration of diversity mechanisms and local search enhancements is essential for efficient global optimization.

2025-06-20 18:26:48 INFO Generating offspring via Crossover...
2025-06-20 18:26:57 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:27:02 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.0001
2025-06-20 18:27:02 INFO FeHistory: [ 8.27858106e+05  1.16860498e+06  1.05058392e+06 ...  2.06423175e+05
  4.69467212e+04 -2.42205780e+02]
2025-06-20 18:27:02 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:27:02 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:27:07 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.0233
2025-06-20 18:27:07 INFO FeHistory: [139902.15311536 114216.98198592 212310.99439742 ...   -673.03031216
   -884.88580088   -889.11976155]
2025-06-20 18:27:07 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:27:07 INFO Good algorithm:
Algorithm Name: AdaptiveDE_Gaussian_Tournament
import numpy as np
import random

# Name: AdaptiveDE_Gaussian_Tournament
# Description: Differential Evolution with adaptive Gaussian sampling and tournament selection for multimodal optimization.
# Code:
class AdaptiveDE_Gaussian_Tournament:
    """
    Combines Differential Evolution, adaptive Gaussian sampling, and tournament selection 
    for efficient multimodal optimization.  Improves exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.2  # Increased rate for higher dimensions
        self.tournament_size = 5


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                mutant = self._mutate(i)
                trial = self._crossover(mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1

                # Tournament Selection
                tournament = random.sample(range(self.population_size), self.tournament_size)
                tournament_fitness = self.fitness_values[tournament]
                winner_index = tournament[np.argmin(tournament_fitness)]
                if trial_fitness < self.fitness_values[winner_index]:
                    new_population.append(trial)
                    self.fitness_values[winner_index] = trial_fitness
                    self.population[winner_index] = trial
                else:
                    new_population.append(self.population[winner_index])

                self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

            self.mutation_scale *= self.mutation_scale_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        population_gaussian = np.random.normal(0, (self.upper_bounds - self.lower_bounds) / 4, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _mutate(self, index):
        a, b, c = self._select_different(index)
        mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
        if np.random.rand() < self.gaussian_sampling_rate:
            mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)
        return np.clip(mutant, self.lower_bounds, self.upper_bounds)

    def _crossover(self, mutant, individual):
        trial = np.where(np.random.rand(self.dim) < 0.5, mutant, individual)
        return trial

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:27:07 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:27:12 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.0185
2025-06-20 18:27:12 INFO FeHistory: [208198.62718112 167475.12846097 219345.33999095 ...   -647.18395765
   -649.55632712   -627.48485282]
2025-06-20 18:27:12 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:27:12 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:27:28 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0125
2025-06-20 18:27:28 INFO FeHistory: [189.94139957 167.38701234 213.56772574 ... 152.30098042  19.20057125
 -46.57099673]
2025-06-20 18:27:28 INFO Expected Optimum FE: -100
2025-06-20 18:27:28 INFO Unimodal AOCC mean: 0.0001
2025-06-20 18:27:28 INFO Multimodal (single component) AOCC mean: 0.0209
2025-06-20 18:27:28 INFO Multimodal (multiple components) AOCC mean: 0.0125
2025-06-20 18:27:28 INFO AOCC mean: 0.0136
2025-06-20 18:27:36 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:27:41 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.2008
2025-06-20 18:27:41 INFO FeHistory: [ 9.50743691e+05  9.75198353e+05  9.61240237e+05 ... -3.82618484e+02
 -3.82618622e+02 -3.82618247e+02]
2025-06-20 18:27:41 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:27:41 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:27:41 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:27:47 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.3288
2025-06-20 18:27:47 INFO FeHistory: [245278.50643422 214731.74097275 268250.0025664  ...   -912.85734913
   -912.85734913   -912.85701576]
2025-06-20 18:27:47 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:27:47 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:27:47 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:27:53 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.3063
2025-06-20 18:27:53 INFO FeHistory: [273396.74263973 160841.74335222 185443.1230915  ...   -656.78895374
   -656.78895374   -656.78895374]
2025-06-20 18:27:53 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:27:53 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:27:53 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:28:09 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0184
2025-06-20 18:28:09 INFO FeHistory: [208.95705591 221.74963204 163.74981125 ... -53.11658976 -54.02067149
 -53.67048251]
2025-06-20 18:28:09 INFO Expected Optimum FE: -100
2025-06-20 18:28:09 INFO Unimodal AOCC mean: 0.2008
2025-06-20 18:28:09 INFO Multimodal (single component) AOCC mean: 0.3175
2025-06-20 18:28:09 INFO Multimodal (multiple components) AOCC mean: 0.0184
2025-06-20 18:28:09 INFO AOCC mean: 0.2136
2025-06-20 18:28:09 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

### Worse code
AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

### Analyze & experience
- Comparing (best) AdaptiveDifferentialEvolutionWithGaussianSampling vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveDifferentialEvolutionWithGaussianSampling incorporates adaptive Gaussian sampling alongside differential evolution, leading to better exploration of the search space and handling of multimodal functions.  EnhancedArchiveGuidedDE, while using an archive, lacks the adaptive exploration mechanisms.  (second best) AdaptiveGaussianMutationDE vs (second worst) AdaptiveMultimodalOptimizerImproved shows AdaptiveGaussianMutationDE's direct use of Gaussian mutation for escaping local optima, while AdaptiveMultimodalOptimizerImproved relies on a more indirect simulated annealing approach. Comparing (1st) vs (2nd), we see that the adaptive Gaussian sampling in AdaptiveDifferentialEvolutionWithGaussianSampling provides a more controlled and efficient way to introduce diversity than the solely mutation-based approach of AdaptiveGaussianMutationDE. (3rd) AdaptiveGaussianSamplingEA vs (4th) AdaptiveMultimodalOptimizerImproved reveals that AdaptiveGaussianSamplingEA's structured approach of tournament selection, Gaussian recombination, and adaptive mutation generally leads to better optimization performance than AdaptiveMultimodalOptimizerImproved's more heuristic-driven local search and simulated annealing. Comparing (second worst) AdaptiveMultimodalOptimizerImproved vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveMultimodalOptimizerImproved employs a local search strategy and tabu list, aiming to avoid redundant exploration, although this is less effective than the adaptive sampling and evolutionary mechanisms used in the higher-ranked algorithms. Overall: The top-performing algorithms consistently utilize adaptive mechanisms (adaptive Gaussian sampling, mutation scaling, sigma decay) and efficient evolutionary operators (differential evolution, tournament selection) to explore and exploit the search space more effectively than the lower-ranked algorithms which rely more on simple random sampling and less sophisticated local search strategies.
- Adaptive sampling and mutation strategies alongside robust evolutionary operators are crucial for high-dimensional optimization.  Combining exploration and exploitation methods, along with adaptive parameter adjustments, significantly improves performance in complex search spaces.  Careful consideration of diversity mechanisms and local search enhancements is essential for efficient global optimization.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:28:09 INFO Generation 3, best so far: 0.21369870938149668
2025-06-20 18:28:09 INFO --- Performing Long-Term Reflection at Generation 3 ---
2025-06-20 18:28:13 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (worst) AdaptiveMultimodalOptimizerImproved (Rank 5), we see that Rank 1 utilizes differential evolution and Gaussian sampling for multimodal exploration, resulting in significantly better AOCC scores.  Rank 5 uses a simpler local search with simulated annealing, lacking the sophisticated exploration strategies of Rank 1.  (second best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2) vs (second worst) AdaptiveGaussianSamplingEA (Rank 4): Rank 2, while very similar to Rank 1, shows a slight performance drop likely due to a simpler population initialization lacking the Gaussian component seen in Rank 1's initialization.  Rank 4 uses adaptive Gaussian sampling but lacks the robustness of differential evolution, hindering its performance. Comparing (1st) vs (2nd), we see that the addition of Gaussian sampling in the population initialization in Rank 1 provides a notable advantage over Rank 2s uniform initialization. (3rd) AdaptiveGaussianMutationDE (Rank 3) vs (4th) AdaptiveGaussianSamplingEA (Rank 4): Both use adaptive Gaussian mutation/sampling, but Rank 3 incorporates differential evolution, providing a more effective search mechanism than Rank 4s evolutionary algorithm structure. Comparing (second worst) AdaptiveGaussianSamplingEA (Rank 4) vs (worst) AdaptiveMultimodalOptimizerImproved (Rank 5): Rank 4 utilizes a more sophisticated evolutionary algorithm framework compared to the simple local search approach in Rank 5.  Overall: The combination of differential evolution and adaptive Gaussian sampling in the population initialization, as seen in Rank 1, is crucial for achieving superior performance in high-dimensional search spaces.  The use of a more sophisticated EA framework also consistently improves performance over simpler local search methods.


**Experience:**

Combining exploration and exploitation techniques within the population initialization is key.  Adaptive sampling methods, especially Gaussian sampling around promising regions, prove very effective.  Sophisticated evolutionary algorithm structures like differential evolution also significantly improve performance over simpler local search methods.

2025-06-20 18:28:13 INFO Generating offspring via Crossover...
2025-06-20 18:28:22 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:28:28 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1511
2025-06-20 18:28:28 INFO FeHistory: [ 5.09968890e+05  6.15736758e+05  9.46098311e+05 ... -3.82442337e+02
 -3.82442337e+02 -3.82442337e+02]
2025-06-20 18:28:28 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:28:28 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Differential evolution with hybrid initialization combining uniform and Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian sampling) 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.3 # Increased rate for hybrid approach

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_uniform = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_uniform
        population_uniform = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_uniform, self.dim))
        # Center Gaussian around the mean of the bounds for better exploration
        mean = (self.upper_bounds + self.lower_bounds) / 2
        std_dev = (self.upper_bounds - self.lower_bounds) / 4  
        population_gaussian = np.random.normal(mean, std_dev, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_uniform, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:28:28 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:28:33 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2928
2025-06-20 18:28:33 INFO FeHistory: [214292.91045432 164056.49889652 301479.98963805 ...   -912.85716754
   -912.85716754   -912.85716754]
2025-06-20 18:28:33 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:28:33 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Differential evolution with hybrid initialization combining uniform and Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian sampling) 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.3 # Increased rate for hybrid approach

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_uniform = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_uniform
        population_uniform = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_uniform, self.dim))
        # Center Gaussian around the mean of the bounds for better exploration
        mean = (self.upper_bounds + self.lower_bounds) / 2
        std_dev = (self.upper_bounds - self.lower_bounds) / 4  
        population_gaussian = np.random.normal(mean, std_dev, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_uniform, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:28:33 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:28:39 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.3028
2025-06-20 18:28:39 INFO FeHistory: [213177.65934025 231022.82613309 138400.33267324 ...   -656.7889252
   -656.7889252    -656.7889252 ]
2025-06-20 18:28:39 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:28:39 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Differential evolution with hybrid initialization combining uniform and Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian sampling) 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.3 # Increased rate for hybrid approach

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_uniform = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_uniform
        population_uniform = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_uniform, self.dim))
        # Center Gaussian around the mean of the bounds for better exploration
        mean = (self.upper_bounds + self.lower_bounds) / 2
        std_dev = (self.upper_bounds - self.lower_bounds) / 4  
        population_gaussian = np.random.normal(mean, std_dev, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_uniform, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:28:39 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:28:56 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0196
2025-06-20 18:28:56 INFO FeHistory: [216.9074402  180.43788762 152.70700203 ... -49.84809114 -53.11777526
 -53.22251683]
2025-06-20 18:28:56 INFO Expected Optimum FE: -100
2025-06-20 18:28:56 INFO Unimodal AOCC mean: 0.1511
2025-06-20 18:28:56 INFO Multimodal (single component) AOCC mean: 0.2978
2025-06-20 18:28:56 INFO Multimodal (multiple components) AOCC mean: 0.0196
2025-06-20 18:28:56 INFO AOCC mean: 0.1916
2025-06-20 18:29:05 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:29:11 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1782
2025-06-20 18:29:11 INFO FeHistory: [ 3.31572632e+05  5.31083839e+05  4.27718037e+05 ... -3.82570711e+02
 -3.82570711e+02 -3.82570711e+02]
2025-06-20 18:29:11 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:29:11 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianInitialization
# Description: Differential evolution with Gaussian-enhanced initialization for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianInitialization:
    """
    Combines differential evolution with a Gaussian-enhanced initialization to efficiently explore multimodal landscapes.  The initialization strategically places individuals around the initial best solution found through Gaussian sampling, improving exploration.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population, initial_best_solution, initial_best_fitness = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall = initial_best_solution
        self.best_fitness_overall = initial_best_fitness


        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        # Gaussian sampling for initial solution
        initial_population = np.random.normal(0, 20, size=(10, self.dim)) # Sample around 0 to initially explore
        initial_population = np.clip(initial_population, self.lower_bounds, self.upper_bounds)
        initial_fitness = objective_function(initial_population)
        self.eval_count += 10
        best_index = np.argmin(initial_fitness)
        best_solution = initial_population[best_index]
        best_fitness = initial_fitness[best_index]

        # Gaussian distribution around initial best
        population = np.random.normal(best_solution, 10, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population, best_solution, best_fitness


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:29:11 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:29:17 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.3002
2025-06-20 18:29:17 INFO FeHistory: [107787.73910067  93027.07370536  93395.443381   ...   -912.85723234
   -912.85723234   -912.85723234]
2025-06-20 18:29:17 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:29:17 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianInitialization
# Description: Differential evolution with Gaussian-enhanced initialization for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianInitialization:
    """
    Combines differential evolution with a Gaussian-enhanced initialization to efficiently explore multimodal landscapes.  The initialization strategically places individuals around the initial best solution found through Gaussian sampling, improving exploration.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population, initial_best_solution, initial_best_fitness = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall = initial_best_solution
        self.best_fitness_overall = initial_best_fitness


        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        # Gaussian sampling for initial solution
        initial_population = np.random.normal(0, 20, size=(10, self.dim)) # Sample around 0 to initially explore
        initial_population = np.clip(initial_population, self.lower_bounds, self.upper_bounds)
        initial_fitness = objective_function(initial_population)
        self.eval_count += 10
        best_index = np.argmin(initial_fitness)
        best_solution = initial_population[best_index]
        best_fitness = initial_fitness[best_index]

        # Gaussian distribution around initial best
        population = np.random.normal(best_solution, 10, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population, best_solution, best_fitness


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:29:17 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:29:23 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2883
2025-06-20 18:29:23 INFO FeHistory: [84724.58918404 89168.24069679 78475.19368397 ...  -656.78865074
  -656.78865074  -656.78865074]
2025-06-20 18:29:23 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:29:23 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithGaussianInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianInitialization
# Description: Differential evolution with Gaussian-enhanced initialization for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianInitialization:
    """
    Combines differential evolution with a Gaussian-enhanced initialization to efficiently explore multimodal landscapes.  The initialization strategically places individuals around the initial best solution found through Gaussian sampling, improving exploration.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population, initial_best_solution, initial_best_fitness = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall = initial_best_solution
        self.best_fitness_overall = initial_best_fitness


        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        # Gaussian sampling for initial solution
        initial_population = np.random.normal(0, 20, size=(10, self.dim)) # Sample around 0 to initially explore
        initial_population = np.clip(initial_population, self.lower_bounds, self.upper_bounds)
        initial_fitness = objective_function(initial_population)
        self.eval_count += 10
        best_index = np.argmin(initial_fitness)
        best_solution = initial_population[best_index]
        best_fitness = initial_fitness[best_index]

        # Gaussian distribution around initial best
        population = np.random.normal(best_solution, 10, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population, best_solution, best_fitness


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:29:23 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:29:39 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0064
2025-06-20 18:29:39 INFO FeHistory: [139.37517474 125.43310228 165.80419578 ... -16.74157817 -15.26740228
 -26.51909481]
2025-06-20 18:29:39 INFO Expected Optimum FE: -100
2025-06-20 18:29:39 INFO Unimodal AOCC mean: 0.1782
2025-06-20 18:29:39 INFO Multimodal (single component) AOCC mean: 0.2942
2025-06-20 18:29:39 INFO Multimodal (multiple components) AOCC mean: 0.0064
2025-06-20 18:29:39 INFO AOCC mean: 0.1933
2025-06-20 18:29:39 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (worst) AdaptiveMultimodalOptimizerImproved (Rank 5), we see that Rank 1 utilizes differential evolution and Gaussian sampling for multimodal exploration, resulting in significantly better AOCC scores.  Rank 5 uses a simpler local search with simulated annealing, lacking the sophisticated exploration strategies of Rank 1.  (second best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2) vs (second worst) AdaptiveGaussianSamplingEA (Rank 4): Rank 2, while very similar to Rank 1, shows a slight performance drop likely due to a simpler population initialization lacking the Gaussian component seen in Rank 1's initialization.  Rank 4 uses adaptive Gaussian sampling but lacks the robustness of differential evolution, hindering its performance. Comparing (1st) vs (2nd), we see that the addition of Gaussian sampling in the population initialization in Rank 1 provides a notable advantage over Rank 2s uniform initialization. (3rd) AdaptiveGaussianMutationDE (Rank 3) vs (4th) AdaptiveGaussianSamplingEA (Rank 4): Both use adaptive Gaussian mutation/sampling, but Rank 3 incorporates differential evolution, providing a more effective search mechanism than Rank 4s evolutionary algorithm structure. Comparing (second worst) AdaptiveGaussianSamplingEA (Rank 4) vs (worst) AdaptiveMultimodalOptimizerImproved (Rank 5): Rank 4 utilizes a more sophisticated evolutionary algorithm framework compared to the simple local search approach in Rank 5.  Overall: The combination of differential evolution and adaptive Gaussian sampling in the population initialization, as seen in Rank 1, is crucial for achieving superior performance in high-dimensional search spaces.  The use of a more sophisticated EA framework also consistently improves performance over simpler local search methods.
- Combining exploration and exploitation techniques within the population initialization is key.  Adaptive sampling methods, especially Gaussian sampling around promising regions, prove very effective.  Sophisticated evolutionary algorithm structures like differential evolution also significantly improve performance over simpler local search methods.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:29:39 INFO Generation 4, best so far: 0.21369870938149668
2025-06-20 18:29:39 INFO --- Performing Long-Term Reflection at Generation 4 ---
2025-06-20 18:29:43 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 1) vs (worst) `AdaptiveGaussianMutationDE` (Rank 5), we see that Rank 1 incorporates both differential evolution and Gaussian sampling in its initialization, leading to a more diverse initial population and better exploration of the search space.  Rank 5 only uses uniform random initialization, which is less effective in high-dimensional spaces.  (second best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 2) vs (second worst) `AdaptiveDifferentialEvolutionWithHybridInitialization` (Rank 4) shows that even a small difference in Gaussian sampling rate (0.1 vs 0.3) and Gaussian sampling center affects performance. Rank 2 centers its Gaussian sampling around 0, while Rank 4 centers it around the mean of the bounds, impacting exploration efficiency. Comparing (1st) vs (2nd), we see almost identical algorithms with a crucial difference: Rank 1 uses a hybrid initialization combining DE and Gaussian sampling while Rank 2 only uses DE and applies Gaussian sampling in the evolutionary process;  (3rd) vs (4th) demonstrates the importance of strategic initialization. Rank 3 focuses on an initial Gaussian sampling phase to find a promising solution, then initializing the rest of the population around it, offering better convergence; Rank 4 uses a hybrid approach but doesn't strategically position its individuals, resulting in poorer performance. Comparing (second worst) vs (worst), we see that the hybrid initialization of Rank 4 is still superior to the solely uniform initialization of Rank 5.  Overall: Strategic hybrid initialization combining uniform and Gaussian sampling, adapting the mutation scale, and intelligently placing initial individuals based on preliminary sampling show significant improvements in performance.

**Experience:**

Effective population initialization for high-dimensional problems requires a hybrid approach combining different sampling methods to ensure diversity and focus exploration.  Strategic placement of initial individuals based on preliminary sampling phases can significantly improve the EA's convergence speed and solution quality.

2025-06-20 18:29:43 INFO Generating offspring via Crossover...
2025-06-20 18:29:52 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:29:57 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.0000
2025-06-20 18:29:57 INFO FeHistory: [1025110.13242747  790516.09078269  839031.05511334 ...   16583.35675181
   26941.8151573    38174.09574563]
2025-06-20 18:29:57 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:29:57 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:30:03 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.0000
2025-06-20 18:30:03 INFO FeHistory: [121308.65175276 183401.22529775 275304.57605544 ...  10205.27849662
   4444.989999     3112.24915737]
2025-06-20 18:30:03 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:30:03 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:30:09 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.0000
2025-06-20 18:30:09 INFO FeHistory: [203785.43324532 143985.13053221 217322.18352983 ...   3777.98959504
   6242.69728608   5054.55869817]
2025-06-20 18:30:09 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:30:09 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:30:25 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0000
2025-06-20 18:30:25 INFO FeHistory: [195.04457966 187.41802933 232.69455347 ...  76.60452215  97.5826962
  94.58134514]
2025-06-20 18:30:25 INFO Expected Optimum FE: -100
2025-06-20 18:30:25 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:30:25 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:30:25 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:30:25 INFO AOCC mean: 0.0000
2025-06-20 18:30:34 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:30:39 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1836
2025-06-20 18:30:39 INFO FeHistory: [ 7.76309554e+05  9.42124235e+05  5.84371891e+05 ... -3.82581737e+02
 -3.82581737e+02 -3.82581737e+02]
2025-06-20 18:30:39 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:30:39 INFO Good algorithm:
Algorithm Name: GuidedGaussianDE
import numpy as np
import random

# Name: GuidedGaussianDE
# Description: Hybrid DE with guided Gaussian sampling for efficient multimodal optimization.
# Code:
class GuidedGaussianDE:
    """
    Combines Differential Evolution with a guided Gaussian sampling initialization 
    to efficiently explore multimodal landscapes.  Initializes a portion of the 
    population using Gaussian sampling guided by a preliminary search, improving 
    diversity and convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_init_rate = 0.2 # Percentage of population initialized with guided Gaussian

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        num_guided = int(self.population_size * self.gaussian_init_rate)
        guided_init = self._guided_gaussian_init(objective_function, num_guided)
        remaining_init = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size - num_guided, self.dim))
        return np.concatenate((guided_init, remaining_init))

    def _guided_gaussian_init(self, objective_function, num_samples):
        # Initial exploration to find a good starting point
        initial_sample = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(100, self.dim))
        fitness = objective_function(initial_sample)
        best_index = np.argmin(fitness)
        best_solution = initial_sample[best_index]
        
        # Gaussian sampling around best solution
        return np.random.normal(loc=best_solution, scale=(self.upper_bounds-self.lower_bounds)/4, size=(num_samples, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:30:39 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:30:45 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2429
2025-06-20 18:30:45 INFO FeHistory: [181307.8233633  154343.10468143 171374.19490336 ...   -912.85339943
   -912.85339943   -912.85339943]
2025-06-20 18:30:45 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:30:45 INFO Good algorithm:
Algorithm Name: GuidedGaussianDE
import numpy as np
import random

# Name: GuidedGaussianDE
# Description: Hybrid DE with guided Gaussian sampling for efficient multimodal optimization.
# Code:
class GuidedGaussianDE:
    """
    Combines Differential Evolution with a guided Gaussian sampling initialization 
    to efficiently explore multimodal landscapes.  Initializes a portion of the 
    population using Gaussian sampling guided by a preliminary search, improving 
    diversity and convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_init_rate = 0.2 # Percentage of population initialized with guided Gaussian

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        num_guided = int(self.population_size * self.gaussian_init_rate)
        guided_init = self._guided_gaussian_init(objective_function, num_guided)
        remaining_init = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size - num_guided, self.dim))
        return np.concatenate((guided_init, remaining_init))

    def _guided_gaussian_init(self, objective_function, num_samples):
        # Initial exploration to find a good starting point
        initial_sample = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(100, self.dim))
        fitness = objective_function(initial_sample)
        best_index = np.argmin(fitness)
        best_solution = initial_sample[best_index]
        
        # Gaussian sampling around best solution
        return np.random.normal(loc=best_solution, scale=(self.upper_bounds-self.lower_bounds)/4, size=(num_samples, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:30:45 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:30:51 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2666
2025-06-20 18:30:51 INFO FeHistory: [110605.60531607 223175.01843793 238621.37682118 ...   -656.78810914
   -656.78810914   -656.78810914]
2025-06-20 18:30:51 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:30:51 INFO Good algorithm:
Algorithm Name: GuidedGaussianDE
import numpy as np
import random

# Name: GuidedGaussianDE
# Description: Hybrid DE with guided Gaussian sampling for efficient multimodal optimization.
# Code:
class GuidedGaussianDE:
    """
    Combines Differential Evolution with a guided Gaussian sampling initialization 
    to efficiently explore multimodal landscapes.  Initializes a portion of the 
    population using Gaussian sampling guided by a preliminary search, improving 
    diversity and convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_init_rate = 0.2 # Percentage of population initialized with guided Gaussian

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        num_guided = int(self.population_size * self.gaussian_init_rate)
        guided_init = self._guided_gaussian_init(objective_function, num_guided)
        remaining_init = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size - num_guided, self.dim))
        return np.concatenate((guided_init, remaining_init))

    def _guided_gaussian_init(self, objective_function, num_samples):
        # Initial exploration to find a good starting point
        initial_sample = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(100, self.dim))
        fitness = objective_function(initial_sample)
        best_index = np.argmin(fitness)
        best_solution = initial_sample[best_index]
        
        # Gaussian sampling around best solution
        return np.random.normal(loc=best_solution, scale=(self.upper_bounds-self.lower_bounds)/4, size=(num_samples, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:30:51 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:31:07 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0270
2025-06-20 18:31:07 INFO FeHistory: [174.18770424 170.97175487 171.18626745 ... -57.91089651 -60.2217543
 -60.30542008]
2025-06-20 18:31:07 INFO Expected Optimum FE: -100
2025-06-20 18:31:07 INFO Good algorithm:
Algorithm Name: GuidedGaussianDE
import numpy as np
import random

# Name: GuidedGaussianDE
# Description: Hybrid DE with guided Gaussian sampling for efficient multimodal optimization.
# Code:
class GuidedGaussianDE:
    """
    Combines Differential Evolution with a guided Gaussian sampling initialization 
    to efficiently explore multimodal landscapes.  Initializes a portion of the 
    population using Gaussian sampling guided by a preliminary search, improving 
    diversity and convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_init_rate = 0.2 # Percentage of population initialized with guided Gaussian

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        num_guided = int(self.population_size * self.gaussian_init_rate)
        guided_init = self._guided_gaussian_init(objective_function, num_guided)
        remaining_init = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size - num_guided, self.dim))
        return np.concatenate((guided_init, remaining_init))

    def _guided_gaussian_init(self, objective_function, num_samples):
        # Initial exploration to find a good starting point
        initial_sample = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(100, self.dim))
        fitness = objective_function(initial_sample)
        best_index = np.argmin(fitness)
        best_solution = initial_sample[best_index]
        
        # Gaussian sampling around best solution
        return np.random.normal(loc=best_solution, scale=(self.upper_bounds-self.lower_bounds)/4, size=(num_samples, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:31:07 INFO Unimodal AOCC mean: 0.1836
2025-06-20 18:31:07 INFO Multimodal (single component) AOCC mean: 0.2548
2025-06-20 18:31:07 INFO Multimodal (multiple components) AOCC mean: 0.0270
2025-06-20 18:31:07 INFO AOCC mean: 0.1800
2025-06-20 18:31:07 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 1) vs (worst) `AdaptiveGaussianMutationDE` (Rank 5), we see that Rank 1 incorporates both differential evolution and Gaussian sampling in its initialization, leading to a more diverse initial population and better exploration of the search space.  Rank 5 only uses uniform random initialization, which is less effective in high-dimensional spaces.  (second best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 2) vs (second worst) `AdaptiveDifferentialEvolutionWithHybridInitialization` (Rank 4) shows that even a small difference in Gaussian sampling rate (0.1 vs 0.3) and Gaussian sampling center affects performance. Rank 2 centers its Gaussian sampling around 0, while Rank 4 centers it around the mean of the bounds, impacting exploration efficiency. Comparing (1st) vs (2nd), we see almost identical algorithms with a crucial difference: Rank 1 uses a hybrid initialization combining DE and Gaussian sampling while Rank 2 only uses DE and applies Gaussian sampling in the evolutionary process;  (3rd) vs (4th) demonstrates the importance of strategic initialization. Rank 3 focuses on an initial Gaussian sampling phase to find a promising solution, then initializing the rest of the population around it, offering better convergence; Rank 4 uses a hybrid approach but doesn't strategically position its individuals, resulting in poorer performance. Comparing (second worst) vs (worst), we see that the hybrid initialization of Rank 4 is still superior to the solely uniform initialization of Rank 5.  Overall: Strategic hybrid initialization combining uniform and Gaussian sampling, adapting the mutation scale, and intelligently placing initial individuals based on preliminary sampling show significant improvements in performance.
- Effective population initialization for high-dimensional problems requires a hybrid approach combining different sampling methods to ensure diversity and focus exploration.  Strategic placement of initial individuals based on preliminary sampling phases can significantly improve the EA's convergence speed and solution quality.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:31:07 INFO Generation 5, best so far: 0.21369870938149668
2025-06-20 18:31:07 INFO --- Performing Long-Term Reflection at Generation 5 ---
2025-06-20 18:31:12 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (worst) AdaptiveGaussianMutationDE (Rank 5), we see that Rank 1 incorporates a hybrid initialization strategy combining uniform and Gaussian sampling, leading to more diverse initial population and better exploration of the search space.  Rank 5 uses only uniform random initialization, which is less effective in high-dimensional spaces.  Rank 1 also adaptively adjusts the mutation scale, further enhancing its exploration capabilities.

(second best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2) vs (second worst) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4):  The key difference is in the initialization. Rank 2 uses a simpler uniform random initialization, while Rank 4 employs a hybrid approach similar to Rank 1 but with a higher Gaussian sampling rate. This higher rate, while seemingly beneficial in the hybrid approach, does not always result in better performance in this instance, suggesting an optimal balance needs to be found between uniform and Gaussian sampling.

Comparing (1st) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (2nd) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2):  Rank 1's initialization includes a Gaussian component,  partially initializing the population using Gaussian sampling. Rank 2 solely relies on uniform random initialization. This difference, while seemingly minor, is significant, resulting in improved AOCC score.

(3rd) AdaptiveDifferentialEvolutionWithGaussianInitialization (Rank 3) vs (4th) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4): Both use hybrid initialization but differ in their approach. Rank 3 initially samples, finds a best solution, and then initializes the population around it. Rank 4 combines uniform and Gaussian samples directly.  Rank 3's approach, focusing on a good initial solution, proves more effective.

Comparing (second worst) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4) vs (worst) AdaptiveGaussianMutationDE (Rank 5): Both use differential evolution, but differ significantly in their initialization strategies and how Gaussian sampling is applied.  Rank 4 uses a hybrid approach, while Rank 5 applies Gaussian perturbation in the mutation phase instead of initialization.  The hybrid initialization strategy is superior.

Overall: The most significant difference among the heuristics lies in their population initialization strategies.  Hybrid approaches combining uniform and Gaussian sampling, particularly those that smartly leverage an initial best solution (Rank 1 and 3), consistently outperform pure uniform random or Gaussian perturbation within the DE framework.


**Experience:**

Effective population initialization for high-dimensional problems requires strategies that promote diversity and guide the search towards promising regions.  Hybrid methods combining uniform and Gaussian sampling, potentially centered around an initially found good solution, are particularly valuable.  Careful tuning of the Gaussian sampling rate is also crucial.

2025-06-20 18:31:12 INFO Generating offspring via Crossover...
2025-06-20 18:31:21 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:31:27 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1161
2025-06-20 18:31:27 INFO FeHistory: [ 1.16877270e+06  1.24475023e+06  9.28197088e+05 ... -3.81728382e+02
 -3.81728382e+02 -3.81728382e+02]
2025-06-20 18:31:27 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:31:27 INFO Good algorithm:
Algorithm Name: AdaptiveHybridInitializationDE
import numpy as np
import random

# Name: AdaptiveHybridInitializationDE
# Description: Differential Evolution with hybrid initialization combining uniform and Gaussian sampling around an initial best solution.
# Code:
class AdaptiveHybridInitializationDE:
    """
    Differential Evolution algorithm with a hybrid initialization strategy that combines uniform random sampling 
    with Gaussian sampling centered around an initially found good solution to enhance exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.gaussian_sampling_rate = 0.3  # Adjust this parameter as needed
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        # Initial sampling to find a good starting point
        initial_sample = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(10, self.dim))
        initial_fitness = objective_function(initial_sample)
        self.eval_count += 10
        best_index = np.argmin(initial_fitness)
        best_solution = initial_sample[best_index]

        # Hybrid initialization
        pop_size_uniform = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_uniform
        population_uniform = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_uniform, self.dim))
        population_gaussian = np.random.normal(best_solution, (self.upper_bounds - self.lower_bounds) / 4, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_uniform, population_gaussian))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:31:27 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:31:32 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2769
2025-06-20 18:31:32 INFO FeHistory: [145366.96875662 235854.90169041 240943.72713011 ...   -912.85638228
   -912.85638228   -912.85638228]
2025-06-20 18:31:32 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:31:32 INFO Good algorithm:
Algorithm Name: AdaptiveHybridInitializationDE
import numpy as np
import random

# Name: AdaptiveHybridInitializationDE
# Description: Differential Evolution with hybrid initialization combining uniform and Gaussian sampling around an initial best solution.
# Code:
class AdaptiveHybridInitializationDE:
    """
    Differential Evolution algorithm with a hybrid initialization strategy that combines uniform random sampling 
    with Gaussian sampling centered around an initially found good solution to enhance exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.gaussian_sampling_rate = 0.3  # Adjust this parameter as needed
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        # Initial sampling to find a good starting point
        initial_sample = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(10, self.dim))
        initial_fitness = objective_function(initial_sample)
        self.eval_count += 10
        best_index = np.argmin(initial_fitness)
        best_solution = initial_sample[best_index]

        # Hybrid initialization
        pop_size_uniform = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_uniform
        population_uniform = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_uniform, self.dim))
        population_gaussian = np.random.normal(best_solution, (self.upper_bounds - self.lower_bounds) / 4, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_uniform, population_gaussian))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:31:32 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:31:38 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2752
2025-06-20 18:31:38 INFO FeHistory: [260020.8482243  234152.2244589  158280.45629085 ...   -656.78855492
   -656.78855492   -656.78855492]
2025-06-20 18:31:38 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:31:38 INFO Good algorithm:
Algorithm Name: AdaptiveHybridInitializationDE
import numpy as np
import random

# Name: AdaptiveHybridInitializationDE
# Description: Differential Evolution with hybrid initialization combining uniform and Gaussian sampling around an initial best solution.
# Code:
class AdaptiveHybridInitializationDE:
    """
    Differential Evolution algorithm with a hybrid initialization strategy that combines uniform random sampling 
    with Gaussian sampling centered around an initially found good solution to enhance exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.gaussian_sampling_rate = 0.3  # Adjust this parameter as needed
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        # Initial sampling to find a good starting point
        initial_sample = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(10, self.dim))
        initial_fitness = objective_function(initial_sample)
        self.eval_count += 10
        best_index = np.argmin(initial_fitness)
        best_solution = initial_sample[best_index]

        # Hybrid initialization
        pop_size_uniform = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_uniform
        population_uniform = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_uniform, self.dim))
        population_gaussian = np.random.normal(best_solution, (self.upper_bounds - self.lower_bounds) / 4, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_uniform, population_gaussian))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:31:38 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:31:54 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0205
2025-06-20 18:31:54 INFO FeHistory: [180.64845198 152.62397796 175.00066637 ... -57.72549977 -56.70939304
 -56.58688768]
2025-06-20 18:31:54 INFO Expected Optimum FE: -100
2025-06-20 18:31:54 INFO Good algorithm:
Algorithm Name: AdaptiveHybridInitializationDE
import numpy as np
import random

# Name: AdaptiveHybridInitializationDE
# Description: Differential Evolution with hybrid initialization combining uniform and Gaussian sampling around an initial best solution.
# Code:
class AdaptiveHybridInitializationDE:
    """
    Differential Evolution algorithm with a hybrid initialization strategy that combines uniform random sampling 
    with Gaussian sampling centered around an initially found good solution to enhance exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.gaussian_sampling_rate = 0.3  # Adjust this parameter as needed
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        # Initial sampling to find a good starting point
        initial_sample = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(10, self.dim))
        initial_fitness = objective_function(initial_sample)
        self.eval_count += 10
        best_index = np.argmin(initial_fitness)
        best_solution = initial_sample[best_index]

        # Hybrid initialization
        pop_size_uniform = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_uniform
        population_uniform = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_uniform, self.dim))
        population_gaussian = np.random.normal(best_solution, (self.upper_bounds - self.lower_bounds) / 4, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_uniform, population_gaussian))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:31:54 INFO Unimodal AOCC mean: 0.1161
2025-06-20 18:31:54 INFO Multimodal (single component) AOCC mean: 0.2760
2025-06-20 18:31:54 INFO Multimodal (multiple components) AOCC mean: 0.0205
2025-06-20 18:31:54 INFO AOCC mean: 0.1722
2025-06-20 18:32:04 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:32:09 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1528
2025-06-20 18:32:09 INFO FeHistory: [ 7.21013455e+05  9.71525886e+05  1.15104558e+06 ... -3.82461891e+02
 -3.82461891e+02 -3.82461891e+02]
2025-06-20 18:32:09 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:32:09 INFO Good algorithm:
Algorithm Name: HybridDEwithAdaptiveGaussianInitialization
import numpy as np
import random

# Name: HybridDEwithAdaptiveGaussianInitialization
# Description: Differential Evolution with adaptive Gaussian initialization for multimodal optimization.
# Code:
class HybridDEwithAdaptiveGaussianInitialization:
    """
    Combines Differential Evolution with an adaptive Gaussian initialization 
    strategy to efficiently explore multimodal landscapes.  Initializes a 
    subset of the population using Gaussian sampling around an initially 
    found good solution, promoting diversity and focusing search in promising areas.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.gaussian_init_rate = 0.3  # Percentage of population initialized with Gaussian
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        num_gaussian = int(self.population_size * self.gaussian_init_rate)
        initial_population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size - num_gaussian, self.dim))
        
        # Find an initial good solution
        initial_solution = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        initial_fitness = objective_function(initial_solution.reshape(1,-1))[0]
        self.eval_count +=1

        #Gaussian sampling around the initial solution
        gaussian_population = np.random.normal(initial_solution, 0.2 * (self.upper_bounds - self.lower_bounds), size=(num_gaussian, self.dim))
        gaussian_population = np.clip(gaussian_population, self.lower_bounds, self.upper_bounds)

        return np.concatenate((initial_population, gaussian_population))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population.reshape(-1, self.dim))
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:32:09 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:32:15 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2841
2025-06-20 18:32:15 INFO FeHistory: [156444.43819301 203425.18108383 235879.96560854 ...   -912.85702212
   -912.85702212   -912.85702212]
2025-06-20 18:32:15 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:32:15 INFO Good algorithm:
Algorithm Name: HybridDEwithAdaptiveGaussianInitialization
import numpy as np
import random

# Name: HybridDEwithAdaptiveGaussianInitialization
# Description: Differential Evolution with adaptive Gaussian initialization for multimodal optimization.
# Code:
class HybridDEwithAdaptiveGaussianInitialization:
    """
    Combines Differential Evolution with an adaptive Gaussian initialization 
    strategy to efficiently explore multimodal landscapes.  Initializes a 
    subset of the population using Gaussian sampling around an initially 
    found good solution, promoting diversity and focusing search in promising areas.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.gaussian_init_rate = 0.3  # Percentage of population initialized with Gaussian
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        num_gaussian = int(self.population_size * self.gaussian_init_rate)
        initial_population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size - num_gaussian, self.dim))
        
        # Find an initial good solution
        initial_solution = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        initial_fitness = objective_function(initial_solution.reshape(1,-1))[0]
        self.eval_count +=1

        #Gaussian sampling around the initial solution
        gaussian_population = np.random.normal(initial_solution, 0.2 * (self.upper_bounds - self.lower_bounds), size=(num_gaussian, self.dim))
        gaussian_population = np.clip(gaussian_population, self.lower_bounds, self.upper_bounds)

        return np.concatenate((initial_population, gaussian_population))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population.reshape(-1, self.dim))
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:32:15 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:32:20 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.3072
2025-06-20 18:32:20 INFO FeHistory: [152278.40850037 112767.34908296 144987.65933597 ...   -656.78891352
   -656.78891352   -656.78891352]
2025-06-20 18:32:20 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:32:20 INFO Good algorithm:
Algorithm Name: HybridDEwithAdaptiveGaussianInitialization
import numpy as np
import random

# Name: HybridDEwithAdaptiveGaussianInitialization
# Description: Differential Evolution with adaptive Gaussian initialization for multimodal optimization.
# Code:
class HybridDEwithAdaptiveGaussianInitialization:
    """
    Combines Differential Evolution with an adaptive Gaussian initialization 
    strategy to efficiently explore multimodal landscapes.  Initializes a 
    subset of the population using Gaussian sampling around an initially 
    found good solution, promoting diversity and focusing search in promising areas.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.gaussian_init_rate = 0.3  # Percentage of population initialized with Gaussian
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population(objective_function)
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, objective_function):
        num_gaussian = int(self.population_size * self.gaussian_init_rate)
        initial_population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size - num_gaussian, self.dim))
        
        # Find an initial good solution
        initial_solution = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        initial_fitness = objective_function(initial_solution.reshape(1,-1))[0]
        self.eval_count +=1

        #Gaussian sampling around the initial solution
        gaussian_population = np.random.normal(initial_solution, 0.2 * (self.upper_bounds - self.lower_bounds), size=(num_gaussian, self.dim))
        gaussian_population = np.clip(gaussian_population, self.lower_bounds, self.upper_bounds)

        return np.concatenate((initial_population, gaussian_population))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population.reshape(-1, self.dim))
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:32:20 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:32:37 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0148
2025-06-20 18:32:37 INFO FeHistory: [188.60647883 176.12403344 181.65016043 ... -45.15211247 -43.32574621
 -44.73564491]
2025-06-20 18:32:37 INFO Expected Optimum FE: -100
2025-06-20 18:32:37 INFO Unimodal AOCC mean: 0.1528
2025-06-20 18:32:37 INFO Multimodal (single component) AOCC mean: 0.2957
2025-06-20 18:32:37 INFO Multimodal (multiple components) AOCC mean: 0.0148
2025-06-20 18:32:37 INFO AOCC mean: 0.1897
2025-06-20 18:32:37 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (worst) AdaptiveGaussianMutationDE (Rank 5), we see that Rank 1 incorporates a hybrid initialization strategy combining uniform and Gaussian sampling, leading to more diverse initial population and better exploration of the search space.  Rank 5 uses only uniform random initialization, which is less effective in high-dimensional spaces.  Rank 1 also adaptively adjusts the mutation scale, further enhancing its exploration capabilities.

(second best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2) vs (second worst) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4):  The key difference is in the initialization. Rank 2 uses a simpler uniform random initialization, while Rank 4 employs a hybrid approach similar to Rank 1 but with a higher Gaussian sampling rate. This higher rate, while seemingly beneficial in the hybrid approach, does not always result in better performance in this instance, suggesting an optimal balance needs to be found between uniform and Gaussian sampling.

Comparing (1st) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (2nd) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2):  Rank 1's initialization includes a Gaussian component,  partially initializing the population using Gaussian sampling. Rank 2 solely relies on uniform random initialization. This difference, while seemingly minor, is significant, resulting in improved AOCC score.

(3rd) AdaptiveDifferentialEvolutionWithGaussianInitialization (Rank 3) vs (4th) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4): Both use hybrid initialization but differ in their approach. Rank 3 initially samples, finds a best solution, and then initializes the population around it. Rank 4 combines uniform and Gaussian samples directly.  Rank 3's approach, focusing on a good initial solution, proves more effective.

Comparing (second worst) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4) vs (worst) AdaptiveGaussianMutationDE (Rank 5): Both use differential evolution, but differ significantly in their initialization strategies and how Gaussian sampling is applied.  Rank 4 uses a hybrid approach, while Rank 5 applies Gaussian perturbation in the mutation phase instead of initialization.  The hybrid initialization strategy is superior.

Overall: The most significant difference among the heuristics lies in their population initialization strategies.  Hybrid approaches combining uniform and Gaussian sampling, particularly those that smartly leverage an initial best solution (Rank 1 and 3), consistently outperform pure uniform random or Gaussian perturbation within the DE framework.
- Effective population initialization for high-dimensional problems requires strategies that promote diversity and guide the search towards promising regions.  Hybrid methods combining uniform and Gaussian sampling, potentially centered around an initially found good solution, are particularly valuable.  Careful tuning of the Gaussian sampling rate is also crucial.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:32:37 INFO Generation 6, best so far: 0.21369870938149668
2025-06-20 18:32:37 INFO --- Performing Long-Term Reflection at Generation 6 ---
2025-06-20 18:32:42 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (worst) HybridDEwithAdaptiveGaussianInitialization (Rank 5), we see that Rank 1 uses a more sophisticated initialization strategy by combining uniform and Gaussian sampling within the `_initialize_population` function, whereas Rank 5 uses a simpler hybrid approach with less control over the distribution of initial samples.  Rank 1 also adaptively adjusts the `gaussian_sampling_rate` while Rank 5 uses a fixed rate.  Rank 1's approach leads to better exploration of the search space.

(second best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2) vs (second worst) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4): Rank 2's initialization is purely uniform random, which lacks the targeted exploration of Rank 4s hybrid approach. The difference lies primarily in the initialization method. Rank 4 intelligently centers its Gaussian sampling around the mean of the bounds, leading to more efficient exploration.

Comparing (1st) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (2nd) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2), we see that the main difference is the initialization. Rank 1 combines DE with Gaussian sampling in initialization, achieving better diversity and potentially escaping local optima more effectively than Rank 2's purely uniform random initialization.

(3rd) AdaptiveDifferentialEvolutionWithGaussianInitialization (Rank 3) vs (4th) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4):  Rank 3 focuses on creating the initial population around an initially-found good solution, further refining the exploration near promising regions. Rank 4 uses a hybrid approach but lacks the focused initialization strategy of Rank 3.

Comparing (second worst) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4) vs (worst) HybridDEwithAdaptiveGaussianInitialization (Rank 5), we see that Rank 4 has a more balanced hybrid approach, incorporating both uniform and Gaussian sampling. Rank 5, however, centers its Gaussian sampling around a single initial solution, potentially limiting exploration if the initial solution is poor.  Rank 4's improved diversity via a more balanced hybrid method accounts for the performance difference.

Overall: The best-performing algorithms utilize a hybrid initialization that combines uniform and Gaussian sampling, strategically placing individuals to enhance exploration and escape local optima. Adaptive strategies for adjusting parameters during the optimization process, as seen in the best algorithms, are also crucial for efficient search. The worst performing algorithm has less sophisticated methods for population initialization and lack adaptiveness.


**Experience:**

Employing hybrid initialization strategies that adaptively balance uniform and Gaussian sampling, guided by the problem's characteristics, is crucial for high-dimensional optimization.  Incorporating mechanisms to adjust exploration parameters during optimization is essential for improved robustness and performance.

2025-06-20 18:32:42 INFO Generating offspring via Crossover...
2025-06-20 18:32:51 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:32:57 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.2190
2025-06-20 18:32:57 INFO FeHistory: [ 1.13053078e+06  1.06423951e+06  1.14871403e+06 ... -3.82613958e+02
 -3.82613958e+02 -3.82613958e+02]
2025-06-20 18:32:57 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:32:57 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE
import numpy as np
import random

# Name: AdaptiveHybridDE
# Description: Combines Differential Evolution with adaptive hybrid initialization for multimodal optimization.
# Code:
class AdaptiveHybridDE:
    """
    Combines Differential Evolution with an adaptive hybrid initialization 
    strategy for efficient multimodal optimization.  The initialization
    mixes uniform random sampling with Gaussian sampling centered around the 
    mean of the bounds, adaptively adjusting the Gaussian sampling rate.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.2 # Initial Gaussian sampling rate
        self.gaussian_sampling_rate_decay = 0.95 # Decay rate for gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            self.gaussian_sampling_rate *= self.gaussian_sampling_rate_decay #Adaptive decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        mean = (self.upper_bounds + self.lower_bounds) / 2
        std = (self.upper_bounds - self.lower_bounds) / 4  # Adjust standard deviation as needed.
        population_gaussian = np.random.normal(mean, std, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:32:57 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:33:02 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2677
2025-06-20 18:33:02 INFO FeHistory: [240265.8130331  272407.68467364 374366.87650502 ...   -912.85595901
   -912.85595901   -912.85595901]
2025-06-20 18:33:02 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:33:02 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE
import numpy as np
import random

# Name: AdaptiveHybridDE
# Description: Combines Differential Evolution with adaptive hybrid initialization for multimodal optimization.
# Code:
class AdaptiveHybridDE:
    """
    Combines Differential Evolution with an adaptive hybrid initialization 
    strategy for efficient multimodal optimization.  The initialization
    mixes uniform random sampling with Gaussian sampling centered around the 
    mean of the bounds, adaptively adjusting the Gaussian sampling rate.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.2 # Initial Gaussian sampling rate
        self.gaussian_sampling_rate_decay = 0.95 # Decay rate for gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            self.gaussian_sampling_rate *= self.gaussian_sampling_rate_decay #Adaptive decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        mean = (self.upper_bounds + self.lower_bounds) / 2
        std = (self.upper_bounds - self.lower_bounds) / 4  # Adjust standard deviation as needed.
        population_gaussian = np.random.normal(mean, std, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:33:02 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:33:08 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2635
2025-06-20 18:33:08 INFO FeHistory: [167082.46015401 164637.46495935 129454.05219594 ...   -656.78751586
   -656.78751586   -656.78751586]
2025-06-20 18:33:08 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:33:08 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE
import numpy as np
import random

# Name: AdaptiveHybridDE
# Description: Combines Differential Evolution with adaptive hybrid initialization for multimodal optimization.
# Code:
class AdaptiveHybridDE:
    """
    Combines Differential Evolution with an adaptive hybrid initialization 
    strategy for efficient multimodal optimization.  The initialization
    mixes uniform random sampling with Gaussian sampling centered around the 
    mean of the bounds, adaptively adjusting the Gaussian sampling rate.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.2 # Initial Gaussian sampling rate
        self.gaussian_sampling_rate_decay = 0.95 # Decay rate for gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            self.gaussian_sampling_rate *= self.gaussian_sampling_rate_decay #Adaptive decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        mean = (self.upper_bounds + self.lower_bounds) / 2
        std = (self.upper_bounds - self.lower_bounds) / 4  # Adjust standard deviation as needed.
        population_gaussian = np.random.normal(mean, std, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:33:08 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:33:24 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0219
2025-06-20 18:33:24 INFO FeHistory: [215.14999133 184.54616174 223.66348591 ... -53.45368591 -57.73914218
 -54.74900561]
2025-06-20 18:33:24 INFO Expected Optimum FE: -100
2025-06-20 18:33:24 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE
import numpy as np
import random

# Name: AdaptiveHybridDE
# Description: Combines Differential Evolution with adaptive hybrid initialization for multimodal optimization.
# Code:
class AdaptiveHybridDE:
    """
    Combines Differential Evolution with an adaptive hybrid initialization 
    strategy for efficient multimodal optimization.  The initialization
    mixes uniform random sampling with Gaussian sampling centered around the 
    mean of the bounds, adaptively adjusting the Gaussian sampling rate.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.2 # Initial Gaussian sampling rate
        self.gaussian_sampling_rate_decay = 0.95 # Decay rate for gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            self.gaussian_sampling_rate *= self.gaussian_sampling_rate_decay #Adaptive decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        mean = (self.upper_bounds + self.lower_bounds) / 2
        std = (self.upper_bounds - self.lower_bounds) / 4  # Adjust standard deviation as needed.
        population_gaussian = np.random.normal(mean, std, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:33:24 INFO Unimodal AOCC mean: 0.2190
2025-06-20 18:33:24 INFO Multimodal (single component) AOCC mean: 0.2656
2025-06-20 18:33:24 INFO Multimodal (multiple components) AOCC mean: 0.0219
2025-06-20 18:33:24 INFO AOCC mean: 0.1930
2025-06-20 18:33:34 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:33:39 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1447
2025-06-20 18:33:39 INFO FeHistory: [ 1.12136027e+06  3.88544822e+05  7.51554587e+05 ... -3.82472275e+02
 -3.82472964e+02 -3.82472622e+02]
2025-06-20 18:33:39 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:33:39 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE
import numpy as np
import random

# Name: AdaptiveHybridDE
# Description: Combines Differential Evolution with adaptive hybrid initialization for multimodal optimization.
# Code:
class AdaptiveHybridDE:
    """
    Combines Differential Evolution with adaptive hybrid initialization (uniform and Gaussian) for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling
        self.hybrid_rate = 0.5 # Ratio of Gaussian vs Uniform sampling in initialization

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale
            self.gaussian_sampling_rate *= 0.99 #Decay Gaussian sampling rate


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        population = np.empty((self.population_size, self.dim))
        for i in range(self.population_size):
            if np.random.rand() < self.hybrid_rate:
                # Gaussian sampling centered around the mean of the bounds
                mean = (self.lower_bounds + self.upper_bounds) / 2
                std = (self.upper_bounds - self.lower_bounds) / 6  # Adjust std as needed
                population[i] = np.random.normal(mean, std, self.dim)
                population[i] = np.clip(population[i], self.lower_bounds, self.upper_bounds)
            else:
                # Uniform sampling
                population[i] = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return population

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:33:39 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:33:45 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2766
2025-06-20 18:33:45 INFO FeHistory: [ 97321.94114704 121921.47983161 191986.18526882 ...   -912.85650894
   -912.85650894   -912.85650894]
2025-06-20 18:33:45 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:33:45 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE
import numpy as np
import random

# Name: AdaptiveHybridDE
# Description: Combines Differential Evolution with adaptive hybrid initialization for multimodal optimization.
# Code:
class AdaptiveHybridDE:
    """
    Combines Differential Evolution with adaptive hybrid initialization (uniform and Gaussian) for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling
        self.hybrid_rate = 0.5 # Ratio of Gaussian vs Uniform sampling in initialization

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale
            self.gaussian_sampling_rate *= 0.99 #Decay Gaussian sampling rate


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        population = np.empty((self.population_size, self.dim))
        for i in range(self.population_size):
            if np.random.rand() < self.hybrid_rate:
                # Gaussian sampling centered around the mean of the bounds
                mean = (self.lower_bounds + self.upper_bounds) / 2
                std = (self.upper_bounds - self.lower_bounds) / 6  # Adjust std as needed
                population[i] = np.random.normal(mean, std, self.dim)
                population[i] = np.clip(population[i], self.lower_bounds, self.upper_bounds)
            else:
                # Uniform sampling
                population[i] = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return population

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:33:45 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:33:51 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2991
2025-06-20 18:33:51 INFO FeHistory: [242705.30140407 184163.76368726 103457.58991924 ...   -656.78887879
   -656.78887878   -656.78887879]
2025-06-20 18:33:51 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:33:51 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE
import numpy as np
import random

# Name: AdaptiveHybridDE
# Description: Combines Differential Evolution with adaptive hybrid initialization for multimodal optimization.
# Code:
class AdaptiveHybridDE:
    """
    Combines Differential Evolution with adaptive hybrid initialization (uniform and Gaussian) for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling
        self.hybrid_rate = 0.5 # Ratio of Gaussian vs Uniform sampling in initialization

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale
            self.gaussian_sampling_rate *= 0.99 #Decay Gaussian sampling rate


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        population = np.empty((self.population_size, self.dim))
        for i in range(self.population_size):
            if np.random.rand() < self.hybrid_rate:
                # Gaussian sampling centered around the mean of the bounds
                mean = (self.lower_bounds + self.upper_bounds) / 2
                std = (self.upper_bounds - self.lower_bounds) / 6  # Adjust std as needed
                population[i] = np.random.normal(mean, std, self.dim)
                population[i] = np.clip(population[i], self.lower_bounds, self.upper_bounds)
            else:
                # Uniform sampling
                population[i] = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return population

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:33:51 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:34:07 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0155
2025-06-20 18:34:07 INFO FeHistory: [201.92428723 192.96671188 190.17785493 ... -46.4991602  -44.57819402
 -45.71191709]
2025-06-20 18:34:07 INFO Expected Optimum FE: -100
2025-06-20 18:34:07 INFO Unimodal AOCC mean: 0.1447
2025-06-20 18:34:07 INFO Multimodal (single component) AOCC mean: 0.2879
2025-06-20 18:34:07 INFO Multimodal (multiple components) AOCC mean: 0.0155
2025-06-20 18:34:07 INFO AOCC mean: 0.1840
2025-06-20 18:34:07 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (worst) HybridDEwithAdaptiveGaussianInitialization (Rank 5), we see that Rank 1 uses a more sophisticated initialization strategy by combining uniform and Gaussian sampling within the `_initialize_population` function, whereas Rank 5 uses a simpler hybrid approach with less control over the distribution of initial samples.  Rank 1 also adaptively adjusts the `gaussian_sampling_rate` while Rank 5 uses a fixed rate.  Rank 1's approach leads to better exploration of the search space.

(second best) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2) vs (second worst) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4): Rank 2's initialization is purely uniform random, which lacks the targeted exploration of Rank 4s hybrid approach. The difference lies primarily in the initialization method. Rank 4 intelligently centers its Gaussian sampling around the mean of the bounds, leading to more efficient exploration.

Comparing (1st) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 1) vs (2nd) AdaptiveDifferentialEvolutionWithGaussianSampling (Rank 2), we see that the main difference is the initialization. Rank 1 combines DE with Gaussian sampling in initialization, achieving better diversity and potentially escaping local optima more effectively than Rank 2's purely uniform random initialization.

(3rd) AdaptiveDifferentialEvolutionWithGaussianInitialization (Rank 3) vs (4th) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4):  Rank 3 focuses on creating the initial population around an initially-found good solution, further refining the exploration near promising regions. Rank 4 uses a hybrid approach but lacks the focused initialization strategy of Rank 3.

Comparing (second worst) AdaptiveDifferentialEvolutionWithHybridInitialization (Rank 4) vs (worst) HybridDEwithAdaptiveGaussianInitialization (Rank 5), we see that Rank 4 has a more balanced hybrid approach, incorporating both uniform and Gaussian sampling. Rank 5, however, centers its Gaussian sampling around a single initial solution, potentially limiting exploration if the initial solution is poor.  Rank 4's improved diversity via a more balanced hybrid method accounts for the performance difference.

Overall: The best-performing algorithms utilize a hybrid initialization that combines uniform and Gaussian sampling, strategically placing individuals to enhance exploration and escape local optima. Adaptive strategies for adjusting parameters during the optimization process, as seen in the best algorithms, are also crucial for efficient search. The worst performing algorithm has less sophisticated methods for population initialization and lack adaptiveness.
- Employing hybrid initialization strategies that adaptively balance uniform and Gaussian sampling, guided by the problem's characteristics, is crucial for high-dimensional optimization.  Incorporating mechanisms to adjust exploration parameters during optimization is essential for improved robustness and performance.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:34:07 INFO Generation 7, best so far: 0.21369870938149668
2025-06-20 18:34:07 INFO --- Performing Long-Term Reflection at Generation 7 ---
2025-06-20 18:34:11 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 1) vs (worst) `AdaptiveDifferentialEvolutionWithHybridInitialization` (Rank 5), we see that Rank 1 incorporates adaptive Gaussian sampling *during* the optimization process, not just during initialization. This allows for continuous exploration of promising regions, while Rank 5 relies solely on initial hybrid sampling. Rank 1's adaptive mutation scale also contributes to its better performance.  (second best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 2) is almost identical to Rank 1, the only difference is in the docstring and the population initialization. This shows that even small differences in initialization can impact performance but less than adaptive sampling during optimization. Comparing (1st) vs (2nd), we see that the slight difference in initialization (Rank 1 uses both DE and Gaussian, Rank 2 only DE) results in a negligible performance difference in AOCC score, suggesting that adaptive Gaussian sampling during the search is the most crucial factor. (3rd) `AdaptiveDifferentialEvolutionWithGaussianInitialization` vs (4th) `AdaptiveHybridDE`, both use Gaussian sampling but in different ways. Rank 3 centers the Gaussian around the initial best solution, while Rank 4 centers it around the mean of the bounds.  Rank 3's approach of focusing on a promising region shows a slight improvement. Comparing (second worst) `AdaptiveHybridDE` vs (worst) `AdaptiveDifferentialEvolutionWithHybridInitialization`, both use hybrid initialization, but Rank 4 incorporates an adaptive decay rate for Gaussian sampling, which contributes to a slightly better AOCC score compared to the static rate in Rank 5. Overall: The most significant factor influencing performance appears to be the incorporation of adaptive Gaussian sampling *throughout* the optimization process (during mutation) rather than solely relying on sophisticated initialization strategies.  Adaptive decay of the mutation scale and Gaussian sampling rate also positively affect performance.

**Experience:**

Adaptive sampling during the optimization process is crucial for high-dimensional problems with wide bounds.  Initial population diversity is important, but continuous exploration is more critical.  Fine-tuning parameter decay rates warrants further investigation.

2025-06-20 18:34:11 INFO Generating offspring via Crossover...
2025-06-20 18:34:21 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:34:27 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1864
2025-06-20 18:34:27 INFO FeHistory: [ 6.47724944e+05  8.98793414e+05  9.66387741e+05 ... -3.82580919e+02
 -3.82580919e+02 -3.82580919e+02]
2025-06-20 18:34:27 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:34:27 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with hybrid initialization and adaptive Gaussian sampling during optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian) and 
    adaptive Gaussian sampling during the optimization process for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.1  # Percentage of population updated with Gaussian sampling
        self.gaussian_sampling_std_decay = 0.95 # Decay rate for Gaussian std

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        gaussian_std = (self.upper_bounds - self.lower_bounds) / 4

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, gaussian_std, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            gaussian_std *= self.gaussian_sampling_std_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        population_gaussian = np.random.normal(0, (self.upper_bounds - self.lower_bounds) / 4, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:34:27 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:34:33 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2697
2025-06-20 18:34:33 INFO FeHistory: [221032.04832217 230335.66525677 163007.99965951 ...   -912.85652318
   -912.85652318   -912.85652318]
2025-06-20 18:34:33 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:34:33 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with hybrid initialization and adaptive Gaussian sampling during optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian) and 
    adaptive Gaussian sampling during the optimization process for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.1  # Percentage of population updated with Gaussian sampling
        self.gaussian_sampling_std_decay = 0.95 # Decay rate for Gaussian std

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        gaussian_std = (self.upper_bounds - self.lower_bounds) / 4

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, gaussian_std, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            gaussian_std *= self.gaussian_sampling_std_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        population_gaussian = np.random.normal(0, (self.upper_bounds - self.lower_bounds) / 4, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:34:33 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:34:39 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2918
2025-06-20 18:34:39 INFO FeHistory: [191807.41910016 233293.22030543 218574.8759799  ...   -656.78872598
   -656.78872598   -656.78872598]
2025-06-20 18:34:39 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:34:39 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with hybrid initialization and adaptive Gaussian sampling during optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian) and 
    adaptive Gaussian sampling during the optimization process for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.1  # Percentage of population updated with Gaussian sampling
        self.gaussian_sampling_std_decay = 0.95 # Decay rate for Gaussian std

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        gaussian_std = (self.upper_bounds - self.lower_bounds) / 4

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, gaussian_std, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            gaussian_std *= self.gaussian_sampling_std_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        population_gaussian = np.random.normal(0, (self.upper_bounds - self.lower_bounds) / 4, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:34:39 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:34:56 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0212
2025-06-20 18:34:56 INFO FeHistory: [222.86791233 163.02316884 209.45644456 ... -55.40435316 -56.53110087
 -58.06321181]
2025-06-20 18:34:56 INFO Expected Optimum FE: -100
2025-06-20 18:34:56 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with hybrid initialization and adaptive Gaussian sampling during optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian) and 
    adaptive Gaussian sampling during the optimization process for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.1  # Percentage of population updated with Gaussian sampling
        self.gaussian_sampling_std_decay = 0.95 # Decay rate for Gaussian std

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        gaussian_std = (self.upper_bounds - self.lower_bounds) / 4

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, gaussian_std, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            gaussian_std *= self.gaussian_sampling_std_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        population_gaussian = np.random.normal(0, (self.upper_bounds - self.lower_bounds) / 4, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:34:56 INFO Unimodal AOCC mean: 0.1864
2025-06-20 18:34:56 INFO Multimodal (single component) AOCC mean: 0.2808
2025-06-20 18:34:56 INFO Multimodal (multiple components) AOCC mean: 0.0212
2025-06-20 18:34:56 INFO AOCC mean: 0.1923
2025-06-20 18:35:05 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:35:11 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1796
2025-06-20 18:35:11 INFO FeHistory: [ 8.12461077e+05  7.18728804e+05  7.87703022e+05 ... -3.82590737e+02
 -3.82590737e+02 -3.82590711e+02]
2025-06-20 18:35:11 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:35:11 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Differential evolution with hybrid initialization and adaptive Gaussian mutation.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy and adaptive Gaussian mutation 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale
            self.gaussian_sampling_rate *= 0.99 #Decay Gaussian sampling rate

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Hybrid initialization: Mix of uniform and Gaussian around the mean
        pop_size = self.population_size
        dim = self.dim
        pop = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size, dim))
        
        #Gaussian component centered around mean of bounds
        mean = (self.upper_bounds + self.lower_bounds) / 2
        gaussian_samples = np.random.normal(loc=mean, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(pop_size // 2, dim))
        gaussian_samples = np.clip(gaussian_samples, self.lower_bounds, self.upper_bounds)
        
        pop[:pop_size//2] = gaussian_samples #replace half population with gaussian samples

        return pop


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:35:11 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:35:16 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2913
2025-06-20 18:35:16 INFO FeHistory: [147769.06686188 232660.28090555 164558.46622066 ...   -912.85709903
   -912.85709903   -912.85709903]
2025-06-20 18:35:16 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:35:16 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Differential evolution with hybrid initialization and adaptive Gaussian mutation.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy and adaptive Gaussian mutation 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale
            self.gaussian_sampling_rate *= 0.99 #Decay Gaussian sampling rate

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Hybrid initialization: Mix of uniform and Gaussian around the mean
        pop_size = self.population_size
        dim = self.dim
        pop = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size, dim))
        
        #Gaussian component centered around mean of bounds
        mean = (self.upper_bounds + self.lower_bounds) / 2
        gaussian_samples = np.random.normal(loc=mean, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(pop_size // 2, dim))
        gaussian_samples = np.clip(gaussian_samples, self.lower_bounds, self.upper_bounds)
        
        pop[:pop_size//2] = gaussian_samples #replace half population with gaussian samples

        return pop


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:35:16 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:35:22 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.3085
2025-06-20 18:35:22 INFO FeHistory: [112005.24783902 231401.86761693 177471.62665787 ...   -656.78890498
   -656.78890498   -656.78890498]
2025-06-20 18:35:22 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:35:22 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Differential evolution with hybrid initialization and adaptive Gaussian mutation.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy and adaptive Gaussian mutation 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale
            self.gaussian_sampling_rate *= 0.99 #Decay Gaussian sampling rate

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Hybrid initialization: Mix of uniform and Gaussian around the mean
        pop_size = self.population_size
        dim = self.dim
        pop = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size, dim))
        
        #Gaussian component centered around mean of bounds
        mean = (self.upper_bounds + self.lower_bounds) / 2
        gaussian_samples = np.random.normal(loc=mean, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(pop_size // 2, dim))
        gaussian_samples = np.clip(gaussian_samples, self.lower_bounds, self.upper_bounds)
        
        pop[:pop_size//2] = gaussian_samples #replace half population with gaussian samples

        return pop


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:35:22 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:35:38 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0261
2025-06-20 18:35:38 INFO FeHistory: [189.31039883 188.45883155 206.68516232 ... -67.05272109 -64.19185915
 -63.79687993]
2025-06-20 18:35:38 INFO Expected Optimum FE: -100
2025-06-20 18:35:38 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Differential evolution with hybrid initialization and adaptive Gaussian mutation.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy and adaptive Gaussian mutation 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale
            self.gaussian_sampling_rate *= 0.99 #Decay Gaussian sampling rate

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Hybrid initialization: Mix of uniform and Gaussian around the mean
        pop_size = self.population_size
        dim = self.dim
        pop = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size, dim))
        
        #Gaussian component centered around mean of bounds
        mean = (self.upper_bounds + self.lower_bounds) / 2
        gaussian_samples = np.random.normal(loc=mean, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(pop_size // 2, dim))
        gaussian_samples = np.clip(gaussian_samples, self.lower_bounds, self.upper_bounds)
        
        pop[:pop_size//2] = gaussian_samples #replace half population with gaussian samples

        return pop


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:35:38 INFO Unimodal AOCC mean: 0.1796
2025-06-20 18:35:38 INFO Multimodal (single component) AOCC mean: 0.2999
2025-06-20 18:35:38 INFO Multimodal (multiple components) AOCC mean: 0.0261
2025-06-20 18:35:38 INFO AOCC mean: 0.2013
2025-06-20 18:35:38 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 1) vs (worst) `AdaptiveDifferentialEvolutionWithHybridInitialization` (Rank 5), we see that Rank 1 incorporates adaptive Gaussian sampling *during* the optimization process, not just during initialization. This allows for continuous exploration of promising regions, while Rank 5 relies solely on initial hybrid sampling. Rank 1's adaptive mutation scale also contributes to its better performance.  (second best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 2) is almost identical to Rank 1, the only difference is in the docstring and the population initialization. This shows that even small differences in initialization can impact performance but less than adaptive sampling during optimization. Comparing (1st) vs (2nd), we see that the slight difference in initialization (Rank 1 uses both DE and Gaussian, Rank 2 only DE) results in a negligible performance difference in AOCC score, suggesting that adaptive Gaussian sampling during the search is the most crucial factor. (3rd) `AdaptiveDifferentialEvolutionWithGaussianInitialization` vs (4th) `AdaptiveHybridDE`, both use Gaussian sampling but in different ways. Rank 3 centers the Gaussian around the initial best solution, while Rank 4 centers it around the mean of the bounds.  Rank 3's approach of focusing on a promising region shows a slight improvement. Comparing (second worst) `AdaptiveHybridDE` vs (worst) `AdaptiveDifferentialEvolutionWithHybridInitialization`, both use hybrid initialization, but Rank 4 incorporates an adaptive decay rate for Gaussian sampling, which contributes to a slightly better AOCC score compared to the static rate in Rank 5. Overall: The most significant factor influencing performance appears to be the incorporation of adaptive Gaussian sampling *throughout* the optimization process (during mutation) rather than solely relying on sophisticated initialization strategies.  Adaptive decay of the mutation scale and Gaussian sampling rate also positively affect performance.
- Adaptive sampling during the optimization process is crucial for high-dimensional problems with wide bounds.  Initial population diversity is important, but continuous exploration is more critical.  Fine-tuning parameter decay rates warrants further investigation.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:35:38 INFO Generation 8, best so far: 0.21369870938149668
2025-06-20 18:35:38 INFO --- Performing Long-Term Reflection at Generation 8 ---
2025-06-20 18:35:44 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 1) vs (worst) `AdaptiveHybridDE` (Rank 5), we see that Rank 1 uses a more sophisticated initialization strategy that combines uniform and Gaussian sampling, leading to a better exploration of the search space. Rank 5 also uses a hybrid approach but its Gaussian sampling rate decays too aggressively, potentially hindering exploration.  Rank 1 also maintains a constant gaussian sampling rate, allowing for continuous exploration throughout the optimization process.


(second best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 2) vs (second worst) `AdaptiveDifferentialEvolutionWithGaussianInitialization` (Rank 4):  Rank 2's simpler uniform random initialization is less effective than Rank 1's hybrid method, but significantly outperforms Rank 4's Gaussian-centered initialization around an initially found solution, which might lead to premature convergence. Rank 2's broader initial exploration seems beneficial.


Comparing (1st) vs (2nd), we see that the inclusion of Gaussian sampling in the initialization (Rank 1) provides a slight edge over pure uniform random initialization (Rank 2), leading to a marginal improvement in the AOCC score. The adaptive nature of the mutation scale is common to both.


(3rd) vs (4th): Rank 3 (`AdaptiveDifferentialEvolutionWithHybridInitialization`)  achieves better results than Rank 4 because its hybrid initialization strategy samples around the mean of the bounds instead of solely around an initially found point (which might be local optima). This provides a more diverse starting point and improved exploration.


Comparing (second worst) vs (worst), we see that both Rank 4 and Rank 5 employ some form of Gaussian initialization. However, Rank 4 is slightly better, suggesting the decay in Gaussian sampling rate employed by Rank 5 is too rapid, leading to suboptimal performance.


Overall: The top-performing algorithms use a hybrid initialization strategy combining uniform random sampling with Gaussian sampling, with Rank 1 offering the most balanced approach.  Adaptively decaying the mutation scale seems beneficial, whereas decaying the Gaussian sampling rate too rapidly can be detrimental.  Focusing on initial population diversity is key.


**Experience:**

Developing effective population initialization strategies requires balancing exploration and exploitation.  Hybrid approaches that combine different sampling methods offer advantages over simpler methods. Adaptive parameter tuning based on the optimization progress is crucial for robust performance.  Careful consideration of decay rates for adaptive parameters is essential to avoid premature convergence.

2025-06-20 18:35:44 INFO Generating offspring via Crossover...
2025-06-20 18:35:54 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:35:59 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.2035
2025-06-20 18:35:59 INFO FeHistory: [ 7.29728322e+05  1.01975201e+06  6.01826577e+05 ... -3.82619360e+02
 -3.82619043e+02 -3.82618900e+02]
2025-06-20 18:35:59 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:35:59 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDEWithImprovedInitialization
import numpy as np
import random

# Name: AdaptiveHybridDEWithImprovedInitialization
# Description: Hybrid Differential Evolution with improved initialization for multimodal optimization.
# Code:
class AdaptiveHybridDEWithImprovedInitialization:
    """
    Combines Differential Evolution with adaptive Gaussian sampling and a sophisticated initialization 
    strategy for efficient multimodal optimization.  The initialization balances uniform and Gaussian 
    sampling to enhance exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.3 # Increased rate for better exploration

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        # Improved Gaussian sampling: around the mean of bounds with controlled variance
        mean_bounds = (self.upper_bounds + self.lower_bounds) / 2
        std_bounds = (self.upper_bounds - self.lower_bounds) / 6 # Reduced standard deviation

        population_gaussian = np.random.normal(mean_bounds, std_bounds, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:35:59 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:36:06 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.3141
2025-06-20 18:36:06 INFO FeHistory: [247653.12554625 257493.13665495 164056.86160613 ...   -912.85734928
   -912.8573493    -912.85734931]
2025-06-20 18:36:06 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:36:06 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDEWithImprovedInitialization
import numpy as np
import random

# Name: AdaptiveHybridDEWithImprovedInitialization
# Description: Hybrid Differential Evolution with improved initialization for multimodal optimization.
# Code:
class AdaptiveHybridDEWithImprovedInitialization:
    """
    Combines Differential Evolution with adaptive Gaussian sampling and a sophisticated initialization 
    strategy for efficient multimodal optimization.  The initialization balances uniform and Gaussian 
    sampling to enhance exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.3 # Increased rate for better exploration

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        # Improved Gaussian sampling: around the mean of bounds with controlled variance
        mean_bounds = (self.upper_bounds + self.lower_bounds) / 2
        std_bounds = (self.upper_bounds - self.lower_bounds) / 6 # Reduced standard deviation

        population_gaussian = np.random.normal(mean_bounds, std_bounds, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:36:06 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:36:11 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.3010
2025-06-20 18:36:11 INFO FeHistory: [118170.56965735 144879.89797426 292484.77269379 ...   -656.78871097
   -656.78887417   -656.78889096]
2025-06-20 18:36:11 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:36:11 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDEWithImprovedInitialization
import numpy as np
import random

# Name: AdaptiveHybridDEWithImprovedInitialization
# Description: Hybrid Differential Evolution with improved initialization for multimodal optimization.
# Code:
class AdaptiveHybridDEWithImprovedInitialization:
    """
    Combines Differential Evolution with adaptive Gaussian sampling and a sophisticated initialization 
    strategy for efficient multimodal optimization.  The initialization balances uniform and Gaussian 
    sampling to enhance exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.3 # Increased rate for better exploration

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian Perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1 - self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        # Improved Gaussian sampling: around the mean of bounds with controlled variance
        mean_bounds = (self.upper_bounds + self.lower_bounds) / 2
        std_bounds = (self.upper_bounds - self.lower_bounds) / 6 # Reduced standard deviation

        population_gaussian = np.random.normal(mean_bounds, std_bounds, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:36:11 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:36:28 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0172
2025-06-20 18:36:28 INFO FeHistory: [152.15468107 180.31881476 196.44397949 ... -47.39907039 -49.13684438
 -47.92597931]
2025-06-20 18:36:28 INFO Expected Optimum FE: -100
2025-06-20 18:36:28 INFO Unimodal AOCC mean: 0.2035
2025-06-20 18:36:28 INFO Multimodal (single component) AOCC mean: 0.3076
2025-06-20 18:36:28 INFO Multimodal (multiple components) AOCC mean: 0.0172
2025-06-20 18:36:28 INFO AOCC mean: 0.2090
2025-06-20 18:36:37 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:36:43 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.2041
2025-06-20 18:36:43 INFO FeHistory: [ 4.40435829e+05  5.72050016e+05  1.18933056e+06 ... -3.82618690e+02
 -3.82619346e+02 -3.82619360e+02]
2025-06-20 18:36:43 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:36:43 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDEWithImprovedInitialization
import numpy as np
import random

# Name: AdaptiveHybridDEWithImprovedInitialization
# Description: Hybrid Differential Evolution with improved initialization for multimodal optimization.
# Code:
class AdaptiveHybridDEWithImprovedInitialization:
    """
    Combines Differential Evolution with adaptive Gaussian sampling and a hybrid initialization 
    strategy for efficient exploration of multimodal landscapes.  Improves upon previous 
    approaches by using a more robust initialization that balances exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.2 #probability of performing gaussian sampling


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Hybrid initialization:  Combines uniform and Gaussian sampling around the mean
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(int(self.population_size * 0.8), self.dim))
        mean = (self.upper_bounds + self.lower_bounds) / 2
        gaussian_samples = np.random.normal(loc=mean, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(int(self.population_size * 0.2), self.dim))  # Gaussian samples around the mean
        gaussian_samples = np.clip(gaussian_samples, self.lower_bounds, self.upper_bounds) #clip to bounds
        population = np.concatenate((population, gaussian_samples))
        return population

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:36:43 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:36:49 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.3176
2025-06-20 18:36:49 INFO FeHistory: [196937.89207084 208243.13125405 200183.00075702 ...   -912.85715516
   -912.85732617   -912.85726622]
2025-06-20 18:36:49 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:36:49 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDEWithImprovedInitialization
import numpy as np
import random

# Name: AdaptiveHybridDEWithImprovedInitialization
# Description: Hybrid Differential Evolution with improved initialization for multimodal optimization.
# Code:
class AdaptiveHybridDEWithImprovedInitialization:
    """
    Combines Differential Evolution with adaptive Gaussian sampling and a hybrid initialization 
    strategy for efficient exploration of multimodal landscapes.  Improves upon previous 
    approaches by using a more robust initialization that balances exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.2 #probability of performing gaussian sampling


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Hybrid initialization:  Combines uniform and Gaussian sampling around the mean
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(int(self.population_size * 0.8), self.dim))
        mean = (self.upper_bounds + self.lower_bounds) / 2
        gaussian_samples = np.random.normal(loc=mean, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(int(self.population_size * 0.2), self.dim))  # Gaussian samples around the mean
        gaussian_samples = np.clip(gaussian_samples, self.lower_bounds, self.upper_bounds) #clip to bounds
        population = np.concatenate((population, gaussian_samples))
        return population

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:36:49 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:36:54 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.3031
2025-06-20 18:36:54 INFO FeHistory: [197570.64804542 163447.43628892 220416.9962077  ...   -656.78895657
   -656.78895652   -656.78895652]
2025-06-20 18:36:54 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:36:54 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDEWithImprovedInitialization
import numpy as np
import random

# Name: AdaptiveHybridDEWithImprovedInitialization
# Description: Hybrid Differential Evolution with improved initialization for multimodal optimization.
# Code:
class AdaptiveHybridDEWithImprovedInitialization:
    """
    Combines Differential Evolution with adaptive Gaussian sampling and a hybrid initialization 
    strategy for efficient exploration of multimodal landscapes.  Improves upon previous 
    approaches by using a more robust initialization that balances exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.2 #probability of performing gaussian sampling


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Hybrid initialization:  Combines uniform and Gaussian sampling around the mean
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(int(self.population_size * 0.8), self.dim))
        mean = (self.upper_bounds + self.lower_bounds) / 2
        gaussian_samples = np.random.normal(loc=mean, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(int(self.population_size * 0.2), self.dim))  # Gaussian samples around the mean
        gaussian_samples = np.clip(gaussian_samples, self.lower_bounds, self.upper_bounds) #clip to bounds
        population = np.concatenate((population, gaussian_samples))
        return population

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:36:54 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:37:11 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0240
2025-06-20 18:37:11 INFO FeHistory: [168.56789687 175.82779216 227.22890394 ... -65.77169692 -66.17354448
 -65.85192007]
2025-06-20 18:37:11 INFO Expected Optimum FE: -100
2025-06-20 18:37:11 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDEWithImprovedInitialization
import numpy as np
import random

# Name: AdaptiveHybridDEWithImprovedInitialization
# Description: Hybrid Differential Evolution with improved initialization for multimodal optimization.
# Code:
class AdaptiveHybridDEWithImprovedInitialization:
    """
    Combines Differential Evolution with adaptive Gaussian sampling and a hybrid initialization 
    strategy for efficient exploration of multimodal landscapes.  Improves upon previous 
    approaches by using a more robust initialization that balances exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.2 #probability of performing gaussian sampling


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Hybrid initialization:  Combines uniform and Gaussian sampling around the mean
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(int(self.population_size * 0.8), self.dim))
        mean = (self.upper_bounds + self.lower_bounds) / 2
        gaussian_samples = np.random.normal(loc=mean, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(int(self.population_size * 0.2), self.dim))  # Gaussian samples around the mean
        gaussian_samples = np.clip(gaussian_samples, self.lower_bounds, self.upper_bounds) #clip to bounds
        population = np.concatenate((population, gaussian_samples))
        return population

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:37:11 INFO Unimodal AOCC mean: 0.2041
2025-06-20 18:37:11 INFO Multimodal (single component) AOCC mean: 0.3104
2025-06-20 18:37:11 INFO Multimodal (multiple components) AOCC mean: 0.0240
2025-06-20 18:37:11 INFO AOCC mean: 0.2122
2025-06-20 18:37:11 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 1) vs (worst) `AdaptiveHybridDE` (Rank 5), we see that Rank 1 uses a more sophisticated initialization strategy that combines uniform and Gaussian sampling, leading to a better exploration of the search space. Rank 5 also uses a hybrid approach but its Gaussian sampling rate decays too aggressively, potentially hindering exploration.  Rank 1 also maintains a constant gaussian sampling rate, allowing for continuous exploration throughout the optimization process.


(second best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 2) vs (second worst) `AdaptiveDifferentialEvolutionWithGaussianInitialization` (Rank 4):  Rank 2's simpler uniform random initialization is less effective than Rank 1's hybrid method, but significantly outperforms Rank 4's Gaussian-centered initialization around an initially found solution, which might lead to premature convergence. Rank 2's broader initial exploration seems beneficial.


Comparing (1st) vs (2nd), we see that the inclusion of Gaussian sampling in the initialization (Rank 1) provides a slight edge over pure uniform random initialization (Rank 2), leading to a marginal improvement in the AOCC score. The adaptive nature of the mutation scale is common to both.


(3rd) vs (4th): Rank 3 (`AdaptiveDifferentialEvolutionWithHybridInitialization`)  achieves better results than Rank 4 because its hybrid initialization strategy samples around the mean of the bounds instead of solely around an initially found point (which might be local optima). This provides a more diverse starting point and improved exploration.


Comparing (second worst) vs (worst), we see that both Rank 4 and Rank 5 employ some form of Gaussian initialization. However, Rank 4 is slightly better, suggesting the decay in Gaussian sampling rate employed by Rank 5 is too rapid, leading to suboptimal performance.


Overall: The top-performing algorithms use a hybrid initialization strategy combining uniform random sampling with Gaussian sampling, with Rank 1 offering the most balanced approach.  Adaptively decaying the mutation scale seems beneficial, whereas decaying the Gaussian sampling rate too rapidly can be detrimental.  Focusing on initial population diversity is key.
- Developing effective population initialization strategies requires balancing exploration and exploitation.  Hybrid approaches that combine different sampling methods offer advantages over simpler methods. Adaptive parameter tuning based on the optimization progress is crucial for robust performance.  Careful consideration of decay rates for adaptive parameters is essential to avoid premature convergence.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:37:11 INFO Generation 9, best so far: 0.21369870938149668
2025-06-20 18:37:11 INFO --- Performing Long-Term Reflection at Generation 9 ---
2025-06-20 18:37:15 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 1) vs (worst) `AdaptiveDifferentialEvolutionWithHybridInitialization` (Rank 5), we see that Rank 1 uses a more sophisticated initialization combining uniform and Gaussian sampling within the `_initialize_population` method, while Rank 5 only partially replaces the uniform population with Gaussian samples.  Rank 1 also maintains a consistent Gaussian sampling rate, whereas Rank 5 decays this rate, potentially hindering exploration later in the optimization process.  Rank 1s `_initialize_population` method also carefully handles the relative sizes of the uniform and gaussian components.

(second best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 2) vs (second worst) `AdaptiveHybridDEWithImprovedInitialization` (Rank 4): Rank 2 uses simpler, purely uniform random initialization, while Rank 4 uses a hybrid approach.  The simpler approach (Rank 2) surprisingly performs better. The increased gaussian sampling rate in Rank 4 may be detrimental.

Comparing (1st) vs (2nd), we see that the only difference is the initialization strategy. Rank 1's hybrid initialization strategy slightly outperforms Rank 2's purely uniform random initialization, but the difference is small.

(3rd) `AdaptiveHybridDEWithImprovedInitialization` (Rank 3) vs (4th) `AdaptiveHybridDEWithImprovedInitialization` (Rank 4): The key difference lies in the `gaussian_sampling_rate` and the standard deviation used in Gaussian sampling within the `_initialize_population` method.  Rank 3 uses a lower rate (0.2) and implicitly a higher standard deviation (based on range/4), while Rank 4 has a higher rate (0.3) and a lower standard deviation (range/6).  The lower rate and higher standard deviation of Rank 3 leads to better performance.

Comparing (second worst) vs (worst), we see that both use hybrid initialization; however, Rank 4 manages the gaussian component more carefully.  Rank 5's decaying gaussian sampling rate likely reduces exploration in later generations.


Overall: The best performing algorithms utilize a hybrid initialization strategy incorporating both uniform and Gaussian sampling, but the proportion and standard deviation of the Gaussian component are critical.  Decaying the Gaussian sampling rate appears to negatively impact performance.


**Experience:**

Careful consideration of initialization strategy is crucial for high-dimensional problems.  A hybrid approach combining uniform and Gaussian sampling, with parameters tuned to balance exploration and exploitation, proves highly effective.  Avoid decaying the exploration rate too aggressively.

2025-06-20 18:37:15 INFO Generating offspring via Crossover...
2025-06-20 18:37:24 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:37:30 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1886
2025-06-20 18:37:30 INFO FeHistory: [ 9.53245908e+05  9.61145155e+05  9.75539552e+05 ... -3.82597642e+02
 -3.82597642e+02 -3.82597642e+02]
2025-06-20 18:37:30 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:37:30 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with a hybrid initialization for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy 
    (uniform and Gaussian) for efficient multimodal optimization.  The Gaussian
    component helps escape local optima early in the search.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.2 # Percentage of population initialized with Gaussian sampling
        self.gaussian_std_dev_factor = 0.25 #Factor to determine Gaussian standard deviation

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        gaussian_std_dev = (self.upper_bounds - self.lower_bounds) * self.gaussian_std_dev_factor
        population_gaussian = np.random.normal(0, gaussian_std_dev, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:37:30 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:37:36 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2939
2025-06-20 18:37:36 INFO FeHistory: [276702.4896179  226232.84833188 270773.73545126 ...   -912.85710754
   -912.85710754   -912.85710754]
2025-06-20 18:37:36 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:37:36 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with a hybrid initialization for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy 
    (uniform and Gaussian) for efficient multimodal optimization.  The Gaussian
    component helps escape local optima early in the search.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.2 # Percentage of population initialized with Gaussian sampling
        self.gaussian_std_dev_factor = 0.25 #Factor to determine Gaussian standard deviation

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        gaussian_std_dev = (self.upper_bounds - self.lower_bounds) * self.gaussian_std_dev_factor
        population_gaussian = np.random.normal(0, gaussian_std_dev, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:37:36 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:37:42 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.2938
2025-06-20 18:37:42 INFO FeHistory: [183869.54456357 197970.46207149 150967.344606   ...   -656.78875152
   -656.78875152   -656.78875152]
2025-06-20 18:37:42 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:37:42 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with a hybrid initialization for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy 
    (uniform and Gaussian) for efficient multimodal optimization.  The Gaussian
    component helps escape local optima early in the search.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.gaussian_sampling_rate = 0.2 # Percentage of population initialized with Gaussian sampling
        self.gaussian_std_dev_factor = 0.25 #Factor to determine Gaussian standard deviation

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        pop_size_de = int((1-self.gaussian_sampling_rate) * self.population_size)
        pop_size_gaussian = self.population_size - pop_size_de
        population_de = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(pop_size_de, self.dim))
        gaussian_std_dev = (self.upper_bounds - self.lower_bounds) * self.gaussian_std_dev_factor
        population_gaussian = np.random.normal(0, gaussian_std_dev, size=(pop_size_gaussian, self.dim))
        population_gaussian = np.clip(population_gaussian, self.lower_bounds, self.upper_bounds)
        return np.concatenate((population_de, population_gaussian))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:37:42 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:37:58 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0162
2025-06-20 18:37:58 INFO FeHistory: [185.05312639 176.13576135 195.5430687  ... -49.16124966 -51.04339233
 -48.63957018]
2025-06-20 18:37:58 INFO Expected Optimum FE: -100
2025-06-20 18:37:58 INFO Unimodal AOCC mean: 0.1886
2025-06-20 18:37:58 INFO Multimodal (single component) AOCC mean: 0.2939
2025-06-20 18:37:58 INFO Multimodal (multiple components) AOCC mean: 0.0162
2025-06-20 18:37:58 INFO AOCC mean: 0.1981
2025-06-20 18:38:07 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:38:13 INFO Run function 4 complete. FEHistory len: 50000, AOCC: 0.1344
2025-06-20 18:38:13 INFO FeHistory: [ 1.34034782e+06  7.90230980e+05  7.53834980e+05 ... -3.82232821e+02
 -3.82232821e+02 -3.82232821e+02]
2025-06-20 18:38:13 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:38:13 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with a hybrid initialization combining uniform and Gaussian sampling for better exploration.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian sampling) 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.2 # Probability of Gaussian sampling in initialization


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        population_size = self.population_size
        dim = self.dim
        lower = self.lower_bounds
        upper = self.upper_bounds
        
        num_gaussian = int(population_size * self.gaussian_sampling_rate)
        num_uniform = population_size - num_gaussian

        gaussian_population = np.random.normal(loc=(lower + upper)/2, scale=(upper-lower)/4, size=(num_gaussian, dim))
        uniform_population = np.random.uniform(lower, upper, size=(num_uniform, dim))

        population = np.concatenate((gaussian_population, uniform_population), axis=0)
        return population


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:38:13 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:38:19 INFO Run function 7 complete. FEHistory len: 50000, AOCC: 0.2959
2025-06-20 18:38:19 INFO FeHistory: [135920.82893839 193168.21251085 159014.89953921 ...   -912.85713202
   -912.85713202   -912.85713202]
2025-06-20 18:38:19 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:38:19 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with a hybrid initialization combining uniform and Gaussian sampling for better exploration.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian sampling) 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.2 # Probability of Gaussian sampling in initialization


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        population_size = self.population_size
        dim = self.dim
        lower = self.lower_bounds
        upper = self.upper_bounds
        
        num_gaussian = int(population_size * self.gaussian_sampling_rate)
        num_uniform = population_size - num_gaussian

        gaussian_population = np.random.normal(loc=(lower + upper)/2, scale=(upper-lower)/4, size=(num_gaussian, dim))
        uniform_population = np.random.uniform(lower, upper, size=(num_uniform, dim))

        population = np.concatenate((gaussian_population, uniform_population), axis=0)
        return population


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:38:19 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:38:25 INFO Run function 8 complete. FEHistory len: 50000, AOCC: 0.3057
2025-06-20 18:38:25 INFO FeHistory: [ 93471.97101659 123357.82624337 139433.25936491 ...   -656.78885645
   -656.78885645   -656.78885645]
2025-06-20 18:38:25 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:38:25 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with a hybrid initialization combining uniform and Gaussian sampling for better exploration.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian sampling) 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.2 # Probability of Gaussian sampling in initialization


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        population_size = self.population_size
        dim = self.dim
        lower = self.lower_bounds
        upper = self.upper_bounds
        
        num_gaussian = int(population_size * self.gaussian_sampling_rate)
        num_uniform = population_size - num_gaussian

        gaussian_population = np.random.normal(loc=(lower + upper)/2, scale=(upper-lower)/4, size=(num_gaussian, dim))
        uniform_population = np.random.uniform(lower, upper, size=(num_uniform, dim))

        population = np.concatenate((gaussian_population, uniform_population), axis=0)
        return population


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:38:25 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:38:42 INFO Run function 24 complete. FEHistory len: 50000, AOCC: 0.0229
2025-06-20 18:38:42 INFO FeHistory: [174.10144705 209.81060802 194.14210206 ... -61.26323496 -62.46807905
 -62.426329  ]
2025-06-20 18:38:42 INFO Expected Optimum FE: -100
2025-06-20 18:38:42 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithHybridInitialization
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithHybridInitialization
# Description: Improves DE with a hybrid initialization combining uniform and Gaussian sampling for better exploration.
# Code:
class AdaptiveDifferentialEvolutionWithHybridInitialization:
    """
    Combines differential evolution with a hybrid initialization strategy (uniform and Gaussian sampling) 
    for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.2 # Probability of Gaussian sampling in initialization


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        population_size = self.population_size
        dim = self.dim
        lower = self.lower_bounds
        upper = self.upper_bounds
        
        num_gaussian = int(population_size * self.gaussian_sampling_rate)
        num_uniform = population_size - num_gaussian

        gaussian_population = np.random.normal(loc=(lower + upper)/2, scale=(upper-lower)/4, size=(num_gaussian, dim))
        uniform_population = np.random.uniform(lower, upper, size=(num_uniform, dim))

        population = np.concatenate((gaussian_population, uniform_population), axis=0)
        return population


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:38:42 INFO Unimodal AOCC mean: 0.1344
2025-06-20 18:38:42 INFO Multimodal (single component) AOCC mean: 0.3008
2025-06-20 18:38:42 INFO Multimodal (multiple components) AOCC mean: 0.0229
2025-06-20 18:38:42 INFO AOCC mean: 0.1897
2025-06-20 18:38:42 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveDifferentialEvolutionWithGaussianSampling
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithGaussianSampling
# Description: Combines differential evolution with adaptive Gaussian sampling for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithGaussianSampling:
    """
    Combines differential evolution with adaptive Gaussian sampling for efficient exploration of multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8  # Initial mutation scale
        self.mutation_scale_decay = 0.99  # Decay factor for mutation scale
        self.gaussian_sampling_rate = 0.1 #probability of performing gaussian sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                if np.random.rand() < self.gaussian_sampling_rate:
                    mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)

                # Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                # Crossover (Binomial Crossover)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                # Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay  # Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 1) vs (worst) `AdaptiveDifferentialEvolutionWithHybridInitialization` (Rank 5), we see that Rank 1 uses a more sophisticated initialization combining uniform and Gaussian sampling within the `_initialize_population` method, while Rank 5 only partially replaces the uniform population with Gaussian samples.  Rank 1 also maintains a consistent Gaussian sampling rate, whereas Rank 5 decays this rate, potentially hindering exploration later in the optimization process.  Rank 1s `_initialize_population` method also carefully handles the relative sizes of the uniform and gaussian components.

(second best) `AdaptiveDifferentialEvolutionWithGaussianSampling` (Rank 2) vs (second worst) `AdaptiveHybridDEWithImprovedInitialization` (Rank 4): Rank 2 uses simpler, purely uniform random initialization, while Rank 4 uses a hybrid approach.  The simpler approach (Rank 2) surprisingly performs better. The increased gaussian sampling rate in Rank 4 may be detrimental.

Comparing (1st) vs (2nd), we see that the only difference is the initialization strategy. Rank 1's hybrid initialization strategy slightly outperforms Rank 2's purely uniform random initialization, but the difference is small.

(3rd) `AdaptiveHybridDEWithImprovedInitialization` (Rank 3) vs (4th) `AdaptiveHybridDEWithImprovedInitialization` (Rank 4): The key difference lies in the `gaussian_sampling_rate` and the standard deviation used in Gaussian sampling within the `_initialize_population` method.  Rank 3 uses a lower rate (0.2) and implicitly a higher standard deviation (based on range/4), while Rank 4 has a higher rate (0.3) and a lower standard deviation (range/6).  The lower rate and higher standard deviation of Rank 3 leads to better performance.

Comparing (second worst) vs (worst), we see that both use hybrid initialization; however, Rank 4 manages the gaussian component more carefully.  Rank 5's decaying gaussian sampling rate likely reduces exploration in later generations.


Overall: The best performing algorithms utilize a hybrid initialization strategy incorporating both uniform and Gaussian sampling, but the proportion and standard deviation of the Gaussian component are critical.  Decaying the Gaussian sampling rate appears to negatively impact performance.
- Careful consideration of initialization strategy is crucial for high-dimensional problems.  A hybrid approach combining uniform and Gaussian sampling, with parameters tuned to balance exploration and exploitation, proves highly effective.  Avoid decaying the exploration rate too aggressively.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:38:42 INFO Generation 10, best so far: 0.21369870938149668
2025-06-20 18:38:42 INFO --- Performing Long-Term Reflection at Generation 10 ---
2025-06-20 18:38:45 INFO Full response text: **Analysis:**

Comparing (best) vs (worst), we see that the best-performing algorithm (`AdaptiveDifferentialEvolutionWithGaussianSampling` - Rank 1) uses a hybrid initialization strategy combining uniform and Gaussian sampling, while the worst (`AdaptiveDifferentialEvolutionWithHybridInitialization` - Rank 5) only uses a hybrid initialization but with less sophisticated parameter tuning (e.g., Gaussian sampling rate decay).  The best algorithm also shows better control over the mutation scale decay and Gaussian sampling rate. (second best) vs (second worst): The second-best algorithm (Rank 2) is almost identical to the best, differing only in the slightly less sophisticated initialization method.  The second worst (Rank 4) employs a similar structure but with a higher Gaussian sampling rate, leading to less effective exploitation. Comparing (1st) vs (2nd), we see that the primary difference lies in the initialization method; Rank 1 incorporates a percentage of Gaussian samples, leading to better diversity initially. (3rd) vs (4th): Both use hybrid initialization, but Rank 3 tunes the Gaussian sampling rate more effectively. Comparing (second worst) vs (worst), we see that both use hybrid initialization, but Rank 4 has a better-tuned Gaussian sampling rate and doesn't decay it, resulting in slightly better performance. Overall: The top-performing algorithms prioritize a balanced hybrid population initialization and finely-tuned adaptive mutation parameters (Gaussian sampling rate and mutation scale decay). Less successful algorithms show less precise tuning and potentially less diverse initial populations.

**Experience:**

Designing effective heuristics requires careful consideration of both exploration and exploitation, balanced through population initialization and adaptive parameter adjustments.  Fine-tuning parameters such as Gaussian sampling rate and mutation scale decay is crucial for optimal performance, and hybrid initialization strategies can improve diversity.

2025-06-20 18:38:45 INFO Generating offspring via Crossover...
2025-06-20 18:38:54 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
