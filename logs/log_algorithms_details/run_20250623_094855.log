2025-06-23 09:48:56 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 09:49:01 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1830
2025-06-23 09:49:01 INFO FeHistory: [-701.32057953 -701.29815051 -701.27866704 ... -701.84184036 -701.84739778
 -701.80340378]
2025-06-23 09:49:01 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 09:49:01 INFO Good algorithm:
Algorithm Name: AdaptiveLevyFlightOptimizer
import numpy as np
from scipy.spatial.distance import cdist

# Name: AdaptiveLevyFlightOptimizer
# Description:  A multimodal optimizer using adaptive Levy flights and topological archive management.

class AdaptiveLevyFlightOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.step_size = 0.1 * (self.upper_bounds - self.lower_bounds)
        self.step_size_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._levy_flight(population, self.step_size)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(population)
            self.eval_count += len(population)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.step_size *= self.step_size_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _levy_flight(self, population, step_size):
        # Levy flight using Mantegna's algorithm
        u = np.random.randn(self.population_size, self.dim)
        v = np.random.randn(self.population_size, self.dim)
        step = (u / np.abs(v)**(1/2)) * step_size
        offspring = population + step
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        return next_gen

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        distances = cdist(combined[:, :-1], combined[:, :-1], 'euclidean')
        np.fill_diagonal(distances, np.inf)
        
        new_archive = []
        while len(new_archive) < self.archive_size and len(combined)>0:
            best_index = np.argmin(combined[:, -1])
            new_archive.append(combined[best_index])
            combined = np.delete(combined, best_index, 0)


        return np.array(new_archive)
2025-06-23 09:49:01 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 09:49:06 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1140
2025-06-23 09:49:06 INFO FeHistory: [-222.58795674 -221.04306648 -221.28530771 ... -227.89552776 -227.69703515
 -226.49941711]
2025-06-23 09:49:06 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 09:49:06 INFO Good algorithm:
Algorithm Name: AdaptiveLevyFlightOptimizer
import numpy as np
from scipy.spatial.distance import cdist

# Name: AdaptiveLevyFlightOptimizer
# Description:  A multimodal optimizer using adaptive Levy flights and topological archive management.

class AdaptiveLevyFlightOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.step_size = 0.1 * (self.upper_bounds - self.lower_bounds)
        self.step_size_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._levy_flight(population, self.step_size)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(population)
            self.eval_count += len(population)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.step_size *= self.step_size_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _levy_flight(self, population, step_size):
        # Levy flight using Mantegna's algorithm
        u = np.random.randn(self.population_size, self.dim)
        v = np.random.randn(self.population_size, self.dim)
        step = (u / np.abs(v)**(1/2)) * step_size
        offspring = population + step
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        return next_gen

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        distances = cdist(combined[:, :-1], combined[:, :-1], 'euclidean')
        np.fill_diagonal(distances, np.inf)
        
        new_archive = []
        while len(new_archive) < self.archive_size and len(combined)>0:
            best_index = np.argmin(combined[:, -1])
            new_archive.append(combined[best_index])
            combined = np.delete(combined, best_index, 0)


        return np.array(new_archive)
2025-06-23 09:49:06 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 09:49:22 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0072
2025-06-23 09:49:22 INFO FeHistory: [181.16749418 209.92924002 196.6327871  ... -36.8257143  -37.1746195
 -37.36468267]
2025-06-23 09:49:22 INFO Expected Optimum FE: -100
2025-06-23 09:49:22 INFO Unimodal AOCC mean: 0.1830
2025-06-23 09:49:22 INFO Multimodal (single component) AOCC mean: 0.1140
2025-06-23 09:49:22 INFO Multimodal (multiple components) AOCC mean: 0.0072
2025-06-23 09:49:22 INFO AOCC mean: 0.1014
2025-06-23 09:50:43 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 09:50:50 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1751
2025-06-23 09:50:50 INFO FeHistory: [-701.31068093 -701.2888864  -701.28465758 ... -701.35729609 -701.35729609
 -701.35729609]
2025-06-23 09:50:50 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 09:50:50 INFO Good algorithm:
Algorithm Name: LatinHypercubeDEwithAdaptiveArchive
import numpy as np
from scipy.stats import qmc

# Name: LatinHypercubeDEwithAdaptiveArchive
# Description: Combines Latin Hypercube sampling, Differential Evolution, and an adaptive archive for robust multimodal optimization.
# Code:
class LatinHypercubeDEwithAdaptiveArchive:
    """
    Combines Latin Hypercube sampling, Differential Evolution, and an adaptive archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.scale_factor = 1.0  # Adaptive scaling factor
        self.scale_decay = 0.95
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            initial_sample = self.sampler.random(n=1)
            self.best_solution_overall = self._scale_sample(initial_sample)[0]
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling(self.population_size)
        fitness = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness)


        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            combined_pop = np.vstack((offspring, population))
            combined_fit = np.concatenate((offspring_fitness, fitness))
            sorted_indices = np.argsort(combined_fit)
            population = combined_pop[sorted_indices[:self.population_size]]
            fitness = combined_fit[sorted_indices[:self.population_size]]

            self._update_best(population, fitness)
            self.scale_factor *= self.scale_decay
            self.F *= self.scale_factor  # Adaptive F scaling
            self.CR *= self.scale_factor  # Adaptive CR Scaling
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness, offspring_fitness)))

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _latin_hypercube_sampling(self, num_samples):
        sample = self.sampler.random(n=num_samples)
        return self._scale_sample(sample)

    def _scale_sample(self, sample):
        return sample * (self.upper_bounds - self.lower_bounds) + self.lower_bounds

    def _differential_evolution(self, population, fitness):
        new_population = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
            trial_fitness = objective_function(trial.reshape(1, -1))
            if trial_fitness[0] < fitness[i]:
                new_population[i] = trial
            else:
                new_population[i] = population[i]
        return new_population

    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Prioritize adding solutions near the best found solution
        distances_to_best = np.linalg.norm(population - self.best_solution_overall, axis=1)
        combined = combined[np.argsort(distances_to_best)]
        
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

def objective_function(x):  #Example objective function, replace with your GNBG functions
    return np.sum(x**2, axis=1)

2025-06-23 09:50:50 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 09:50:57 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1022
2025-06-23 09:50:57 INFO FeHistory: [-221.7380145  -220.11479016 -221.31760662 ... -224.78444075 -224.78444075
 -224.78444075]
2025-06-23 09:50:57 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 09:50:57 INFO Good algorithm:
Algorithm Name: LatinHypercubeDEwithAdaptiveArchive
import numpy as np
from scipy.stats import qmc

# Name: LatinHypercubeDEwithAdaptiveArchive
# Description: Combines Latin Hypercube sampling, Differential Evolution, and an adaptive archive for robust multimodal optimization.
# Code:
class LatinHypercubeDEwithAdaptiveArchive:
    """
    Combines Latin Hypercube sampling, Differential Evolution, and an adaptive archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.scale_factor = 1.0  # Adaptive scaling factor
        self.scale_decay = 0.95
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            initial_sample = self.sampler.random(n=1)
            self.best_solution_overall = self._scale_sample(initial_sample)[0]
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling(self.population_size)
        fitness = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness)


        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            combined_pop = np.vstack((offspring, population))
            combined_fit = np.concatenate((offspring_fitness, fitness))
            sorted_indices = np.argsort(combined_fit)
            population = combined_pop[sorted_indices[:self.population_size]]
            fitness = combined_fit[sorted_indices[:self.population_size]]

            self._update_best(population, fitness)
            self.scale_factor *= self.scale_decay
            self.F *= self.scale_factor  # Adaptive F scaling
            self.CR *= self.scale_factor  # Adaptive CR Scaling
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness, offspring_fitness)))

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _latin_hypercube_sampling(self, num_samples):
        sample = self.sampler.random(n=num_samples)
        return self._scale_sample(sample)

    def _scale_sample(self, sample):
        return sample * (self.upper_bounds - self.lower_bounds) + self.lower_bounds

    def _differential_evolution(self, population, fitness):
        new_population = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
            trial_fitness = objective_function(trial.reshape(1, -1))
            if trial_fitness[0] < fitness[i]:
                new_population[i] = trial
            else:
                new_population[i] = population[i]
        return new_population

    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Prioritize adding solutions near the best found solution
        distances_to_best = np.linalg.norm(population - self.best_solution_overall, axis=1)
        combined = combined[np.argsort(distances_to_best)]
        
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

def objective_function(x):  #Example objective function, replace with your GNBG functions
    return np.sum(x**2, axis=1)

2025-06-23 09:50:57 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 09:51:14 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 09:51:14 INFO FeHistory: [179.07087951 207.42443345 175.46578495 ... 131.41462409 131.41462409
 131.41462409]
2025-06-23 09:51:14 INFO Expected Optimum FE: -100
2025-06-23 09:51:14 INFO Unimodal AOCC mean: 0.1751
2025-06-23 09:51:14 INFO Multimodal (single component) AOCC mean: 0.1022
2025-06-23 09:51:14 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 09:51:14 INFO AOCC mean: 0.0924
