2025-06-23 14:02:33 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:02:33 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:02:33 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:02:33 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:02:33 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:02:38 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1795
2025-06-23 14:02:38 INFO FeHistory: [-701.3372145  -701.32894619 -701.31717544 ... -701.55560434 -701.52475818
 -701.50916029]
2025-06-23 14:02:38 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:02:38 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8 # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate

        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        #Initial Evaluation
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        best_index = np.argmin(fitness_values)
        self.best_solution_overall = self.population[best_index].copy()
        self.best_fitness_overall = fitness_values[best_index]


        while self.eval_count < self.budget:
            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)

                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)  #Keep within bounds

                trial = np.zeros_like(self.population[i])
                for j in range(self.dim):
                    if random.random() < self.CR:
                        trial[j] = mutant[j]
                    else:
                        trial[j] = self.population[i][j]

                offspring[i] = trial

            # Evaluation of offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < fitness_values[i]:
                    self.population[i] = offspring[i]
                    fitness_values[i] = offspring_fitness[i]

                    if fitness_values[i] < self.best_fitness_overall:
                        self.best_solution_overall = self.population[i].copy()
                        self.best_fitness_overall = fitness_values[i]

            #Adaptive Local Search (Optional, uncomment for enhanced performance)
            # if self.eval_count % (self.population_size * 5) == 0: #Perform Local Search every 5 generations
            #    self.local_search(objective_function)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    # def local_search(self, objective_function): #Optional Local Search (Hill Climbing)
    #     current_solution = self.best_solution_overall.copy()
    #     current_fitness = self.best_fitness_overall
    #     step_size = 0.1

    #     while self.eval_count < self.budget: #Check budget within the local search
    #         improved = False
    #         for i in range(self.dim):
    #             temp_solution = current_solution.copy()
    #             temp_solution[i] += step_size
    #             temp_solution = np.clip(temp_solution, self.lower_bounds, self.upper_bounds)
    #             temp_fitness = objective_function(temp_solution.reshape(1,-1))[0] #Evaluate single solution
    #             self.eval_count += 1
    #             if temp_fitness < current_fitness:
    #                 current_solution = temp_solution
    #                 current_fitness = temp_fitness
    #                 improved = True
    #             temp_solution[i] -= 2 * step_size
    #             temp_solution = np.clip(temp_solution, self.lower_bounds, self.upper_bounds)
    #             temp_fitness = objective_function(temp_solution.reshape(1,-1))[0]
    #             self.eval_count += 1
    #             if temp_fitness < current_fitness:
    #                 current_solution = temp_solution
    #                 current_fitness = temp_fitness
    #                 improved = True
    #         if not improved:
    #             break
    #         if current_fitness < self.best_fitness_overall:
    #             self.best_solution_overall = current_solution.copy()
    #             self.best_fitness_overall = current_fitness

2025-06-23 14:02:38 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:02:38 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1806
2025-06-23 14:02:38 INFO FeHistory: [-701.28198964 -701.32111412 -701.34846022 ... -701.53717593 -701.57436963
 -701.61160569]
2025-06-23 14:02:38 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:02:38 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.local_search_iterations = 5 #Number of iterations for local search


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)


        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness = objective_function(new_population)
            self.eval_count += self.population_size

            self.population = np.concatenate((self.population, new_population))
            fitness = np.concatenate((fitness, new_fitness))
            
            self.population, fitness = self._selection(self.population, fitness)
            
            #Adaptive Local Search
            best_idx = np.argmin(fitness)
            best_solution = self.population[best_idx]
            
            improved = True
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(best_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))
                self.eval_count +=1
                if neighbor_fitness[0] < fitness[best_idx]:
                    best_solution = neighbor
                    fitness[best_idx] = neighbor_fitness[0]
                    improved = True
                else:
                    improved = False
            if improved:
                self.population[best_idx] = best_solution

            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        new_population = np.zeros_like(self.population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            v = self.population[a] + self.F * (self.population[b] - self.population[c])
            
            for j in range(self.dim):
                if random.random() < self.CR:
                    new_population[i, j] = v[j]
                else:
                    new_population[i, j] = self.population[i, j]
            
            new_population[i] = np.clip(new_population[i], self.lower_bounds, self.upper_bounds)
        return new_population
    
    def _selection(self, population, fitness):
        combined = np.column_stack((population, fitness))
        sorted_pop = combined[combined[:, -1].argsort()]
        return sorted_pop[:, :-1][:self.population_size], sorted_pop[:, -1][:self.population_size]
        

    def _find_best(self, population, fitness):
        best_idx = np.argmin(fitness)
        best_solution = population[best_idx]
        best_fitness = fitness[best_idx]
        return best_solution, best_fitness
    
    def _generate_neighbor(self, solution):
        neighbor = solution + np.random.normal(0, 0.1, self.dim) # Adjust the standard deviation as needed
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor
2025-06-23 14:02:38 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:02:39 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1793
2025-06-23 14:02:39 INFO FeHistory: [-701.3057558  -701.30153169 -701.30801946 ... -701.49718141 -701.51143125
 -701.52771844]
2025-06-23 14:02:39 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:02:39 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 #  Adjust as needed for problem difficulty
        self.F = 0.8 # Differential Evolution scaling factor
        self.CR = 0.9 # Crossover rate

        self.mutation_rate = 0.1 # Initial mutation rate, adaptive
        self.local_search_radius = 0.5 * (self.upper_bounds[0] - self.lower_bounds[0]) # Initialize local search radius


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = float('inf')
        
        # Initialize population
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        # Main optimization loop
        while self.eval_count < self.budget:
            # Differential Evolution
            for i in range(self.population_size):
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.copy(population[i])
                for j in range(self.dim):
                    if random.random() < self.CR:
                        trial[j] = mutant[j]

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    population[i] = trial
                    fitness_values[i] = trial_fitness

            # Adaptive Mutation and Local Search
            best_index = np.argmin(fitness_values)
            best_solution = population[best_index]
            best_fitness = fitness_values[best_index]


            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            # Adaptive components
            if self.eval_count % (self.population_size * 5) == 0:  # Adaptive every 5 generations
                if best_fitness > acceptance_threshold: # Increase exploration if far from optimum
                    self.mutation_rate = min(self.mutation_rate * 1.1, 0.5)
                    self.local_search_radius *= 1.1 #Wider exploration
                else:
                     self.mutation_rate *= 0.9 #Reduce exploration when approaching optimum
                     self.local_search_radius *=0.9  # Narrower exploration

            # Local Search (optional, to enhance exploitation)
            local_search_solution = self.local_search(best_solution, objective_function, self.local_search_radius)
            if local_search_solution[1] < self.best_fitness_overall:
                self.best_solution_overall = local_search_solution[0]
                self.best_fitness_overall = local_search_solution[1]


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def local_search(self, solution, objective_function, radius):
        best_local_solution = solution
        best_local_fitness = objective_function(solution.reshape(1, -1))[0]
        self.eval_count +=1
        for i in range(10): # number of local search iterations
            perturbation = np.random.uniform(-radius, radius, self.dim)
            neighbor = np.clip(solution + perturbation, self.lower_bounds, self.upper_bounds)
            neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
            self.eval_count += 1
            if neighbor_fitness < best_local_fitness:
                best_local_solution = neighbor
                best_local_fitness = neighbor_fitness
                
        return best_local_solution, best_local_fitness

2025-06-23 14:02:39 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:02:39 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1787
2025-06-23 14:02:39 INFO FeHistory: [-701.2905109  -701.32765508 -701.33520793 ... -701.45115306 -701.42383077
 -701.41660288]
2025-06-23 14:02:39 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:02:39 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.F = 0.8 #Differential Evolution scaling factor
        self.CR = 0.9 #Differential Evolution crossover rate
        self.mutation_rate = 0.1 #Initial mutation rate
        self.adaptive_factor = 0.95 #factor to adjust mutation rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                #Differential Evolution
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)

                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                trial = np.clip(np.random.rand(self.dim) < self.CR, 0, 1) * mutant + (1 - np.clip(np.random.rand(self.dim) < self.CR, 0, 1)) * self.population[i]
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds)

                #Local Search (Optional - add a simple local search here if needed)
                #Example: Simple hill climbing.  Comment this section out if you do not want it.
                # for j in range(self.dim):
                #     temp = trial.copy()
                #     temp[j] += 0.1
                #     temp_fitness = objective_function(temp.reshape(1,-1))[0]
                #     self.eval_count +=1
                #     if temp_fitness < objective_function(trial.reshape(1,-1))[0]:
                #         trial = temp

                trial_fitness = objective_function(trial.reshape(1,-1))[0]
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values[i] = trial_fitness
                    if trial_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = trial_fitness
                        self.best_solution_overall = trial
                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)

            #Adaptive Mutation Rate based on convergence speed
            if self.best_fitness_overall < acceptance_threshold:
                self.mutation_rate *= self.adaptive_factor

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate_final' : self.mutation_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-23 14:02:39 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:02:43 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1064
2025-06-23 14:02:43 INFO FeHistory: [-222.60311541 -222.36484348 -222.80498795 ... -222.22448634 -221.52231072
 -222.21675424]
2025-06-23 14:02:43 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:02:43 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8 # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate

        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        #Initial Evaluation
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        best_index = np.argmin(fitness_values)
        self.best_solution_overall = self.population[best_index].copy()
        self.best_fitness_overall = fitness_values[best_index]


        while self.eval_count < self.budget:
            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)

                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)  #Keep within bounds

                trial = np.zeros_like(self.population[i])
                for j in range(self.dim):
                    if random.random() < self.CR:
                        trial[j] = mutant[j]
                    else:
                        trial[j] = self.population[i][j]

                offspring[i] = trial

            # Evaluation of offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < fitness_values[i]:
                    self.population[i] = offspring[i]
                    fitness_values[i] = offspring_fitness[i]

                    if fitness_values[i] < self.best_fitness_overall:
                        self.best_solution_overall = self.population[i].copy()
                        self.best_fitness_overall = fitness_values[i]

            #Adaptive Local Search (Optional, uncomment for enhanced performance)
            # if self.eval_count % (self.population_size * 5) == 0: #Perform Local Search every 5 generations
            #    self.local_search(objective_function)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    # def local_search(self, objective_function): #Optional Local Search (Hill Climbing)
    #     current_solution = self.best_solution_overall.copy()
    #     current_fitness = self.best_fitness_overall
    #     step_size = 0.1

    #     while self.eval_count < self.budget: #Check budget within the local search
    #         improved = False
    #         for i in range(self.dim):
    #             temp_solution = current_solution.copy()
    #             temp_solution[i] += step_size
    #             temp_solution = np.clip(temp_solution, self.lower_bounds, self.upper_bounds)
    #             temp_fitness = objective_function(temp_solution.reshape(1,-1))[0] #Evaluate single solution
    #             self.eval_count += 1
    #             if temp_fitness < current_fitness:
    #                 current_solution = temp_solution
    #                 current_fitness = temp_fitness
    #                 improved = True
    #             temp_solution[i] -= 2 * step_size
    #             temp_solution = np.clip(temp_solution, self.lower_bounds, self.upper_bounds)
    #             temp_fitness = objective_function(temp_solution.reshape(1,-1))[0]
    #             self.eval_count += 1
    #             if temp_fitness < current_fitness:
    #                 current_solution = temp_solution
    #                 current_fitness = temp_fitness
    #                 improved = True
    #         if not improved:
    #             break
    #         if current_fitness < self.best_fitness_overall:
    #             self.best_solution_overall = current_solution.copy()
    #             self.best_fitness_overall = current_fitness

2025-06-23 14:02:43 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:02:43 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1094
2025-06-23 14:02:43 INFO FeHistory: [-222.15155501 -221.8286324  -221.47416018 ... -222.02192242 -222.51597996
 -223.27617651]
2025-06-23 14:02:43 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:02:43 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.local_search_iterations = 5 #Number of iterations for local search


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)


        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness = objective_function(new_population)
            self.eval_count += self.population_size

            self.population = np.concatenate((self.population, new_population))
            fitness = np.concatenate((fitness, new_fitness))
            
            self.population, fitness = self._selection(self.population, fitness)
            
            #Adaptive Local Search
            best_idx = np.argmin(fitness)
            best_solution = self.population[best_idx]
            
            improved = True
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(best_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))
                self.eval_count +=1
                if neighbor_fitness[0] < fitness[best_idx]:
                    best_solution = neighbor
                    fitness[best_idx] = neighbor_fitness[0]
                    improved = True
                else:
                    improved = False
            if improved:
                self.population[best_idx] = best_solution

            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        new_population = np.zeros_like(self.population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            v = self.population[a] + self.F * (self.population[b] - self.population[c])
            
            for j in range(self.dim):
                if random.random() < self.CR:
                    new_population[i, j] = v[j]
                else:
                    new_population[i, j] = self.population[i, j]
            
            new_population[i] = np.clip(new_population[i], self.lower_bounds, self.upper_bounds)
        return new_population
    
    def _selection(self, population, fitness):
        combined = np.column_stack((population, fitness))
        sorted_pop = combined[combined[:, -1].argsort()]
        return sorted_pop[:, :-1][:self.population_size], sorted_pop[:, -1][:self.population_size]
        

    def _find_best(self, population, fitness):
        best_idx = np.argmin(fitness)
        best_solution = population[best_idx]
        best_fitness = fitness[best_idx]
        return best_solution, best_fitness
    
    def _generate_neighbor(self, solution):
        neighbor = solution + np.random.normal(0, 0.1, self.dim) # Adjust the standard deviation as needed
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor
2025-06-23 14:02:43 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:02:43 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1804
2025-06-23 14:02:43 INFO FeHistory: [-701.28331796 -701.31736542 -701.29106024 ... -701.54114591 -701.53750541
 -701.58524887]
2025-06-23 14:02:43 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:02:43 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Differential Evolution crossover rate
        self.niche_radius = 0.5  # Adjust based on problem scale
        self.population = self.initialize_population()


    def initialize_population(self):
        return np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))

    def differential_evolution(self, population):
        new_population = np.zeros_like(population)
        for i in range(self.population_size):
            # Select three random individuals (excluding the current one)
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            # Mutation
            mutant = population[a] + self.F * (population[b] - population[c])

            # Boundary handling (clipping)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Crossover
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])

            new_population[i] = trial
        return new_population

    def niching(self, population, fitness_values):
        # Simple niching: remove solutions too close to existing ones
        unique_population = [population[0]]
        unique_fitness = [fitness_values[0]]
        for i in range(1, len(population)):
            too_close = False
            for j in range(len(unique_population)):
                distance = np.linalg.norm(population[i] - unique_population[j])
                if distance < self.niche_radius:
                    too_close = True
                    break
            if not too_close:
                unique_population.append(population[i])
                unique_fitness.append(fitness_values[i])
        return np.array(unique_population), np.array(unique_fitness)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population = self.initialize_population()

        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        best_index = np.argmin(fitness_values)
        self.best_solution_overall = self.population[best_index]
        self.best_fitness_overall = fitness_values[best_index]

        while self.eval_count < self.budget:
            new_population = self.differential_evolution(self.population)
            new_fitness_values = objective_function(new_population)
            self.eval_count += self.population_size

            combined_population = np.concatenate((self.population, new_population))
            combined_fitness = np.concatenate((fitness_values, new_fitness_values))
            
            #Sort to find best solutions (before niching)
            sorted_indices = np.argsort(combined_fitness)
            combined_population = combined_population[sorted_indices]
            combined_fitness = combined_fitness[sorted_indices]

            # Niching
            self.population, fitness_values = self.niching(combined_population[:self.population_size], combined_fitness[:self.population_size])

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness_values[best_index]
                self.best_solution_overall = self.population[best_index]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-23 14:02:43 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:02:44 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1071
2025-06-23 14:02:44 INFO FeHistory: [-222.84933799 -222.67710159 -221.51748349 ... -221.098618   -223.39273346
 -222.01352168]
2025-06-23 14:02:44 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:02:44 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 #  Adjust as needed for problem difficulty
        self.F = 0.8 # Differential Evolution scaling factor
        self.CR = 0.9 # Crossover rate

        self.mutation_rate = 0.1 # Initial mutation rate, adaptive
        self.local_search_radius = 0.5 * (self.upper_bounds[0] - self.lower_bounds[0]) # Initialize local search radius


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = float('inf')
        
        # Initialize population
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        # Main optimization loop
        while self.eval_count < self.budget:
            # Differential Evolution
            for i in range(self.population_size):
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.copy(population[i])
                for j in range(self.dim):
                    if random.random() < self.CR:
                        trial[j] = mutant[j]

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    population[i] = trial
                    fitness_values[i] = trial_fitness

            # Adaptive Mutation and Local Search
            best_index = np.argmin(fitness_values)
            best_solution = population[best_index]
            best_fitness = fitness_values[best_index]


            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            # Adaptive components
            if self.eval_count % (self.population_size * 5) == 0:  # Adaptive every 5 generations
                if best_fitness > acceptance_threshold: # Increase exploration if far from optimum
                    self.mutation_rate = min(self.mutation_rate * 1.1, 0.5)
                    self.local_search_radius *= 1.1 #Wider exploration
                else:
                     self.mutation_rate *= 0.9 #Reduce exploration when approaching optimum
                     self.local_search_radius *=0.9  # Narrower exploration

            # Local Search (optional, to enhance exploitation)
            local_search_solution = self.local_search(best_solution, objective_function, self.local_search_radius)
            if local_search_solution[1] < self.best_fitness_overall:
                self.best_solution_overall = local_search_solution[0]
                self.best_fitness_overall = local_search_solution[1]


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def local_search(self, solution, objective_function, radius):
        best_local_solution = solution
        best_local_fitness = objective_function(solution.reshape(1, -1))[0]
        self.eval_count +=1
        for i in range(10): # number of local search iterations
            perturbation = np.random.uniform(-radius, radius, self.dim)
            neighbor = np.clip(solution + perturbation, self.lower_bounds, self.upper_bounds)
            neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
            self.eval_count += 1
            if neighbor_fitness < best_local_fitness:
                best_local_solution = neighbor
                best_local_fitness = neighbor_fitness
                
        return best_local_solution, best_local_fitness

2025-06-23 14:02:44 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:02:45 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1101
2025-06-23 14:02:45 INFO FeHistory: [-221.47931805 -221.42229844 -221.97703204 ... -222.24911724 -222.84791912
 -223.00977498]
2025-06-23 14:02:45 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:02:45 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.F = 0.8 #Differential Evolution scaling factor
        self.CR = 0.9 #Differential Evolution crossover rate
        self.mutation_rate = 0.1 #Initial mutation rate
        self.adaptive_factor = 0.95 #factor to adjust mutation rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                #Differential Evolution
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)

                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                trial = np.clip(np.random.rand(self.dim) < self.CR, 0, 1) * mutant + (1 - np.clip(np.random.rand(self.dim) < self.CR, 0, 1)) * self.population[i]
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds)

                #Local Search (Optional - add a simple local search here if needed)
                #Example: Simple hill climbing.  Comment this section out if you do not want it.
                # for j in range(self.dim):
                #     temp = trial.copy()
                #     temp[j] += 0.1
                #     temp_fitness = objective_function(temp.reshape(1,-1))[0]
                #     self.eval_count +=1
                #     if temp_fitness < objective_function(trial.reshape(1,-1))[0]:
                #         trial = temp

                trial_fitness = objective_function(trial.reshape(1,-1))[0]
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values[i] = trial_fitness
                    if trial_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = trial_fitness
                        self.best_solution_overall = trial
                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)

            #Adaptive Mutation Rate based on convergence speed
            if self.best_fitness_overall < acceptance_threshold:
                self.mutation_rate *= self.adaptive_factor

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate_final' : self.mutation_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-23 14:02:45 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:02:54 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1077
2025-06-23 14:02:54 INFO FeHistory: [-221.84327778 -221.90808933 -220.88598172 ... -221.67787957 -221.11155413
 -222.41611956]
2025-06-23 14:02:54 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:02:54 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Differential Evolution crossover rate
        self.niche_radius = 0.5  # Adjust based on problem scale
        self.population = self.initialize_population()


    def initialize_population(self):
        return np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))

    def differential_evolution(self, population):
        new_population = np.zeros_like(population)
        for i in range(self.population_size):
            # Select three random individuals (excluding the current one)
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            # Mutation
            mutant = population[a] + self.F * (population[b] - population[c])

            # Boundary handling (clipping)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Crossover
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])

            new_population[i] = trial
        return new_population

    def niching(self, population, fitness_values):
        # Simple niching: remove solutions too close to existing ones
        unique_population = [population[0]]
        unique_fitness = [fitness_values[0]]
        for i in range(1, len(population)):
            too_close = False
            for j in range(len(unique_population)):
                distance = np.linalg.norm(population[i] - unique_population[j])
                if distance < self.niche_radius:
                    too_close = True
                    break
            if not too_close:
                unique_population.append(population[i])
                unique_fitness.append(fitness_values[i])
        return np.array(unique_population), np.array(unique_fitness)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population = self.initialize_population()

        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        best_index = np.argmin(fitness_values)
        self.best_solution_overall = self.population[best_index]
        self.best_fitness_overall = fitness_values[best_index]

        while self.eval_count < self.budget:
            new_population = self.differential_evolution(self.population)
            new_fitness_values = objective_function(new_population)
            self.eval_count += self.population_size

            combined_population = np.concatenate((self.population, new_population))
            combined_fitness = np.concatenate((fitness_values, new_fitness_values))
            
            #Sort to find best solutions (before niching)
            sorted_indices = np.argsort(combined_fitness)
            combined_population = combined_population[sorted_indices]
            combined_fitness = combined_fitness[sorted_indices]

            # Niching
            self.population, fitness_values = self.niching(combined_population[:self.population_size], combined_fitness[:self.population_size])

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness_values[best_index]
                self.best_solution_overall = self.population[best_index]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-23 14:02:54 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:03:01 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:03:01 INFO FeHistory: [188.24735218 153.6581945  219.48468801 ...  82.28956889  91.66549622
  82.60148228]
2025-06-23 14:03:01 INFO Expected Optimum FE: -100
2025-06-23 14:03:01 INFO Unimodal AOCC mean: 0.1795
2025-06-23 14:03:01 INFO Multimodal (single component) AOCC mean: 0.1064
2025-06-23 14:03:01 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:03:01 INFO AOCC mean: 0.0953
2025-06-23 14:03:01 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:03:01 ERROR Can not run the algorithm
2025-06-23 14:03:01 INFO Run function 2 complete. FEHistory len: 300, AOCC: 0.1753
2025-06-23 14:03:01 INFO FeHistory: [-701.30576946 -701.28756824 -701.34554914 -701.31374942 -701.3200939
 -701.28559159 -701.29124091 -701.30109822 -701.29346677 -701.32117679
 -701.29293796 -701.30076628 -701.30220989 -701.33589452 -701.3041279
 -701.29144341 -701.32902315 -701.30870079 -701.29947465 -701.33278956
 -701.28902809 -701.32663592 -701.31800617 -701.3233585  -701.32463489
 -701.31573907 -701.30698607 -701.2893998  -701.33765944 -701.30865036
 -701.34597491 -701.29812709 -701.34250438 -701.31966437 -701.3093574
 -701.28183743 -701.31936579 -701.29345414 -701.31167563 -701.28937959
 -701.29420038 -701.31821102 -701.29867369 -701.3276257  -701.28775097
 -701.29827872 -701.34207399 -701.31100184 -701.313667   -701.27570796
 -701.34609959 -701.3220674  -701.30620716 -701.31627383 -701.32278873
 -701.31217476 -701.27303529 -701.34611574 -701.2914994  -701.31859618
 -701.34297596 -701.30982384 -701.30861437 -701.29904182 -701.29832415
 -701.29678978 -701.32406921 -701.32238636 -701.3178551  -701.31787769
 -701.29178053 -701.29710807 -701.30567735 -701.30999008 -701.32385244
 -701.31019851 -701.3662184  -701.3072307  -701.29753223 -701.29765547
 -701.29336739 -701.29649438 -701.31778838 -701.3124428  -701.28774017
 -701.32333696 -701.31056407 -701.30788723 -701.32197327 -701.3093397
 -701.2754477  -701.28042748 -701.33313015 -701.29168896 -701.29725879
 -701.2885754  -701.31477081 -701.29648291 -701.33529506 -701.31992382
 -701.2977718  -701.28994957 -701.2785048  -701.32031828 -701.32129666
 -701.28851197 -701.25205204 -701.27004813 -701.33278327 -701.27814506
 -701.29042861 -701.3162564  -701.26396399 -701.30143633 -701.29477789
 -701.28406792 -701.33427685 -701.30302644 -701.27504985 -701.27510005
 -701.28443442 -701.30515189 -701.29039972 -701.26692725 -701.27602261
 -701.27238282 -701.29480581 -701.28078966 -701.31355004 -701.28288878
 -701.27698977 -701.28235147 -701.25514953 -701.28345619 -701.30555455
 -701.29283369 -701.28664945 -701.30601886 -701.29943446 -701.29519356
 -701.24565641 -701.30353113 -701.25421504 -701.28191076 -701.30951882
 -701.278803   -701.30200944 -701.28371975 -701.28912045 -701.27103458
 -701.3080235  -701.26841988 -701.27160797 -701.28142065 -701.26633309
 -701.28812487 -701.29471438 -701.29660709 -701.32986061 -701.29016938
 -701.25347154 -701.30668187 -701.30543791 -701.29252907 -701.27986851
 -701.34715858 -701.2807045  -701.2997964  -701.29611864 -701.25763604
 -701.30199764 -701.30384536 -701.2808663  -701.28015106 -701.26623022
 -701.30517544 -701.28973597 -701.30170477 -701.29508398 -701.26713848
 -701.27102947 -701.27455502 -701.27365833 -701.26999629 -701.29212696
 -701.2816102  -701.31921879 -701.29747191 -701.30753273 -701.26484987
 -701.29262829 -701.27256304 -701.29242791 -701.27278844 -701.30864893
 -701.29988288 -701.27282914 -701.28182317 -701.2973279  -701.29810235
 -701.29043367 -701.29896337 -701.29525266 -701.29073076 -701.2715315
 -701.30213401 -701.30032558 -701.30444556 -701.30713945 -701.28398913
 -701.30753585 -701.31550046 -701.30329852 -701.29990307 -701.34158268
 -701.31728059 -701.26630752 -701.27666184 -701.29911208 -701.32947926
 -701.32043676 -701.30101371 -701.32632578 -701.2890373  -701.30148948
 -701.27502362 -701.27943667 -701.30078342 -701.2848487  -701.30381605
 -701.29855826 -701.30719309 -701.29820662 -701.26571469 -701.27484124
 -701.28884817 -701.31492137 -701.33038281 -701.28707463 -701.28154147
 -701.30542419 -701.27974507 -701.26096083 -701.2951543  -701.2760769
 -701.29439846 -701.30617302 -701.29394523 -701.2622764  -701.32494239
 -701.305064   -701.29908064 -701.3085422  -701.31597587 -701.26533473
 -701.29611817 -701.30982452 -701.26993365 -701.30172122 -701.30348652
 -701.2745491  -701.29429256 -701.30446917 -701.26728756 -701.2700029
 -701.28309162 -701.28541959 -701.30705195 -701.30798139 -701.28864494
 -701.3054785  -701.27674986 -701.26851491 -701.28911218 -701.28171288
 -701.28349487 -701.28961174 -701.32085726 -701.29632705 -701.31084507
 -701.30084919 -701.29671125 -701.28470831 -701.32497682 -701.29194413
 -701.28628354 -701.31724213 -701.30249083 -701.26693829 -701.28579965
 -701.26697198 -701.29886359 -701.30039045 -701.26738868 -701.30344261
 -701.2829987  -701.31502846 -701.27657364 -701.32452247 -701.28129429]
2025-06-23 14:03:01 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:03:01 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate
        

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness_values = objective_function(new_population)
            self.eval_count += self.population_size
            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, new_population, new_fitness_values)
            fitness_values = np.concatenate((fitness_values, new_fitness_values))
            self._adapt_mutation_rate() # Adjust mutation rate based on performance
            
        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        new_population = np.zeros_like(self.population)
        for i in range(self.population_size):
            # Randomly select three different individuals
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            # Mutation
            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])

            # Crossover
            j_rand = random.randint(0, self.dim - 1)
            for j in range(self.dim):
                if random.random() < self.CR or j == j_rand:
                    new_population[i, j] = mutant[j]
                else:
                    new_population[i, j] = self.population[i, j]

            # Boundary handling
            new_population[i] = np.clip(new_population[i], self.lower_bounds, self.upper_bounds)

        return new_population

    def _selection(self, old_pop, old_fit, new_pop, new_fit):
        combined_pop = np.concatenate((old_pop, new_pop))
        combined_fit = np.concatenate((old_fit, new_fit))
        indices = np.argsort(combined_fit)
        selected_pop = combined_pop[indices[:self.population_size]]
        selected_fit = combined_fit[indices[:self.population_size]]
        best_solution, best_fitness = self._update_best(selected_pop, selected_fit)
        return selected_pop, best_solution, best_fitness


    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness

    def _adapt_mutation_rate(self):
        # Simple adaptive strategy: Reduce mutation rate if consistently no improvement
        if self.best_fitness_overall == float('inf'):
            self.mutation_rate = min(self.mutation_rate * 1.1, 1)
        elif self.best_fitness_overall > self.best_fitness_overall:
            self.mutation_rate = max(self.mutation_rate * 0.9, 0.01) # prevents mutation rate from dropping too low

2025-06-23 14:03:01 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:03:01 ERROR Can not run the algorithm
2025-06-23 14:03:01 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:03:01 INFO FeHistory: [147.39862744 196.96859989 200.86720722 ...  60.58132758  73.33469397
  74.91750332]
2025-06-23 14:03:01 INFO Expected Optimum FE: -100
2025-06-23 14:03:01 INFO Unimodal AOCC mean: 0.1806
2025-06-23 14:03:01 INFO Multimodal (single component) AOCC mean: 0.1094
2025-06-23 14:03:01 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:03:01 INFO AOCC mean: 0.0967
2025-06-23 14:03:01 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:03:01 INFO Run function 15 complete. FEHistory len: 300, AOCC: 0.1015
2025-06-23 14:03:01 INFO FeHistory: [-224.02170353 -222.13832919 -220.98199894 -222.67749574 -222.6085434
 -220.8757231  -220.39598312 -220.91913904 -222.68017079 -220.31094788
 -220.79464235 -223.07284658 -221.63509411 -222.89967227 -221.07455801
 -222.15123543 -222.56968331 -222.42176537 -220.25301894 -222.96655645
 -220.75653294 -222.405772   -222.22297624 -222.15496244 -222.40995691
 -222.47923218 -221.95980965 -220.62768446 -223.41763487 -220.84622963
 -222.06198787 -221.75550253 -221.73922449 -221.95034287 -221.66739761
 -220.60540076 -220.46842374 -222.05852714 -221.61481114 -220.70933979
 -221.79528401 -221.79247678 -223.22348785 -221.23602864 -221.6030986
 -221.83578635 -220.8409061  -221.26010696 -221.59341845 -221.32468091
 -220.92727865 -222.03044369 -223.24333442 -222.15201249 -221.00351852
 -221.36808467 -223.47693014 -221.17582796 -222.14154384 -222.40319749
 -222.22511973 -222.82576496 -220.82425707 -222.31368198 -222.90632825
 -220.81100158 -220.61680223 -222.21991862 -222.61124758 -221.43231667
 -221.31610407 -221.27034107 -222.98304304 -221.71892705 -221.27444288
 -222.32714364 -221.09482908 -221.43406496 -220.77416731 -220.35900703
 -222.01232677 -222.14945437 -221.66520895 -224.61730183 -223.95090941
 -222.74805384 -221.62950198 -220.54244494 -222.1698691  -222.51798377
 -222.17067629 -222.21845077 -222.77214086 -222.76234988 -221.83740954
 -222.17424901 -222.02720602 -221.84357736 -222.59659024 -221.79043036
 -222.08196985 -220.77864673 -223.13674582 -221.55150907 -222.83569248
 -220.17399329 -221.75557735 -221.90389809 -222.56031    -220.57637845
 -221.68629901 -222.89911545 -224.04533294 -221.80819323 -221.62562902
 -220.71773976 -220.74128094 -221.80757503 -220.81307645 -222.01534264
 -221.64766288 -220.30042101 -221.24593959 -220.91615384 -221.92425257
 -221.43458009 -221.26912901 -221.7299421  -220.69535472 -221.63161027
 -221.46986816 -222.0277881  -221.1898845  -222.04728799 -220.67136915
 -222.55171039 -221.05824734 -221.90356201 -221.74246539 -221.80620687
 -220.74234021 -220.45398739 -221.62530957 -221.96111203 -220.85413093
 -222.41802337 -222.07264724 -222.37427286 -222.19298436 -221.65272835
 -220.48852384 -221.78940291 -222.06751763 -219.92775898 -220.62968062
 -221.25101132 -221.57686933 -222.12460527 -223.06012523 -221.16126563
 -220.80144132 -222.66990183 -220.12997803 -220.40970635 -221.10963631
 -222.84063474 -221.15091155 -222.0263236  -221.46232196 -223.44592541
 -221.34597165 -221.75962103 -223.16201825 -224.12165458 -221.40571335
 -221.07792896 -220.84726099 -222.87215056 -221.98083847 -222.54223855
 -220.52540838 -222.0565318  -221.24387451 -221.40977827 -221.72270101
 -221.38178304 -222.068739   -223.41356104 -222.81916488 -221.21873148
 -222.88855663 -220.46166015 -221.7926437  -221.92603448 -221.35132951
 -221.88950896 -222.21270701 -221.74428848 -221.13630728 -222.38869407
 -221.5615416  -221.81792609 -223.09306555 -221.56226888 -220.69680856
 -221.30973162 -222.24758777 -220.66808757 -221.78846965 -221.72763674
 -223.00010689 -221.45137349 -223.07060483 -221.62408899 -221.75545274
 -222.60285847 -222.40452132 -222.05489738 -221.51266822 -219.96995273
 -222.51887654 -221.70457891 -221.25020815 -221.53039686 -221.44931277
 -221.54459586 -221.64914255 -221.98180623 -223.72027747 -221.47211363
 -221.30271893 -221.65311089 -222.01225082 -221.92749937 -222.09189019
 -220.71482631 -221.18835294 -222.21719996 -222.47478175 -220.98015888
 -223.59287106 -222.35815305 -220.88764357 -221.80376858 -220.83618333
 -220.53666522 -221.74937672 -222.22574338 -220.64512731 -221.24948641
 -222.00234435 -220.76042922 -221.66611067 -222.67499316 -220.96465494
 -221.34055129 -222.9938661  -221.34393817 -221.70818471 -222.2678095
 -220.51573711 -223.38246364 -220.45255379 -221.93503955 -221.50200708
 -222.02769235 -221.11554908 -220.00332185 -221.00461288 -222.20520453
 -222.82197674 -222.70619435 -222.16051953 -222.13604784 -221.25053053
 -222.48029138 -221.11459402 -220.46644266 -221.66884732 -222.91142316
 -221.31671163 -222.54916035 -220.80990187 -220.54325815 -220.99590972
 -222.14338221 -221.70880645 -220.66081787 -223.2763743  -221.95813924
 -221.92895929 -221.62143973 -222.90684784 -221.34603015 -223.1055949
 -222.00086084 -222.72121499 -221.5690131  -220.26344976 -222.12828927]
2025-06-23 14:03:01 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:03:01 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate
        

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness_values = objective_function(new_population)
            self.eval_count += self.population_size
            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, new_population, new_fitness_values)
            fitness_values = np.concatenate((fitness_values, new_fitness_values))
            self._adapt_mutation_rate() # Adjust mutation rate based on performance
            
        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        new_population = np.zeros_like(self.population)
        for i in range(self.population_size):
            # Randomly select three different individuals
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            # Mutation
            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])

            # Crossover
            j_rand = random.randint(0, self.dim - 1)
            for j in range(self.dim):
                if random.random() < self.CR or j == j_rand:
                    new_population[i, j] = mutant[j]
                else:
                    new_population[i, j] = self.population[i, j]

            # Boundary handling
            new_population[i] = np.clip(new_population[i], self.lower_bounds, self.upper_bounds)

        return new_population

    def _selection(self, old_pop, old_fit, new_pop, new_fit):
        combined_pop = np.concatenate((old_pop, new_pop))
        combined_fit = np.concatenate((old_fit, new_fit))
        indices = np.argsort(combined_fit)
        selected_pop = combined_pop[indices[:self.population_size]]
        selected_fit = combined_fit[indices[:self.population_size]]
        best_solution, best_fitness = self._update_best(selected_pop, selected_fit)
        return selected_pop, best_solution, best_fitness


    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness

    def _adapt_mutation_rate(self):
        # Simple adaptive strategy: Reduce mutation rate if consistently no improvement
        if self.best_fitness_overall == float('inf'):
            self.mutation_rate = min(self.mutation_rate * 1.1, 1)
        elif self.best_fitness_overall > self.best_fitness_overall:
            self.mutation_rate = max(self.mutation_rate * 0.9, 0.01) # prevents mutation rate from dropping too low

2025-06-23 14:03:01 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:03:01 ERROR Can not run the algorithm
2025-06-23 14:03:01 INFO Run function 24 complete. FEHistory len: 300, AOCC: 0.0000
2025-06-23 14:03:01 INFO FeHistory: [203.82281052 217.86747479 171.61326642 184.55861254 217.07243977
 200.78140785 162.58795157 210.16871591 174.72318724 154.46222439
 178.01891962 219.24992231 165.90091052 196.13406777 209.24469914
 210.02625146 189.98818329 163.85290022 144.39751036 152.22946818
 140.23823666 228.2512293  202.18385916 190.10171969 193.80373116
 211.48245117 179.16032538 199.65187078 161.80285883 163.76652
 208.10039897 200.38843655 158.08119437 162.26721889 197.58957339
 176.0403699  207.27875225 187.42781548 169.40443935 134.78701551
 155.51916169 199.59517988 206.51038826 183.81446192 187.89299004
 201.82941618 170.6443314  208.41799602 171.97249335 186.33781274
 219.97461928 179.22719306 200.24919795 195.53686677 151.09823129
 143.24571262 160.3618243  207.86832032 175.77479488 191.29789605
 179.90784629 192.12551018 188.75621044 198.92103278 160.64092076
 197.85759236 172.26075466 198.1439499  189.13660325 196.50636787
 160.3172421  228.50401303 171.88018346 199.23811237 186.40606294
 213.99424488 212.23920487 191.4364069  218.46371918 184.92160248
 187.44012251 189.03497416 192.25356483 199.51980636 193.90133384
 180.90355157 201.92727635 205.05386015 226.11206489 180.75442874
 170.27644409 213.8515696  187.22482842 176.2873537  198.35605795
 188.6612623  196.92639846 207.85775234 203.01753444 224.4111773
 205.73868576 175.69383356 206.35625141 171.02566386 182.01708188
 200.57919249 213.42264745 207.36207916 220.17844905 155.99853453
 191.68994551 192.76559504 206.79637503 192.99246336 175.83467336
 213.07075836 198.53669418 212.5694329  201.41433212 210.35561855
 199.56720542 179.37130724 213.98723756 215.4703116  169.67016819
 234.16150356 207.71117896 200.10615886 171.1774037  218.96727532
 191.31665412 188.50536411 223.42816713 211.34966571 211.23114861
 215.66368463 222.41640799 193.17212513 194.52000895 202.89597414
 224.53430685 179.13535958 194.08072211 178.47095789 190.87714041
 172.0601564  221.0259075  225.66188735 217.44638237 179.88273538
 203.19788308 211.72751478 204.28396507 232.44845143 198.44509355
 221.29761928 186.7422786  181.08792814 232.23304521 211.84585399
 184.74333473 228.15599121 227.31485454 255.00642426 153.81756694
 207.99556863 215.61550108 221.5246644  212.89448394 217.8075223
 190.28120949 198.07796139 222.58574085 209.29327146 223.71582085
 207.4713477  211.61293342 202.8672181  225.55385256 174.4504117
 218.04617735 179.94727727 231.45363084 209.43995579 153.91127525
 190.90747313 195.4959976  203.89097737 210.87421053 208.96928508
 224.09552523 205.55803023 208.0330065  150.07887669 232.75302844
 186.97561364 221.86280571 156.78789018 202.34585356 237.22238717
 194.33654249 181.36786122 233.25819685 248.73314483 204.12809944
 211.74610278 179.39503691 150.31970808 242.12887712 200.5359683
 185.22147183 201.73304382 173.07569771 180.28549383 174.33850175
 215.69686703 139.45697268 207.61981362 192.20330627 230.034252
 206.33426904 209.10558013 221.14266929 196.85146459 222.3248194
 184.93244    184.08338977 209.42197091 224.708623   211.10685527
 193.64024038 150.14323808 225.07655208 189.19035324 195.15571353
 174.54388818 233.12748993 215.51483823 181.71619395 195.87688549
 150.77288299 218.884167   216.40717035 204.4463985  193.15255239
 200.31584606 220.85405237 217.67363772 172.20438845 173.76554309
 254.58804789 229.44479668 195.66152691 222.35702303 209.68877665
 207.97947528 199.86348525 175.12341247 158.9800388  221.65006393
 182.52264158 227.57154415 197.15068389 214.94826003 161.50537893
 145.8901875  217.69304612 223.94424311 183.42184309 220.9707251
 195.63753851 218.32331245 188.9976404  205.50276081 186.29458899
 201.12667942 237.95842289 208.76405583 182.01902654 196.78703357
 222.5558464  238.18946427 180.7392908  211.74519231 242.75764637
 210.01597389 176.24284423 183.93450173 213.25166252 164.19137389
 209.80388723 185.99193396 194.26902504 200.29894777 200.16999948
 189.85453525 198.44252103 202.09939956 197.27119372 200.93333152]
2025-06-23 14:03:01 INFO Expected Optimum FE: -100
2025-06-23 14:03:01 INFO Unimodal AOCC mean: 0.1753
2025-06-23 14:03:01 INFO Multimodal (single component) AOCC mean: 0.1015
2025-06-23 14:03:01 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:03:01 INFO AOCC mean: 0.0923
2025-06-23 14:03:01 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:03:02 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:03:02 INFO FeHistory: [185.00338325 193.23028356 184.30468471 ...  77.10488802  77.94630676
 101.45158788]
2025-06-23 14:03:02 INFO Expected Optimum FE: -100
2025-06-23 14:03:02 INFO Unimodal AOCC mean: 0.1793
2025-06-23 14:03:02 INFO Multimodal (single component) AOCC mean: 0.1071
2025-06-23 14:03:02 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:03:02 INFO AOCC mean: 0.0955
2025-06-23 14:03:02 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:03:02 ERROR Can not run the algorithm
2025-06-23 14:03:03 INFO Run function 2 complete. FEHistory len: 100, AOCC: 0.1756
2025-06-23 14:03:03 INFO FeHistory: [-701.30137945 -701.30488187 -701.31187583 -701.32052945 -701.30952703
 -701.28721411 -701.29464236 -701.29865129 -701.28319262 -701.31199904
 -701.29748659 -701.31738484 -701.32093618 -701.2963632  -701.30979828
 -701.31702807 -701.27860689 -701.32010248 -701.36379005 -701.33690965
 -701.26244185 -701.29510594 -701.32156239 -701.31908035 -701.30564509
 -701.33160705 -701.29837499 -701.28180129 -701.30059262 -701.29718716
 -701.29974666 -701.29023238 -701.321571   -701.31183203 -701.3151507
 -701.30634817 -701.30010721 -701.29895815 -701.32786203 -701.33517944
 -701.30648589 -701.29267717 -701.34498285 -701.29531343 -701.27449179
 -701.28400714 -701.32352791 -701.30964398 -701.30747423 -701.30287779
 -701.34446013 -701.32702026 -701.29081398 -701.2968531  -701.32108116
 -701.30397246 -701.31068186 -701.29209881 -701.30635834 -701.30337332
 -701.26565087 -701.34585604 -701.29131495 -701.35436705 -701.29734903
 -701.29483968 -701.31778743 -701.37720968 -701.37305143 -701.27490478
 -701.28820023 -701.32165784 -701.27233175 -701.26541014 -701.28290395
 -701.28618834 -701.28472002 -701.31155312 -701.31363254 -701.28909886
 -701.3118039  -701.30518626 -701.34340009 -701.28904077 -701.33506823
 -701.30819429 -701.32651069 -701.31171432 -701.32136461 -701.34745253
 -701.29430704 -701.31043971 -701.31253126 -701.31141919 -701.32269193
 -701.31480683 -701.31890073 -701.31737908 -701.27675857 -701.30827612]
2025-06-23 14:03:03 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:03:03 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 # Adjust as needed
        self.population = None
        self.F = 0.8 # Differential Evolution scaling factor
        self.CR = 0.9 # Differential Evolution crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness_values = objective_function(new_population)
            self.eval_count += self.population_size
            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, new_population, new_fitness_values)
            fitness_values = np.concatenate((fitness_values, new_fitness_values))
            
            # Adaptive Mutation: Increase mutation rate if stuck
            if self.eval_count > self.budget * 0.2 and self.best_fitness_overall > acceptance_threshold and self._is_stagnant(fitness_values):
                self.mutation_rate *= 1.1
                
        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'final_mutation_rate': self.mutation_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        offspring = np.copy(self.population)
        for i in range(self.population_size):
            r1, r2, r3 = random.sample(range(self.population_size), 3)
            while r1 == i or r2 == i or r3 == i:
                r1, r2, r3 = random.sample(range(self.population_size), 3)
            mutant = self.population[r1] + self.F * (self.population[r2] - self.population[r3])
            
            #Clamp Values
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            
            trial = np.copy(self.population[i])
            crossover_points = np.random.rand(self.dim) < self.CR
            trial[crossover_points] = mutant[crossover_points]
            
            #Add Mutation
            mutation_points = np.random.rand(self.dim) < self.mutation_rate
            trial[mutation_points] = np.random.uniform(self.lower_bounds, self.upper_bounds, np.sum(mutation_points))
            
            offspring[i] = trial
        return offspring

    def _selection(self, population, fitness_values, offspring, offspring_fitness_values):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness_values))
        indices = np.argsort(combined_fitness)
        selected_population = combined_population[indices[:self.population_size]]
        selected_fitness = combined_fitness[indices[:self.population_size]]
        best_solution, best_fitness = self._update_best(selected_population, selected_fitness)
        return selected_population, best_solution, best_fitness

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness
    
    def _is_stagnant(self, fitness_values):
        # Check if the best fitness has not improved for a certain number of iterations.
        # Adjust the parameters (e.g., num_iterations) as needed
        num_iterations = 10
        if len(fitness_values) < num_iterations * self.population_size:
             return False
        return np.min(fitness_values[-num_iterations*self.population_size:]) == np.min(fitness_values)

2025-06-23 14:03:03 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:03:03 ERROR Can not run the algorithm
2025-06-23 14:03:03 INFO Run function 15 complete. FEHistory len: 100, AOCC: 0.1028
2025-06-23 14:03:03 INFO FeHistory: [-221.7317636  -221.17113435 -223.23399752 -223.11497079 -222.3635513
 -221.45431409 -222.6519949  -222.4682589  -222.2493695  -221.98079963
 -221.46038722 -220.86724836 -221.28944242 -221.52832519 -221.29239732
 -221.67664101 -221.62128084 -223.70894098 -221.91329606 -221.54208641
 -221.46043008 -220.45071003 -221.92719972 -221.52396847 -221.16635644
 -223.11787934 -221.90617626 -221.08923979 -221.55463313 -222.56473277
 -221.10455399 -222.85376591 -221.60031109 -223.8424215  -220.90034639
 -221.12601484 -222.50429645 -222.91693769 -221.37701006 -221.97371783
 -223.31992022 -222.2601215  -222.01728519 -222.05198508 -224.91432046
 -222.8128766  -221.98055586 -221.413472   -222.133867   -221.07829641
 -223.17367887 -221.11269929 -221.27450879 -221.71454466 -221.16011806
 -222.56142734 -221.24285533 -222.83907523 -221.69353175 -222.10820962
 -221.60123586 -221.4104673  -222.35183244 -223.32465819 -221.65524156
 -222.08343554 -222.3874065  -221.04770741 -222.87425477 -222.62375771
 -221.99959179 -222.13749193 -222.25012673 -221.10468853 -223.14414567
 -222.41444201 -221.85160509 -223.75794965 -221.23118912 -220.76398322
 -221.27664299 -222.34939079 -222.02348321 -224.01497142 -221.74171915
 -222.12445824 -222.75416553 -222.61724685 -222.07934417 -221.2038213
 -221.75967501 -222.26415172 -223.47503155 -221.94300285 -221.70615467
 -221.73653127 -221.99558558 -223.52401255 -221.86174083 -221.49222276]
2025-06-23 14:03:03 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:03:03 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 # Adjust as needed
        self.population = None
        self.F = 0.8 # Differential Evolution scaling factor
        self.CR = 0.9 # Differential Evolution crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness_values = objective_function(new_population)
            self.eval_count += self.population_size
            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, new_population, new_fitness_values)
            fitness_values = np.concatenate((fitness_values, new_fitness_values))
            
            # Adaptive Mutation: Increase mutation rate if stuck
            if self.eval_count > self.budget * 0.2 and self.best_fitness_overall > acceptance_threshold and self._is_stagnant(fitness_values):
                self.mutation_rate *= 1.1
                
        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'final_mutation_rate': self.mutation_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        offspring = np.copy(self.population)
        for i in range(self.population_size):
            r1, r2, r3 = random.sample(range(self.population_size), 3)
            while r1 == i or r2 == i or r3 == i:
                r1, r2, r3 = random.sample(range(self.population_size), 3)
            mutant = self.population[r1] + self.F * (self.population[r2] - self.population[r3])
            
            #Clamp Values
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            
            trial = np.copy(self.population[i])
            crossover_points = np.random.rand(self.dim) < self.CR
            trial[crossover_points] = mutant[crossover_points]
            
            #Add Mutation
            mutation_points = np.random.rand(self.dim) < self.mutation_rate
            trial[mutation_points] = np.random.uniform(self.lower_bounds, self.upper_bounds, np.sum(mutation_points))
            
            offspring[i] = trial
        return offspring

    def _selection(self, population, fitness_values, offspring, offspring_fitness_values):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness_values))
        indices = np.argsort(combined_fitness)
        selected_population = combined_population[indices[:self.population_size]]
        selected_fitness = combined_fitness[indices[:self.population_size]]
        best_solution, best_fitness = self._update_best(selected_population, selected_fitness)
        return selected_population, best_solution, best_fitness

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness
    
    def _is_stagnant(self, fitness_values):
        # Check if the best fitness has not improved for a certain number of iterations.
        # Adjust the parameters (e.g., num_iterations) as needed
        num_iterations = 10
        if len(fitness_values) < num_iterations * self.population_size:
             return False
        return np.min(fitness_values[-num_iterations*self.population_size:]) == np.min(fitness_values)

2025-06-23 14:03:03 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:03:03 ERROR Can not run the algorithm
2025-06-23 14:03:03 INFO Run function 24 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-23 14:03:03 INFO FeHistory: [189.96321254 212.0286238  198.71757156 181.91163154 209.64807631
 196.17193791 162.27441756 210.47069859 190.19013809 197.32769009
 186.49371764 182.47080949 187.75539984 157.71615738 172.92936045
 189.6250074  192.30091835 201.88044484 201.9801062  172.93993597
 190.71695709 158.31176047 196.62049031 199.56857031 191.10194456
 171.31746441 194.58395346 220.33157705 171.88120225 208.4166017
 167.15705956 200.35714904 206.99157431 213.38016126 164.52930457
 177.85320361 221.32919023 161.9458919  196.51591814 189.70527189
 155.98126435 173.35288516 212.04515343 174.74046195 215.21165835
 178.68330299 212.29381417 212.52814221 212.98325753 206.73932234
 173.504928   203.44525933 176.22879257 218.6792902  172.71516998
 191.05388933 173.06949852 176.22393289 202.39845943 206.90546893
 204.01613926 192.26865227 179.24589995 211.1329879  199.24886663
 178.38667338 155.6577404  169.95724717 181.51207116 185.65449859
 179.64360926 175.17810506 194.4303874  183.97500547 213.27680121
 149.49208802 195.51329954 192.6054199  186.90517499 217.4549152
 183.4376485  125.20271648 257.5671848  223.39776986 174.73869297
 211.81088087 196.13719872 210.62250512 233.9888051  172.57330373
 186.74525432 180.22611994 142.65054596 221.54884211 184.73010414
 180.39671151 187.00682229 195.0045828  218.58459114 173.59589595]
2025-06-23 14:03:03 INFO Expected Optimum FE: -100
2025-06-23 14:03:03 INFO Unimodal AOCC mean: 0.1756
2025-06-23 14:03:03 INFO Multimodal (single component) AOCC mean: 0.1028
2025-06-23 14:03:03 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:03:03 INFO AOCC mean: 0.0928
2025-06-23 14:03:03 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:03:04 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:03:04 INFO FeHistory: [144.63681699 217.30779982 186.64684342 ... 105.77236809 128.89212224
 131.51999258]
2025-06-23 14:03:04 INFO Expected Optimum FE: -100
2025-06-23 14:03:04 INFO Unimodal AOCC mean: 0.1787
2025-06-23 14:03:04 INFO Multimodal (single component) AOCC mean: 0.1101
2025-06-23 14:03:04 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:03:04 INFO AOCC mean: 0.0963
2025-06-23 14:03:04 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:03:07 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1803
2025-06-23 14:03:07 INFO FeHistory: [-701.30281366 -701.31340356 -701.27708944 ... -701.55205349 -701.5351028
 -701.57710116]
2025-06-23 14:03:07 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:03:07 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.F = 0.8 # Differential evolution scaling factor
        self.CR = 0.9 # Differential evolution crossover rate
        self.niche_radius = 0.1 #Adjust this parameter to control the niching effect


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self.get_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = self.differential_evolution()
            new_fitness_values = objective_function(new_population)
            self.eval_count += len(new_fitness_values)
            self.population, self.fitness_values = self.selection(self.population, self.fitness_values, new_population, new_fitness_values)
            best_solution, best_fitness = self.get_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            
            #Adaptive Mutation: increase F and CR if stuck in local optima
            if self.eval_count > self.budget * 0.7 and self.best_fitness_overall > acceptance_threshold:
                self.F += 0.1 * (1 - self.F)
                self.CR += 0.1 * (1 - self.CR)
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def differential_evolution(self):
        offspring = np.zeros_like(self.population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
            trial = np.zeros_like(mutant)
            jrand = random.randint(0, self.dim -1)
            for j in range(self.dim):
                if random.random() < self.CR or j == jrand:
                    trial[j] = mutant[j]
                else:
                    trial[j] = self.population[i][j]
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring


    def selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_population = np.vstack((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fitness)
        return combined_population[sorted_indices[:self.population_size]], combined_fitness[sorted_indices[:self.population_size]]

    def get_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-23 14:03:07 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:03:07 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1785
2025-06-23 14:03:07 INFO FeHistory: [-701.30001615 -701.31556919 -701.31602013 ... -701.41616351 -701.40568259
 -701.43869844]
2025-06-23 14:03:07 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:03:07 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate
        self.population = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                # Differential Evolution
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                trial = np.clip(np.random.binomial(1,self.CR,size=self.dim)*mutant + (1-np.random.binomial(1,self.CR,size=self.dim))*self.population[i],self.lower_bounds,self.upper_bounds)

                #Adaptive Mutation - Increase mutation rate if stuck
                if self.best_fitness_overall == np.min(fitness_values) and self.eval_count>self.population_size*5:
                    self.mutation_rate = min(1.0, self.mutation_rate * 1.1)

                #Local Search - small perturbation if close to solution
                if abs(np.min(fitness_values) - self.best_fitness_overall)<0.01 and self.eval_count > self.population_size*5:
                    trial = trial + np.random.normal(0,0.1,size=self.dim)
                    trial = np.clip(trial,self.lower_bounds, self.upper_bounds)


                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values = np.concatenate((fitness_values[:i], trial_fitness, fitness_values[i+1:]))
                    if trial_fitness < self.best_fitness_overall:
                        self.best_solution_overall = trial
                        self.best_fitness_overall = trial_fitness
                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate':self.mutation_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-23 14:03:07 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:03:12 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1081
2025-06-23 14:03:12 INFO FeHistory: [-222.0053343  -221.8372189  -220.89190946 ... -223.59342754 -223.82291687
 -220.75087341]
2025-06-23 14:03:12 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:03:12 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.F = 0.8 # Differential evolution scaling factor
        self.CR = 0.9 # Differential evolution crossover rate
        self.niche_radius = 0.1 #Adjust this parameter to control the niching effect


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self.get_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = self.differential_evolution()
            new_fitness_values = objective_function(new_population)
            self.eval_count += len(new_fitness_values)
            self.population, self.fitness_values = self.selection(self.population, self.fitness_values, new_population, new_fitness_values)
            best_solution, best_fitness = self.get_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            
            #Adaptive Mutation: increase F and CR if stuck in local optima
            if self.eval_count > self.budget * 0.7 and self.best_fitness_overall > acceptance_threshold:
                self.F += 0.1 * (1 - self.F)
                self.CR += 0.1 * (1 - self.CR)
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def differential_evolution(self):
        offspring = np.zeros_like(self.population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
            trial = np.zeros_like(mutant)
            jrand = random.randint(0, self.dim -1)
            for j in range(self.dim):
                if random.random() < self.CR or j == jrand:
                    trial[j] = mutant[j]
                else:
                    trial[j] = self.population[i][j]
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring


    def selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_population = np.vstack((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fitness)
        return combined_population[sorted_indices[:self.population_size]], combined_fitness[sorted_indices[:self.population_size]]

    def get_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-23 14:03:12 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:03:14 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1098
2025-06-23 14:03:14 INFO FeHistory: [-221.91667803 -222.43743648 -222.56172847 ... -222.48420882 -221.29278778
 -224.30300539]
2025-06-23 14:03:14 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:03:14 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate
        self.population = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                # Differential Evolution
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                trial = np.clip(np.random.binomial(1,self.CR,size=self.dim)*mutant + (1-np.random.binomial(1,self.CR,size=self.dim))*self.population[i],self.lower_bounds,self.upper_bounds)

                #Adaptive Mutation - Increase mutation rate if stuck
                if self.best_fitness_overall == np.min(fitness_values) and self.eval_count>self.population_size*5:
                    self.mutation_rate = min(1.0, self.mutation_rate * 1.1)

                #Local Search - small perturbation if close to solution
                if abs(np.min(fitness_values) - self.best_fitness_overall)<0.01 and self.eval_count > self.population_size*5:
                    trial = trial + np.random.normal(0,0.1,size=self.dim)
                    trial = np.clip(trial,self.lower_bounds, self.upper_bounds)


                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values = np.concatenate((fitness_values[:i], trial_fitness, fitness_values[i+1:]))
                    if trial_fitness < self.best_fitness_overall:
                        self.best_solution_overall = trial
                        self.best_fitness_overall = trial_fitness
                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate':self.mutation_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-23 14:03:14 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:03:17 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:03:17 INFO FeHistory: [179.59411394 193.93676523 160.71665594 ...  61.90731776  78.61985924
 116.21186655]
2025-06-23 14:03:17 INFO Expected Optimum FE: -100
2025-06-23 14:03:17 INFO Unimodal AOCC mean: 0.1804
2025-06-23 14:03:17 INFO Multimodal (single component) AOCC mean: 0.1077
2025-06-23 14:03:17 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:03:17 INFO AOCC mean: 0.0960
2025-06-23 14:03:17 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:03:19 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1800
2025-06-23 14:03:19 INFO FeHistory: [-701.31172384 -701.35244914 -701.3313347  ... -701.56274573 -701.56274231
 -701.56276087]
2025-06-23 14:03:19 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:03:19 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_rate = 0.1  # Initial mutation rate
        self.niche_radius = 0.5 # Initial niche radius
        self.sigma = 0.5 #Initial standard deviation for Gaussian mutation

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall, self.best_fitness_overall = self.find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.population = np.concatenate((self.population, offspring))
            self.fitness_values = np.concatenate((self.fitness_values, offspring_fitness))

            self.population, self.fitness_values = self.selection(self.population, self.fitness_values)

            best_solution, best_fitness = self.find_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            self.adapt_parameters()


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate': self.mutation_rate,
            'niche_radius': self.niche_radius
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self):
        offspring = []
        for i in range(self.population_size // 2):
            parent1_index = random.randint(0, len(self.population) - 1)
            parent2_index = random.randint(0, len(self.population) - 1)
            while parent2_index == parent1_index: # Ensure different parents
                parent2_index = random.randint(0, len(self.population)-1)

            child1, child2 = self.crossover(self.population[parent1_index], self.population[parent2_index])
            child1 = self.mutate(child1)
            child2 = self.mutate(child2)
            offspring.extend([child1, child2])
        return np.array(offspring)

    def crossover(self, parent1, parent2):
        crossover_point = random.randint(1, self.dim -1)
        child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))
        child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))
        return child1, child2

    def mutate(self, individual):
        mutated_individual = individual + np.random.normal(0, self.sigma, size=self.dim)
        mutated_individual = np.clip(mutated_individual, self.lower_bounds, self.upper_bounds) #Keeps it within bounds
        return mutated_individual


    def selection(self, population, fitness_values):
      # Niche based selection.  Keep best individuals from each niche.
      sorted_indices = np.argsort(fitness_values)
      selected_population = []
      selected_fitness = []
      occupied_niches = set()

      for i in sorted_indices:
          individual = population[i]
          is_in_niche = False
          for j in occupied_niches:
              if np.linalg.norm(individual - population[j]) < self.niche_radius:
                  is_in_niche = True
                  break
          if not is_in_niche:
              selected_population.append(individual)
              selected_fitness.append(fitness_values[i])
              occupied_niches.add(len(selected_population) -1)
              if len(selected_population) == self.population_size:
                  break
      return np.array(selected_population), np.array(selected_fitness)

    def find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def adapt_parameters(self):
        #Simple adaptive mechanism: reduce mutation rate and niche radius over time
        self.mutation_rate *= 0.99
        self.sigma *= 0.99
        self.niche_radius *= 0.99


2025-06-23 14:03:19 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:03:19 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1753
2025-06-23 14:03:19 INFO FeHistory: [-701.30403315 -701.32157107 -701.31080519 ... -701.28282168 -701.27745527
 -701.27743084]
2025-06-23 14:03:19 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:03:19 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalEvolutionaryStrategy
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.5  # Initial mutation strength
        self.sigma_decay = 0.99 # Decay rate of mutation strength
        self.niche_radius = 20 # Radius for niching

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.population, fitness_values = self._selection(self.population, fitness_values, offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)
            self.sigma *= self.sigma_decay #Adaptive mutation

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'sigma': self.sigma
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = []
        for i in range(self.population_size):
            offspring.append(self.population[i] + np.random.normal(0, self.sigma, self.dim))
        return np.array(offspring)
    

    def _selection(self, parents, parent_fitness, offspring, offspring_fitness):
        combined_population = np.vstack((parents, offspring))
        combined_fitness = np.concatenate((parent_fitness, offspring_fitness))

        #Niching to handle multimodality
        selected_population = []
        selected_fitness = []
        while len(selected_population) < self.population_size:
            best_index = np.argmin(combined_fitness)
            best_solution = combined_population[best_index]
            is_unique = True
            for sol in selected_population:
                if np.linalg.norm(sol- best_solution) < self.niche_radius:
                    is_unique = False
                    break
            if is_unique:
                selected_population.append(best_solution)
                selected_fitness.append(combined_fitness[best_index])
            combined_population = np.delete(combined_population, best_index, axis=0)
            combined_fitness = np.delete(combined_fitness, best_index)


        return np.array(selected_population), np.array(selected_fitness)

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]

        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return self.best_solution_overall, self.best_fitness_overall

2025-06-23 14:03:19 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:03:30 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:03:30 INFO FeHistory: [177.17827997 124.11326091 154.71885904 ...  78.7819205   91.90802577
  63.74614494]
2025-06-23 14:03:30 INFO Expected Optimum FE: -100
2025-06-23 14:03:30 INFO Unimodal AOCC mean: 0.1803
2025-06-23 14:03:30 INFO Multimodal (single component) AOCC mean: 0.1081
2025-06-23 14:03:30 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:03:30 INFO AOCC mean: 0.0961
2025-06-23 14:03:33 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:03:33 INFO FeHistory: [178.25570353 130.87737454 216.23752427 ... 103.84133423 118.14643298
 108.64397587]
2025-06-23 14:03:33 INFO Expected Optimum FE: -100
2025-06-23 14:03:33 INFO Unimodal AOCC mean: 0.1785
2025-06-23 14:03:33 INFO Multimodal (single component) AOCC mean: 0.1098
2025-06-23 14:03:33 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:03:33 INFO AOCC mean: 0.0961
2025-06-23 14:03:33 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1199
2025-06-23 14:03:33 INFO FeHistory: [-221.21508595 -222.95306418 -222.74364317 ... -228.11106649 -228.11104997
 -228.11105991]
2025-06-23 14:03:33 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:03:33 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_rate = 0.1  # Initial mutation rate
        self.niche_radius = 0.5 # Initial niche radius
        self.sigma = 0.5 #Initial standard deviation for Gaussian mutation

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall, self.best_fitness_overall = self.find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.population = np.concatenate((self.population, offspring))
            self.fitness_values = np.concatenate((self.fitness_values, offspring_fitness))

            self.population, self.fitness_values = self.selection(self.population, self.fitness_values)

            best_solution, best_fitness = self.find_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            self.adapt_parameters()


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate': self.mutation_rate,
            'niche_radius': self.niche_radius
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self):
        offspring = []
        for i in range(self.population_size // 2):
            parent1_index = random.randint(0, len(self.population) - 1)
            parent2_index = random.randint(0, len(self.population) - 1)
            while parent2_index == parent1_index: # Ensure different parents
                parent2_index = random.randint(0, len(self.population)-1)

            child1, child2 = self.crossover(self.population[parent1_index], self.population[parent2_index])
            child1 = self.mutate(child1)
            child2 = self.mutate(child2)
            offspring.extend([child1, child2])
        return np.array(offspring)

    def crossover(self, parent1, parent2):
        crossover_point = random.randint(1, self.dim -1)
        child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))
        child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))
        return child1, child2

    def mutate(self, individual):
        mutated_individual = individual + np.random.normal(0, self.sigma, size=self.dim)
        mutated_individual = np.clip(mutated_individual, self.lower_bounds, self.upper_bounds) #Keeps it within bounds
        return mutated_individual


    def selection(self, population, fitness_values):
      # Niche based selection.  Keep best individuals from each niche.
      sorted_indices = np.argsort(fitness_values)
      selected_population = []
      selected_fitness = []
      occupied_niches = set()

      for i in sorted_indices:
          individual = population[i]
          is_in_niche = False
          for j in occupied_niches:
              if np.linalg.norm(individual - population[j]) < self.niche_radius:
                  is_in_niche = True
                  break
          if not is_in_niche:
              selected_population.append(individual)
              selected_fitness.append(fitness_values[i])
              occupied_niches.add(len(selected_population) -1)
              if len(selected_population) == self.population_size:
                  break
      return np.array(selected_population), np.array(selected_fitness)

    def find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def adapt_parameters(self):
        #Simple adaptive mechanism: reduce mutation rate and niche radius over time
        self.mutation_rate *= 0.99
        self.sigma *= 0.99
        self.niche_radius *= 0.99


2025-06-23 14:03:33 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:03:36 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1157
2025-06-23 14:03:36 INFO FeHistory: [-222.66647464 -221.85408056 -222.05115021 ... -222.63017544 -222.49922587
 -222.30142451]
2025-06-23 14:03:36 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:03:36 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalEvolutionaryStrategy
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.5  # Initial mutation strength
        self.sigma_decay = 0.99 # Decay rate of mutation strength
        self.niche_radius = 20 # Radius for niching

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.population, fitness_values = self._selection(self.population, fitness_values, offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)
            self.sigma *= self.sigma_decay #Adaptive mutation

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'sigma': self.sigma
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = []
        for i in range(self.population_size):
            offspring.append(self.population[i] + np.random.normal(0, self.sigma, self.dim))
        return np.array(offspring)
    

    def _selection(self, parents, parent_fitness, offspring, offspring_fitness):
        combined_population = np.vstack((parents, offspring))
        combined_fitness = np.concatenate((parent_fitness, offspring_fitness))

        #Niching to handle multimodality
        selected_population = []
        selected_fitness = []
        while len(selected_population) < self.population_size:
            best_index = np.argmin(combined_fitness)
            best_solution = combined_population[best_index]
            is_unique = True
            for sol in selected_population:
                if np.linalg.norm(sol- best_solution) < self.niche_radius:
                    is_unique = False
                    break
            if is_unique:
                selected_population.append(best_solution)
                selected_fitness.append(combined_fitness[best_index])
            combined_population = np.delete(combined_population, best_index, axis=0)
            combined_fitness = np.delete(combined_fitness, best_index)


        return np.array(selected_population), np.array(selected_fitness)

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]

        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return self.best_solution_overall, self.best_fitness_overall

2025-06-23 14:03:36 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:04:53 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:04:53 INFO FeHistory: [157.01841748 163.11873119 196.12825176 ...  28.42535994  28.42524129
  28.42532225]
2025-06-23 14:04:53 INFO Expected Optimum FE: -100
2025-06-23 14:04:53 INFO Unimodal AOCC mean: 0.1800
2025-06-23 14:04:53 INFO Multimodal (single component) AOCC mean: 0.1199
2025-06-23 14:04:53 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:04:53 INFO AOCC mean: 0.1000
2025-06-23 14:04:58 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:04:58 INFO FeHistory: [213.92697659 192.14040396 178.40881433 ... 168.89783002 171.76822155
 177.60760554]
2025-06-23 14:04:58 INFO Expected Optimum FE: -100
2025-06-23 14:04:58 INFO Unimodal AOCC mean: 0.1753
2025-06-23 14:04:58 INFO Multimodal (single component) AOCC mean: 0.1157
2025-06-23 14:04:58 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:04:58 INFO AOCC mean: 0.0970
2025-06-23 14:07:40 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1755
2025-06-23 14:07:40 INFO FeHistory: [-701.37314277 -701.31828144 -701.29460533 ... -701.29180927 -701.29179724
 -701.29180018]
2025-06-23 14:07:40 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:07:40 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.population_size = 100 # Adjust as needed
        self.F = 0.8 # Differential evolution scaling factor
        self.CR = 0.9 # Differential evolution crossover rate
        self.mutation_rate = 0.1 # Adaptive mutation rate
        self.local_search_iterations = 5 #Number of iterations for local search

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self.find_best(self.population, fitness_values)


        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                #Differential Evolution
                a, b, c = self.select_different(i)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                trial = self.crossover(self.population[i], mutant)
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds) #Bound constraints

                #Adaptive Mutation
                if random.random() < self.mutation_rate :
                    trial = self.mutate(trial)

                #Local Search
                trial = self.local_search(trial, objective_function)
                
                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                
                if trial_fitness[0] < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values[i] = trial_fitness[0]
                    if trial_fitness[0] < self.best_fitness_overall:
                         self.best_solution_overall, self.best_fitness_overall = trial, trial_fitness[0]

                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)
            self.adaptive_mutation_rate()

        if self.best_solution_overall is None and self.dim > 0 : # Fallback
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
            self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def select_different(self, i):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == i or b == i or c == i:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def crossover(self, x, v):
        u = np.zeros_like(x)
        for i in range(self.dim):
            if random.random() < self.CR:
                u[i] = v[i]
            else:
                u[i] = x[i]
        return u

    def mutate(self, x):
        index = random.randint(0, self.dim - 1)
        x[index] += np.random.normal(0, 0.5)  #Small gaussian mutation
        return x

    def local_search(self, x, objective_function):
        for _ in range(self.local_search_iterations):
            best_neighbor = x.copy()
            best_neighbor_fitness = objective_function(best_neighbor.reshape(1,-1))
            for i in range(self.dim):
              for j in range(2): # explore both directions
                  neighbor = x.copy()
                  neighbor[i] += (-1)**j * 0.1 # Adjust step size as needed
                  neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                  neighbor_fitness = objective_function(neighbor.reshape(1,-1))
                  if neighbor_fitness[0] < best_neighbor_fitness[0]:
                    best_neighbor = neighbor
                    best_neighbor_fitness = neighbor_fitness
            x = best_neighbor
            self.eval_count+=1
        return x


    def adaptive_mutation_rate(self):
        # Adjust mutation rate based on convergence - simple example
        if self.eval_count > 0.8 * self.budget:
            self.mutation_rate *= 0.5 # Reduce mutation closer to the end

    def find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-23 14:07:40 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:11:05 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1012
2025-06-23 14:11:05 INFO FeHistory: [-222.62480167 -221.82842787 -222.40176403 ... -220.80136006 -220.79816242
 -220.7997625 ]
2025-06-23 14:11:05 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:11:05 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.population_size = 100 # Adjust as needed
        self.F = 0.8 # Differential evolution scaling factor
        self.CR = 0.9 # Differential evolution crossover rate
        self.mutation_rate = 0.1 # Adaptive mutation rate
        self.local_search_iterations = 5 #Number of iterations for local search

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self.find_best(self.population, fitness_values)


        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                #Differential Evolution
                a, b, c = self.select_different(i)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                trial = self.crossover(self.population[i], mutant)
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds) #Bound constraints

                #Adaptive Mutation
                if random.random() < self.mutation_rate :
                    trial = self.mutate(trial)

                #Local Search
                trial = self.local_search(trial, objective_function)
                
                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                
                if trial_fitness[0] < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values[i] = trial_fitness[0]
                    if trial_fitness[0] < self.best_fitness_overall:
                         self.best_solution_overall, self.best_fitness_overall = trial, trial_fitness[0]

                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)
            self.adaptive_mutation_rate()

        if self.best_solution_overall is None and self.dim > 0 : # Fallback
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
            self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def select_different(self, i):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == i or b == i or c == i:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def crossover(self, x, v):
        u = np.zeros_like(x)
        for i in range(self.dim):
            if random.random() < self.CR:
                u[i] = v[i]
            else:
                u[i] = x[i]
        return u

    def mutate(self, x):
        index = random.randint(0, self.dim - 1)
        x[index] += np.random.normal(0, 0.5)  #Small gaussian mutation
        return x

    def local_search(self, x, objective_function):
        for _ in range(self.local_search_iterations):
            best_neighbor = x.copy()
            best_neighbor_fitness = objective_function(best_neighbor.reshape(1,-1))
            for i in range(self.dim):
              for j in range(2): # explore both directions
                  neighbor = x.copy()
                  neighbor[i] += (-1)**j * 0.1 # Adjust step size as needed
                  neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                  neighbor_fitness = objective_function(neighbor.reshape(1,-1))
                  if neighbor_fitness[0] < best_neighbor_fitness[0]:
                    best_neighbor = neighbor
                    best_neighbor_fitness = neighbor_fitness
            x = best_neighbor
            self.eval_count+=1
        return x


    def adaptive_mutation_rate(self):
        # Adjust mutation rate based on convergence - simple example
        if self.eval_count > 0.8 * self.budget:
            self.mutation_rate *= 0.5 # Reduce mutation closer to the end

    def find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-23 14:11:05 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:24:32 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:24:32 INFO FeHistory: [184.69212105 201.33153212 207.73512314 ... 175.0694273  175.71789584
 176.14197133]
2025-06-23 14:24:32 INFO Expected Optimum FE: -100
2025-06-23 14:24:32 INFO Unimodal AOCC mean: 0.1755
2025-06-23 14:24:32 INFO Multimodal (single component) AOCC mean: 0.1012
2025-06-23 14:24:32 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:24:32 INFO AOCC mean: 0.0922
2025-06-23 14:26:16 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:16 ERROR Can not run the algorithm
2025-06-23 14:26:16 INFO Run function 2 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-23 14:26:16 INFO FeHistory: []
2025-06-23 14:26:16 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:26:16 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:26:16 ERROR Can not run the algorithm
2025-06-23 14:26:16 INFO Run function 15 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-23 14:26:16 INFO FeHistory: []
2025-06-23 14:26:16 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:26:16 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:26:16 ERROR Can not run the algorithm
2025-06-23 14:26:16 INFO Run function 24 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-23 14:26:16 INFO FeHistory: []
2025-06-23 14:26:16 INFO Expected Optimum FE: -100
2025-06-23 14:26:16 INFO Unimodal AOCC mean: 0.0000
2025-06-23 14:26:16 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 14:26:16 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:26:16 INFO AOCC mean: 0.0000
2025-06-23 14:26:16 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:21 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1987
2025-06-23 14:26:21 INFO FeHistory: [-701.31158976 -701.31270831 -701.29977631 ... -702.49872038 -702.5110515
 -702.49468486]
2025-06-23 14:26:21 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:26:21 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveDE
import numpy as np
import random

# Name: ArchiveGuidedAdaptiveDE
# Description: Adaptive Differential Evolution guided by an archive to escape local optima in multimodal landscapes.
class ArchiveGuidedAdaptiveDE:
    """
    Combines Differential Evolution with an archive and adaptive scaling factor to handle multimodal optimization problems.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5  # Initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation (Elitism)
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available), promoting diversity
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity and better fitness in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

2025-06-23 14:26:21 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:26:27 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1108
2025-06-23 14:26:27 INFO FeHistory: [-221.14289322 -221.53840105 -222.1608683  ... -222.83678308 -223.41581075
 -223.07139766]
2025-06-23 14:26:27 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:26:27 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveDE
import numpy as np
import random

# Name: ArchiveGuidedAdaptiveDE
# Description: Adaptive Differential Evolution guided by an archive to escape local optima in multimodal landscapes.
class ArchiveGuidedAdaptiveDE:
    """
    Combines Differential Evolution with an archive and adaptive scaling factor to handle multimodal optimization problems.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5  # Initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation (Elitism)
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available), promoting diversity
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity and better fitness in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

2025-06-23 14:26:27 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:26:45 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0523
2025-06-23 14:26:45 INFO FeHistory: [162.37758462 191.35054986 167.47882104 ... -88.71440765 -88.65290169
 -88.62838694]
2025-06-23 14:26:45 INFO Expected Optimum FE: -100
2025-06-23 14:26:45 INFO Unimodal AOCC mean: 0.1987
2025-06-23 14:26:45 INFO Multimodal (single component) AOCC mean: 0.1108
2025-06-23 14:26:45 INFO Multimodal (multiple components) AOCC mean: 0.0523
2025-06-23 14:26:45 INFO AOCC mean: 0.1206
2025-06-23 14:26:45 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:45 ERROR Can not run the algorithm
2025-06-23 14:26:45 INFO Run function 2 complete. FEHistory len: 306, AOCC: 0.1751
2025-06-23 14:26:45 INFO FeHistory: [-701.31194955 -701.28707768 -701.32939337 -701.30918816 -701.2862089
 -701.30613023 -701.28295202 -701.33878631 -701.29316384 -701.30265791
 -701.3022299  -701.32811054 -701.29730451 -701.29413566 -701.32902614
 -701.30226466 -701.34573321 -701.2944994  -701.34647346 -701.32982588
 -701.29905005 -701.32949929 -701.27914786 -701.3176366  -701.31188841
 -701.30197784 -701.32832939 -701.29030053 -701.29116002 -701.30607556
 -701.32964307 -701.35601211 -701.30517013 -701.29757813 -701.27605916
 -701.30665381 -701.31174905 -701.29368299 -701.32370008 -701.32973976
 -701.31270716 -701.30127097 -701.29820162 -701.29342443 -701.29805981
 -701.3091981  -701.31168645 -701.3326216  -701.31281718 -701.32301752
 -701.33784998 -701.29797296 -701.28831893 -701.29328991 -701.31375334
 -701.3335331  -701.31344565 -701.32671136 -701.31995242 -701.32846941
 -701.33593319 -701.32342192 -701.2958328  -701.3349394  -701.30658752
 -701.28736732 -701.30004192 -701.32442467 -701.30034436 -701.30282579
 -701.3210872  -701.28201048 -701.29731777 -701.34237393 -701.33223871
 -701.31801629 -701.27791609 -701.3098763  -701.33720591 -701.31193236
 -701.32376377 -701.31649499 -701.3229757  -701.33015513 -701.31833884
 -701.28468079 -701.29284349 -701.2913123  -701.31620844 -701.32179527
 -701.32668309 -701.33480517 -701.30489268 -701.28511901 -701.31580665
 -701.31653546 -701.30691821 -701.30057229 -701.29645668 -701.30378412
 -701.31195384 -701.2867832  -701.32915898 -701.30933944 -701.28637377
 -701.30634403 -701.28297561 -701.33875406 -701.29353854 -701.30270724
 -701.30238648 -701.32809318 -701.29689893 -701.29421036 -701.328798
 -701.30235251 -701.34606817 -701.29410859 -701.34672297 -701.32968515
 -701.2990857  -701.32966244 -701.27960842 -701.31793703 -701.31185081
 -701.30159189 -701.32821507 -701.29040296 -701.29113674 -701.30609378
 -701.3294896  -701.35590077 -701.30506443 -701.29778333 -701.27600872
 -701.3062911  -701.31164545 -701.29395821 -701.32335056 -701.32969398
 -701.31294081 -701.30116289 -701.29847265 -701.29358075 -701.29775698
 -701.30942396 -701.31151289 -701.33255638 -701.31252511 -701.32301927
 -701.33727732 -701.29746272 -701.28811908 -701.29352113 -701.31358049
 -701.33356651 -701.31342862 -701.32644145 -701.31987822 -701.32908729
 -701.33633395 -701.32326997 -701.29553385 -701.33462263 -701.30666886
 -701.28731771 -701.29989416 -701.32435404 -701.30036022 -701.30268973
 -701.32141899 -701.2818693  -701.29728311 -701.34242006 -701.33248421
 -701.31802538 -701.27780086 -701.31013063 -701.33748131 -701.3125076
 -701.32399261 -701.31672937 -701.32292017 -701.32961004 -701.31839671
 -701.28473657 -701.29263691 -701.29121625 -701.31639251 -701.32191481
 -701.32630118 -701.33481861 -701.30470783 -701.28545494 -701.31572086
 -701.31676655 -701.30717417 -701.3002811  -701.29665968 -701.30371835
 -701.35601211 -701.35600344 -701.356009   -701.35601358 -701.35601033
 -701.35600422 -701.35691196 -701.35576341 -701.34641508 -701.34620648
 -701.34565708 -701.34575825 -701.34229853 -701.34272577 -701.33914411
 -701.3389595  -701.33781747 -701.33723326 -701.33695506 -701.3372601
 -701.33623538 -701.33619959 -701.33494828 -701.33512719 -701.33530952
 -701.33487739 -701.3335621  -701.33331837 -701.33277297 -701.33209335
 -701.33249167 -701.33223037 -701.33008595 -701.33002457 -701.3299465
 -701.32985951 -701.3296609  -701.33003074 -701.32969684 -701.32953903
 -701.32889905 -701.32966688 -701.32913108 -701.32935082 -701.32900467
 -701.32917263 -701.32898545 -701.32875507 -701.32840513 -701.32850669
 -701.32777541 -701.32790571 -701.32646641 -701.32690736 -701.32675215
 -701.32657189 -701.32428126 -701.32420838 -701.32417914 -701.32405228
 -701.32347545 -701.32352738 -701.32299356 -701.32332919 -701.32326862
 -701.32315759 -701.32314008 -701.32282211 -701.32215407 -701.32162802
 -701.32148026 -701.32108903 -701.31982044 -701.31980681 -701.31873571
 -701.31841115 -701.31797333 -701.31806307 -701.31797824 -701.31764901
 -701.31693634 -701.31642701 -701.31641833 -701.3163901  -701.3157921
 -701.3158577  -701.31616545 -701.31584554 -701.31369974 -701.31358583
 -701.31343792 -701.31386579 -701.31305866 -701.31348783 -701.31296351
 -701.31242514 -701.31247977 -701.31205221 -701.31206685 -701.3119829
 -701.31203368 -701.31212954 -701.31177273 -701.31157434 -701.31180489
 -701.31155916]
2025-06-23 14:26:45 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:26:45 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEAwithLocalSearch
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEAwithLocalSearch
# Description: Combines adaptive Gaussian mutation, an archive for diversity, and local search to escape local optima in multimodal landscapes.
class AdaptiveGaussianArchiveEAwithLocalSearch:
    """Combines adaptive Gaussian mutation, an archive for diversity, and local search to escape local optima."""
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution = None
        self.best_fitness = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.sigma = 0.5  # Initial standard deviation for Gaussian mutation
        self.sigma_decay = 0.99 # Decay rate for sigma

        self.local_search_iterations = 5

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(population)
        self.eval_count += self.population_size

        for i in range(self.population_size):
            self._update_best(population[i], fitness[i])
            self._add_to_archive(population[i], fitness[i])

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            for i in range(self.population_size):
                self._update_best(offspring[i], offspring_fitness[i])
                self._add_to_archive(offspring[i], offspring_fitness[i])

            population = self._select_population(population, fitness, offspring, offspring_fitness)
            fitness = np.concatenate((fitness,offspring_fitness))
            fitness = fitness[fitness.argsort()][:self.population_size]
            population = population[fitness.argsort()][:self.population_size]

            # Adaptive Local Search on the best solution
            self.best_solution = self._local_search(self.best_solution, objective_function)
            
            self.sigma *= self.sigma_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness
        }
        return self.best_solution, self.best_fitness, optimization_info

    def _generate_offspring(self, population):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            parent = population[i]
            mutation = np.random.normal(0, self.sigma, self.dim)
            offspring[i] = np.clip(parent + mutation, self.lower_bounds, self.upper_bounds)
        return offspring

    def _update_best(self, solution, fitness):
        if fitness < self.best_fitness:
            self.best_fitness = fitness
            self.best_solution = solution

    def _add_to_archive(self, solution, fitness):
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_solution, worst_fitness = max(self.archive, key=lambda item: item[1])
            if fitness < worst_fitness:
                self.archive.remove((worst_solution, worst_fitness))
                self.archive.append((solution, fitness))

    def _select_population(self, population, fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        sorted_indices = np.argsort(combined_fitness)
        return combined_population[sorted_indices[:self.population_size]]

    def _local_search(self, solution, objective_function):
        current_solution = solution.copy()
        current_fitness = objective_function(current_solution.reshape(1, -1))[0]
        for _ in range(self.local_search_iterations):
            neighbor = current_solution + np.random.normal(0, 0.01, self.dim)
            neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
            neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
            if neighbor_fitness < current_fitness:
                current_solution = neighbor
                current_fitness = neighbor_fitness
        return current_solution
2025-06-23 14:26:45 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:26:45 ERROR Can not run the algorithm
2025-06-23 14:26:45 INFO Run function 15 complete. FEHistory len: 306, AOCC: 0.1023
2025-06-23 14:26:45 INFO FeHistory: [-220.85396629 -221.07410539 -221.08573986 -223.50297688 -221.58062469
 -220.86076275 -224.28536822 -223.70282607 -221.43492123 -221.67923271
 -221.04978667 -222.14069933 -220.71978707 -222.69953071 -223.7178621
 -220.38142809 -222.37427919 -221.78078309 -224.68818364 -222.28208179
 -222.15477341 -221.79166185 -221.29625509 -221.0844235  -222.21826846
 -222.02461046 -222.6095702  -220.29834212 -221.77285581 -222.86408361
 -223.6176463  -220.62114784 -221.66593099 -221.46416198 -222.69835312
 -222.38994125 -223.00282222 -221.83199911 -223.92768368 -221.10685266
 -221.77019478 -222.03740708 -220.71101935 -223.15798247 -222.58704977
 -222.35035832 -222.20751809 -223.8153087  -222.00574775 -220.76835817
 -220.94778877 -223.85952122 -222.95038604 -220.53965571 -223.09632955
 -223.61690002 -221.73428124 -222.44331642 -222.05706675 -220.33509564
 -221.40668906 -222.6989109  -222.21595203 -222.68793211 -221.5073579
 -223.75741322 -222.91972265 -221.77269996 -221.11023796 -222.29109567
 -221.18391596 -222.55675346 -220.65681254 -220.95072023 -223.23836868
 -221.86007026 -221.4754988  -222.06263605 -223.27310429 -222.1767123
 -221.51417183 -223.38009568 -221.008655   -223.57427527 -221.7438919
 -220.54150549 -220.56941306 -220.73777394 -221.61539916 -224.1130814
 -221.38644702 -220.65058256 -222.33765155 -223.23511526 -220.32564323
 -221.99061583 -223.07820722 -221.22406383 -223.33366096 -220.42039775
 -220.81181654 -221.15243759 -221.0293008  -223.5986078  -221.45450339
 -220.84284892 -224.27870369 -223.62119757 -221.43471112 -221.73162845
 -221.06726628 -222.13566737 -220.73983996 -222.88616718 -223.73389739
 -220.36323373 -222.29791256 -221.69246917 -224.65583967 -222.41870886
 -222.11044272 -221.78930543 -221.296971   -221.12761702 -222.28057619
 -222.07584256 -222.76616525 -220.34965511 -221.76049628 -222.97318375
 -223.53395516 -220.62654294 -221.57072521 -221.43021631 -222.68069374
 -222.32923517 -222.97441893 -221.82109311 -223.99481182 -221.02328288
 -221.82733066 -222.01906042 -220.63255252 -223.19494203 -222.62604096
 -222.34554661 -222.19994129 -223.95113835 -222.00457006 -220.77910771
 -220.97361326 -223.89398225 -222.87674602 -220.52056373 -223.17845391
 -223.55129781 -221.80385412 -222.45552545 -222.16167075 -220.36406294
 -221.42019243 -222.68440148 -222.29494708 -222.62643563 -221.50132521
 -223.69072147 -222.92535614 -221.70067744 -221.06696547 -222.22030138
 -221.25737336 -222.51557888 -220.66220171 -220.94891861 -223.27656666
 -221.95194625 -221.43246851 -222.01089909 -223.20580383 -222.07595693
 -221.53951302 -223.30691879 -220.99434667 -223.60776612 -221.78919151
 -220.53697225 -220.58550945 -220.77320617 -221.62523371 -224.16429049
 -221.37714404 -220.75216411 -222.26033884 -223.24658216 -220.33007703
 -222.00057498 -223.15420332 -221.18746812 -223.4437658  -220.41014423
 -224.68818364 -224.68906596 -224.68958303 -224.68918175 -224.68769115
 -224.6890085  -224.80609483 -224.59748031 -224.16799555 -224.22604668
 -224.16606966 -224.09289085 -223.9179782  -223.95706063 -223.83029034
 -223.73730277 -223.71934528 -223.90862238 -223.80154654 -223.56963392
 -223.64377473 -223.69307029 -223.78236492 -223.70818925 -223.54303886
 -223.58068539 -223.73262035 -223.51080652 -223.5198605  -223.54652701
 -223.5117113  -223.46575892 -223.55801359 -223.42192851 -223.40599652
 -223.424322   -223.3704227  -223.34534665 -223.41285575 -223.23763175
 -223.28999964 -223.13408726 -223.01446403 -223.21770299 -223.25052546
 -223.2367295  -223.13527066 -223.12813004 -222.86832974 -223.01715626
 -222.99890069 -222.83082507 -222.9523847  -222.99353934 -222.87340476
 -222.8848162  -222.85292029 -222.73780889 -222.72291172 -222.66180924
 -222.76486656 -222.61564843 -222.74453738 -222.64953693 -222.56237855
 -222.61269216 -222.67884343 -222.68289603 -222.60465336 -222.49054291
 -222.44325012 -222.48983241 -222.36321763 -222.40713039 -222.38933616
 -222.35374909 -222.34684119 -222.36064854 -222.31052645 -222.34655832
 -222.30057168 -222.28786626 -222.12545582 -222.28840025 -222.30926071
 -222.23490858 -222.18854057 -222.24692336 -222.14949195 -222.19734926
 -222.17960279 -222.14039787 -222.11640039 -222.07619578 -222.13065863
 -222.0681901  -222.03218783 -222.08095565 -222.19220841 -222.06232252
 -222.10647351 -222.03200484 -222.05549339 -222.18222143 -222.00307161
 -221.98456352]
2025-06-23 14:26:45 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:26:45 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEAwithLocalSearch
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEAwithLocalSearch
# Description: Combines adaptive Gaussian mutation, an archive for diversity, and local search to escape local optima in multimodal landscapes.
class AdaptiveGaussianArchiveEAwithLocalSearch:
    """Combines adaptive Gaussian mutation, an archive for diversity, and local search to escape local optima."""
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution = None
        self.best_fitness = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.sigma = 0.5  # Initial standard deviation for Gaussian mutation
        self.sigma_decay = 0.99 # Decay rate for sigma

        self.local_search_iterations = 5

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(population)
        self.eval_count += self.population_size

        for i in range(self.population_size):
            self._update_best(population[i], fitness[i])
            self._add_to_archive(population[i], fitness[i])

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            for i in range(self.population_size):
                self._update_best(offspring[i], offspring_fitness[i])
                self._add_to_archive(offspring[i], offspring_fitness[i])

            population = self._select_population(population, fitness, offspring, offspring_fitness)
            fitness = np.concatenate((fitness,offspring_fitness))
            fitness = fitness[fitness.argsort()][:self.population_size]
            population = population[fitness.argsort()][:self.population_size]

            # Adaptive Local Search on the best solution
            self.best_solution = self._local_search(self.best_solution, objective_function)
            
            self.sigma *= self.sigma_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness
        }
        return self.best_solution, self.best_fitness, optimization_info

    def _generate_offspring(self, population):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            parent = population[i]
            mutation = np.random.normal(0, self.sigma, self.dim)
            offspring[i] = np.clip(parent + mutation, self.lower_bounds, self.upper_bounds)
        return offspring

    def _update_best(self, solution, fitness):
        if fitness < self.best_fitness:
            self.best_fitness = fitness
            self.best_solution = solution

    def _add_to_archive(self, solution, fitness):
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_solution, worst_fitness = max(self.archive, key=lambda item: item[1])
            if fitness < worst_fitness:
                self.archive.remove((worst_solution, worst_fitness))
                self.archive.append((solution, fitness))

    def _select_population(self, population, fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        sorted_indices = np.argsort(combined_fitness)
        return combined_population[sorted_indices[:self.population_size]]

    def _local_search(self, solution, objective_function):
        current_solution = solution.copy()
        current_fitness = objective_function(current_solution.reshape(1, -1))[0]
        for _ in range(self.local_search_iterations):
            neighbor = current_solution + np.random.normal(0, 0.01, self.dim)
            neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
            neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
            if neighbor_fitness < current_fitness:
                current_solution = neighbor
                current_fitness = neighbor_fitness
        return current_solution
2025-06-23 14:26:45 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:26:45 ERROR Can not run the algorithm
2025-06-23 14:26:45 INFO Run function 24 complete. FEHistory len: 306, AOCC: 0.0000
2025-06-23 14:26:45 INFO FeHistory: [190.91222454 204.40458395 167.06538287 218.19457055 180.78786384
 166.27358855 202.12750079 186.81576257 189.22440721 172.33319677
 167.25936748 187.15402867 196.76635985 203.43149339 210.76365811
 144.04312501 217.68147927 207.32443349 186.09804318 201.35812309
 169.03936966 198.94558742 206.6463276  180.86810571 209.28622468
 182.58061205 202.36727996 215.47299629 191.4600875  168.24753673
 183.43827935 183.53999937 192.55171589 193.4420895  191.39099533
 175.54089435 190.54553762 176.16617271 196.55443847 179.24229697
 200.79076806 206.74912556 181.37663716 197.20807907 173.29667193
 183.39642988 123.12036673 180.64255021 187.77978238 191.36617233
 174.94440502 182.18749634 169.83116426 164.94624895 203.49204455
 176.92608498 168.6054943  196.63596618 166.28244702 184.78044586
 192.4811623  191.73528238 174.22090091 187.31578872 172.5184278
 182.80900023 170.17521755 156.65443578 172.25022742 152.25738783
 180.027821   196.32070816 208.0162301  204.36021786 186.45478283
 159.12243561 208.6231545  204.15924991 200.89012882 165.67812966
 158.86063138 163.77049951 193.42924359 166.14012376 184.96633831
 210.63881852 194.5680913  196.31832393 164.73503022 172.54260141
 161.80739521 194.30098582 182.94593433 194.19395359 196.23141389
 183.72890077 174.98854459 176.46733    181.16838532 149.70685106
 192.35814113 204.54988865 167.1860327  219.44490549 180.53248606
 165.5110761  200.31441002 186.25594885 198.10324616 171.92328087
 168.19942675 189.42507134 195.55436186 206.80415641 208.48022814
 145.23224658 208.49939868 207.46158167 186.11622906 201.15607413
 167.55148257 198.12783155 203.94437762 183.58881557 214.42035194
 178.9141835  204.44595532 213.70417946 191.22266209 170.47540572
 186.45082758 175.61791844 188.35904105 197.54357808 181.35004995
 175.90315424 187.30617354 175.03214976 198.67423231 179.13983982
 201.22566838 206.07003335 185.61866517 194.11041607 180.71986889
 187.60564403 121.65259995 183.27431634 189.32672042 185.75214353
 172.97513038 178.61223644 172.62169166 164.2284432  200.23277997
 175.28879333 166.3580784  196.08971303 165.25456447 184.73871562
 191.97298823 193.07219083 176.94845217 184.94443897 167.1402817
 182.31242592 172.35066114 159.41844942 175.81537228 156.1694229
 180.24889155 195.34016262 207.46677671 202.40550963 181.98530532
 145.15284177 209.3950063  205.2578093  202.89222    164.32049339
 159.59349151 161.10855053 192.08916729 167.85962438 186.16896947
 214.78988402 197.82386226 195.04808018 160.3199734  169.63614861
 160.36702002 189.32759105 187.2176869  193.35337669 191.78727615
 182.94121439 173.62705126 176.69953844 179.67447456 146.38714899
 121.65259995 121.64789387 121.72154666 121.63764698 121.72775224
 121.6502973  123.28449189 119.10041917 142.88207535 145.02768984
 145.39236933 148.81863132 149.30453731 155.82398124 162.84754279
 158.90265507 158.21920946 159.4549079  153.75197675 159.47482379
 164.26725964 161.32794789 166.67987103 162.95093378 170.40193571
 167.08746201 165.06925227 167.86849754 162.28125266 166.85872663
 167.63696982 165.64275917 167.07774927 163.1295897  162.39458146
 159.26788984 167.6609067  166.79744002 174.15162144 162.75136945
 172.76506143 165.9204773  172.54426049 169.6560347  170.85699031
 164.96407376 168.63246466 171.56312124 170.55588976 168.98814995
 169.30857439 173.45063667 167.24996488 173.03519633 170.77898139
 174.74895316 174.89129984 173.46943748 173.00733916 176.99453017
 173.79111695 172.65891325 180.75459645 175.98383723 181.42535698
 176.25976231 179.0081578  175.30943833 177.02104247 175.17331141
 176.57418408 173.85538135 178.40446276 175.91757714 174.69950244
 178.07471119 181.38244522 177.98981841 180.74191965 183.059435
 170.66936049 178.6243628  182.64725887 178.90492207 182.0348814
 178.15896155 181.31675694 182.24750333 179.0069581  172.99685118
 186.07772706 184.80649173 181.72710235 184.30388042 183.16916372
 178.36482569 185.46379568 187.14565224 180.32448863 185.60020585
 181.95783536 184.21834382 175.05433605 195.64723596 185.22940927
 186.09703021]
2025-06-23 14:26:45 INFO Expected Optimum FE: -100
2025-06-23 14:26:45 INFO Unimodal AOCC mean: 0.1751
2025-06-23 14:26:45 INFO Multimodal (single component) AOCC mean: 0.1023
2025-06-23 14:26:45 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:26:45 INFO AOCC mean: 0.0925
