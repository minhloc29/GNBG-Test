2025-06-22 21:50:16 INFO Initializing first population
2025-06-22 21:50:16 INFO Initializing population from 7 seed files...
2025-06-22 21:50:16 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:50:20 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1746
2025-06-22 21:50:20 INFO FeHistory: [-334.02493442 -333.94704547 -333.98296346 ... -336.23884429 -336.23883228
 -336.23882456]
2025-06-22 21:50:20 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:50:20 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Standard Deviation for Gaussian Sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Adaptive Gaussian Sampling
            parents = self.tournament_selection(fitness_values, k=5)  # Tournament Selection
            offspring = self.gaussian_mutation(parents, self.sigma)

            # Bounds handling
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update population and best solution
            self.population = np.concatenate((self.population, offspring))
            fitness_values = np.concatenate((fitness_values, offspring_fitness))

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            # Adaptive Sigma
            self.sigma *= 0.99  # Gradually reduce sigma for finer search later.

            # Elitism
            sorted_pop = self.population[np.argsort(fitness_values)]
            self.population = sorted_pop[:self.population_size]
            fitness_values = fitness_values[np.argsort(fitness_values)][:self.population_size]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2  # Select half the population as parents
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

2025-06-22 21:50:20 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:50:25 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.3391
2025-06-22 21:50:25 INFO FeHistory: [198810.08953647 151890.42550717 284934.08897237 ...  -1002.47907853
  -1002.47907844  -1002.47907844]
2025-06-22 21:50:25 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:50:25 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Standard Deviation for Gaussian Sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Adaptive Gaussian Sampling
            parents = self.tournament_selection(fitness_values, k=5)  # Tournament Selection
            offspring = self.gaussian_mutation(parents, self.sigma)

            # Bounds handling
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update population and best solution
            self.population = np.concatenate((self.population, offspring))
            fitness_values = np.concatenate((fitness_values, offspring_fitness))

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            # Adaptive Sigma
            self.sigma *= 0.99  # Gradually reduce sigma for finer search later.

            # Elitism
            sorted_pop = self.population[np.argsort(fitness_values)]
            self.population = sorted_pop[:self.population_size]
            fitness_values = fitness_values[np.argsort(fitness_values)][:self.population_size]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2  # Select half the population as parents
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

2025-06-22 21:50:25 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:50:42 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2435
2025-06-22 21:50:42 INFO FeHistory: [ 12.97878404  31.94575582  11.57408136 ... -99.99822053 -99.99810988
 -99.99795005]
2025-06-22 21:50:42 INFO Expected Optimum FE: -100
2025-06-22 21:50:42 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Standard Deviation for Gaussian Sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Adaptive Gaussian Sampling
            parents = self.tournament_selection(fitness_values, k=5)  # Tournament Selection
            offspring = self.gaussian_mutation(parents, self.sigma)

            # Bounds handling
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update population and best solution
            self.population = np.concatenate((self.population, offspring))
            fitness_values = np.concatenate((fitness_values, offspring_fitness))

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            # Adaptive Sigma
            self.sigma *= 0.99  # Gradually reduce sigma for finer search later.

            # Elitism
            sorted_pop = self.population[np.argsort(fitness_values)]
            self.population = sorted_pop[:self.population_size]
            fitness_values = fitness_values[np.argsort(fitness_values)][:self.population_size]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2  # Select half the population as parents
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

2025-06-22 21:50:42 INFO Unimodal AOCC mean: 0.1746
2025-06-22 21:50:42 INFO Multimodal (single component) AOCC mean: 0.3391
2025-06-22 21:50:42 INFO Multimodal (multiple components) AOCC mean: 0.2435
2025-06-22 21:50:42 INFO AOCC mean: 0.2524
2025-06-22 21:50:42 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:50:48 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1756
2025-06-22 21:50:48 INFO FeHistory: [-334.08010313 -334.02384768 -334.02084541 ... -336.24611574 -336.2460032
 -336.24651038]
2025-06-22 21:50:48 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:50:48 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-22 21:50:48 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:50:54 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.2802
2025-06-22 21:50:54 INFO FeHistory: [163923.28593559 225041.25143823 238193.85814332 ...  -1002.47906023
  -1002.47906093  -1002.47905151]
2025-06-22 21:50:54 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:50:54 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-22 21:50:54 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:51:13 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.1930
2025-06-22 21:51:13 INFO FeHistory: [ 29.07487288  22.1253008   31.00676949 ... -99.86209373 -99.89206765
 -99.86023904]
2025-06-22 21:51:13 INFO Expected Optimum FE: -100
2025-06-22 21:51:13 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-22 21:51:13 INFO Unimodal AOCC mean: 0.1756
2025-06-22 21:51:13 INFO Multimodal (single component) AOCC mean: 0.2802
2025-06-22 21:51:13 INFO Multimodal (multiple components) AOCC mean: 0.1930
2025-06-22 21:51:13 INFO AOCC mean: 0.2163
2025-06-22 21:51:13 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:51:18 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1754
2025-06-22 21:51:18 INFO FeHistory: [-333.89617663 -333.90596662 -333.90738305 ... -336.22214066 -336.22214217
 -336.22213556]
2025-06-22 21:51:18 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:51:18 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-22 21:51:18 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:51:24 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.3741
2025-06-22 21:51:24 INFO FeHistory: [257542.8851854  316811.56500766 318610.76271078 ...  -1002.47907865
  -1002.47907857  -1002.4790786 ]
2025-06-22 21:51:24 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:51:24 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-22 21:51:24 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:51:41 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2612
2025-06-22 21:51:41 INFO FeHistory: [ 35.06266813  21.85325473  30.44799637 ... -99.99881842 -99.99859296
 -99.99861401]
2025-06-22 21:51:41 INFO Expected Optimum FE: -100
2025-06-22 21:51:41 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-22 21:51:41 INFO Unimodal AOCC mean: 0.1754
2025-06-22 21:51:41 INFO Multimodal (single component) AOCC mean: 0.3741
2025-06-22 21:51:41 INFO Multimodal (multiple components) AOCC mean: 0.2612
2025-06-22 21:51:41 INFO AOCC mean: 0.2702
2025-06-22 21:51:41 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:51:46 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1941
2025-06-22 21:51:46 INFO FeHistory: [-334.02253917 -333.98327352 -333.90698502 ... -335.80376264 -335.80376264
 -335.80376264]
2025-06-22 21:51:46 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:51:46 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithEnhancedInitialization
import numpy as np
from scipy.optimize import minimize

class AdaptiveDifferentialEvolutionWithEnhancedInitialization:
    """
    Combines Differential Evolution with enhanced initialization near known optima and local search for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], known_optimum=None):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.known_optimum = known_optimum  # Allow for None if no known optimum

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.local_search_freq = 5 # Perform local search every 5 generations

    def initialize_population(self, num_samples):
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(num_samples, self.dim))
        
        if self.known_optimum is not None:
            num_near_optimum = int(0.3 * num_samples) # 30% near the optimum
            noise_scale = 20 # Adjust noise scale as needed. Experiment with this!
            noise = np.random.normal(scale=noise_scale, size=(num_near_optimum, self.dim))
            population[:num_near_optimum, :] = self.known_optimum + noise
            population[:num_near_optimum, :] = np.clip(population[:num_near_optimum, :], self.lower_bounds, self.upper_bounds)

        return population

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self.initialize_population(self.population_size)
        fitness = objective_function(population)
        self.eval_count += len(fitness)
        best_solution = population[np.argmin(fitness)]
        best_fitness = np.min(fitness)
        self.best_solution_overall = best_solution
        self.best_fitness_overall = best_fitness

        generation = 0
        while self.eval_count < self.budget:
            # Differential Evolution
            new_population = np.zeros_like(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if trial_fitness[0] < fitness[i]:
                    new_population[i] = trial
                    fitness[i] = trial_fitness[0]
                else:
                    new_population[i] = population[i]

            population = new_population
            best_solution = population[np.argmin(fitness)]
            best_fitness = np.min(fitness)

            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            # Local Search
            if generation % self.local_search_freq == 0:
                result = minimize(objective_function, best_solution, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
                if result.fun < best_fitness:
                    best_fitness = result.fun
                    best_solution = result.x
                    self.eval_count += result.nfev

                    if best_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = best_fitness
                        self.best_solution_overall = best_solution

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info









2025-06-22 21:51:46 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:51:52 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:51:52 INFO FeHistory: [239529.97546674 278886.52298607 147361.18235973 ...  11280.98348326
  11280.98348982  11280.98348459]
2025-06-22 21:51:52 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:51:52 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:52:10 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0373
2025-06-22 21:52:10 INFO FeHistory: [ 22.04608409  51.0484308   39.27060293 ... -24.37694952  -6.7208981
 -11.24489387]
2025-06-22 21:52:10 INFO Expected Optimum FE: -100
2025-06-22 21:52:10 INFO Unimodal AOCC mean: 0.1941
2025-06-22 21:52:10 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:52:10 INFO Multimodal (multiple components) AOCC mean: 0.0373
2025-06-22 21:52:10 INFO AOCC mean: 0.0771
2025-06-22 21:52:10 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:52:19 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1724
2025-06-22 21:52:19 INFO FeHistory: [-333.978033   -333.97322596 -333.978033   ... -335.16992589 -335.87242578
 -335.02501635]
2025-06-22 21:52:19 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:52:19 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-22 21:52:19 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:52:28 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:52:28 INFO FeHistory: [267608.25668721 149029.92270569 267608.25668721 ...  44253.62092598
  43377.68155253  54413.23948581]
2025-06-22 21:52:28 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:52:28 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:53:00 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0203
2025-06-22 21:53:00 INFO FeHistory: [ 16.39664304  24.68445659  16.39664304 ... -31.55309644 -37.66074215
  -7.83716312]
2025-06-22 21:53:00 INFO Expected Optimum FE: -100
2025-06-22 21:53:00 INFO Unimodal AOCC mean: 0.1724
2025-06-22 21:53:00 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:53:00 INFO Multimodal (multiple components) AOCC mean: 0.0203
2025-06-22 21:53:00 INFO AOCC mean: 0.0642
2025-06-22 21:53:00 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:53:05 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1709
2025-06-22 21:53:05 INFO FeHistory: [-334.03760009 -333.89949357 -334.01181426 ... -336.17097667 -336.19646553
 -336.20412814]
2025-06-22 21:53:05 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:53:05 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-22 21:53:05 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:53:10 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0563
2025-06-22 21:53:10 INFO FeHistory: [199806.12275156 225815.97667944 129041.18081027 ...  -1001.754186
  -1001.57968541  -1001.67004154]
2025-06-22 21:53:10 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:53:10 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:53:27 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.1360
2025-06-22 21:53:27 INFO FeHistory: [ 29.09101154  20.65937912  20.7016864  ... -99.45413998 -99.5909158
 -99.47067193]
2025-06-22 21:53:27 INFO Expected Optimum FE: -100
2025-06-22 21:53:27 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-22 21:53:27 INFO Unimodal AOCC mean: 0.1709
2025-06-22 21:53:27 INFO Multimodal (single component) AOCC mean: 0.0563
2025-06-22 21:53:27 INFO Multimodal (multiple components) AOCC mean: 0.1360
2025-06-22 21:53:27 INFO AOCC mean: 0.1211
2025-06-22 21:53:27 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:54:27 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 21:56:05 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1755
2025-06-22 21:56:05 INFO FeHistory: [-334.00306569 -333.95949353 -333.94813325 ... -336.22925681 -336.22925135
 -336.22925297]
2025-06-22 21:56:05 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:56:05 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:56:05 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:58:51 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.3753
2025-06-22 21:58:51 INFO FeHistory: [216897.41771499 388014.81758661 274127.41710536 ...  -1002.47907864
  -1002.4790786   -1002.47907864]
2025-06-22 21:58:51 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:58:51 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:58:51 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:01:41 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2618
2025-06-22 22:01:41 INFO FeHistory: [ 37.67127966   9.49618857  47.7097544  ... -99.99884074 -99.99849752
 -99.99871997]
2025-06-22 22:01:41 INFO Expected Optimum FE: -100
2025-06-22 22:01:41 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 22:01:41 INFO Unimodal AOCC mean: 0.1755
2025-06-22 22:01:41 INFO Multimodal (single component) AOCC mean: 0.3753
2025-06-22 22:01:41 INFO Multimodal (multiple components) AOCC mean: 0.2618
2025-06-22 22:01:41 INFO AOCC mean: 0.2709
2025-06-22 22:03:15 INFO Started evolutionary loop, best so far: 0.27087516450180577
2025-06-22 22:03:15 INFO Population length is: 15
2025-06-22 22:03:15 INFO --- Performing Long-Term Reflection at Generation 1 ---
2025-06-22 22:03:15 INFO Reflection Prompt: ### Problem Description
Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    
### List heuristics
Below is a list of design heuristics ranked from best to worst based on their average AOCC score, where higher is better, across a subset of the GNBG benchmark. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score: 2.7088e-01 |             AOCC Score on Unimodal instances: 1.7551e-01 |             AOCC Score on Multimodal instances with a single component: 3.7534e-01 |             AOCC Score on Multimodal instances with multiple components: 2.6178e-01)
# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: Seed from AdaptiveGaussianSamplingEAwithArchive
# Code:
```python
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 2 (Overall AOCC Score: 2.7022e-01 |             AOCC Score on Unimodal instances: 1.7537e-01 |             AOCC Score on Multimodal instances with a single component: 3.7410e-01 |             AOCC Score on Multimodal instances with multiple components: 2.6119e-01)
# Name: AdaptiveGaussianSamplingEA
# Description: Seed from AdaptiveGaussianSamplingEA
# Code:
```python
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
```

### Rank 3 (Overall AOCC Score: 2.5240e-01 |             AOCC Score on Unimodal instances: 1.7464e-01 |             AOCC Score on Multimodal instances with a single component: 3.3907e-01 |             AOCC Score on Multimodal instances with multiple components: 2.4350e-01)
# Name: AdaptiveGaussianSamplingEA
# Description: Seed from AdaptiveGaussianSamplingEA
# Code:
```python
import numpy as np

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Standard Deviation for Gaussian Sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Adaptive Gaussian Sampling
            parents = self.tournament_selection(fitness_values, k=5)  # Tournament Selection
            offspring = self.gaussian_mutation(parents, self.sigma)

            # Bounds handling
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update population and best solution
            self.population = np.concatenate((self.population, offspring))
            fitness_values = np.concatenate((fitness_values, offspring_fitness))

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            # Adaptive Sigma
            self.sigma *= 0.99  # Gradually reduce sigma for finer search later.

            # Elitism
            sorted_pop = self.population[np.argsort(fitness_values)]
            self.population = sorted_pop[:self.population_size]
            fitness_values = fitness_values[np.argsort(fitness_values)][:self.population_size]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2  # Select half the population as parents
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

```

### Rank 4 (Overall AOCC Score: 2.1625e-01 |             AOCC Score on Unimodal instances: 1.7558e-01 |             AOCC Score on Multimodal instances with a single component: 2.8018e-01 |             AOCC Score on Multimodal instances with multiple components: 1.9300e-01)
# Name: AdaptiveGaussianMutationDE
# Description: Seed from AdaptiveGaussianMutationDE
# Code:
```python
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
```

### Rank 5 (Overall AOCC Score: 1.2107e-01 |             AOCC Score on Unimodal instances: 1.7091e-01 |             AOCC Score on Multimodal instances with a single component: 5.6334e-02 |             AOCC Score on Multimodal instances with multiple components: 1.3597e-01)
# Name: EnhancedArchiveGuidedDE
# Description: Seed from EnhancedArchiveGuidedDE
# Code:
```python
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
```

### Rank 6 (Overall AOCC Score: 7.7138e-02 |             AOCC Score on Unimodal instances: 1.9409e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 3.7321e-02)
# Name: AdaptiveDifferentialEvolutionWithEnhancedInitialization
# Description: Seed from AdaptiveDifferentialEvolutionWithEnhancedInitialization
# Code:
```python
import numpy as np
from scipy.optimize import minimize

class AdaptiveDifferentialEvolutionWithEnhancedInitialization:
    """
    Combines Differential Evolution with enhanced initialization near known optima and local search for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], known_optimum=None):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.known_optimum = known_optimum  # Allow for None if no known optimum

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.local_search_freq = 5 # Perform local search every 5 generations

    def initialize_population(self, num_samples):
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(num_samples, self.dim))
        
        if self.known_optimum is not None:
            num_near_optimum = int(0.3 * num_samples) # 30% near the optimum
            noise_scale = 20 # Adjust noise scale as needed. Experiment with this!
            noise = np.random.normal(scale=noise_scale, size=(num_near_optimum, self.dim))
            population[:num_near_optimum, :] = self.known_optimum + noise
            population[:num_near_optimum, :] = np.clip(population[:num_near_optimum, :], self.lower_bounds, self.upper_bounds)

        return population

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self.initialize_population(self.population_size)
        fitness = objective_function(population)
        self.eval_count += len(fitness)
        best_solution = population[np.argmin(fitness)]
        best_fitness = np.min(fitness)
        self.best_solution_overall = best_solution
        self.best_fitness_overall = best_fitness

        generation = 0
        while self.eval_count < self.budget:
            # Differential Evolution
            new_population = np.zeros_like(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if trial_fitness[0] < fitness[i]:
                    new_population[i] = trial
                    fitness[i] = trial_fitness[0]
                else:
                    new_population[i] = population[i]

            population = new_population
            best_solution = population[np.argmin(fitness)]
            best_fitness = np.min(fitness)

            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            # Local Search
            if generation % self.local_search_freq == 0:
                result = minimize(objective_function, best_solution, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
                if result.fun < best_fitness:
                    best_fitness = result.fun
                    best_solution = result.x
                    self.eval_count += result.nfev

                    if best_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = best_fitness
                        self.best_solution_overall = best_solution

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info









```

### Rank 7 (Overall AOCC Score: 7.2038e-02 |             AOCC Score on Unimodal instances: 1.9644e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 1.9678e-02)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining Differential Evolution with a local search and adaptive mutation to escape local optima effectively in high-dimensional multimodal landscapes.
# Code:
```python
import numpy as np
from scipy.optimize import minimize

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Differential Evolution crossover rate
        self.mutation_rate_decay = 0.95 # Decay factor for mutation rate

        self.initial_mutation_rate = 0.2 # Initialize with larger rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = float('inf')
        
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness = objective_function(population)
        self.eval_count += self.population_size

        current_mutation_rate = self.initial_mutation_rate

        while self.eval_count < self.budget:
            # Differential Evolution
            offspring = np.copy(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds) #Bounds handling

                trial = np.copy(population[i])
                jrand = np.random.randint(0, self.dim)
                for j in range(self.dim):
                    if np.random.rand() < self.CR or j == jrand:
                        trial[j] = mutant[j]
                
                #Add adaptive mutation to escape local optima
                trial = trial + current_mutation_rate * np.random.normal(0,1, self.dim)
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds)

                trial_fitness = objective_function(trial.reshape(1,-1))
                self.eval_count += 1
                if trial_fitness < fitness[i]:
                    offspring[i] = trial
                    fitness[i] = trial_fitness

            population = offspring


            # Local Search (optional - improves convergence speed for unimodal)
            best_index = np.argmin(fitness)
            best_solution = population[best_index]
            res = minimize(lambda x: objective_function(x.reshape(1,-1)), best_solution, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
            local_search_solution = res.x
            local_search_fitness = res.fun
            self.eval_count += res.nfev
            if local_search_fitness < fitness[best_index]:
                population[best_index] = local_search_solution
                fitness[best_index] = local_search_fitness

            # Update overall best
            best_index_overall = np.argmin(fitness)
            if fitness[best_index_overall] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[best_index_overall]
                self.best_solution_overall = population[best_index_overall]

            current_mutation_rate *= self.mutation_rate_decay #Decay mutation rate over time


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

```

### Rank 8 (Overall AOCC Score: 6.7687e-02 |             AOCC Score on Unimodal instances: 1.5284e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 5.0219e-02)
# Name: AdaptiveMultimodalEvolutionaryStrategy
# Description: An evolutionary strategy employing adaptive mutation and a population diversification mechanism to efficiently explore multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  #Adjust as needed
        self.population = None
        self.fitness = None
        self.mutation_rate = 0.1  #Initial mutation rate
        self.diversity_threshold = 0.8 #Parameter for diversity check


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        self.fitness = np.full(self.population_size, np.inf)
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')


        while self.eval_count < self.budget:
            # Evaluate population
            for i in range(self.population_size):
                if np.isinf(self.fitness[i]): #only evaluate if not already evaluated
                    self.fitness[i] = objective_function(self.population[i:i+1])[0]
                    self.eval_count += 1

            # Update best solution
            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_index]
                self.best_solution_overall = self.population[best_index].copy()


            # Selection (tournament selection)
            parents = []
            for _ in range(self.population_size // 2):
                tournament = random.sample(range(self.population_size), 5) #tournament size 5
                winner1 = min(tournament, key=lambda i: self.fitness[i])
                tournament = random.sample(range(self.population_size), 5)
                winner2 = min(tournament, key=lambda i: self.fitness[i])
                parents.extend([winner1, winner2])

            #Recombination (simple averaging)
            offspring = np.zeros((self.population_size, self.dim))
            for i in range(0, self.population_size, 2):
                offspring[i] = (self.population[parents[i]] + self.population[parents[i+1]]) / 2
                offspring[i+1] = (self.population[parents[i]] + self.population[parents[i+1]]) / 2

            # Mutation (adaptive)
            for i in range(self.population_size):
                mutation = np.random.normal(0, self.mutation_rate * (self.upper_bounds - self.lower_bounds), self.dim)
                offspring[i] = np.clip(offspring[i] + mutation, self.lower_bounds, self.upper_bounds)

            #Diversity Check and adjustment
            diversity = np.std(offspring)
            if diversity < self.diversity_threshold * np.std(self.population):
                #inject some diversity if needed. Replace a portion of the offspring with random solutions
                num_random = int(0.2 * self.population_size)
                random_indices = random.sample(range(self.population_size), num_random)
                offspring[random_indices] = np.random.uniform(self.lower_bounds, self.upper_bounds, (num_random, self.dim))
                self.mutation_rate *= 1.1 #increase mutation if low diversity


            self.population = offspring.copy()
            self.fitness = np.full(self.population_size, np.inf) #reset fitness for new population

        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

```

### Rank 9 (Overall AOCC Score: 6.4233e-02 |             AOCC Score on Unimodal instances: 1.7244e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 2.0256e-02)
# Name: AdaptiveMultimodalOptimizerImproved
# Description: Seed from AdaptiveMultimodalOptimizerImproved
# Code:
```python
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








```

### Rank 10 (Overall AOCC Score: 6.2241e-02 |             AOCC Score on Unimodal instances: 1.4981e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 3.6916e-02)
# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A Differential Evolution algorithm enhanced with adaptive mutation and clustering to escape local optima and explore diverse regions of the search space efficiently.
# Code:
```python
import numpy as np
from sklearn.cluster import KMeans

class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness = None
        self.F = 0.8  # Differential weight (adaptive)
        self.CR = 0.9 # Crossover rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(self.fitness)]
        self.best_fitness_overall = np.min(self.fitness)

        while self.eval_count < self.budget:
            # Adaptive mutation strategy based on fitness spread
            fitness_spread = np.max(self.fitness) - np.min(self.fitness)
            self.F = 0.5 + 0.5 * (fitness_spread / np.mean(self.fitness)) # Adjust F dynamically

            # Clustering to identify diverse regions
            kmeans = KMeans(n_clusters=min(10, self.population_size // 2), random_state=0).fit(self.population)
            cluster_centers = kmeans.cluster_centers_

            offspring = []
            for i in range(self.population_size):
                # Select parents based on clusters
                cluster_idx = kmeans.labels_[i]
                parent1 = self.population[i]
                parent2 = self.population[kmeans.labels_ == cluster_idx][np.random.choice(np.sum(kmeans.labels_ == cluster_idx))]
                parent3 = self.population[np.random.choice(self.population_size)] # Random third vector
                
                mutant = parent1 + self.F * (parent2 - parent3)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds) # bound constraints
                
                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, parent1)
                offspring.append(trial_vector)

            offspring = np.array(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((self.fitness, offspring_fitness))
            
            idx_sorted = np.argsort(combined_fitness)
            self.population = combined_population[idx_sorted[:self.population_size]]
            self.fitness = combined_fitness[idx_sorted[:self.population_size]]
            
            best_idx = np.argmin(self.fitness)
            if self.fitness[best_idx] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_idx]
                self.best_solution_overall = self.population[best_idx]


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

```

### Rank 11 (Overall AOCC Score: 5.4671e-02 |             AOCC Score on Unimodal instances: 1.4722e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 1.6797e-02)
# Name: AdaptiveMultimodalEvolutionaryStrategy
# Description: An evolutionary strategy employing adaptive mutation and a niching mechanism to escape local optima in multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 50  # Adjust as needed
        self.population = None
        self.mutation_rate = 0.1  # Initial mutation rate
        self.mutation_decay = 0.99  # Decay factor for mutation rate
        self.niches = [] # List to store niches (best solutions found so far)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population,fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(self.population)
            self.eval_count += len(self.population)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population,fitness_values)
            self.mutation_rate *= self.mutation_decay #Adaptive Mutation


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = []
        for i in range(self.population_size):
            parent = self.population[i]
            offspring_solution = parent + np.random.normal(0, self.mutation_rate, self.dim)  #Gaussian mutation
            offspring_solution = np.clip(offspring_solution, self.lower_bounds, self.upper_bounds) #Clamp to bounds
            offspring.append(offspring_solution)
        return np.array(offspring)

    def _selection(self, parents, parent_fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((parents, offspring))
        combined_fitness = np.concatenate((parent_fitness, offspring_fitness))
        
        indices = np.argsort(combined_fitness)
        selected_indices = indices[:self.population_size]
        selected_population = combined_population[selected_indices]
        selected_fitness = combined_fitness[selected_indices]
        
        best_solution, best_fitness = self._update_best(selected_population,selected_fitness)
        
        return selected_population, best_solution, best_fitness

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]

        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness
```

### Rank 12 (Overall AOCC Score: 5.3933e-02 |             AOCC Score on Unimodal instances: 1.4713e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 1.4672e-02)
# Name: AdaptiveMultimodalEvolutionaryStrategy
# Description: An evolutionary strategy employing adaptive mutation and population diversification to efficiently explore multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.mutation_rate = 0.1  # Initial mutation rate
        self.mutation_decay = 0.99 # Decay factor for mutation rate
        self.population = self.initialize_population()


    def initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))

    def mutate(self, individual):
        mutation = np.random.normal(0, self.mutation_rate, self.dim)
        offspring = individual + mutation
        offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds) #Keep offspring within bounds
        return offspring

    def diversify_population(self):
        # Introduce diversity by replacing a fraction of the worst-performing individuals
        num_to_replace = int(0.2 * self.population_size) #Replace 20% of population
        indices_to_replace = np.argsort(self.fitness_values)[:num_to_replace]
        self.population[indices_to_replace] = self.initialize_population()[ :num_to_replace]

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.fitness_values = np.full(self.population_size, np.inf) # Initialize fitness values
        self.population = self.initialize_population()

        while self.eval_count < self.budget:
            self.fitness_values = objective_function(self.population)
            self.eval_count += self.population_size

            best_index = np.argmin(self.fitness_values)
            best_solution = self.population[best_index]
            best_fitness = self.fitness_values[best_index]

            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            # Adapt mutation rate based on progress
            self.mutation_rate *= self.mutation_decay

            # Generate offspring through mutation
            offspring = np.apply_along_axis(self.mutate, 1, self.population)

            #Combine parent and offspring
            combined_population = np.vstack((self.population, offspring))
            combined_fitness = objective_function(combined_population)
            self.eval_count += self.population_size
            
            # Selection: keep the best individuals
            sorted_indices = np.argsort(combined_fitness)
            self.population = combined_population[sorted_indices[:self.population_size]]
            self.fitness_values = combined_fitness[sorted_indices[:self.population_size]]


            # Diversify if needed (check for stagnation)
            if self.eval_count > self.budget * 0.7 and np.std(self.fitness_values) < acceptance_threshold * 10:
                self.diversify_population()


        if self.best_solution_overall is None:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

```

### Rank 13 (Overall AOCC Score: 5.2493e-02 |             AOCC Score on Unimodal instances: 1.4688e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 1.0602e-02)
# Name: AdaptiveMultimodalEvolutionaryStrategy
# Description: An evolutionary strategy using adaptive mutation and population diversification to escape local optima in multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 50  # Adjust as needed
        self.mutation_rate = 0.1 # Initial mutation rate
        self.population = None

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        else:
            self.population = np.array([])
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        while self.eval_count < self.budget:
            fitness_values = objective_function(self.population)
            self.eval_count += self.population_size

            # Update best solution
            for i, fitness in enumerate(fitness_values):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = self.population[i]

            # Selection (tournament selection)
            parents = self.tournament_selection(self.population, fitness_values, tournament_size=5)

            # Recombination (intermediate recombination)
            offspring = self.intermediate_recombination(parents)

            # Mutation (adaptive mutation)
            mutated_offspring = self.adaptive_mutation(offspring, self.mutation_rate)

            # Population update
            self.population = np.concatenate((parents, mutated_offspring))  #Elitism

            # Diversity check and adjustment
            self.mutation_rate = max(0.01, self.mutation_rate * (1 - 0.01*np.std(fitness_values))) # Reduce mutation if solutions are similar


        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate_history' : [self.mutation_rate] #only saves last value for brevity
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, population, fitness_values, tournament_size):
        selected_indices = []
        for _ in range(self.population_size // 2):  # Select half the population
            tournament = random.sample(range(len(population)), tournament_size)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_indices.append(winner_index)
        return population[selected_indices]


    def intermediate_recombination(self, parents):
        offspring = np.zeros_like(parents)
        for i in range(0, len(parents), 2):
            offspring[i] = (parents[i] + parents[i+1]) / 2
            offspring[i+1] = (parents[i] + parents[i+1]) / 2
        return offspring

    def adaptive_mutation(self, offspring, mutation_rate):
        mutated_offspring = offspring + np.random.normal(0, mutation_rate * (self.upper_bounds - self.lower_bounds), size=offspring.shape)
        mutated_offspring = np.clip(mutated_offspring, self.lower_bounds, self.upper_bounds)
        return mutated_offspring

```

### Rank 14 (Overall AOCC Score: 5.1365e-02 |             AOCC Score on Unimodal instances: 1.5069e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 3.4049e-03)
# Name: AdaptiveMultimodalEvolutionaryStrategy
# Description: A hybrid evolutionary strategy employing adaptive mutation and a niching mechanism to efficiently explore multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.5  # Initial mutation strength
        self.niche_radius = 0.1  # Adjust to control niching strength

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        for i, fitness in enumerate(fitness_values):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = self.population[i]


        while self.eval_count < self.budget:
            offspring = self.generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Selection and Niching
            combined_population = np.vstack((self.population, offspring))
            combined_fitness = np.concatenate((fitness_values, offspring_fitness))

            sorted_indices = np.argsort(combined_fitness)
            
            new_population = []
            new_fitness = []
            for i in sorted_indices[:self.population_size]:
                is_unique = True
                for sol in new_population:
                  if np.linalg.norm(combined_population[i] - sol) < self.niche_radius:
                    is_unique = False
                    break
                if is_unique:
                  new_population.append(combined_population[i])
                  new_fitness.append(combined_fitness[i])

            #Adaptive Mutation
            best_ind = np.argmin(new_fitness)
            if len(new_fitness) > 0:
                self.sigma *= 1 + 0.1*(new_fitness[best_ind]-np.mean(new_fitness))

            self.population = np.array(new_population)
            fitness_values = np.array(new_fitness)


            for i, fitness in enumerate(fitness_values):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = self.population[i]

            self.sigma = max(0.01, min(self.sigma, 10)) # keep sigma in reasonable bounds

        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def generate_offspring(self):
        offspring = []
        for i in range(self.population_size):
            parent = self.population[i]
            mutation = np.random.normal(0, self.sigma, self.dim)
            child = np.clip(parent + mutation, self.lower_bounds, self.upper_bounds)
            offspring.append(child)
        return np.array(offspring)

```

### Rank 15 (Overall AOCC Score: 4.9004e-02 |             AOCC Score on Unimodal instances: 1.4663e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 3.8199e-04)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining differential evolution with adaptive mutation strength and a niching mechanism to escape local optima in multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.niche_radius = 0.1  # Adjust based on problem scale
        self.mutation_strength = 0.5  # Initial mutation strength, adaptively adjusted

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, self.fitness_values)


        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self._mutate(self.population[a], self.population[b], self.population[c])
                trial = self._crossover(self.population[i], mutant)
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds) # Ensure bounds
                fitness_trial = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if fitness_trial < self.fitness_values[i]:
                    new_population.append(trial)
                    self.fitness_values[i] = fitness_trial[0]
                else:
                    new_population.append(self.population[i])
                self.best_solution_overall, self.best_fitness_overall = self._update_best(np.array(new_population), self.fitness_values)

            self.population = np.array(new_population)
            self._adapt_mutation_strength() # Adaptive Mechanism

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _select_different(self, i):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == i or b == i or c == i:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _mutate(self, x_a, x_b, x_c):
        return x_a + self.F * (x_b - x_c)

    def _crossover(self, x, v):
        u = np.copy(x)
        for i in range(self.dim):
            if random.random() < self.CR:
                u[i] = v[i]
        return u

    def _update_best(self, population, fitnesses):
        best_index = np.argmin(fitnesses)
        best_solution = population[best_index]
        best_fitness = fitnesses[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness

    def _adapt_mutation_strength(self):
      #Simple adaptive mechanism: Reduce mutation strength if stuck in local optima
        if self.eval_count > self.budget * 0.7 and self.best_fitness_overall > 1e-4 : # Adjust thresholds as needed
            self.mutation_strength *= 0.9
        elif self.eval_count < self.budget * 0.3 and self.best_fitness_overall > 1e-4: #Increase for early exploration
            self.mutation_strength *= 1.1
        self.F = max(0.1, min(1, self.mutation_strength)) # Keep F within bounds

```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-22 22:03:20 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEAwithArchive vs (worst) AdaptiveMultimodalOptimizer, we see the archive mechanism significantly improves performance, particularly on multimodal problems.  The archive stores diverse solutions, preventing premature convergence. (second best) AdaptiveGaussianSamplingEA vs (second worst) AdaptiveDifferentialEvolutionWithEnhancedInitialization shows that adaptive Gaussian sampling is more effective than the enhanced initialization and local search strategy, especially in higher dimensional spaces, while also being simpler. Comparing (1st) vs (2nd), we see that adding an archive improves exploration and exploitation, especially for difficult multimodal problems. (3rd) AdaptiveGaussianSamplingEA vs (4th) AdaptiveGaussianMutationDE reveals that simpler mutation strategies (Gaussian) can outperform more complex ones (DE), especially when combined with adaptive sampling. Comparing (second worst) AdaptiveDifferentialEvolutionWithEnhancedInitialization vs (worst) AdaptiveMultimodalOptimizerImproved highlights the limitations of solely relying on local search and perturbation for high-dimensional multimodal problems;  a population-based approach like DE provides better global exploration.  Overall:  Adaptive mechanisms, archives, and Gaussian mutation consistently show superior performance compared to strategies heavily reliant on local search or complex DE variants. The archive in particular enhances exploration without drastically increasing computational cost.


**Experience:**

Prioritizing adaptive mechanisms and efficient exploration techniques is crucial.  Simple, well-integrated techniques are often superior to overly complex ones.  An archive to maintain diversity is highly beneficial for multimodal problems in high-dimensional spaces.

2025-06-22 22:03:21 INFO Full response text: * **Keywords:** Adaptive, efficient, diverse, simple, multimodal, high-dimensional.

* **Advice:** Focus on integrating simple, adaptive mechanisms for efficient exploration.  Maintain a diverse archive of solutions, especially for complex problem spaces.  Prioritize elegance and practicality over excessive complexity.

* **Avoid:** Overly complex algorithms,  techniques without clear benefits,  neglecting diversity in solutions.

* **Explanation:**  Simple, well-integrated heuristics are more likely to be robust and computationally efficient. A diverse archive allows the algorithm to escape local optima and explore a wider range of solutions in high-dimensional spaces, vital for multimodal problems.  Avoid unnecessary intricacy   effectiveness trumps sophistication.

2025-06-22 22:03:21 INFO Generating offspring via Crossover...
2025-06-22 22:03:28 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:03:33 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1629
2025-06-22 22:03:33 INFO FeHistory: [-333.98059425 -333.99235087 -334.00714921 ... -335.75242609 -335.76525135
 -335.77440953]
2025-06-22 22:03:33 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:03:33 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Size of the archive to maintain diversity
        self.archive = None
        self.archive_fitness = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.archive = self.population.copy()
        fitness_values = objective_function(self.population)
        self.archive_fitness = fitness_values.copy()
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            parents = self.tournament_selection(self.archive_fitness, k=5)
            offspring = self.gaussian_mutation(parents, self.sigma)
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Update archive
            self.archive = np.concatenate((self.archive, offspring))
            self.archive_fitness = np.concatenate((self.archive_fitness, offspring_fitness))

            #Maintain archive size
            sorted_archive_indices = np.argsort(self.archive_fitness)
            self.archive = self.archive[sorted_archive_indices[:self.archive_size]]
            self.archive_fitness = self.archive_fitness[sorted_archive_indices[:self.archive_size]]

            best_index = np.argmin(self.archive_fitness)
            if self.archive_fitness[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.archive[best_index]
                self.best_fitness_overall = self.archive_fitness[best_index]
            self.sigma *= 0.99


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.archive[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

2025-06-22 22:03:33 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:03:37 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0806
2025-06-22 22:03:37 INFO FeHistory: [165856.68137285 145423.40762612  86464.11822519 ...  -1002.32465892
  -1002.28008461  -1002.37887713]
2025-06-22 22:03:37 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:03:37 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:03:53 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.1274
2025-06-22 22:03:53 INFO FeHistory: [ 22.90748227  13.88137733  25.38369796 ... -99.52022197 -99.54786519
 -99.45454826]
2025-06-22 22:03:53 INFO Expected Optimum FE: -100
2025-06-22 22:03:53 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Size of the archive to maintain diversity
        self.archive = None
        self.archive_fitness = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.archive = self.population.copy()
        fitness_values = objective_function(self.population)
        self.archive_fitness = fitness_values.copy()
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            parents = self.tournament_selection(self.archive_fitness, k=5)
            offspring = self.gaussian_mutation(parents, self.sigma)
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Update archive
            self.archive = np.concatenate((self.archive, offspring))
            self.archive_fitness = np.concatenate((self.archive_fitness, offspring_fitness))

            #Maintain archive size
            sorted_archive_indices = np.argsort(self.archive_fitness)
            self.archive = self.archive[sorted_archive_indices[:self.archive_size]]
            self.archive_fitness = self.archive_fitness[sorted_archive_indices[:self.archive_size]]

            best_index = np.argmin(self.archive_fitness)
            if self.archive_fitness[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.archive[best_index]
                self.best_fitness_overall = self.archive_fitness[best_index]
            self.sigma *= 0.99


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.archive[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

2025-06-22 22:03:53 INFO Unimodal AOCC mean: 0.1629
2025-06-22 22:03:53 INFO Multimodal (single component) AOCC mean: 0.0806
2025-06-22 22:03:53 INFO Multimodal (multiple components) AOCC mean: 0.1274
2025-06-22 22:03:53 INFO AOCC mean: 0.1236
2025-06-22 22:04:02 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:04:02 ERROR Can not run the algorithm
2025-06-22 22:04:02 INFO Run function 5 complete. FEHistory len: 50, AOCC: 0.1470
2025-06-22 22:04:02 INFO FeHistory: [-333.97881137 -334.00267986 -334.02395884 -333.94988145 -333.96601673
 -333.99292255 -333.98853252 -334.0286268  -334.03639264 -333.93048085
 -333.96740311 -333.98511266 -333.97634013 -333.96959675 -333.99469409
 -334.03449341 -334.0313047  -333.97407618 -333.97247561 -334.1191503
 -334.05431346 -334.0536054  -333.97706504 -333.97536356 -333.97430034
 -334.02488039 -334.00647346 -334.00342628 -334.00795158 -334.09572361
 -333.92989    -333.86247689 -333.94387053 -333.94912617 -333.99709282
 -333.95642382 -334.08340574 -334.03610527 -333.9538076  -333.95843109
 -333.98299097 -333.94921181 -333.94217966 -333.98707391 -333.97729184
 -333.95893066 -333.97086184 -334.0330326  -333.92200352 -333.98717465]
2025-06-22 22:04:02 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:04:02 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveGaussianMutationEA
import numpy as np
import random

class ArchiveGuidedAdaptiveGaussianMutationEA:
    """
    An evolutionary algorithm combining an archive and adaptive Gaussian mutation 
    for efficient exploration and exploitation in multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 50
        self.archive_size = 100
        self.archive = []
        self.mutation_rate = 0.5 # Initial mutation rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        while self.eval_count < self.budget:
            fitness_values = objective_function(self.population)
            self.eval_count += self.population_size

            #Update best solution
            for i, fitness in enumerate(fitness_values):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = self.population[i]

            # Update archive
            self.update_archive(self.population, fitness_values)

            # Selection (tournament selection)
            parents = self.tournament_selection(self.population, fitness_values, tournament_size=5)

            # Recombination (intermediate recombination)
            offspring = self.intermediate_recombination(parents)

            # Mutation (adaptive Gaussian mutation)
            mutated_offspring = self.adaptive_gaussian_mutation(offspring)

            # Population update (Elitism)
            self.population = np.concatenate((parents, mutated_offspring))


        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, population, fitness_values, tournament_size):
        selected_indices = []
        for _ in range(self.population_size // 2):
            tournament = random.sample(range(len(population)), tournament_size)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_indices.append(winner_index)
        return population[selected_indices]

    def intermediate_recombination(self, parents):
        offspring = np.zeros_like(parents)
        for i in range(0, len(parents), 2):
            offspring[i] = (parents[i] + parents[i+1]) / 2
            offspring[i+1] = (parents[i] + parents[i+1]) / 2
        return offspring

    def adaptive_gaussian_mutation(self, offspring):
        #Adaptive mutation rate based on population diversity
        std_dev = np.std(offspring)
        mutation_rate = max(0.01, self.mutation_rate * (1 - 0.01*std_dev) )
        mutated_offspring = offspring + np.random.normal(0, mutation_rate * (self.upper_bounds - self.lower_bounds), size=offspring.shape)
        mutated_offspring = np.clip(mutated_offspring, self.lower_bounds, self.upper_bounds)
        return mutated_offspring

    def update_archive(self, population, fitness_values):
        for i in range(len(population)):
            if len(self.archive) < self.archive_size:
                self.archive.append((population[i], fitness_values[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if fitness_values[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (population[i], fitness_values[i])

2025-06-22 22:04:02 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:04:02 ERROR Can not run the algorithm
2025-06-22 22:04:02 INFO Run function 12 complete. FEHistory len: 50, AOCC: 0.0000
2025-06-22 22:04:02 INFO FeHistory: [224952.02380584 229218.20204474 240267.6717652  147212.48555123
 340042.8444734  108681.01570964 162181.32883831 210570.4503272
 301201.01407756 161581.94315603 146064.47740317 206739.85286681
 295837.07700492 336863.03151272 190423.41764094 109337.01330574
 334252.24147609 150224.35681983 270979.71185876 212736.95584064
 190986.71871427 217477.70414088 157217.60135535 204793.1054869
 287686.58398893 216617.64505426 148119.99448194 246745.80991722
 274219.96769393 208873.91032927 255217.72745719 183382.01442757
 223947.61344272 216225.67452097 317204.86746872 285322.37477323
 174282.45310767 191215.47408574 123762.63742202 117705.42643934
 194234.77340078 218084.09761817 158490.8872965  493021.48431721
 261172.25413192 313758.4018507  185122.51192583 283582.11667761
 351937.48122731 243731.62821484]
2025-06-22 22:04:02 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:04:02 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:04:02 ERROR Can not run the algorithm
2025-06-22 22:04:03 INFO Run function 23 complete. FEHistory len: 50, AOCC: 0.0000
2025-06-22 22:04:03 INFO FeHistory: [34.35060593 30.18911745 31.02505733 26.60583341 32.21399882 30.41897526
  7.17704068 39.83414524 39.685673   25.2903158   4.1154522  29.96322219
 39.15199829 19.65691474 38.03172175 22.74284601 20.90553438  1.76713098
 15.77346503 17.04951644 26.50866553  8.14342125 44.3981432  34.48802659
 33.68685087 32.52775023 42.36922219 38.00232573 21.73624598 18.24034843
 20.20514059 10.57217195 43.85552646 21.42982134 16.9735673  23.46208997
 26.554723   13.27438241 14.89685294 49.3227958  11.76475699 33.57309672
 29.55522354 27.8684263  25.02102894 24.96799032 27.19776097 28.74644145
 30.72820348 41.41525305]
2025-06-22 22:04:03 INFO Expected Optimum FE: -100
2025-06-22 22:04:03 INFO Unimodal AOCC mean: 0.1470
2025-06-22 22:04:03 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 22:04:03 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 22:04:03 INFO AOCC mean: 0.0490
2025-06-22 22:04:10 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:04:15 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1749
2025-06-22 22:04:15 INFO FeHistory: [-333.96132526 -333.98833774 -333.94675344 ... -336.30994142 -336.30991045
 -336.30990856]
2025-06-22 22:04:15 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:04:15 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 100 # Size of the archive
        self.archive = None
        self.archive_fitness = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.archive = self.population.copy()
        self.archive_fitness = fitness_values.copy()

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness_values)


        while self.eval_count < self.budget:
            #Adaptive Gaussian Sampling from diverse archive
            parents = self.tournament_selection(self.archive_fitness, k=5)
            offspring = self.gaussian_mutation(parents, self.sigma)
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Update archive
            self.archive = np.concatenate((self.archive, offspring))
            self.archive_fitness = np.concatenate((self.archive_fitness, offspring_fitness))
            self._manage_archive()

            #Update best solution
            best_solution, best_fitness = self._find_best(self.archive, self.archive_fitness)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            
            self.sigma *= 0.99 #Decay sigma


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _manage_archive(self):
        # Maintain archive size by removing worst solutions.
        sorted_indices = np.argsort(self.archive_fitness)
        self.archive = self.archive[sorted_indices[:self.archive_size]]
        self.archive_fitness = self.archive_fitness[sorted_indices[:self.archive_size]]

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.archive[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-22 22:04:15 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:04:19 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.3398
2025-06-22 22:04:19 INFO FeHistory: [252204.05997257 266545.26178705 259850.6041049  ...  -1002.47907854
  -1002.47907843  -1002.47907855]
2025-06-22 22:04:19 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:04:19 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 100 # Size of the archive
        self.archive = None
        self.archive_fitness = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.archive = self.population.copy()
        self.archive_fitness = fitness_values.copy()

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness_values)


        while self.eval_count < self.budget:
            #Adaptive Gaussian Sampling from diverse archive
            parents = self.tournament_selection(self.archive_fitness, k=5)
            offspring = self.gaussian_mutation(parents, self.sigma)
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Update archive
            self.archive = np.concatenate((self.archive, offspring))
            self.archive_fitness = np.concatenate((self.archive_fitness, offspring_fitness))
            self._manage_archive()

            #Update best solution
            best_solution, best_fitness = self._find_best(self.archive, self.archive_fitness)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            
            self.sigma *= 0.99 #Decay sigma


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _manage_archive(self):
        # Maintain archive size by removing worst solutions.
        sorted_indices = np.argsort(self.archive_fitness)
        self.archive = self.archive[sorted_indices[:self.archive_size]]
        self.archive_fitness = self.archive_fitness[sorted_indices[:self.archive_size]]

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.archive[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-22 22:04:19 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:04:36 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2445
2025-06-22 22:04:36 INFO FeHistory: [ 22.47564675  17.1832459   21.48352735 ... -99.99797788 -99.99774474
 -99.99754339]
2025-06-22 22:04:36 INFO Expected Optimum FE: -100
2025-06-22 22:04:36 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 100 # Size of the archive
        self.archive = None
        self.archive_fitness = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.archive = self.population.copy()
        self.archive_fitness = fitness_values.copy()

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness_values)


        while self.eval_count < self.budget:
            #Adaptive Gaussian Sampling from diverse archive
            parents = self.tournament_selection(self.archive_fitness, k=5)
            offspring = self.gaussian_mutation(parents, self.sigma)
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Update archive
            self.archive = np.concatenate((self.archive, offspring))
            self.archive_fitness = np.concatenate((self.archive_fitness, offspring_fitness))
            self._manage_archive()

            #Update best solution
            best_solution, best_fitness = self._find_best(self.archive, self.archive_fitness)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            
            self.sigma *= 0.99 #Decay sigma


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _manage_archive(self):
        # Maintain archive size by removing worst solutions.
        sorted_indices = np.argsort(self.archive_fitness)
        self.archive = self.archive[sorted_indices[:self.archive_size]]
        self.archive_fitness = self.archive_fitness[sorted_indices[:self.archive_size]]

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.archive[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-22 22:04:36 INFO Unimodal AOCC mean: 0.1749
2025-06-22 22:04:36 INFO Multimodal (single component) AOCC mean: 0.3398
2025-06-22 22:04:36 INFO Multimodal (multiple components) AOCC mean: 0.2445
2025-06-22 22:04:36 INFO AOCC mean: 0.2531
2025-06-22 22:04:46 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:05:46 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 22:07:26 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1767
2025-06-22 22:07:26 INFO FeHistory: [-334.1129523  -334.0313602  -334.02422885 ... -336.32983453 -336.32982848
 -336.32983038]
2025-06-22 22:07:26 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:07:26 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA
# Description: Adaptive Gaussian EA with archive for robust multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian mutation with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs tournament selection for efficient parent selection.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initialize sigma adaptively
        self.sigma_decay = 0.99
        self.archive = []
        self.tournament_size = 5


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
            self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
            self.eval_count += 1
        else:
            self.best_solution_overall = np.array([])
            self.best_fitness_overall = 0


        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay  #Adaptive Sigma reduction

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 22:07:26 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:10:13 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.3754
2025-06-22 22:10:13 INFO FeHistory: [162537.73208438 112672.71590461 229653.1378247  ...  -1002.47907861
  -1002.47907856  -1002.47907864]
2025-06-22 22:10:13 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:10:13 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA
# Description: Adaptive Gaussian EA with archive for robust multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian mutation with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs tournament selection for efficient parent selection.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initialize sigma adaptively
        self.sigma_decay = 0.99
        self.archive = []
        self.tournament_size = 5


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
            self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
            self.eval_count += 1
        else:
            self.best_solution_overall = np.array([])
            self.best_fitness_overall = 0


        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay  #Adaptive Sigma reduction

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 22:10:13 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:13:01 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2609
2025-06-22 22:13:01 INFO FeHistory: [ 23.19632088  29.62696947  54.51991437 ... -99.9984016  -99.99863963
 -99.99857222]
2025-06-22 22:13:01 INFO Expected Optimum FE: -100
2025-06-22 22:13:01 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA
# Description: Adaptive Gaussian EA with archive for robust multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian mutation with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs tournament selection for efficient parent selection.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initialize sigma adaptively
        self.sigma_decay = 0.99
        self.archive = []
        self.tournament_size = 5


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
            self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
            self.eval_count += 1
        else:
            self.best_solution_overall = np.array([])
            self.best_fitness_overall = 0


        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay  #Adaptive Sigma reduction

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 22:13:01 INFO Unimodal AOCC mean: 0.1767
2025-06-22 22:13:01 INFO Multimodal (single component) AOCC mean: 0.3754
2025-06-22 22:13:01 INFO Multimodal (multiple components) AOCC mean: 0.2609
2025-06-22 22:13:01 INFO AOCC mean: 0.2710
2025-06-22 22:13:11 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:13:11 ERROR Can not run the algorithm
2025-06-22 22:13:11 INFO Run function 5 complete. FEHistory len: 150, AOCC: 0.1464
2025-06-22 22:13:11 INFO FeHistory: [-333.96579552 -333.89831743 -333.96772192 -334.0125688  -333.95643565
 -333.95939068 -334.0335882  -334.0546547  -334.01838252 -334.02068985
 -334.02125464 -333.98067384 -333.94846948 -334.00841422 -333.98630341
 -333.99727604 -333.9811523  -333.99455077 -334.02554953 -333.94279037
 -333.96088759 -334.01351304 -333.98440053 -334.01563115 -333.97948834
 -334.03779035 -333.96295916 -334.07330649 -333.97221757 -333.96783115
 -333.98514698 -334.00769662 -333.98639158 -333.93156674 -333.94761444
 -333.96201856 -333.98730358 -333.99490283 -333.9269532  -333.93688453
 -333.94393821 -333.96406989 -333.9414476  -333.94833922 -333.97954697
 -333.98105975 -334.06945402 -334.00359292 -334.06399471 -333.98194652
 -333.96634124 -333.89825039 -333.96766208 -334.01215209 -333.9568433
 -333.95911005 -334.03323371 -334.05465269 -334.01850455 -334.02123916
 -334.02100908 -333.98125556 -333.94835006 -334.00818313 -333.98640999
 -333.99717163 -333.98145611 -333.9945711  -334.02515634 -333.94318615
 -333.96043655 -334.01306376 -333.98453657 -334.01424933 -333.97958739
 -334.03745861 -333.96288876 -334.07382263 -333.97179119 -333.96738706
 -333.98504044 -334.00824934 -333.98696458 -333.93142229 -333.94721626
 -333.9624783  -333.9865249  -333.99450484 -333.92578839 -333.93645192
 -333.94381073 -333.96434402 -333.94081802 -333.94880276 -333.97992449
 -333.98109413 -334.07047002 -334.00465236 -334.06332325 -333.98290724
 -334.07382263 -334.07330649 -334.07047002 -334.06945402 -334.06399471
 -334.06332325 -334.0546547  -334.05465269 -334.03779035 -334.03745861
 -334.0335882  -334.03323371 -334.02554953 -334.02515634 -334.02125464
 -334.02123916 -334.02100908 -334.02068985 -334.01850455 -334.01838252
 -334.01563115 -334.01424933 -334.01351304 -334.01306376 -334.0125688
 -334.01215209 -334.00841422 -334.00824934 -334.00818313 -334.00769662
 -334.00465236 -334.00359292 -333.99727604 -333.99717163 -333.99490283
 -333.9945711  -333.99455077 -333.99450484 -333.98730358 -333.98696458
 -333.9865249  -333.98640999 -333.98639158 -333.98630341 -333.98514698
 -333.98504044 -333.98453657 -333.98440053 -333.98290724 -333.98194652]
2025-06-22 22:13:11 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:13:11 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian mutation with an archive to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 50
        self.archive_size = 100
        self.archive = []
        self.mutation_rate = 0.5 # Initial mutation rate
        self.mutation_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(self.population)
            self.eval_count += len(self.population)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

            self._update_archive(self.population, fitness_values) #Update archive
            self.mutation_rate *= self.mutation_decay #Adaptive Mutation


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = []
        for i in range(self.population_size):
            # Sample parent from population or archive for diversity
            if random.random() < 0.5 and len(self.archive) > 0:
                parent = random.choice(self.archive)
            else:
                parent = self.population[i]

            offspring_solution = parent + np.random.normal(0, self.mutation_rate, self.dim)
            offspring_solution = np.clip(offspring_solution, self.lower_bounds, self.upper_bounds)
            offspring.append(offspring_solution)
        return np.array(offspring)

    def _selection(self, parents, parent_fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((parents, offspring))
        combined_fitness = np.concatenate((parent_fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        selected_indices = indices[:self.population_size]
        selected_population = combined_population[selected_indices]
        selected_fitness = combined_fitness[selected_indices]
        best_solution, best_fitness = self._update_best(selected_population, selected_fitness)
        return selected_population, best_solution, best_fitness

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness

    def _update_archive(self, population, fitness_values):
        for i in range(len(population)):
            solution = population[i]
            fitness = fitness_values[i]
            self._add_to_archive(solution, fitness)

    def _add_to_archive(self, solution, fitness):
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_solution, worst_fitness = max(self.archive, key=lambda item: item[1])
            if fitness < worst_fitness:
                self.archive.remove((worst_solution, worst_fitness))
                self.archive.append((solution, fitness))

2025-06-22 22:13:11 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:13:11 ERROR Can not run the algorithm
2025-06-22 22:13:11 INFO Run function 12 complete. FEHistory len: 150, AOCC: 0.0000
2025-06-22 22:13:11 INFO FeHistory: [302394.30867632 195632.78751134 257598.88461778 302437.57579183
 257945.46555841 190689.53696224 163107.56313126 178873.30007899
 186921.87389738 294300.10856074 300222.49570494 141137.06696848
 202706.86288023 120328.78868402 528596.30352723 168826.51239187
 118976.8107248  549614.00131896 296015.8570328  227694.66105577
 183353.2359871  244736.40247612 163419.83443884 104499.43267947
 185027.40657007 263239.78153914 144676.9442068  127901.02562958
 168468.01143417 258608.07092348 422022.24570934 271021.1147493
 105100.41806551 130914.93822282 194763.58922931 264135.01532329
 294301.60130368 268577.36333955 368898.50192885 131124.79809713
 182169.15130094 233804.11841085 283907.69892148 193889.73699293
 510310.73836255 189777.89218148 359257.76427152 280021.13143147
 215777.53834802 151120.95985253 302567.6599369  193666.77783564
 258821.5289396  305813.02630725 254159.35339308 193945.95155322
 172755.2689691  180695.04884484 188439.24581191 297265.40962686
 300780.4059489  145124.88238227 202149.6051374  120923.73320574
 535992.4254261  164785.94424627 116213.56909177 550034.79513022
 286323.58365881 227439.54759027 182038.78762545 242899.71725103
 171000.79937203 104215.86126761 186219.67922772 263545.37703089
 147756.74941512 126465.58438134 167616.87169056 265797.62625842
 424377.63158629 269774.19437027 104694.45530229 139169.59428276
 190364.71341519 261937.44772819 289386.51189368 269853.57318094
 378595.01688579 132517.63832193 180162.92898245 226014.20389123
 282382.01395196 187536.84919601 511012.15366803 182830.56029428
 366929.35390834 261413.87809238 213093.283316   150461.89892784
 104215.86126761 104499.43267947 104694.45530229 105100.41806551
 116213.56909177 118976.8107248  120328.78868402 120923.73320574
 126465.58438134 127901.02562958 130914.93822282 131124.79809713
 132517.63832193 139169.59428276 141137.06696848 144676.9442068
 145124.88238227 147756.74941512 150461.89892784 151120.95985253
 163107.56313126 163419.83443884 164785.94424627 167616.87169056
 168468.01143417 168826.51239187 171000.79937203 172755.2689691
 178873.30007899 180162.92898245 180695.04884484 182038.78762545
 182169.15130094 182830.56029428 183353.2359871  185027.40657007
 186219.67922772 186921.87389738 187536.84919601 188439.24581191
 189777.89218148 190364.71341519 190689.53696224 193666.77783564
 193889.73699293 193945.95155322 194763.58922931 195632.78751134
 202149.6051374  202706.86288023]
2025-06-22 22:13:11 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:13:11 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:13:11 ERROR Can not run the algorithm
2025-06-22 22:13:12 INFO Run function 23 complete. FEHistory len: 150, AOCC: 0.0001
2025-06-22 22:13:12 INFO FeHistory: [41.05089179 32.61195712 19.98746276 26.23575953 38.40120757 19.51025806
  4.38670026 26.43155197 35.91053853 12.85691855 18.98238012  5.89806302
 33.82046232 33.09830931  3.4917167  38.18894122 33.29337665 20.64112968
 38.09655658 19.79691148 16.58253069 32.05276295 22.53692119 30.15137724
 30.60688746 17.39339996 24.12160072 25.18204368 28.70135681 32.28660756
 26.74004829  9.44253234 19.96965317 39.36818002 30.47939716 43.59933369
 11.64148958 44.50413034 22.51868659 29.77042422 26.791536   43.95746609
 15.0990546  30.29131499 25.98322553 33.24004341 24.72829046  1.8135252
 30.2893976  16.62803841 43.22479728 31.93488981 19.25332868 27.75070601
 42.80035261 22.5954235   2.46087337 26.53680793 37.42683336  8.25343906
 19.61080304  2.32055229 36.15295505 34.46662748  4.20987976 40.31858476
 35.7738255  21.71748918 39.50606944 19.60762138 15.38553868 34.35344594
 19.43850343 29.5224959  34.52850523 17.78233728 24.74622854 25.80006943
 30.92726666 29.80682451 26.23807008 10.65740764 20.03564011 29.19893611
 30.18671603 39.52863683  6.37646722 45.35080945 20.52907766 33.92870594
 24.75327776 45.9362358  15.35384142 30.81407123 24.71561318 32.91755141
 21.05254973 -0.30903816 28.10773455 16.72180557 -0.30903816  1.8135252
  2.32055229  2.46087337  3.4917167   4.20987976  4.38670026  5.89806302
  6.37646722  8.25343906  9.44253234 10.65740764 11.64148958 12.85691855
 15.0990546  15.35384142 15.38553868 16.58253069 16.62803841 16.72180557
 17.39339996 17.78233728 18.98238012 19.25332868 19.43850343 19.51025806
 19.60762138 19.61080304 19.79691148 19.96965317 19.98746276 20.03564011
 20.52907766 20.64112968 21.05254973 21.71748918 22.51868659 22.53692119
 22.5954235  24.12160072 24.71561318 24.72829046 24.74622854 24.75327776
 25.18204368 25.80006943 25.98322553 26.23575953 26.23807008 26.43155197]
2025-06-22 22:13:12 INFO Expected Optimum FE: -100
2025-06-22 22:13:12 INFO Unimodal AOCC mean: 0.1464
2025-06-22 22:13:12 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 22:13:12 INFO Multimodal (multiple components) AOCC mean: 0.0001
2025-06-22 22:13:12 INFO AOCC mean: 0.0488
2025-06-22 22:13:22 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:14:22 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 22:14:57 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1807
2025-06-22 22:14:57 INFO FeHistory: [-333.97004709 -333.97481393 -333.943758   ... -336.22256005 -336.22256005
 -336.22256005]
2025-06-22 22:14:57 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:14:57 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for efficient multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-22 22:14:57 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:16:34 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.6530
2025-06-22 22:16:34 INFO FeHistory: [166852.85243311 257368.53761998 415529.68814152 ...  -1002.4790787
  -1002.4790787   -1002.4790787 ]
2025-06-22 22:16:34 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:16:34 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for efficient multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-22 22:16:34 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:18:17 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.4736
2025-06-22 22:18:17 INFO FeHistory: [ 43.46034942  54.04532458  52.29005474 ... -99.99999996 -99.99999996
 -99.99999996]
2025-06-22 22:18:17 INFO Expected Optimum FE: -100
2025-06-22 22:18:17 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for efficient multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-22 22:18:17 INFO Unimodal AOCC mean: 0.1807
2025-06-22 22:18:17 INFO Multimodal (single component) AOCC mean: 0.6530
2025-06-22 22:18:17 INFO Multimodal (multiple components) AOCC mean: 0.4736
2025-06-22 22:18:17 INFO AOCC mean: 0.4357
2025-06-22 22:18:25 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:18:30 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1527
2025-06-22 22:18:30 INFO FeHistory: [-333.99558778 -334.01019148 -334.01339675 ... -334.64277096 -334.6172337
 -334.57266841]
2025-06-22 22:18:30 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:18:30 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Adaptive Gaussian sampling EA enhanced with an archive to maintain diversity and escape local optima in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  # Size of the archive
        self.archive = None
        self.archive_fitness = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial sigma

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.archive = np.empty((0, self.dim))
        self.archive_fitness = np.array([])

        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Selection
            parents = self.tournament_selection(fitness_values, k=5)

            # Gaussian Mutation
            offspring = self.gaussian_mutation(parents, self.sigma)
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Archive Update
            self.update_archive(offspring, offspring_fitness)
            
            # Combine Population and Archive (for selection)
            combined_population = np.vstack((self.population, self.archive))
            combined_fitness = np.concatenate((fitness_values, self.archive_fitness))

            # Elitism
            sorted_indices = np.argsort(combined_fitness)
            self.population = combined_population[sorted_indices][:self.population_size]
            fitness_values = combined_fitness[sorted_indices][:self.population_size]

            # Update Best Solution
            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            #Adaptive Sigma (reduce exploration over time)
            self.sigma *= 0.999

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive = np.vstack((self.archive, offspring[i]))
                self.archive_fitness = np.append(self.archive_fitness, offspring_fitness[i])
            else:
                worst_index = np.argmax(self.archive_fitness)
                if offspring_fitness[i] < self.archive_fitness[worst_index]:
                    self.archive[worst_index] = offspring[i]
                    self.archive_fitness[worst_index] = offspring_fitness[i]
2025-06-22 22:18:30 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:18:35 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 22:18:35 INFO FeHistory: [215223.01403657 347688.7967982  137896.7645851  ...  13013.72079811
  22838.79374971  20173.58318724]
2025-06-22 22:18:35 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:18:35 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:18:51 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0411
2025-06-22 22:18:51 INFO FeHistory: [ 23.43617445  23.59447274  30.6612902  ... -43.01334379 -39.08873423
 -45.09103188]
2025-06-22 22:18:51 INFO Expected Optimum FE: -100
2025-06-22 22:18:51 INFO Unimodal AOCC mean: 0.1527
2025-06-22 22:18:51 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 22:18:51 INFO Multimodal (multiple components) AOCC mean: 0.0411
2025-06-22 22:18:51 INFO AOCC mean: 0.0646
2025-06-22 22:18:59 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:19:04 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1488
2025-06-22 22:19:04 INFO FeHistory: [-334.00965917 -333.98935398 -333.99296301 ... -334.3481409  -334.34926311
 -334.34942586]
2025-06-22 22:19:04 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:19:04 INFO Good algorithm:
Algorithm Name: AdaptiveArchiveGuidedGaussianMutation
import numpy as np
import random

# Name: AdaptiveArchiveGuidedGaussianMutation
# Description: Combines adaptive Gaussian mutation with an archive for efficient multimodal optimization.
# Code:
class AdaptiveArchiveGuidedGaussianMutation:
    """
    Combines adaptive Gaussian mutation with an archive to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # Heuristic for population size
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.sigma = 0.5  # Initial Gaussian mutation strength

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            self.update_archive(offspring, offspring_fitness)

            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)

            #Adapt mutation strength
            self.sigma = self.sigma * (1 + 0.1 * np.random.randn()) #Adaptive sigma
            self.sigma = np.clip(self.sigma, 0.1, 1.5) #Keep sigma in reasonable range

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive),
            'final_sigma': self.sigma
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        for i in range(self.population_size):
            #Gaussian Mutation with adaptive sigma
            mutation = np.random.normal(0, self.sigma, self.dim)
            offspring[i] = population[i] + mutation
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

2025-06-22 22:19:04 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:19:08 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 22:19:08 INFO FeHistory: [159675.54092663 330673.51852889 238552.3113716  ...  23393.30164389
  23271.6648856   22644.15034644]
2025-06-22 22:19:08 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:19:08 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:19:24 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0151
2025-06-22 22:19:24 INFO FeHistory: [ 27.03882813  46.94823503  27.17945994 ... -30.68330569 -30.61603137
 -30.77884457]
2025-06-22 22:19:24 INFO Expected Optimum FE: -100
2025-06-22 22:19:24 INFO Unimodal AOCC mean: 0.1488
2025-06-22 22:19:24 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 22:19:24 INFO Multimodal (multiple components) AOCC mean: 0.0151
2025-06-22 22:19:24 INFO AOCC mean: 0.0546
2025-06-22 22:19:36 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:20:36 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 22:22:16 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1541
2025-06-22 22:22:16 INFO FeHistory: [-334.07591421 -333.97135702 -333.89699449 ... -334.40193193 -334.49882634
 -334.51496172]
2025-06-22 22:22:16 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:22:16 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.initial_sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma = self.initial_sigma.copy() #Using a copy to avoid modifying original
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )
            self._update_best(offspring, offspring_fitness)
            self._adapt_sigma() #Adaptive sigma adjustment

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_sigma(self):
        #Adaptive sigma adjustment based on exploration/exploitation balance
        if self.eval_count > self.budget * 0.7 and self.best_fitness_overall > 1e-4: #Reduce sigma if stuck in local optima
            self.sigma *= 0.95
        elif self.eval_count < self.budget * 0.3 and self.best_fitness_overall > 1e-4: #Increase early exploration
            self.sigma *= 1.05
        self.sigma = np.clip(self.sigma, 0.01 * self.initial_sigma, 10 * self.initial_sigma) #Keep sigma within reasonable bounds

2025-06-22 22:22:16 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:25:06 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 22:25:06 INFO FeHistory: [ 9.86608096e+04  2.52940010e+05  3.30144563e+05 ... -1.17933506e+02
 -3.58845343e+02 -4.52378496e+02]
2025-06-22 22:25:06 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:25:06 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:28:35 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0515
2025-06-22 22:28:35 INFO FeHistory: [ 17.53126015  35.61377771  32.95894852 ... -45.77172137 -45.99882779
 -52.03836065]
2025-06-22 22:28:35 INFO Expected Optimum FE: -100
2025-06-22 22:28:35 INFO Unimodal AOCC mean: 0.1541
2025-06-22 22:28:35 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 22:28:35 INFO Multimodal (multiple components) AOCC mean: 0.0515
2025-06-22 22:28:35 INFO AOCC mean: 0.0686
2025-06-22 22:28:44 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:28:51 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1503
2025-06-22 22:28:51 INFO FeHistory: [-334.01338807 -333.9470882  -333.98651423 ... -334.38842397 -334.38842693
 -334.38842385]
2025-06-22 22:28:51 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:28:51 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: An evolutionary strategy using adaptive Gaussian sampling and an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingEAwithArchive:
    """
    An evolutionary strategy combining adaptive Gaussian sampling and an archive 
    to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Size of the archive
        self.archive = []
        self.sigma = 0.5 #Initial standard deviation for Gaussian sampling
        self.sigma_decay = 0.99 # Decay factor for sigma

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        self.fitness = np.full(self.population_size, np.inf)
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        while self.eval_count < self.budget:
            #Evaluate Population
            for i in range(self.population_size):
                if np.isinf(self.fitness[i]):
                    self.fitness[i] = objective_function(self.population[i:i+1])[0]
                    self.eval_count +=1

            #Update best solution
            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_index]
                self.best_solution_overall = self.population[best_index].copy()

            #Update Archive
            for i in range(self.population_size):
                if len(self.archive) < self.archive_size:
                    self.archive.append((self.population[i], self.fitness[i]))
                else:
                    worst_index = np.argmax([f for _,f in self.archive])
                    if self.fitness[i] < self.archive[worst_index][1]:
                        self.archive[worst_index] = (self.population[i], self.fitness[i])

            #Selection (Tournament Selection from archive + population)
            parents = []
            candidates = list(self.population) + [x for x,_ in self.archive]
            fitnesses = list(self.fitness) + [f for _,f in self.archive]
            for _ in range(self.population_size//2):
                tournament = random.sample(range(len(candidates)),5)
                winner1 = min(tournament, key=lambda i: fitnesses[i])
                tournament = random.sample(range(len(candidates)),5)
                winner2 = min(tournament, key=lambda i: fitnesses[i])
                parents.extend([winner1,winner2])

            #Recombination (Simple Averaging)
            offspring = np.zeros((self.population_size, self.dim))
            for i in range(0, self.population_size, 2):
                offspring[i] = (candidates[parents[i]] + candidates[parents[i+1]])/2
                offspring[i+1] = (candidates[parents[i]] + candidates[parents[i+1]])/2

            #Mutation (Adaptive Gaussian)
            for i in range(self.population_size):
                mutation = np.random.normal(0, self.sigma, self.dim)
                offspring[i] = np.clip(offspring[i] + mutation, self.lower_bounds, self.upper_bounds)

            self.population = offspring.copy()
            self.fitness = np.full(self.population_size, np.inf)
            self.sigma *= self.sigma_decay #decay sigma over time


        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
2025-06-22 22:28:51 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:28:58 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 22:28:58 INFO FeHistory: [396952.14841639 139869.21756989 107961.48828076 ...  19568.96629644
  19568.96670718  19568.96650817]
2025-06-22 22:28:58 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:28:58 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:29:20 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0390
2025-06-22 22:29:20 INFO FeHistory: [ 34.49820318  12.02215155  45.29245589 ... -59.7886871  -59.78868732
 -59.78868662]
2025-06-22 22:29:20 INFO Expected Optimum FE: -100
2025-06-22 22:29:20 INFO Unimodal AOCC mean: 0.1503
2025-06-22 22:29:20 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 22:29:20 INFO Multimodal (multiple components) AOCC mean: 0.0390
2025-06-22 22:29:20 INFO AOCC mean: 0.0631
2025-06-22 22:29:20 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

### Better code
AdaptiveMultimodalEvolutionaryStrategy
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  #Adjust as needed
        self.population = None
        self.fitness = None
        self.mutation_rate = 0.1  #Initial mutation rate
        self.diversity_threshold = 0.8 #Parameter for diversity check


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        self.fitness = np.full(self.population_size, np.inf)
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')


        while self.eval_count < self.budget:
            # Evaluate population
            for i in range(self.population_size):
                if np.isinf(self.fitness[i]): #only evaluate if not already evaluated
                    self.fitness[i] = objective_function(self.population[i:i+1])[0]
                    self.eval_count += 1

            # Update best solution
            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_index]
                self.best_solution_overall = self.population[best_index].copy()


            # Selection (tournament selection)
            parents = []
            for _ in range(self.population_size // 2):
                tournament = random.sample(range(self.population_size), 5) #tournament size 5
                winner1 = min(tournament, key=lambda i: self.fitness[i])
                tournament = random.sample(range(self.population_size), 5)
                winner2 = min(tournament, key=lambda i: self.fitness[i])
                parents.extend([winner1, winner2])

            #Recombination (simple averaging)
            offspring = np.zeros((self.population_size, self.dim))
            for i in range(0, self.population_size, 2):
                offspring[i] = (self.population[parents[i]] + self.population[parents[i+1]]) / 2
                offspring[i+1] = (self.population[parents[i]] + self.population[parents[i+1]]) / 2

            # Mutation (adaptive)
            for i in range(self.population_size):
                mutation = np.random.normal(0, self.mutation_rate * (self.upper_bounds - self.lower_bounds), self.dim)
                offspring[i] = np.clip(offspring[i] + mutation, self.lower_bounds, self.upper_bounds)

            #Diversity Check and adjustment
            diversity = np.std(offspring)
            if diversity < self.diversity_threshold * np.std(self.population):
                #inject some diversity if needed. Replace a portion of the offspring with random solutions
                num_random = int(0.2 * self.population_size)
                random_indices = random.sample(range(self.population_size), num_random)
                offspring[random_indices] = np.random.uniform(self.lower_bounds, self.upper_bounds, (num_random, self.dim))
                self.mutation_rate *= 1.1 #increase mutation if low diversity


            self.population = offspring.copy()
            self.fitness = np.full(self.population_size, np.inf) #reset fitness for new population

        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


### Worse code
AdaptiveMultimodalOptimizer
import numpy as np
from scipy.optimize import minimize

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Differential Evolution crossover rate
        self.mutation_rate_decay = 0.95 # Decay factor for mutation rate

        self.initial_mutation_rate = 0.2 # Initialize with larger rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = float('inf')
        
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness = objective_function(population)
        self.eval_count += self.population_size

        current_mutation_rate = self.initial_mutation_rate

        while self.eval_count < self.budget:
            # Differential Evolution
            offspring = np.copy(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds) #Bounds handling

                trial = np.copy(population[i])
                jrand = np.random.randint(0, self.dim)
                for j in range(self.dim):
                    if np.random.rand() < self.CR or j == jrand:
                        trial[j] = mutant[j]
                
                #Add adaptive mutation to escape local optima
                trial = trial + current_mutation_rate * np.random.normal(0,1, self.dim)
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds)

                trial_fitness = objective_function(trial.reshape(1,-1))
                self.eval_count += 1
                if trial_fitness < fitness[i]:
                    offspring[i] = trial
                    fitness[i] = trial_fitness

            population = offspring


            # Local Search (optional - improves convergence speed for unimodal)
            best_index = np.argmin(fitness)
            best_solution = population[best_index]
            res = minimize(lambda x: objective_function(x.reshape(1,-1)), best_solution, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
            local_search_solution = res.x
            local_search_fitness = res.fun
            self.eval_count += res.nfev
            if local_search_fitness < fitness[best_index]:
                population[best_index] = local_search_solution
                fitness[best_index] = local_search_fitness

            # Update overall best
            best_index_overall = np.argmin(fitness)
            if fitness[best_index_overall] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[best_index_overall]
                self.best_solution_overall = population[best_index_overall]

            current_mutation_rate *= self.mutation_rate_decay #Decay mutation rate over time


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEAwithArchive vs (worst) AdaptiveMultimodalOptimizer, we see the archive mechanism significantly improves performance, particularly on multimodal problems.  The archive stores diverse solutions, preventing premature convergence. (second best) AdaptiveGaussianSamplingEA vs (second worst) AdaptiveDifferentialEvolutionWithEnhancedInitialization shows that adaptive Gaussian sampling is more effective than the enhanced initialization and local search strategy, especially in higher dimensional spaces, while also being simpler. Comparing (1st) vs (2nd), we see that adding an archive improves exploration and exploitation, especially for difficult multimodal problems. (3rd) AdaptiveGaussianSamplingEA vs (4th) AdaptiveGaussianMutationDE reveals that simpler mutation strategies (Gaussian) can outperform more complex ones (DE), especially when combined with adaptive sampling. Comparing (second worst) AdaptiveDifferentialEvolutionWithEnhancedInitialization vs (worst) AdaptiveMultimodalOptimizerImproved highlights the limitations of solely relying on local search and perturbation for high-dimensional multimodal problems;  a population-based approach like DE provides better global exploration.  Overall:  Adaptive mechanisms, archives, and Gaussian mutation consistently show superior performance compared to strategies heavily reliant on local search or complex DE variants. The archive in particular enhances exploration without drastically increasing computational cost.
- * **Keywords:** Adaptive, efficient, diverse, simple, multimodal, high-dimensional.

* **Advice:** Focus on integrating simple, adaptive mechanisms for efficient exploration.  Maintain a diverse archive of solutions, especially for complex problem spaces.  Prioritize elegance and practicality over excessive complexity.

* **Avoid:** Overly complex algorithms,  techniques without clear benefits,  neglecting diversity in solutions.

* **Explanation:**  Simple, well-integrated heuristics are more likely to be robust and computationally efficient. A diverse archive allows the algorithm to escape local optima and explore a wider range of solutions in high-dimensional spaces, vital for multimodal problems.  Avoid unnecessary intricacy   effectiveness trumps sophistication.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-22 22:29:20 INFO Mutation prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

Current heuristics:
AdaptiveGaussianArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for efficient multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])


Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- * **Keywords:** Adaptive, efficient, diverse, simple, multimodal, high-dimensional.

* **Advice:** Focus on integrating simple, adaptive mechanisms for efficient exploration.  Maintain a diverse archive of solutions, especially for complex problem spaces.  Prioritize elegance and practicality over excessive complexity.

* **Avoid:** Overly complex algorithms,  techniques without clear benefits,  neglecting diversity in solutions.

* **Explanation:**  Simple, well-integrated heuristics are more likely to be robust and computationally efficient. A diverse archive allows the algorithm to escape local optima and explore a wider range of solutions in high-dimensional spaces, vital for multimodal problems.  Avoid unnecessary intricacy   effectiveness trumps sophistication.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-22 22:30:07 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:31:07 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 22:33:31 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1527
2025-06-22 22:33:31 INFO FeHistory: [-334.0588024  -333.97881315 -334.00475306 ... -334.53313084 -334.53452171
 -334.53538257]
2025-06-22 22:33:31 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:33:31 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithArchive
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description:  Combines differential evolution with an adaptive mutation strategy and an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionWithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_three_distinct(self, i, population_size):
        candidates = list(range(population_size))
        candidates.remove(i)
        a = random.choice(candidates)
        candidates.remove(a)
        b = random.choice(candidates)
        candidates.remove(b)
        c = random.choice(candidates)
        return a, b, c

    def _crossover(self, x, v):
        trial = np.copy(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 22:33:31 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:36:54 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 22:36:54 INFO FeHistory: [154358.41879807 222396.8803386  283796.0525004  ...  35900.90918822
  24000.78617373  46834.39789766]
2025-06-22 22:36:54 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:36:54 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:40:32 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0346
2025-06-22 22:40:32 INFO FeHistory: [ 15.24353283  46.70583544   5.15571344 ... -38.27718771 -38.48318955
 -29.46487849]
2025-06-22 22:40:32 INFO Expected Optimum FE: -100
2025-06-22 22:40:32 INFO Unimodal AOCC mean: 0.1527
2025-06-22 22:40:32 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 22:40:32 INFO Multimodal (multiple components) AOCC mean: 0.0346
2025-06-22 22:40:32 INFO AOCC mean: 0.0624
2025-06-22 22:40:32 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:41:32 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 22:42:17 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1554
2025-06-22 22:42:17 INFO FeHistory: [-333.99364422 -333.99198496 -334.00858745 ... -334.73868015 -334.73868015
 -334.73868015]
2025-06-22 22:42:17 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:42:17 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionArchiveEA
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionArchiveEA
# Description: Adaptive Differential Evolution with an archive for robust multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []
        self.k= 0.05 #Adaptive parameter for F

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_F()


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_individuals(i, population)
            v = a + self.F * (b - c)
            u = self._crossover(population[i], v)
            offspring[i] = np.clip(u, self.lower_bounds, self.upper_bounds)
        return offspring

    def _select_individuals(self, i, population):
        indices = list(range(self.population_size))
        indices.remove(i)
        a, b, c = random.sample(indices, 3)
        return population[a], population[b], population[c]


    def _crossover(self, x, v):
      u = np.zeros_like(x)
      jrand = random.randint(0, self.dim - 1)
      for j in range(self.dim):
        if random.random() < self.CR or j == jrand:
          u[j] = v[j]
        else:
          u[j] = x[j]
      return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_F(self):
        if len(self.archive) > 0:
          mean_fitness = np.mean(self.archive[:,-1])
          if self.best_fitness_overall < mean_fitness:
            self.F = max(0.1, self.F - self.k) #Reduce F if solution is better than average
          else:
            self.F = min(1.0, self.F + self.k) #Increase F if solution is worse than average

2025-06-22 22:42:17 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:43:53 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 22:43:53 INFO FeHistory: [231412.32840969 374757.75530059 191532.14206873 ...    641.70708925
    641.70708925    641.70708925]
2025-06-22 22:43:53 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:43:53 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:46:08 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0838
2025-06-22 22:46:08 INFO FeHistory: [ 40.10377305  16.29191878  10.3841109  ... -87.86059435 -87.86059435
 -87.86059435]
2025-06-22 22:46:08 INFO Expected Optimum FE: -100
2025-06-22 22:46:08 INFO Unimodal AOCC mean: 0.1554
2025-06-22 22:46:08 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 22:46:08 INFO Multimodal (multiple components) AOCC mean: 0.0838
2025-06-22 22:46:08 INFO AOCC mean: 0.0797
2025-06-22 22:46:08 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:47:08 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 22:49:32 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1523
2025-06-22 22:49:32 INFO FeHistory: [-333.96541417 -334.04816441 -333.95666743 ... -334.4625118  -334.55556672
 -334.42072394]
2025-06-22 22:49:32 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:49:32 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithArchive
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Adaptive Differential Evolution with an archive to enhance exploration and exploitation in multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionWithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, i, pop_size):
        indices = list(range(pop_size))
        indices.remove(i)
        random.shuffle(indices)
        return indices[0], indices[1], indices[2]

    def _crossover(self, x, v):
        u = np.zeros_like(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                u[j] = v[j]
            else:
                u[j] = x[j]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-22 22:49:32 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 22:52:56 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 22:52:56 INFO FeHistory: [261776.67511485 404552.07370391 243621.06584392 ...  40046.77694504
  49912.98975884  42766.29912408]
2025-06-22 22:52:56 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 22:52:56 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 22:56:31 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0367
2025-06-22 22:56:31 INFO FeHistory: [  2.90636975   9.58658374  24.71571447 ... -48.10699462 -39.26845488
 -50.960726  ]
2025-06-22 22:56:31 INFO Expected Optimum FE: -100
2025-06-22 22:56:31 INFO Unimodal AOCC mean: 0.1523
2025-06-22 22:56:31 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 22:56:31 INFO Multimodal (multiple components) AOCC mean: 0.0367
2025-06-22 22:56:31 INFO AOCC mean: 0.0630
2025-06-22 22:56:31 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 22:57:31 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 22:59:55 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1526
2025-06-22 22:59:55 INFO FeHistory: [-333.97654095 -333.94474095 -334.02816878 ... -334.52826527 -334.50808691
 -334.53990774]
2025-06-22 22:59:55 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 22:59:55 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionArchiveEA
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionArchiveEA
# Description: Combines adaptive differential evolution with an archive for efficient multimodal optimization in high-dimensional spaces.
# Code:
class AdaptiveDifferentialEvolutionArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.copy(population)
        for i in range(self.population_size):
            # Choose three different individuals
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Binomial crossover
            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i, cross_points] = mutant[cross_points]

        return offspring

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 22:59:55 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:03:18 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 23:03:18 INFO FeHistory: [144622.59692954 252739.86173858 186600.35503176 ...  26854.64699738
  54488.21661959  48012.15898257]
2025-06-22 23:03:18 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:03:18 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:06:55 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0365
2025-06-22 23:06:55 INFO FeHistory: [ 43.28210599  12.05461511  30.37987127 ... -28.94298767 -42.22554096
 -35.09697597]
2025-06-22 23:06:55 INFO Expected Optimum FE: -100
2025-06-22 23:06:55 INFO Unimodal AOCC mean: 0.1526
2025-06-22 23:06:55 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:06:55 INFO Multimodal (multiple components) AOCC mean: 0.0365
2025-06-22 23:06:55 INFO AOCC mean: 0.0630
2025-06-22 23:06:55 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:07:55 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 23:10:20 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1524
2025-06-22 23:10:20 INFO FeHistory: [-333.94010019 -333.99713998 -333.99361767 ... -334.48445799 -334.58208968
 -334.44226225]
2025-06-22 23:10:20 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:10:20 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithArchive
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Combines Differential Evolution with an adaptive mutation strategy and an archive for efficient multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_distinct(population, i)  # Select 3 distinct individuals
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds) #Bound the mutant

            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring


    def _select_distinct(self, population, exclude_index):
        indices = list(range(self.population_size))
        indices.remove(exclude_index)
        random.shuffle(indices)
        return population[indices[0]], population[indices[1]], population[indices[2]]


    def _crossover(self, x, v):
        jrand = np.random.randint(0, self.dim)
        y = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                y[j] = v[j]
        return y

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:10:20 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:13:45 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 23:13:45 INFO FeHistory: [174985.41098309 418929.07976983 381911.0278892  ...  39215.94123136
  42405.85865516  54119.3632249 ]
2025-06-22 23:13:45 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:13:45 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:23 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0385
2025-06-22 23:17:23 INFO FeHistory: [ 28.99534854  37.23241679   7.69729001 ... -39.44093057 -41.60644685
 -39.09824108]
2025-06-22 23:17:23 INFO Expected Optimum FE: -100
2025-06-22 23:17:23 INFO Unimodal AOCC mean: 0.1524
2025-06-22 23:17:23 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:23 INFO Multimodal (multiple components) AOCC mean: 0.0385
2025-06-22 23:17:23 INFO AOCC mean: 0.0636
2025-06-22 23:17:23 INFO Perform Harmony Search...
2025-06-22 23:17:35 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:35 ERROR Can not run the algorithm
2025-06-22 23:17:35 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1455
2025-06-22 23:17:35 INFO FeHistory: [-334.00449867]
2025-06-22 23:17:35 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:35 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 534.9663313243025, archive_size: int = 767.1016412189888, sigma_decay: float = 0.9777492213534237, sigma_multiplier: float = 0.4986888104093946, tournament_size:int = 3.9038085598395433):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:35 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:35 ERROR Can not run the algorithm
2025-06-22 23:17:35 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:35 INFO FeHistory: [186720.94892117]
2025-06-22 23:17:35 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:35 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:35 ERROR Can not run the algorithm
2025-06-22 23:17:36 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:36 INFO FeHistory: [32.80492216]
2025-06-22 23:17:36 INFO Expected Optimum FE: -100
2025-06-22 23:17:36 INFO Unimodal AOCC mean: 0.1455
2025-06-22 23:17:36 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:36 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:36 INFO AOCC mean: 0.0485
2025-06-22 23:17:36 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:36 ERROR Can not run the algorithm
2025-06-22 23:17:36 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1459
2025-06-22 23:17:36 INFO FeHistory: [-334.03425516]
2025-06-22 23:17:36 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:36 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 807.1928904943303, archive_size: int = 108.87801631594193, sigma_decay: float = 0.9754333373430424, sigma_multiplier: float = 0.24708201824412507, tournament_size:int = 2.294348380972787):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:36 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:36 ERROR Can not run the algorithm
2025-06-22 23:17:36 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:36 INFO FeHistory: [417331.86363121]
2025-06-22 23:17:36 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:36 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:36 ERROR Can not run the algorithm
2025-06-22 23:17:36 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:36 INFO FeHistory: [34.53112735]
2025-06-22 23:17:36 INFO Expected Optimum FE: -100
2025-06-22 23:17:36 INFO Unimodal AOCC mean: 0.1459
2025-06-22 23:17:36 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:36 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:36 INFO AOCC mean: 0.0486
2025-06-22 23:17:36 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:36 ERROR Can not run the algorithm
2025-06-22 23:17:36 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1466
2025-06-22 23:17:36 INFO FeHistory: [-334.08722402]
2025-06-22 23:17:36 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:36 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 31.54335471547143, archive_size: int = 387.25162924011346, sigma_decay: float = 0.995706453249246, sigma_multiplier: float = 0.21093004803440868, tournament_size:int = 40.232711219881):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:36 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:36 ERROR Can not run the algorithm
2025-06-22 23:17:37 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:37 INFO FeHistory: [153988.19175401]
2025-06-22 23:17:37 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:37 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:37 ERROR Can not run the algorithm
2025-06-22 23:17:37 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:37 INFO FeHistory: [19.92689215]
2025-06-22 23:17:37 INFO Expected Optimum FE: -100
2025-06-22 23:17:37 INFO Unimodal AOCC mean: 0.1466
2025-06-22 23:17:37 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:37 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:37 INFO AOCC mean: 0.0489
2025-06-22 23:17:37 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:37 ERROR Can not run the algorithm
2025-06-22 23:17:37 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1445
2025-06-22 23:17:37 INFO FeHistory: [-333.92221364]
2025-06-22 23:17:37 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:37 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 299.3026021032487, archive_size: int = 784.0176641338247, sigma_decay: float = 0.8741310435032351, sigma_multiplier: float = 0.3306235508048766, tournament_size:int = 8.71293688542376):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:37 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:37 ERROR Can not run the algorithm
2025-06-22 23:17:37 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:37 INFO FeHistory: [300157.6152897]
2025-06-22 23:17:37 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:37 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:37 ERROR Can not run the algorithm
2025-06-22 23:17:37 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:37 INFO FeHistory: [0.62831497]
2025-06-22 23:17:37 INFO Expected Optimum FE: -100
2025-06-22 23:17:37 INFO Unimodal AOCC mean: 0.1445
2025-06-22 23:17:37 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:37 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:37 INFO AOCC mean: 0.0482
2025-06-22 23:17:37 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:37 ERROR Can not run the algorithm
2025-06-22 23:17:38 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1453
2025-06-22 23:17:38 INFO FeHistory: [-333.98526335]
2025-06-22 23:17:38 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:38 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 630.5998167688697, archive_size: int = 890.404154687335, sigma_decay: float = 0.8921703322268263, sigma_multiplier: float = 0.13773694189063163, tournament_size:int = 31.82138332982472):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:38 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:38 ERROR Can not run the algorithm
2025-06-22 23:17:38 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:38 INFO FeHistory: [287739.14520003]
2025-06-22 23:17:38 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:38 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:38 ERROR Can not run the algorithm
2025-06-22 23:17:38 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:38 INFO FeHistory: [34.8189023]
2025-06-22 23:17:38 INFO Expected Optimum FE: -100
2025-06-22 23:17:38 INFO Unimodal AOCC mean: 0.1453
2025-06-22 23:17:38 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:38 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:38 INFO AOCC mean: 0.0484
2025-06-22 23:17:38 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:38 ERROR Can not run the algorithm
2025-06-22 23:17:38 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1449
2025-06-22 23:17:38 INFO FeHistory: [-333.95612282]
2025-06-22 23:17:38 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:38 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 201.2210082036846, archive_size: int = 786.7403874637873, sigma_decay: float = 0.8741310435032351, sigma_multiplier: float = 0.4986888104093946, tournament_size:int = 30.449910069685522):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:38 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:38 ERROR Can not run the algorithm
2025-06-22 23:17:38 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:38 INFO FeHistory: [115540.17074945]
2025-06-22 23:17:38 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:38 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:38 ERROR Can not run the algorithm
2025-06-22 23:17:39 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:39 INFO FeHistory: [23.5408531]
2025-06-22 23:17:39 INFO Expected Optimum FE: -100
2025-06-22 23:17:39 INFO Unimodal AOCC mean: 0.1449
2025-06-22 23:17:39 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:39 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:39 INFO AOCC mean: 0.0483
2025-06-22 23:17:39 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:39 ERROR Can not run the algorithm
2025-06-22 23:17:39 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1449
2025-06-22 23:17:39 INFO FeHistory: [-333.95529669]
2025-06-22 23:17:39 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:39 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 981.1852002461629, archive_size: int = 248.29288964554198, sigma_decay: float = 0.9713175482831308, sigma_multiplier: float = 0.4986888104093946, tournament_size:int = 40.232711219881):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:39 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:39 ERROR Can not run the algorithm
2025-06-22 23:17:39 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:39 INFO FeHistory: [304070.57344567]
2025-06-22 23:17:39 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:39 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:39 ERROR Can not run the algorithm
2025-06-22 23:17:39 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:39 INFO FeHistory: [29.30382033]
2025-06-22 23:17:39 INFO Expected Optimum FE: -100
2025-06-22 23:17:39 INFO Unimodal AOCC mean: 0.1449
2025-06-22 23:17:39 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:39 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:39 INFO AOCC mean: 0.0483
2025-06-22 23:17:39 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:39 ERROR Can not run the algorithm
2025-06-22 23:17:39 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1450
2025-06-22 23:17:39 INFO FeHistory: [-333.96395918]
2025-06-22 23:17:39 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:39 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 463.81590610080116, archive_size: int = 786.7403874637873, sigma_decay: float = 0.846572091327552, sigma_multiplier: float = 0.24708201824412507, tournament_size:int = 29.801868652782673):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:39 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:39 ERROR Can not run the algorithm
2025-06-22 23:17:40 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:40 INFO FeHistory: [218305.83262157]
2025-06-22 23:17:40 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:40 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:40 ERROR Can not run the algorithm
2025-06-22 23:17:40 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:40 INFO FeHistory: [26.06917083]
2025-06-22 23:17:40 INFO Expected Optimum FE: -100
2025-06-22 23:17:40 INFO Unimodal AOCC mean: 0.1450
2025-06-22 23:17:40 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:40 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:40 INFO AOCC mean: 0.0483
2025-06-22 23:17:40 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:40 ERROR Can not run the algorithm
2025-06-22 23:17:40 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1455
2025-06-22 23:17:40 INFO FeHistory: [-333.99908543]
2025-06-22 23:17:40 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:40 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 31.54335471547143, archive_size: int = 696.9857051399031, sigma_decay: float = 0.9763654494221919, sigma_multiplier: float = 0.4986888104093946, tournament_size:int = 23.070928386769094):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:40 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:40 ERROR Can not run the algorithm
2025-06-22 23:17:40 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:40 INFO FeHistory: [216157.29288756]
2025-06-22 23:17:40 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:40 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:40 ERROR Can not run the algorithm
2025-06-22 23:17:40 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:40 INFO FeHistory: [40.10730387]
2025-06-22 23:17:40 INFO Expected Optimum FE: -100
2025-06-22 23:17:40 INFO Unimodal AOCC mean: 0.1455
2025-06-22 23:17:40 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:40 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:40 INFO AOCC mean: 0.0485
2025-06-22 23:17:40 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:17:40 ERROR Can not run the algorithm
2025-06-22 23:17:41 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1446
2025-06-22 23:17:41 INFO FeHistory: [-333.92522863]
2025-06-22 23:17:41 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:17:41 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 31.54335471547143, archive_size: int = 86.05941817567982, sigma_decay: float = 0.8921703322268263, sigma_multiplier: float = 0.5204999462379747, tournament_size:int = 36.26392307694046):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:17:41 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:17:41 ERROR Can not run the algorithm
2025-06-22 23:17:41 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:41 INFO FeHistory: [176863.42512381]
2025-06-22 23:17:41 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:17:41 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:17:41 ERROR Can not run the algorithm
2025-06-22 23:17:41 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 23:17:41 INFO FeHistory: [39.44895517]
2025-06-22 23:17:41 INFO Expected Optimum FE: -100
2025-06-22 23:17:41 INFO Unimodal AOCC mean: 0.1446
2025-06-22 23:17:41 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:17:41 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 23:17:41 INFO AOCC mean: 0.0482
2025-06-22 23:17:41 INFO Generation 2, best so far: 0.4357403339008153
2025-06-22 23:17:41 INFO Population length is: 12
2025-06-22 23:17:41 INFO --- Performing Long-Term Reflection at Generation 2 ---
2025-06-22 23:17:41 INFO Reflection Prompt: ### Problem Description
Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    
### List heuristics
Below is a list of design heuristics ranked from best to worst based on their average AOCC score, where higher is better, across a subset of the GNBG benchmark. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score: 6.8551e-02 |             AOCC Score on Unimodal instances: 1.5413e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 5.1518e-02)
# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for robust multimodal optimization.
# Code:
```python
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.initial_sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma = self.initial_sigma.copy() #Using a copy to avoid modifying original
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )
            self._update_best(offspring, offspring_fitness)
            self._adapt_sigma() #Adaptive sigma adjustment

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_sigma(self):
        #Adaptive sigma adjustment based on exploration/exploitation balance
        if self.eval_count > self.budget * 0.7 and self.best_fitness_overall > 1e-4: #Reduce sigma if stuck in local optima
            self.sigma *= 0.95
        elif self.eval_count < self.budget * 0.3 and self.best_fitness_overall > 1e-4: #Increase early exploration
            self.sigma *= 1.05
        self.sigma = np.clip(self.sigma, 0.01 * self.initial_sigma, 10 * self.initial_sigma) #Keep sigma within reasonable bounds

```

### Rank 2 (Overall AOCC Score: 6.4600e-02 |             AOCC Score on Unimodal instances: 1.5272e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 4.1075e-02)
# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Adaptive Gaussian sampling EA enhanced with an archive to maintain diversity and escape local optima in multimodal landscapes.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Adaptive Gaussian sampling EA enhanced with an archive to maintain diversity and escape local optima in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  # Size of the archive
        self.archive = None
        self.archive_fitness = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial sigma

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.archive = np.empty((0, self.dim))
        self.archive_fitness = np.array([])

        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Selection
            parents = self.tournament_selection(fitness_values, k=5)

            # Gaussian Mutation
            offspring = self.gaussian_mutation(parents, self.sigma)
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Archive Update
            self.update_archive(offspring, offspring_fitness)
            
            # Combine Population and Archive (for selection)
            combined_population = np.vstack((self.population, self.archive))
            combined_fitness = np.concatenate((fitness_values, self.archive_fitness))

            # Elitism
            sorted_indices = np.argsort(combined_fitness)
            self.population = combined_population[sorted_indices][:self.population_size]
            fitness_values = combined_fitness[sorted_indices][:self.population_size]

            # Update Best Solution
            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            #Adaptive Sigma (reduce exploration over time)
            self.sigma *= 0.999

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive = np.vstack((self.archive, offspring[i]))
                self.archive_fitness = np.append(self.archive_fitness, offspring_fitness[i])
            else:
                worst_index = np.argmax(self.archive_fitness)
                if offspring_fitness[i] < self.archive_fitness[worst_index]:
                    self.archive[worst_index] = offspring[i]
                    self.archive_fitness[worst_index] = offspring_fitness[i]
```

### Rank 3 (Overall AOCC Score: 6.3624e-02 |             AOCC Score on Unimodal instances: 1.5241e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 3.8460e-02)
# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Combines Differential Evolution with an adaptive mutation strategy and an archive for efficient multimodal optimization.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Combines Differential Evolution with an adaptive mutation strategy and an archive for efficient multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_distinct(population, i)  # Select 3 distinct individuals
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds) #Bound the mutant

            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring


    def _select_distinct(self, population, exclude_index):
        indices = list(range(self.population_size))
        indices.remove(exclude_index)
        random.shuffle(indices)
        return population[indices[0]], population[indices[1]], population[indices[2]]


    def _crossover(self, x, v):
        jrand = np.random.randint(0, self.dim)
        y = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                y[j] = v[j]
        return y

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 4 (Overall AOCC Score: 6.3095e-02 |             AOCC Score on Unimodal instances: 1.5032e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 3.8971e-02)
# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: An evolutionary strategy using adaptive Gaussian sampling and an archive to efficiently explore multimodal landscapes.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: An evolutionary strategy using adaptive Gaussian sampling and an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingEAwithArchive:
    """
    An evolutionary strategy combining adaptive Gaussian sampling and an archive 
    to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Size of the archive
        self.archive = []
        self.sigma = 0.5 #Initial standard deviation for Gaussian sampling
        self.sigma_decay = 0.99 # Decay factor for sigma

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        self.fitness = np.full(self.population_size, np.inf)
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        while self.eval_count < self.budget:
            #Evaluate Population
            for i in range(self.population_size):
                if np.isinf(self.fitness[i]):
                    self.fitness[i] = objective_function(self.population[i:i+1])[0]
                    self.eval_count +=1

            #Update best solution
            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_index]
                self.best_solution_overall = self.population[best_index].copy()

            #Update Archive
            for i in range(self.population_size):
                if len(self.archive) < self.archive_size:
                    self.archive.append((self.population[i], self.fitness[i]))
                else:
                    worst_index = np.argmax([f for _,f in self.archive])
                    if self.fitness[i] < self.archive[worst_index][1]:
                        self.archive[worst_index] = (self.population[i], self.fitness[i])

            #Selection (Tournament Selection from archive + population)
            parents = []
            candidates = list(self.population) + [x for x,_ in self.archive]
            fitnesses = list(self.fitness) + [f for _,f in self.archive]
            for _ in range(self.population_size//2):
                tournament = random.sample(range(len(candidates)),5)
                winner1 = min(tournament, key=lambda i: fitnesses[i])
                tournament = random.sample(range(len(candidates)),5)
                winner2 = min(tournament, key=lambda i: fitnesses[i])
                parents.extend([winner1,winner2])

            #Recombination (Simple Averaging)
            offspring = np.zeros((self.population_size, self.dim))
            for i in range(0, self.population_size, 2):
                offspring[i] = (candidates[parents[i]] + candidates[parents[i+1]])/2
                offspring[i+1] = (candidates[parents[i]] + candidates[parents[i+1]])/2

            #Mutation (Adaptive Gaussian)
            for i in range(self.population_size):
                mutation = np.random.normal(0, self.sigma, self.dim)
                offspring[i] = np.clip(offspring[i] + mutation, self.lower_bounds, self.upper_bounds)

            self.population = offspring.copy()
            self.fitness = np.full(self.population_size, np.inf)
            self.sigma *= self.sigma_decay #decay sigma over time


        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
```

### Rank 5 (Overall AOCC Score: 6.3021e-02 |             AOCC Score on Unimodal instances: 1.5260e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 3.6466e-02)
# Name: AdaptiveDifferentialEvolutionArchiveEA
# Description: Combines adaptive differential evolution with an archive for efficient multimodal optimization in high-dimensional spaces.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionArchiveEA
# Description: Combines adaptive differential evolution with an archive for efficient multimodal optimization in high-dimensional spaces.
# Code:
class AdaptiveDifferentialEvolutionArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.copy(population)
        for i in range(self.population_size):
            # Choose three different individuals
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Binomial crossover
            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i, cross_points] = mutant[cross_points]

        return offspring

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 6 (Overall AOCC Score: 6.3005e-02 |             AOCC Score on Unimodal instances: 1.5229e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 3.6728e-02)
# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Adaptive Differential Evolution with an archive to enhance exploration and exploitation in multimodal landscapes.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Adaptive Differential Evolution with an archive to enhance exploration and exploitation in multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionWithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, i, pop_size):
        indices = list(range(pop_size))
        indices.remove(i)
        random.shuffle(indices)
        return indices[0], indices[1], indices[2]

    def _crossover(self, x, v):
        u = np.zeros_like(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                u[j] = v[j]
            else:
                u[j] = x[j]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

```

### Rank 7 (Overall AOCC Score: 6.2413e-02 |             AOCC Score on Unimodal instances: 1.5265e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 3.4583e-02)
# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Combines differential evolution with an adaptive mutation strategy and an archive to efficiently explore multimodal landscapes.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description:  Combines differential evolution with an adaptive mutation strategy and an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionWithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_three_distinct(self, i, population_size):
        candidates = list(range(population_size))
        candidates.remove(i)
        a = random.choice(candidates)
        candidates.remove(a)
        b = random.choice(candidates)
        candidates.remove(b)
        c = random.choice(candidates)
        return a, b, c

    def _crossover(self, x, v):
        trial = np.copy(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 8 (Overall AOCC Score: 5.4626e-02 |             AOCC Score on Unimodal instances: 1.4876e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 1.5113e-02)
# Name: AdaptiveArchiveGuidedGaussianMutation
# Description: Combines adaptive Gaussian mutation with an archive for efficient multimodal optimization.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveArchiveGuidedGaussianMutation
# Description: Combines adaptive Gaussian mutation with an archive for efficient multimodal optimization.
# Code:
class AdaptiveArchiveGuidedGaussianMutation:
    """
    Combines adaptive Gaussian mutation with an archive to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # Heuristic for population size
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.sigma = 0.5  # Initial Gaussian mutation strength

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            self.update_archive(offspring, offspring_fitness)

            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)

            #Adapt mutation strength
            self.sigma = self.sigma * (1 + 0.1 * np.random.randn()) #Adaptive sigma
            self.sigma = np.clip(self.sigma, 0.1, 1.5) #Keep sigma in reasonable range

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive),
            'final_sigma': self.sigma
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        for i in range(self.population_size):
            #Gaussian Mutation with adaptive sigma
            mutation = np.random.normal(0, self.sigma, self.dim)
            offspring[i] = population[i] + mutation
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

```

### Rank 9 (Overall AOCC Score: 4.8994e-02 |             AOCC Score on Unimodal instances: 1.4698e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: ArchiveGuidedAdaptiveGaussianMutationEA
# Description: An evolutionary algorithm using an archive and adaptive Gaussian mutation for efficient multimodal optimization.
# Code:
```python
import numpy as np
import random

class ArchiveGuidedAdaptiveGaussianMutationEA:
    """
    An evolutionary algorithm combining an archive and adaptive Gaussian mutation 
    for efficient exploration and exploitation in multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 50
        self.archive_size = 100
        self.archive = []
        self.mutation_rate = 0.5 # Initial mutation rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        while self.eval_count < self.budget:
            fitness_values = objective_function(self.population)
            self.eval_count += self.population_size

            #Update best solution
            for i, fitness in enumerate(fitness_values):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = self.population[i]

            # Update archive
            self.update_archive(self.population, fitness_values)

            # Selection (tournament selection)
            parents = self.tournament_selection(self.population, fitness_values, tournament_size=5)

            # Recombination (intermediate recombination)
            offspring = self.intermediate_recombination(parents)

            # Mutation (adaptive Gaussian mutation)
            mutated_offspring = self.adaptive_gaussian_mutation(offspring)

            # Population update (Elitism)
            self.population = np.concatenate((parents, mutated_offspring))


        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, population, fitness_values, tournament_size):
        selected_indices = []
        for _ in range(self.population_size // 2):
            tournament = random.sample(range(len(population)), tournament_size)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_indices.append(winner_index)
        return population[selected_indices]

    def intermediate_recombination(self, parents):
        offspring = np.zeros_like(parents)
        for i in range(0, len(parents), 2):
            offspring[i] = (parents[i] + parents[i+1]) / 2
            offspring[i+1] = (parents[i] + parents[i+1]) / 2
        return offspring

    def adaptive_gaussian_mutation(self, offspring):
        #Adaptive mutation rate based on population diversity
        std_dev = np.std(offspring)
        mutation_rate = max(0.01, self.mutation_rate * (1 - 0.01*std_dev) )
        mutated_offspring = offspring + np.random.normal(0, mutation_rate * (self.upper_bounds - self.lower_bounds), size=offspring.shape)
        mutated_offspring = np.clip(mutated_offspring, self.lower_bounds, self.upper_bounds)
        return mutated_offspring

    def update_archive(self, population, fitness_values):
        for i in range(len(population)):
            if len(self.archive) < self.archive_size:
                self.archive.append((population[i], fitness_values[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if fitness_values[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (population[i], fitness_values[i])

```

### Rank 10 (Overall AOCC Score: 4.8858e-02 |             AOCC Score on Unimodal instances: 1.4657e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveGaussianArchiveEA
# Description: 
# Code:
```python
import numpy as np
import random

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 31.54335471547143, archive_size: int = 387.25162924011346, sigma_decay: float = 0.995706453249246, sigma_multiplier: float = 0.21093004803440868, tournament_size:int = 40.232711219881):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.tournament_size = tournament_size
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 11 (Overall AOCC Score: 4.8846e-02 |             AOCC Score on Unimodal instances: 1.4640e-01 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 1.3424e-04)
# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: Combines adaptive Gaussian mutation with an archive to efficiently explore multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian mutation with an archive to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 50
        self.archive_size = 100
        self.archive = []
        self.mutation_rate = 0.5 # Initial mutation rate
        self.mutation_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(self.population)
            self.eval_count += len(self.population)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

            self._update_archive(self.population, fitness_values) #Update archive
            self.mutation_rate *= self.mutation_decay #Adaptive Mutation


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = []
        for i in range(self.population_size):
            # Sample parent from population or archive for diversity
            if random.random() < 0.5 and len(self.archive) > 0:
                parent = random.choice(self.archive)
            else:
                parent = self.population[i]

            offspring_solution = parent + np.random.normal(0, self.mutation_rate, self.dim)
            offspring_solution = np.clip(offspring_solution, self.lower_bounds, self.upper_bounds)
            offspring.append(offspring_solution)
        return np.array(offspring)

    def _selection(self, parents, parent_fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((parents, offspring))
        combined_fitness = np.concatenate((parent_fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        selected_indices = indices[:self.population_size]
        selected_population = combined_population[selected_indices]
        selected_fitness = combined_fitness[selected_indices]
        best_solution, best_fitness = self._update_best(selected_population, selected_fitness)
        return selected_population, best_solution, best_fitness

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness

    def _update_archive(self, population, fitness_values):
        for i in range(len(population)):
            solution = population[i]
            fitness = fitness_values[i]
            self._add_to_archive(solution, fitness)

    def _add_to_archive(self, solution, fitness):
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_solution, worst_fitness = max(self.archive, key=lambda item: item[1])
            if fitness < worst_fitness:
                self.archive.remove((worst_solution, worst_fitness))
                self.archive.append((solution, fitness))

```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-22 23:17:45 INFO Full response text: **Analysis:**

Comparing AdaptiveGaussianArchiveEA (best) vs AdaptiveGaussianSamplingEAwithArchive (worst), we see that the best-performing algorithm uses more sophisticated recombination and selection mechanisms (tournament selection and Gaussian recombination) leading to a more efficient exploration of the search space.  The worst algorithm uses simpler averaging recombination and lacks the adaptive sigma adjustment, hindering its ability to converge effectively and escape local optima.

AdaptiveGaussianSamplingWithArchiveEA (second best) vs ArchiveGuidedAdaptiveGaussianMutationEA (second worst) shows a similar trend. AdaptiveGaussianSamplingWithArchiveEA incorporates a more robust archive update mechanism and adaptive sigma adjustment, while ArchiveGuidedAdaptiveGaussianMutationEA uses simpler mutation strategies and lacks sophisticated selection and recombination techniques.

Comparing AdaptiveGaussianArchiveEA (1st) vs AdaptiveGaussianSamplingWithArchiveEA (2nd), we see minor differences. Both use archives and adaptive Gaussian sampling. However, the top performer has a more refined approach in adaptive sigma adjustment and tournament selection resulting in slightly better exploitation.

AdaptiveDifferentialEvolutionWithArchive (3rd) vs AdaptiveArchiveGuidedGaussianMutation (4th) reveals that DE's inherent exploration via vector differences provides a slight edge over simple Gaussian mutation. The adaptive archive helps both, but DE's structured variation provides superior performance in multimodal landscapes.

Comparing AdaptiveGaussianSamplingEAwithArchive (second worst) vs ArchiveGuidedAdaptiveGaussianMutationEA (worst), the use of tournament selection, intermediate recombination, and more refined adaptive mutation in the second worst algorithm contributes to a marginally better performance compared to the adaptive Gaussian mutation used in the worst-performing algorithm. Overall: The superior algorithms consistently employ adaptive mechanisms for mutation strength or selection pressure, incorporate archives for maintaining diversity, and leverage sophisticated recombination operators like Gaussian recombination or differential evolution. The use of tournament selection also seems beneficial.


**Experience:**

Developing effective heuristics requires a balanced approach combining global exploration with local exploitation, adaptive mechanisms for adjusting search parameters based on problem characteristics, and efficient data structures like archives for managing previously visited solutions.  Sophisticated selection and recombination strategies further enhance performance.

2025-06-22 23:17:47 INFO Full response text: * **Keywords:** Adaptive, efficient, exploration-exploitation balance, diversity, simplicity.

* **Advice:** Focus on integrating simple, adaptive mechanisms for parameter control and efficient exploration. Prioritize data structures (like archives) that maintain solution diversity, especially for high-dimensional, multimodal problems.  Emphasize clear, measurable improvements in performance.

* **Avoid:** Overly complex algorithms,  unnecessary features,  lack of adaptability to problem characteristics, neglecting diversity maintenance.

* **Explanation:** Effective heuristics leverage adaptive strategies to balance exploration and exploitation, using efficient data structures to manage search.  Simplicity and adaptability are key to robust performance across different problem instances.  Prioritizing these aspects over sheer complexity leads to more effective and generalizable heuristics.

2025-06-22 23:17:47 INFO Generating offspring via Crossover...
2025-06-22 23:17:57 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:18:57 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 23:21:18 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1500
2025-06-22 23:21:18 INFO FeHistory: [-334.04090966 -333.96272664 -333.97774224 ... -334.26138054 -334.18087183
 -334.25539014]
2025-06-22 23:21:18 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:21:18 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithGaussianMutationAndArchive
import numpy as np
import random

# Name: AdaptiveDEwithGaussianMutationAndArchive
# Description: Combines Differential Evolution, Gaussian mutation, and an archive for robust multimodal optimization.
# Code:
class AdaptiveDEwithGaussianMutationAndArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds) # Initial Gaussian mutation standard deviation
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            #Adapt sigma - reduce exploration over time
            self.sigma *= 0.999

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            #Gaussian Mutation
            trial = trial + np.random.normal(0, self.sigma, self.dim)
            trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
            offspring[i] = trial
        return offspring

    def _select_three_distinct(self, i, population_size):
        candidates = list(range(population_size))
        candidates.remove(i)
        a = random.choice(candidates)
        candidates.remove(a)
        b = random.choice(candidates)
        candidates.remove(b)
        c = random.choice(candidates)
        return a, b, c

    def _crossover(self, x, v):
        trial = np.copy(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:21:18 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:24:39 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 23:24:39 INFO FeHistory: [240453.19067178 163888.6921322  112600.14952969 ... 161307.37383986
  35167.66366511 121748.67142717]
2025-06-22 23:24:39 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:24:39 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:28:14 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0253
2025-06-22 23:28:14 INFO FeHistory: [ 36.6893328   30.46938575  32.46879715 ... -17.22570909 -19.33526701
  -1.62081063]
2025-06-22 23:28:14 INFO Expected Optimum FE: -100
2025-06-22 23:28:14 INFO Unimodal AOCC mean: 0.1500
2025-06-22 23:28:14 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:28:14 INFO Multimodal (multiple components) AOCC mean: 0.0253
2025-06-22 23:28:14 INFO AOCC mean: 0.0584
2025-06-22 23:28:26 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:29:26 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 23:31:48 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1524
2025-06-22 23:31:48 INFO FeHistory: [-333.98993898 -333.94204398 -333.95309721 ... -334.53660989 -334.51601601
 -334.47343382]
2025-06-22 23:31:48 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:31:48 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithGaussianMutationAndArchive
import numpy as np
import random

# Name: AdaptiveDEwithGaussianMutationAndArchive
# Description: Combines Differential Evolution with adaptive Gaussian mutation and an archive for efficient multimodal optimization.
# Code:
class AdaptiveDEwithGaussianMutationAndArchive:
    """
    Combines Differential Evolution (DE) with adaptive Gaussian mutation and an archive to efficiently explore multimodal landscapes.  
    DE provides diverse exploration, Gaussian mutation refines solutions, and the archive maintains diversity and prevents premature convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.sigma = 0.5 # Initial Gaussian mutation standard deviation
        self.sigma_decay = 0.99 # Decay factor for sigma
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            #Gaussian Mutation
            trial = np.clip(trial + np.random.normal(0, self.sigma, self.dim), self.lower_bounds, self.upper_bounds)
            offspring[i] = trial
        return offspring

    def _select_three_distinct(self, i, population_size):
        candidates = list(range(population_size))
        candidates.remove(i)
        a = random.choice(candidates)
        candidates.remove(a)
        b = random.choice(candidates)
        candidates.remove(b)
        c = random.choice(candidates)
        return a, b, c

    def _crossover(self, x, v):
        trial = np.copy(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-22 23:31:48 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:35:11 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 23:35:11 INFO FeHistory: [196068.487055   143087.23922804 241792.08972586 ...  30075.81002052
  74026.31780947  26254.4305536 ]
2025-06-22 23:35:11 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:35:11 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:38:49 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0318
2025-06-22 23:38:49 INFO FeHistory: [ 16.0329983   41.03011628  18.97642296 ... -19.52956865 -24.71743764
 -24.99969134]
2025-06-22 23:38:49 INFO Expected Optimum FE: -100
2025-06-22 23:38:49 INFO Unimodal AOCC mean: 0.1524
2025-06-22 23:38:49 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:38:49 INFO Multimodal (multiple components) AOCC mean: 0.0318
2025-06-22 23:38:49 INFO AOCC mean: 0.0614
2025-06-22 23:38:59 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:39:59 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 23:43:48 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1503
2025-06-22 23:43:48 INFO FeHistory: [-334.08487707 -333.93848625 -333.96109227 ... -334.22575932 -334.24670476
 -334.33969768]
2025-06-22 23:43:48 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:43:48 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchiveAndGaussianMutation
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndGaussianMutation
# Description: Combines adaptive differential evolution with an archive and Gaussian mutation for multimodal optimization.
# Code:
class AdaptiveDEwithArchiveAndGaussianMutation:
    """
    Combines adaptive differential evolution with an archive and Gaussian mutation for efficient multimodal optimization.  
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * self.dim  # Heuristic population size
        self.archive_size = 200
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sigma = 0.5 # Initial Gaussian mutation strength

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self._initialize_population()
        fitness = objective_function(population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(population, fitness)
        self.archive = self._update_archive(population, fitness)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness = self._selection(population, fitness, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness, offspring_fitness)))
            self.best_solution_overall, self.best_fitness_overall = self._update_best(np.vstack((population, offspring)), np.concatenate((fitness, offspring_fitness)))

            #Adapt mutation strength
            self.sigma = self.sigma * (1 + 0.1 * np.random.randn()) #Adaptive sigma
            self.sigma = np.clip(self.sigma, 0.1, 1.5) #Keep sigma in reasonable range

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive),
            'final_sigma': self.sigma
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness):
        offspring = np.copy(population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Gaussian Mutation
            mutant += np.random.normal(0, self.sigma, self.dim)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i, cross_points] = mutant[cross_points]
        return offspring

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 23:43:48 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:48:36 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 23:48:36 INFO FeHistory: [204808.07032517 330002.0460475  247782.8890154  ...  74584.44182331
 145629.40454627  62808.81562854]
2025-06-22 23:48:36 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:48:36 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:53:18 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0252
2025-06-22 23:53:18 INFO FeHistory: [ 39.80087041  16.66840244  32.24414591 ... -29.33839723   7.16945083
 -23.33870878]
2025-06-22 23:53:18 INFO Expected Optimum FE: -100
2025-06-22 23:53:18 INFO Unimodal AOCC mean: 0.1503
2025-06-22 23:53:18 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:53:18 INFO Multimodal (multiple components) AOCC mean: 0.0252
2025-06-22 23:53:18 INFO AOCC mean: 0.0585
2025-06-22 23:53:29 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 23:53:29 ERROR Can not run the algorithm
2025-06-22 23:53:29 INFO Run function 5 complete. FEHistory len: 100, AOCC: 0.1470
2025-06-22 23:53:29 INFO FeHistory: [-333.95275523 -333.97154332 -333.96252976 -334.08595061 -333.99528627
 -333.99289368 -334.05973031 -333.9502486  -333.93551185 -334.04187519
 -334.02113687 -334.10631416 -334.02268601 -334.00686652 -334.03430724
 -333.94191795 -334.0193173  -333.95553246 -333.93811593 -334.01710786
 -333.90074356 -333.9621207  -334.03798947 -334.03229181 -334.12380254
 -333.93317551 -333.98597013 -334.06581143 -333.97899426 -333.89926956
 -333.9818349  -334.01157103 -334.00510812 -333.93576028 -333.99359485
 -333.95685456 -334.04966453 -334.02661792 -333.96209306 -333.98316087
 -333.97669185 -334.06127782 -334.00585861 -334.0172357  -333.9546768
 -334.01793245 -333.95446644 -334.00266599 -333.97219285 -333.92637883
 -333.96528883 -334.0633712  -334.02726979 -334.02361051 -333.98515867
 -333.96983168 -333.96353151 -333.99809756 -333.97380902 -333.94399994
 -334.03703336 -333.97891307 -334.00110173 -334.00740459 -333.94824373
 -333.94438436 -333.91595467 -333.94590572 -333.93016443 -333.9403502
 -333.99081627 -333.9399727  -334.03996932 -334.00662512 -333.99068641
 -333.95018608 -334.02792971 -334.03206731 -334.02196447 -333.90152423
 -333.94793407 -334.0425602  -334.02060655 -334.02879943 -333.95659317
 -333.96943935 -333.9468304  -333.91298488 -334.01277211 -333.96990854
 -334.05166212 -333.95552994 -333.97038622 -333.99005092 -333.95592688
 -334.00923669 -333.98053201 -333.93515436 -334.0024358  -334.05306697]
2025-06-22 23:53:29 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 23:53:29 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchive
import numpy as np
import random

# Name: AdaptiveDEwithArchive
# Description: Adaptive Differential Evolution with an archive to enhance exploration and exploitation in multimodal landscapes.
# Code:
class AdaptiveDEwithArchive:
    """
    Combines adaptive Differential Evolution with an archive to handle multimodal problems efficiently.  
    Adapts mutation strength and uses tournament selection.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)
        
        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(np.vstack((self.population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self.population, fitness_values = self._select_next_generation(self.population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((self.population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._adapt_F()

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = np.zeros_like(self.population)
        for i in range(self.population_size):
            a, b, c = self._tournament_selection(3)  # Tournament selection of 3 individuals
            v = self.population[a] + self.F * (self.population[b] - self.population[c])
            u = np.where(np.random.rand(self.dim) < self.CR, v, self.population[i])
            offspring[i] = np.clip(u, self.lower_bounds, self.upper_bounds)
        return offspring

    def _tournament_selection(self, k):
        indices = np.random.choice(self.population_size, size=k, replace=False)
        fitness_subset = [np.min(objective_function(self.population[indices].reshape(1,-1))) for i in range(len(indices))] # Efficient fitness evaluation
        winner_index = indices[np.argmin(fitness_subset)]
        indices = np.random.choice(self.population_size, size=k-1, replace=False)
        return winner_index, indices[0], indices[1]

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit


    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
        return self.best_solution_overall, self.best_fitness_overall

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_F(self):
        if self.eval_count > self.budget * 0.7 and self.best_fitness_overall > 1e-4:
            self.F *= 0.95
        elif self.eval_count < self.budget * 0.3 and self.best_fitness_overall > 1e-4:
            self.F *= 1.05
        self.F = np.clip(self.F, 0.1, 1.0)

2025-06-22 23:53:29 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 23:53:29 ERROR Can not run the algorithm
2025-06-22 23:53:29 INFO Run function 12 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-22 23:53:29 INFO FeHistory: [351901.78615142 174907.58841227 130044.31070555 136729.9419438
 178913.60102307 281445.84462135 132904.76630111 287097.70651019
 275515.63174245 287030.03554921 329241.62407835 255563.87733029
  89460.19708817 262185.100678   237614.93267447 182603.63271136
 324270.54449726 400720.64797502 254341.80679996 391285.70384627
 316339.0574836  136821.83107833 316111.66493652 179845.27336222
 316706.40139146 275574.7902415   91381.70464813 240925.60242693
 292461.53321247 333164.61837774 288847.92583887 179543.33308921
 172701.05206547 220560.226937   243561.91504042 235649.57805364
 336071.72630801 248766.35409183 232100.45846099 247637.43484354
 287736.50781781 252006.56781166 250247.64842464 164361.16198473
 143488.76865017 177232.07900742 284166.77635704 115295.26278995
 131969.06699899 307615.65453591 311660.69731839 130091.82955054
 146312.95918506 175576.21286796 180385.56025952 225161.07699161
 268787.93402847 285337.1233525  158079.79363902 162536.81458479
 384130.7574449  181018.53064318 184647.93955135 144050.66070233
 170064.11451418 163435.63223939 174442.24956266 228129.96042745
 206130.27547792 183289.97224685 294589.17085897 211718.0222911
 172865.13150224 164846.36540147 210600.04392862 328164.02680973
 158829.49360887 159628.0275391  193117.04460687 118414.67219851
 103657.92047769 269804.29109591 272087.34059045 352181.83947135
 169698.14149254 127223.0848332  445438.50390506 260609.60233037
 371001.27359405 373565.10733982 252806.44737361 297901.5351735
 154604.46265951 170822.79820891 219444.46203305 240149.20473522
 190509.48399631 305813.09008567 168431.2806082  199026.38682289]
2025-06-22 23:53:29 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 23:53:29 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 23:53:30 ERROR Can not run the algorithm
2025-06-22 23:53:30 INFO Run function 23 complete. FEHistory len: 100, AOCC: 0.0032
2025-06-22 23:53:30 INFO FeHistory: [13.6421679  43.08927671 34.72501768 28.51185199 32.91154015 17.44958454
 32.21872888  0.78006275 38.12548804 12.34897096 62.40790395 37.97920281
 22.21932175 30.5778111  21.85404084 49.22887694 46.68951489 25.28426687
 34.58565013 46.02754237 44.38793233 16.18049409 50.32493424  9.7398475
 42.51924867 42.96939533 24.9993905  15.58325488 35.65346244 29.56280788
 24.02931979 18.63588541 15.38296988 19.80662388 57.42313585 42.7497221
 16.75854527 16.17967588 10.16644756 46.41275678 36.90141715 29.50348401
 36.0787211  14.11229073 51.6222104  28.63123453 23.23781629 25.36240787
 21.10123745 30.51098977 26.49731998 -0.60790036 25.55067265 36.83181936
 -7.15225137 36.30289914 25.68997795 35.02058394 39.621628   26.3683411
 16.74589956 27.90983593 30.7015721  36.89032061 55.99108996 16.75521905
 43.41966798 32.731919    5.42371096 14.61289701 18.1521795  29.1696199
 38.85722261 13.62324324 32.79613373 28.13298594 28.20454841 17.24769648
 29.43958606 48.53687415 24.92047831 22.83941667 42.53721443 32.8647627
 26.0937916  17.74503219 -1.68930882  5.47500973 21.17972206  0.17137419
 25.96577292 26.09553565 58.95981259 34.52017898 27.93947201 40.81127524
 28.47804225  8.77757443 18.28090586 49.15801695]
2025-06-22 23:53:30 INFO Expected Optimum FE: -100
2025-06-22 23:53:30 INFO Unimodal AOCC mean: 0.1470
2025-06-22 23:53:30 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 23:53:30 INFO Multimodal (multiple components) AOCC mean: 0.0032
2025-06-22 23:53:30 INFO AOCC mean: 0.0501
2025-06-22 23:53:39 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 01:01:19 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-23 01:17:18 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1668
2025-06-23 01:17:18 INFO FeHistory: [-333.94829822 -334.09820661 -334.00538238 ... -335.52027477 -335.52027477
 -335.52027477]
2025-06-23 01:17:18 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 01:17:18 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithEnhancedArchive
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithEnhancedArchive
# Description: Adaptive Differential Evolution using an enhanced archive for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithEnhancedArchive:
    """
    Combines adaptive differential evolution with an enhanced archive to balance exploration and exploitation in multimodal landscapes.  
    The archive uses a more sophisticated update mechanism to maintain diversity and prevent premature convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.sigma_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count +=1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.F *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            trial = self._crossover(population[i], mutant)
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring

    def _select_distinct(self, i, size):
        candidates = list(range(size))
        candidates.remove(i)
        random.shuffle(candidates)
        return candidates[0], candidates[1], candidates[2]

    def _crossover(self, x, v):
        jrand = random.randint(0, self.dim - 1)
        y = np.copy(x)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                y[j] = v[j]
        return y

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 01:17:18 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 01:55:14 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0423
2025-06-23 01:55:14 INFO FeHistory: [339576.34039388 261960.61614712 200934.10398343 ...   -975.15959978
   -975.15959978   -975.15959978]
2025-06-23 01:55:14 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 01:55:14 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 03:13:23 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.1474
2025-06-23 03:13:23 INFO FeHistory: [ 22.13083705  39.56096026  28.78305502 ... -98.33433152 -98.33433152
 -98.33433152]
2025-06-23 03:13:23 INFO Expected Optimum FE: -100
2025-06-23 03:13:23 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithEnhancedArchive
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithEnhancedArchive
# Description: Adaptive Differential Evolution using an enhanced archive for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithEnhancedArchive:
    """
    Combines adaptive differential evolution with an enhanced archive to balance exploration and exploitation in multimodal landscapes.  
    The archive uses a more sophisticated update mechanism to maintain diversity and prevent premature convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.sigma_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count +=1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.F *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            trial = self._crossover(population[i], mutant)
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring

    def _select_distinct(self, i, size):
        candidates = list(range(size))
        candidates.remove(i)
        random.shuffle(candidates)
        return candidates[0], candidates[1], candidates[2]

    def _crossover(self, x, v):
        jrand = random.randint(0, self.dim - 1)
        y = np.copy(x)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                y[j] = v[j]
        return y

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 03:13:23 INFO Unimodal AOCC mean: 0.1668
2025-06-23 03:13:23 INFO Multimodal (single component) AOCC mean: 0.0423
2025-06-23 03:13:23 INFO Multimodal (multiple components) AOCC mean: 0.1474
2025-06-23 03:13:23 INFO AOCC mean: 0.1189
2025-06-23 03:13:35 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 03:57:22 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-23 05:59:07 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1520
2025-06-23 05:59:07 INFO FeHistory: [-333.93323605 -334.03265362 -333.96344634 ... -334.42969112 -334.45746842
 -334.49063867]
2025-06-23 05:59:07 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 05:59:07 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithGaussianArchive
import numpy as np
import random

class AdaptiveDEwithGaussianArchive:
    """
    Combines Differential Evolution's exploration with Gaussian mutation and an archive for improved multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 100, archive_size: int = 200, F: float = 0.8, CR: float = 0.9,
                 sigma_multiplier: float = 0.1, sigma_decay: float = 0.99):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.F = F
        self.CR = CR
        self.sigma = sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            #Gaussian Mutation
            trial = trial + np.random.normal(0, self.sigma, self.dim)
            trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
            offspring[i] = trial
        return offspring

    def _select_three_distinct(self, i, population_size):
        candidates = list(range(population_size))
        candidates.remove(i)
        a = random.choice(candidates)
        candidates.remove(a)
        b = random.choice(candidates)
        candidates.remove(b)
        c = random.choice(candidates)
        return a, b, c

    def _crossover(self, x, v):
        trial = np.copy(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 05:59:07 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 06:56:29 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 06:56:29 INFO FeHistory: [162150.42459579 156033.6826571  188326.7471442  ...  47949.96458098
  43759.84250707  26480.50968853]
2025-06-23 06:56:29 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 06:56:29 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:09:22 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0378
2025-06-23 07:09:22 INFO FeHistory: [ 43.2610627   36.14292028  44.28001184 ... -48.89528416 -40.84334856
 -41.0146963 ]
2025-06-23 07:09:22 INFO Expected Optimum FE: -100
2025-06-23 07:09:22 INFO Unimodal AOCC mean: 0.1520
2025-06-23 07:09:22 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 07:09:22 INFO Multimodal (multiple components) AOCC mean: 0.0378
2025-06-23 07:09:22 INFO AOCC mean: 0.0633
2025-06-23 07:09:34 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 07:10:34 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-23 07:11:03 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1514
2025-06-23 07:11:03 INFO FeHistory: [-333.97569246 -334.03853927 -333.97480342 ... -334.41197683 -334.33747754
 -334.38488043]
2025-06-23 07:11:03 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 07:11:03 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithArchiveAndGaussianMutation
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchiveAndGaussianMutation
# Description: Combines adaptive DE with an archive and Gaussian mutation for robust multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithArchiveAndGaussianMutation:
    """
    Combines adaptive differential evolution (DE) with an archive and Gaussian mutation 
    for efficient exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.mutation_rate = 0.1 #Gaussian Mutation rate
        self.sigma_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)
        self.archive = self._update_archive(self.population, fitness_values)


        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(self.population)
            self.eval_count += self.population_size
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)
            self.archive = self._update_archive(np.vstack((self.population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self.mutation_rate *= self.sigma_decay #Adaptive gaussian mutation

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = []
        for i in range(self.population_size):
            # Differential Evolution
            a, b, c = self._select_different(i, self.population_size)
            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
            trial = self._crossover(self.population[i], mutant)
            #Gaussian Mutation
            trial = trial + np.random.normal(0, self.mutation_rate, self.dim)
            trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
            offspring.append(trial)
        return np.array(offspring)

    def _select_different(self, i, pop_size):
        candidates = list(range(pop_size))
        candidates.remove(i)
        random.shuffle(candidates)
        return candidates[0], candidates[1], candidates[2]

    def _crossover(self, x, v):
        u = np.zeros_like(x)
        jrand = random.randint(0, self.dim -1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                u[j] = v[j]
            else:
                u[j] = x[j]
        return u

    def _selection(self, parents, parent_fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((parents, offspring))
        combined_fitness = np.concatenate((parent_fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        selected_indices = indices[:self.population_size]
        selected_population = combined_population[selected_indices]
        selected_fitness = combined_fitness[selected_indices]
        best_solution, best_fitness = self._update_best(selected_population, selected_fitness)
        return selected_population, best_solution, best_fitness

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 07:11:03 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 07:12:30 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 07:12:30 INFO FeHistory: [297531.0275995  195693.76032581 194327.05189148 ...  38269.38821931
  51539.33762161  39970.19812384]
2025-06-23 07:12:30 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 07:12:30 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:14:09 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0303
2025-06-23 07:14:09 INFO FeHistory: [ 26.8869613   14.09758776  26.9140357  ... -33.43975103 -19.19086883
 -17.59134799]
2025-06-23 07:14:09 INFO Expected Optimum FE: -100
2025-06-23 07:14:09 INFO Unimodal AOCC mean: 0.1514
2025-06-23 07:14:09 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 07:14:09 INFO Multimodal (multiple components) AOCC mean: 0.0303
2025-06-23 07:14:09 INFO AOCC mean: 0.0606
2025-06-23 07:14:18 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 07:14:23 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1525
2025-06-23 07:14:23 INFO FeHistory: [-334.0726991  -333.94888002 -333.9932688  ... -334.47956528 -334.51987766
 -334.42417083]
2025-06-23 07:14:23 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 07:14:23 INFO Good algorithm:
Algorithm Name: AdaptiveArchiveDE
import numpy as np
import random

# Name: AdaptiveArchiveDE
# Description: Differential Evolution with adaptive archive for multimodal optimization.
# Code:
class AdaptiveArchiveDE:
    """
    Combines Differential Evolution (DE) with an adaptive archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 100, archive_size: int = 200, F: float = 0.8, CR: float = 0.9):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.F = F
        self.CR = CR
        self.archive = []
        self.archive_fitness = []
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(self.population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            self._update_archive(offspring, offspring_fitness)
            
            self.population, fitness_values = self._select_next_generation(self.population, fitness_values, offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            r1, r2, r3 = random.sample(range(self.population_size), 3)
            while r1 == i or r2 == i or r3 == i:  # Ensure distinct indices
                r1, r2, r3 = random.sample(range(self.population_size), 3)
            
            mutant = population[r1] + self.F * (population[r2] - population[r3])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            
            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i] = np.where(cross_points, mutant, population[i])

        return offspring

    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append(offspring[i])
                self.archive_fitness.append(offspring_fitness[i])
            else:
                worst_index = np.argmax(self.archive_fitness)
                if offspring_fitness[i] < self.archive_fitness[worst_index]:
                    self.archive[worst_index] = offspring[i]
                    self.archive_fitness[worst_index] = offspring_fitness[i]

    def _select_next_generation(self, population, pop_fitness, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((pop_fitness, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
        return self.best_solution_overall, self.best_fitness_overall

2025-06-23 07:14:23 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 07:14:28 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 07:14:28 INFO FeHistory: [361594.89351847 166422.70785991 365980.1558966  ...  66079.65252257
  54292.18752256  46224.24076899]
2025-06-23 07:14:28 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 07:14:28 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:14:45 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0353
2025-06-23 07:14:45 INFO FeHistory: [ 32.62014409  18.97418219  50.4306237  ... -51.42996406 -44.72296196
 -46.79415202]
2025-06-23 07:14:45 INFO Expected Optimum FE: -100
2025-06-23 07:14:45 INFO Unimodal AOCC mean: 0.1525
2025-06-23 07:14:45 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 07:14:45 INFO Multimodal (multiple components) AOCC mean: 0.0353
2025-06-23 07:14:45 INFO AOCC mean: 0.0626
2025-06-23 07:14:52 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 07:14:57 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1503
2025-06-23 07:14:57 INFO FeHistory: [-334.02191869 -334.01618414 -333.95631128 ... -334.32216518 -334.26368742
 -334.25520745]
2025-06-23 07:14:57 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 07:14:57 INFO Good algorithm:
Algorithm Name: AdaptiveArchiveDE
import numpy as np
import random

# Name: AdaptiveArchiveDE
# Description: Differential Evolution with adaptive archive for multimodal optimization.
# Code:
class AdaptiveArchiveDE:
    """
    Combines Differential Evolution (DE) with an adaptive archive for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * self.dim  # Heuristic population sizing
        self.archive_size = 100
        self.archive = []
        self.F = 0.8  # Differential weight (can be adapted)
        self.CR = 0.9 # Crossover rate (can be adapted)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            self.update_archive(offspring, offspring_fitness)

            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            r1, r2, r3 = random.sample(range(self.population_size), 3)
            while r1 == i or r2 == i or r3 ==i: #Ensure different indices
                r1, r2, r3 = random.sample(range(self.population_size), 3)

            mutant = population[r1] + self.F * (population[r2] - population[r3])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i] = np.where(cross_points, mutant, population[i])

        return offspring


    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

2025-06-23 07:14:57 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 07:15:02 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 07:15:02 INFO FeHistory: [410371.38965315 164921.58693803 253349.99590034 ...  41945.38821645
  56097.56363983  80633.31475677]
2025-06-23 07:15:02 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 07:15:02 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:15:19 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0253
2025-06-23 07:15:19 INFO FeHistory: [ 24.4135757   35.96348448  21.71200815 ... -16.52355204 -28.0911151
 -15.58733445]
2025-06-23 07:15:19 INFO Expected Optimum FE: -100
2025-06-23 07:15:19 INFO Unimodal AOCC mean: 0.1503
2025-06-23 07:15:19 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 07:15:19 INFO Multimodal (multiple components) AOCC mean: 0.0253
2025-06-23 07:15:19 INFO AOCC mean: 0.0585
2025-06-23 07:15:28 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 07:15:33 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1688
2025-06-23 07:15:33 INFO FeHistory: [-333.98538289 -333.95150374 -333.99709749 ... -335.74154543 -335.74154543
 -335.74154543]
2025-06-23 07:15:33 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 07:15:33 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchive
import numpy as np
import random

# Name: AdaptiveDEwithArchive
# Description: Differential Evolution with adaptive archive for multimodal optimization.
# Code:
class AdaptiveDEwithArchive:
    """
    Combines Differential Evolution (DE) with an adaptive archive to 
    efficiently explore and exploit multimodal landscapes.  DE's inherent
    exploration is enhanced by an archive to maintain diversity and prevent
    premature convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.archive_fitness = []
        self.F = 0.8  # Differential weight (adaptive)
        self.CR = 0.9 # Crossover rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)

        while self.eval_count < self.budget:
            # Evaluate Population
            for i in range(self.population_size):
                if np.isinf(self.fitness[i]):
                    self.fitness[i] = objective_function(self.population[i:i+1])[0]
                    self.eval_count += 1

            # Update best solution
            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_index]
                self.best_solution_overall = self.population[best_index].copy()

            # Update Archive
            for i in range(self.population_size):
                if len(self.archive) < self.archive_size:
                    self.archive.append((self.population[i], self.fitness[i]))
                    self.archive_fitness.append(self.fitness[i])
                else:
                    worst_index = np.argmax(self.archive_fitness)
                    if self.fitness[i] < self.archive_fitness[worst_index]:
                        self.archive[worst_index] = (self.population[i], self.fitness[i])
                        self.archive_fitness[worst_index] = self.fitness[i]

            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = self.select_different(i)  #Randomly select 3 different individuals
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                offspring[i] = self.crossover(self.population[i], mutant, self.CR)
                offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection (Generational)
            combined_population = np.vstack((self.population, offspring))
            combined_fitness = np.concatenate((self.fitness, offspring_fitness))
            sorted_indices = np.argsort(combined_fitness)
            self.population = combined_population[sorted_indices][:self.population_size]
            self.fitness = combined_fitness[sorted_indices][:self.population_size]

            #Adaptive F (reduce exploration over time)
            self.F *= 0.995

        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def select_different(self, i):
        candidates = list(range(self.population_size))
        candidates.remove(i)
        return random.sample(candidates, 3)

    def crossover(self, x, v, CR):
        u = np.copy(x)
        jrand = random.randint(0, self.dim -1)
        for j in range(self.dim):
            if random.random() < CR or j == jrand:
                u[j] = v[j]
        return u

2025-06-23 07:15:33 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 07:15:38 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.1640
2025-06-23 07:15:38 INFO FeHistory: [271128.34390662 171748.2115797  225640.17040703 ...  -1002.26062374
  -1002.26062374  -1002.26062374]
2025-06-23 07:15:38 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 07:15:38 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchive
import numpy as np
import random

# Name: AdaptiveDEwithArchive
# Description: Differential Evolution with adaptive archive for multimodal optimization.
# Code:
class AdaptiveDEwithArchive:
    """
    Combines Differential Evolution (DE) with an adaptive archive to 
    efficiently explore and exploit multimodal landscapes.  DE's inherent
    exploration is enhanced by an archive to maintain diversity and prevent
    premature convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.archive_fitness = []
        self.F = 0.8  # Differential weight (adaptive)
        self.CR = 0.9 # Crossover rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)

        while self.eval_count < self.budget:
            # Evaluate Population
            for i in range(self.population_size):
                if np.isinf(self.fitness[i]):
                    self.fitness[i] = objective_function(self.population[i:i+1])[0]
                    self.eval_count += 1

            # Update best solution
            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_index]
                self.best_solution_overall = self.population[best_index].copy()

            # Update Archive
            for i in range(self.population_size):
                if len(self.archive) < self.archive_size:
                    self.archive.append((self.population[i], self.fitness[i]))
                    self.archive_fitness.append(self.fitness[i])
                else:
                    worst_index = np.argmax(self.archive_fitness)
                    if self.fitness[i] < self.archive_fitness[worst_index]:
                        self.archive[worst_index] = (self.population[i], self.fitness[i])
                        self.archive_fitness[worst_index] = self.fitness[i]

            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = self.select_different(i)  #Randomly select 3 different individuals
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                offspring[i] = self.crossover(self.population[i], mutant, self.CR)
                offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection (Generational)
            combined_population = np.vstack((self.population, offspring))
            combined_fitness = np.concatenate((self.fitness, offspring_fitness))
            sorted_indices = np.argsort(combined_fitness)
            self.population = combined_population[sorted_indices][:self.population_size]
            self.fitness = combined_fitness[sorted_indices][:self.population_size]

            #Adaptive F (reduce exploration over time)
            self.F *= 0.995

        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def select_different(self, i):
        candidates = list(range(self.population_size))
        candidates.remove(i)
        return random.sample(candidates, 3)

    def crossover(self, x, v, CR):
        u = np.copy(x)
        jrand = random.randint(0, self.dim -1)
        for j in range(self.dim):
            if random.random() < CR or j == jrand:
                u[j] = v[j]
        return u

2025-06-23 07:15:38 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:15:55 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.1572
2025-06-23 07:15:55 INFO FeHistory: [ 23.32759936  23.03689994  37.10436512 ... -99.2238292  -99.2238292
 -99.2238292 ]
2025-06-23 07:15:55 INFO Expected Optimum FE: -100
2025-06-23 07:15:55 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchive
import numpy as np
import random

# Name: AdaptiveDEwithArchive
# Description: Differential Evolution with adaptive archive for multimodal optimization.
# Code:
class AdaptiveDEwithArchive:
    """
    Combines Differential Evolution (DE) with an adaptive archive to 
    efficiently explore and exploit multimodal landscapes.  DE's inherent
    exploration is enhanced by an archive to maintain diversity and prevent
    premature convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.archive_fitness = []
        self.F = 0.8  # Differential weight (adaptive)
        self.CR = 0.9 # Crossover rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)

        while self.eval_count < self.budget:
            # Evaluate Population
            for i in range(self.population_size):
                if np.isinf(self.fitness[i]):
                    self.fitness[i] = objective_function(self.population[i:i+1])[0]
                    self.eval_count += 1

            # Update best solution
            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_index]
                self.best_solution_overall = self.population[best_index].copy()

            # Update Archive
            for i in range(self.population_size):
                if len(self.archive) < self.archive_size:
                    self.archive.append((self.population[i], self.fitness[i]))
                    self.archive_fitness.append(self.fitness[i])
                else:
                    worst_index = np.argmax(self.archive_fitness)
                    if self.fitness[i] < self.archive_fitness[worst_index]:
                        self.archive[worst_index] = (self.population[i], self.fitness[i])
                        self.archive_fitness[worst_index] = self.fitness[i]

            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = self.select_different(i)  #Randomly select 3 different individuals
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                offspring[i] = self.crossover(self.population[i], mutant, self.CR)
                offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection (Generational)
            combined_population = np.vstack((self.population, offspring))
            combined_fitness = np.concatenate((self.fitness, offspring_fitness))
            sorted_indices = np.argsort(combined_fitness)
            self.population = combined_population[sorted_indices][:self.population_size]
            self.fitness = combined_fitness[sorted_indices][:self.population_size]

            #Adaptive F (reduce exploration over time)
            self.F *= 0.995

        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def select_different(self, i):
        candidates = list(range(self.population_size))
        candidates.remove(i)
        return random.sample(candidates, 3)

    def crossover(self, x, v, CR):
        u = np.copy(x)
        jrand = random.randint(0, self.dim -1)
        for j in range(self.dim):
            if random.random() < CR or j == jrand:
                u[j] = v[j]
        return u

2025-06-23 07:15:55 INFO Unimodal AOCC mean: 0.1688
2025-06-23 07:15:55 INFO Multimodal (single component) AOCC mean: 0.1640
2025-06-23 07:15:55 INFO Multimodal (multiple components) AOCC mean: 0.1572
2025-06-23 07:15:55 INFO AOCC mean: 0.1633
2025-06-23 07:15:55 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

### Better code
AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: An evolutionary strategy using adaptive Gaussian sampling and an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingEAwithArchive:
    """
    An evolutionary strategy combining adaptive Gaussian sampling and an archive 
    to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Size of the archive
        self.archive = []
        self.sigma = 0.5 #Initial standard deviation for Gaussian sampling
        self.sigma_decay = 0.99 # Decay factor for sigma

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        self.fitness = np.full(self.population_size, np.inf)
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        while self.eval_count < self.budget:
            #Evaluate Population
            for i in range(self.population_size):
                if np.isinf(self.fitness[i]):
                    self.fitness[i] = objective_function(self.population[i:i+1])[0]
                    self.eval_count +=1

            #Update best solution
            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_index]
                self.best_solution_overall = self.population[best_index].copy()

            #Update Archive
            for i in range(self.population_size):
                if len(self.archive) < self.archive_size:
                    self.archive.append((self.population[i], self.fitness[i]))
                else:
                    worst_index = np.argmax([f for _,f in self.archive])
                    if self.fitness[i] < self.archive[worst_index][1]:
                        self.archive[worst_index] = (self.population[i], self.fitness[i])

            #Selection (Tournament Selection from archive + population)
            parents = []
            candidates = list(self.population) + [x for x,_ in self.archive]
            fitnesses = list(self.fitness) + [f for _,f in self.archive]
            for _ in range(self.population_size//2):
                tournament = random.sample(range(len(candidates)),5)
                winner1 = min(tournament, key=lambda i: fitnesses[i])
                tournament = random.sample(range(len(candidates)),5)
                winner2 = min(tournament, key=lambda i: fitnesses[i])
                parents.extend([winner1,winner2])

            #Recombination (Simple Averaging)
            offspring = np.zeros((self.population_size, self.dim))
            for i in range(0, self.population_size, 2):
                offspring[i] = (candidates[parents[i]] + candidates[parents[i+1]])/2
                offspring[i+1] = (candidates[parents[i]] + candidates[parents[i+1]])/2

            #Mutation (Adaptive Gaussian)
            for i in range(self.population_size):
                mutation = np.random.normal(0, self.sigma, self.dim)
                offspring[i] = np.clip(offspring[i] + mutation, self.lower_bounds, self.upper_bounds)

            self.population = offspring.copy()
            self.fitness = np.full(self.population_size, np.inf)
            self.sigma *= self.sigma_decay #decay sigma over time


        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

### Worse code
AdaptiveGaussianSamplingWithArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Adaptive Gaussian sampling EA enhanced with an archive to maintain diversity and escape local optima in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  # Size of the archive
        self.archive = None
        self.archive_fitness = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial sigma

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.archive = np.empty((0, self.dim))
        self.archive_fitness = np.array([])

        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Selection
            parents = self.tournament_selection(fitness_values, k=5)

            # Gaussian Mutation
            offspring = self.gaussian_mutation(parents, self.sigma)
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Archive Update
            self.update_archive(offspring, offspring_fitness)
            
            # Combine Population and Archive (for selection)
            combined_population = np.vstack((self.population, self.archive))
            combined_fitness = np.concatenate((fitness_values, self.archive_fitness))

            # Elitism
            sorted_indices = np.argsort(combined_fitness)
            self.population = combined_population[sorted_indices][:self.population_size]
            fitness_values = combined_fitness[sorted_indices][:self.population_size]

            # Update Best Solution
            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            #Adaptive Sigma (reduce exploration over time)
            self.sigma *= 0.999

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive = np.vstack((self.archive, offspring[i]))
                self.archive_fitness = np.append(self.archive_fitness, offspring_fitness[i])
            else:
                worst_index = np.argmax(self.archive_fitness)
                if offspring_fitness[i] < self.archive_fitness[worst_index]:
                    self.archive[worst_index] = offspring[i]
                    self.archive_fitness[worst_index] = offspring_fitness[i]

### Analyze & experience
- Comparing AdaptiveGaussianArchiveEA (best) vs AdaptiveGaussianSamplingEAwithArchive (worst), we see that the best-performing algorithm uses more sophisticated recombination and selection mechanisms (tournament selection and Gaussian recombination) leading to a more efficient exploration of the search space.  The worst algorithm uses simpler averaging recombination and lacks the adaptive sigma adjustment, hindering its ability to converge effectively and escape local optima.

AdaptiveGaussianSamplingWithArchiveEA (second best) vs ArchiveGuidedAdaptiveGaussianMutationEA (second worst) shows a similar trend. AdaptiveGaussianSamplingWithArchiveEA incorporates a more robust archive update mechanism and adaptive sigma adjustment, while ArchiveGuidedAdaptiveGaussianMutationEA uses simpler mutation strategies and lacks sophisticated selection and recombination techniques.

Comparing AdaptiveGaussianArchiveEA (1st) vs AdaptiveGaussianSamplingWithArchiveEA (2nd), we see minor differences. Both use archives and adaptive Gaussian sampling. However, the top performer has a more refined approach in adaptive sigma adjustment and tournament selection resulting in slightly better exploitation.

AdaptiveDifferentialEvolutionWithArchive (3rd) vs AdaptiveArchiveGuidedGaussianMutation (4th) reveals that DE's inherent exploration via vector differences provides a slight edge over simple Gaussian mutation. The adaptive archive helps both, but DE's structured variation provides superior performance in multimodal landscapes.

Comparing AdaptiveGaussianSamplingEAwithArchive (second worst) vs ArchiveGuidedAdaptiveGaussianMutationEA (worst), the use of tournament selection, intermediate recombination, and more refined adaptive mutation in the second worst algorithm contributes to a marginally better performance compared to the adaptive Gaussian mutation used in the worst-performing algorithm. Overall: The superior algorithms consistently employ adaptive mechanisms for mutation strength or selection pressure, incorporate archives for maintaining diversity, and leverage sophisticated recombination operators like Gaussian recombination or differential evolution. The use of tournament selection also seems beneficial.
- * **Keywords:** Adaptive, efficient, exploration-exploitation balance, diversity, simplicity.

* **Advice:** Focus on integrating simple, adaptive mechanisms for parameter control and efficient exploration. Prioritize data structures (like archives) that maintain solution diversity, especially for high-dimensional, multimodal problems.  Emphasize clear, measurable improvements in performance.

* **Avoid:** Overly complex algorithms,  unnecessary features,  lack of adaptability to problem characteristics, neglecting diversity maintenance.

* **Explanation:** Effective heuristics leverage adaptive strategies to balance exploration and exploitation, using efficient data structures to manage search.  Simplicity and adaptability are key to robust performance across different problem instances.  Prioritizing these aspects over sheer complexity leads to more effective and generalizable heuristics.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-23 07:15:55 INFO Mutation prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

Current heuristics:
AdaptiveGaussianArchiveEA
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for efficient multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])


Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- * **Keywords:** Adaptive, efficient, exploration-exploitation balance, diversity, simplicity.

* **Advice:** Focus on integrating simple, adaptive mechanisms for parameter control and efficient exploration. Prioritize data structures (like archives) that maintain solution diversity, especially for high-dimensional, multimodal problems.  Emphasize clear, measurable improvements in performance.

* **Avoid:** Overly complex algorithms,  unnecessary features,  lack of adaptability to problem characteristics, neglecting diversity maintenance.

* **Explanation:** Effective heuristics leverage adaptive strategies to balance exploration and exploitation, using efficient data structures to manage search.  Simplicity and adaptability are key to robust performance across different problem instances.  Prioritizing these aspects over sheer complexity leads to more effective and generalizable heuristics.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-23 07:16:44 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 07:17:44 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-23 07:19:35 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1537
2025-06-23 07:19:35 INFO FeHistory: [-334.01187369 -333.99061235 -334.02071585 ... -334.43335482 -334.4673121
 -334.49292293]
2025-06-23 07:19:35 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 07:19:35 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionArchiveEA
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionArchiveEA
# Description: Adaptive Differential Evolution with an archive for robust multimodal optimization, balancing exploration and exploitation efficiently.
# Code:
class AdaptiveDifferentialEvolutionArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.dim = dim

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_random_individuals(i, population.shape[0])
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)  # Ensure bounds
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_random_individuals(self, exclude_index, population_size):
        indices = random.sample(range(population_size), 3)
        while exclude_index in indices:
            indices = random.sample(range(population_size), 3)
        return indices

    def _crossover(self, x, v):
        u = np.zeros_like(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                u[j] = v[j]
            else:
                u[j] = x[j]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        # Simple adaptation: Adjust F based on population diversity
        std_dev = np.std(population, axis=0).mean()
        if std_dev < 0.1 * (self.upper_bounds.mean() - self.lower_bounds.mean()):
            self.F = min(1.0, self.F + 0.1)  # Increase exploration
        else:
            self.F = max(0.1, self.F - 0.05) # Decrease exploration

2025-06-23 07:19:35 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 07:22:24 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 07:22:24 INFO FeHistory: [218273.00081513 373919.76128613 183883.37330816 ...  28906.69567546
  48558.80182196  20441.22856731]
2025-06-23 07:22:24 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 07:22:24 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:25:29 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0492
2025-06-23 07:25:29 INFO FeHistory: [ 13.89318884   5.86951913  38.13931294 ... -42.26071829 -43.02319952
 -44.60270545]
2025-06-23 07:25:29 INFO Expected Optimum FE: -100
2025-06-23 07:25:29 INFO Unimodal AOCC mean: 0.1537
2025-06-23 07:25:29 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 07:25:29 INFO Multimodal (multiple components) AOCC mean: 0.0492
2025-06-23 07:25:29 INFO AOCC mean: 0.0676
2025-06-23 07:25:29 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 07:26:29 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-23 07:28:26 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1524
2025-06-23 07:28:26 INFO FeHistory: [-333.99321696 -334.0412735  -333.98534619 ... -334.5806924  -334.55052381
 -334.52567456]
2025-06-23 07:28:26 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 07:28:26 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithArchive
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Combines adaptive differential evolution with an archive for efficient multimodal optimization, emphasizing diversity and exploration-exploitation balance.
# Code:
class AdaptiveDifferentialEvolutionWithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

            # Adaptive parameter control (adjust F and CR based on performance)
            if self.eval_count % (self.budget // 10) == 0: #adjust every 10% of budget
                self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.copy(population)
        for i in range(self.population_size):
            a, b, c = self._select_random_vectors(i, population)
            mutant = a + self.F * (b - c)
            trial = self._crossover(population[i], mutant)
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring

    def _select_random_vectors(self, i, population):
        indices = np.random.choice(self.population_size, 3, replace=False)
        while i in indices:
            indices = np.random.choice(self.population_size, 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]


    def _crossover(self, x, mutant):
        trial = np.copy(x)
        jrand = np.random.randint(self.dim)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = mutant[j]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        # Simple adaptive strategy: Increase F if convergence is slow, decrease otherwise.
        # Adjust CR similarly based on diversity metrics (e.g., standard deviation of population).
        std_dev = np.std(population, axis=0)
        avg_std_dev = np.mean(std_dev)
        
        if avg_std_dev > 0.2*(np.max(self.upper_bounds) - np.min(self.lower_bounds)): #check for high diversity
            self.F = min(1.0, self.F + 0.1)  #Increase exploration
            self.CR = max(0.1, self.CR - 0.1) #Reduce exploitation
        else:
            self.F = max(0.1, self.F - 0.1) #Reduce exploration
            self.CR = min(0.99, self.CR + 0.1) #Increase exploitation
            
        self.F = np.clip(self.F, 0.1, 1.0) # Bound F
        self.CR = np.clip(self.CR, 0.1, 0.99) # Bound CR
2025-06-23 07:28:26 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 07:31:24 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 07:31:24 INFO FeHistory: [250601.27603596 212638.16528507 154295.39719684 ...  35582.97807864
  62191.20692489  40800.02588794]
2025-06-23 07:31:24 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 07:31:24 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:34:32 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0333
2025-06-23 07:34:32 INFO FeHistory: [ 32.75534973  17.54692047  14.4664957  ... -30.21536608 -40.10667457
 -57.87812178]
2025-06-23 07:34:32 INFO Expected Optimum FE: -100
2025-06-23 07:34:32 INFO Unimodal AOCC mean: 0.1524
2025-06-23 07:34:32 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 07:34:32 INFO Multimodal (multiple components) AOCC mean: 0.0333
2025-06-23 07:34:32 INFO AOCC mean: 0.0619
2025-06-23 07:34:32 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 07:35:32 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-23 07:36:54 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1628
2025-06-23 07:36:54 INFO FeHistory: [-333.98488873 -333.94880747 -333.97210522 ... -335.57733415 -335.57733415
 -335.57733415]
2025-06-23 07:36:54 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 07:36:54 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionArchiveEA
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionArchiveEA
# Description:  A Differential Evolution algorithm with adaptive mutation and an archive to maintain diversity and escape local optima in multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

            #Adaptive F and CR (simple example)
            self.F = max(0.1, self.F * (1 - 0.01 * (self.eval_count / self.budget)) ) #decrease F over time
            self.CR = 0.9 + (0.1 * (self.eval_count / self.budget)) # Increase CR slightly

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_different(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds) #bound mutant
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_three_different(self, exclude_index, population_size):
        indices = list(range(population_size))
        indices.remove(exclude_index)
        return random.sample(indices, 3)

    def _crossover(self, x, v):
        u = np.zeros_like(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                u[j] = v[j]
            else:
                u[j] = x[j]
        return u

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 07:36:54 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 07:39:15 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0644
2025-06-23 07:39:15 INFO FeHistory: [141874.49867955 298892.39244106 387455.80833824 ...   -999.45929403
   -999.45929403   -999.45929403]
2025-06-23 07:39:15 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 07:39:15 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:41:51 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.1150
2025-06-23 07:41:51 INFO FeHistory: [ 33.88588639   5.81355637  15.51891113 ... -98.81250745 -98.81250745
 -98.81250745]
2025-06-23 07:41:51 INFO Expected Optimum FE: -100
2025-06-23 07:41:51 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionArchiveEA
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionArchiveEA
# Description:  A Differential Evolution algorithm with adaptive mutation and an archive to maintain diversity and escape local optima in multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

            #Adaptive F and CR (simple example)
            self.F = max(0.1, self.F * (1 - 0.01 * (self.eval_count / self.budget)) ) #decrease F over time
            self.CR = 0.9 + (0.1 * (self.eval_count / self.budget)) # Increase CR slightly

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_different(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds) #bound mutant
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_three_different(self, exclude_index, population_size):
        indices = list(range(population_size))
        indices.remove(exclude_index)
        return random.sample(indices, 3)

    def _crossover(self, x, v):
        u = np.zeros_like(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                u[j] = v[j]
            else:
                u[j] = x[j]
        return u

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 07:41:51 INFO Unimodal AOCC mean: 0.1628
2025-06-23 07:41:51 INFO Multimodal (single component) AOCC mean: 0.0644
2025-06-23 07:41:51 INFO Multimodal (multiple components) AOCC mean: 0.1150
2025-06-23 07:41:51 INFO AOCC mean: 0.1141
2025-06-23 07:41:51 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 07:41:51 ERROR Can not run the algorithm
2025-06-23 07:41:51 INFO Run function 5 complete. FEHistory len: 201, AOCC: 0.1472
2025-06-23 07:41:51 INFO FeHistory: [-333.97001651 -334.02702914 -333.9528508  -334.03256794 -333.98870885
 -333.9920685  -333.98131731 -333.99408253 -333.93912682 -334.0364465
 -333.94927216 -334.00880866 -333.95406504 -333.91815157 -334.08958912
 -333.97916897 -334.05913156 -334.03750119 -333.94062489 -334.03019372
 -333.95734613 -333.97418201 -334.01031078 -333.94439494 -334.00677482
 -334.00018041 -334.00058767 -334.06698196 -333.98288308 -333.99649132
 -333.91783973 -333.92866471 -333.95220293 -334.03306356 -334.03392542
 -333.97618579 -333.94290289 -333.92398597 -333.98375715 -333.96638842
 -333.93684866 -333.96845551 -333.94777224 -333.95898938 -333.96723808
 -333.96783626 -333.99748178 -333.98122892 -334.11497065 -333.94753672
 -333.9441964  -334.03327576 -333.95815375 -333.93543954 -334.04366454
 -333.93950073 -333.95621904 -333.942903   -333.98781072 -334.06481369
 -333.96112923 -333.98807522 -333.96659743 -334.12372252 -333.97772605
 -333.96142436 -334.02070475 -333.95603552 -333.94494114 -333.89947307
 -334.01626688 -333.97349597 -334.00294306 -334.02249359 -333.99399528
 -334.00574459 -333.99479138 -334.13972725 -334.01201177 -334.02074901
 -333.99221115 -333.92309016 -333.92100629 -333.9932598  -333.95732144
 -334.02320279 -333.95774201 -333.9742529  -333.97142963 -334.04933027
 -334.01817403 -333.97320128 -334.03082316 -333.97538041 -334.02010356
 -333.9792583  -333.94589737 -334.0052077  -334.00013577 -334.03318519
 -333.95552141 -333.92437087 -333.93490938 -334.03745184 -333.93296611
 -333.92681552 -333.85864008 -333.95769287 -333.92656443 -333.91021459
 -333.9362529  -333.86996777 -333.95434002 -333.91500674 -333.88088413
 -333.92692333 -333.91690465 -333.98890618 -333.98749873 -333.90894729
 -333.92977174 -333.89332596 -333.9653664  -333.87520711 -333.89256119
 -334.0216747  -333.92703144 -333.92855682 -333.88827975 -333.92279839
 -333.96280734 -333.9015053  -333.87569235 -334.04094243 -333.93525319
 -333.96751309 -333.97469538 -333.93087447 -333.99442921 -333.93715904
 -333.88406482 -333.97274696 -333.93796909 -333.91496905 -333.88216267
 -333.94427885 -333.92192592 -333.97650731 -334.02658141 -333.98396473
 -333.95748338 -333.9895768  -333.91756682 -333.99434411 -334.04888679
 -333.92724617 -333.96381138 -333.97281909 -333.975874   -334.02494352
 -333.87923199 -333.97409453 -333.892675   -333.93654737 -333.93472578
 -333.98643211 -333.93325559 -334.01313902 -333.9989361  -333.93265943
 -333.97611565 -333.95654309 -333.98082517 -333.9407027  -333.92818254
 -334.01925876 -333.92755105 -334.11392495 -333.9372185  -333.9286087
 -333.99140992 -333.94059583 -333.89529418 -333.93806049 -333.90192004
 -334.02313415 -333.9478261  -333.93473808 -333.95958826 -333.98309103
 -333.98354914 -333.9339828  -333.90246908 -333.97807709 -333.8685364
 -333.92842569 -333.89979594 -333.99936485 -333.95640797 -334.01046768
 -333.9668071 ]
2025-06-23 07:41:51 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 07:41:51 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithArchive
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Combines adaptive differential evolution with an archive for efficient multimodal optimization, emphasizing diversity and adaptive parameter control.
# Code:
class AdaptiveDifferentialEvolutionWithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []
        self.k = 0.1 #parameter to control archive update
        

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            #Adapt F and CR based on success rate (Not implemented here for simplicity, but is recommended)
        

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_differents(i,len(population))
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_differents(self, target_index, population_size):
        indices = list(range(population_size))
        indices.remove(target_index)
        random.shuffle(indices)
        return indices[0], indices[1], indices[2]


    def _crossover(self, x, v):
        u = np.copy(x)
        jrand = random.randint(0, self.dim-1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                u[j] = v[j]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        if len(self.archive) < self.archive_size:
            for sol in combined:
                already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
                if not already_present:
                    self.archive.append(sol)
        else:
            for sol in combined:
                if random.random()< self.k:
                    worst_index = np.argmax([x[-1] for x in self.archive])
                    if sol[-1]<self.archive[worst_index][-1]:
                        self.archive[worst_index] = sol

        self.archive.sort(key=lambda x: x[-1])
        return np.array(self.archive)
2025-06-23 07:41:51 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 07:41:51 ERROR Can not run the algorithm
2025-06-23 07:41:51 INFO Run function 12 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-23 07:41:51 INFO FeHistory: [143043.88566662 213408.14467262 362843.82368031 254425.58743831
 433552.04680193 182608.7763908  248313.03591177 355808.92341796
 422052.6830348  258077.07747419 260427.59969475 180171.08479156
 299827.19997402 226112.27269254 226662.97786207 213430.31891582
 315282.82714583 170025.77293498 115215.2195747  194144.57769846
 184168.11751448 280076.36978178 305287.10934388 169324.35038151
 244738.20862649 150509.48219845 356641.89604636 210784.89717855
 102810.07045484 128884.16816635 155823.14098749 147852.33141769
 148085.80877568 240906.54289397 363584.0872268  164109.58592924
 192717.03849955 175544.0071731  101963.08406859 192146.52099993
 338171.46183041 102911.0983802  291010.79199401 245799.36901699
 136928.76290075 305705.35510136 206539.26212007 145249.51536581
 132350.22343093 221664.43913796 205644.36184204 106584.17698799
 344520.07512576 214206.32570338 267082.83396186 242337.43600149
 217896.65675242 190365.33177168 119373.00319542 243163.03985131
 161989.79746729 207275.3388512  227070.85135198 169070.35969038
 208725.56522476 194132.34673504 124856.00980618 369652.52706893
 304712.05530449 161189.6476764  310125.44831836 513828.63629225
 151036.13523785 115533.96843258 165812.50369571 320779.76907366
 340847.61885764 132824.06337179 281515.87172226 193061.08909579
 299430.01292349 307798.3659366  474160.84840933 203420.78244973
 312777.908285   199055.29730996 146680.35831538 410615.03510859
 169131.77958944 336286.15082219 101628.15256419 220303.76580389
 325557.18666141 177402.4530404  149921.75896979 284255.56274267
 126628.02229361 376354.84354939 220041.59059726 183239.96830174
 224478.53145327 399906.94328511 342593.18283466 282027.40592115
 187800.61543998 358556.4330993  295211.22170693 187742.89729773
 307900.62449229 204431.87860881 297325.36238456 305177.2183216
 378993.28937756 337107.92104305 474209.31395743 201408.68686583
 415594.90466374 233262.56456551 311203.51980395 271031.03268936
 179884.99998631 273399.22088574 474004.30283717 616102.13932164
 214934.66614931 499978.77817067 331676.54427954 368799.85034033
 289739.48770526 493955.6476455  397045.3162603  157578.8505521
 369278.93482475 507186.81726228 439057.25275669 104454.7577548
 163393.36326728 285276.2630739  128875.98589189 323692.91872032
 394131.75389443 206072.35542851 435240.80176977 167989.25525445
 233475.02448398 185512.99748492 179282.18899055 208867.1993347
 179012.71060897 328026.12586171 263029.29098763 215587.70955931
 337516.12135277 336453.38814774 330940.23099834 305771.88468812
 255686.4855681  242303.64898876 202616.51708769 209419.15372575
 227099.41120648 303169.54359272 431463.15907063 171002.53108025
 223066.85838097 197540.63404905 354219.24567078 146161.51429074
 158099.42543421 289204.95206524 234088.97185077 198307.35981257
 191141.92440372 342476.71764354 262417.53574887 203429.88125958
 406530.29603171 248695.0355048  115674.08125083 152136.76949854
 141614.29007117 317678.0662365  256227.46444044 322765.78250635
 573799.28899342 192185.53991445 336279.1306714  268107.38649935
 157238.47977205 223682.36549893 190164.86593127 291782.02901162
 338034.63963142 326677.14235558 254374.7071197  171743.46000361
 211107.97328458 409995.31988751 267486.09515072 258203.18028646
 319457.28217943]
2025-06-23 07:41:51 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 07:41:51 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:41:51 ERROR Can not run the algorithm
2025-06-23 07:41:51 INFO Run function 23 complete. FEHistory len: 201, AOCC: 0.0027
2025-06-23 07:41:51 INFO FeHistory: [49.67138661 25.19568774 39.10139481  0.97354582 31.32112274 24.63488633
 34.77447201 -0.58854785 33.60783356 36.95477382 19.98524421 39.11278392
 37.49730084 13.00921396 48.53308038 37.57899283  8.97766792 20.32692312
 37.63205716 27.52408816  3.98629351 30.39544019 11.10226389  9.05897075
 37.30088676 34.85973604 31.51198666 13.24068146 29.64891204 63.8438472
 16.54247831 21.85218614 26.28249377 17.52025456 19.46913145 18.38739736
 18.72581505 21.51498437 11.00713249 18.37502613 42.56924101 51.13051131
 23.60542785 14.37529692 20.38948659 17.25210577 25.81134866 33.46817693
 23.94620281 18.97446931 44.20771747 18.02453659 18.27646477 36.18157209
 27.68012713 37.05970595 19.05877422 10.23359727 43.98009883 30.92267442
  3.76991406 11.78858845  9.18639068 18.74339109 -6.03758877 14.36359527
 37.35572825 12.93276486  4.43874945 31.66612714 26.38041237 30.79414796
 14.51900278  0.39817262 18.47689963 25.90737446 42.99035099 24.74354011
 24.61086255 21.1043665  34.39096006 -0.54519211 13.12660963 13.08196301
 43.31373077 28.2391583  22.25012323 15.12572274 23.7949404  10.49522113
 15.24208314 28.3707963  24.43787405 19.94601064  5.43847828 28.02547726
 16.73594135 15.9326032  43.99463849 10.30211447 40.69999368 46.58587087
 39.28339904 30.1271439  38.84155643 41.91981983 45.77663674 32.60151661
  7.02018223 50.69484254 29.45682301 34.65702364 16.50700079 43.83975654
 33.52544993 21.35543732 22.7628153  31.40111852 -1.97724291 38.38892119
 59.07768785 34.1614424  30.93614602 34.34287003 45.08954426 43.78476177
 53.60355639 21.57333683  6.66896601 47.87766599 18.77811419 44.40298445
 46.41481625 17.55453191 48.17168917 25.64632668 32.06228306 39.03279329
 46.02704525 19.49456118 44.32201621 43.40485727 17.25958413 15.91474664
 28.26536694 56.80471741 56.26276204 18.10811039 45.5726608  21.55692197
 36.40613354  8.87506213 35.45927168 46.32766281 30.20720683 25.91055591
 62.33875878 57.65531899 33.36609837 29.42383268 44.3293289  47.77427061
 65.3854143  46.55180377 18.92275662 54.00401249 21.50587612 44.87930149
 21.55501203 42.47037707 37.69575254 40.39220738 39.9172157  38.69523065
 37.03024635 43.20308992 46.35840429 29.94327288 38.22066614 32.90229858
 32.61028086 25.01395612 27.31314241 27.09585213  2.99962819 51.83634956
 54.89524492 58.42332628 43.51119991 41.53765414 31.39124231 34.31624707
 23.66230521 66.11374673 28.54215293 18.30200547 43.53465145 19.76980624
  9.62854063  8.45404582 49.46683467]
2025-06-23 07:41:51 INFO Expected Optimum FE: -100
2025-06-23 07:41:51 INFO Unimodal AOCC mean: 0.1472
2025-06-23 07:41:51 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 07:41:51 INFO Multimodal (multiple components) AOCC mean: 0.0027
2025-06-23 07:41:51 INFO AOCC mean: 0.0500
2025-06-23 07:41:51 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 07:42:51 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-23 07:44:41 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1522
2025-06-23 07:44:41 INFO FeHistory: [-334.0301392  -334.02260372 -333.95640528 ... -334.48609128 -334.43106699
 -334.45049123]
2025-06-23 07:44:41 INFO Expected Optimum FE: -337.50899809752036
2025-06-23 07:44:41 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithArchive
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description:  Combines differential evolution with an adaptive mutation strategy and an archive to efficiently explore and exploit multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionWithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            # Differential Evolution Mutation
            a, b, c = self._select_different_individuals(population, i)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Crossover
            for j in range(self.dim):
                if random.random() < self.CR:
                    offspring[i, j] = mutant[j]
                else:
                    offspring[i, j] = population[i, j]
        return offspring

    def _select_different_individuals(self, population, i):
        indices = list(range(self.population_size))
        indices.remove(i)
        a, b, c = random.sample(indices, 3)
        return population[a], population[b], population[c]

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 07:44:41 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-23 07:47:30 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 07:47:30 INFO FeHistory: [460281.94021482 325523.96005335 158031.93590824 ...  43958.49272107
 110175.53435732  35116.04483052]
2025-06-23 07:47:30 INFO Expected Optimum FE: -1002.4790787013411
2025-06-23 07:47:30 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-23 07:50:30 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0371
2025-06-23 07:50:30 INFO FeHistory: [ 36.47534479  23.90380738  28.29193355 ... -38.81063963 -40.94608476
 -34.8077555 ]
2025-06-23 07:50:30 INFO Expected Optimum FE: -100
2025-06-23 07:50:30 INFO Unimodal AOCC mean: 0.1522
2025-06-23 07:50:30 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 07:50:30 INFO Multimodal (multiple components) AOCC mean: 0.0371
2025-06-23 07:50:30 INFO AOCC mean: 0.0631
2025-06-23 07:50:30 INFO Perform Harmony Search...
