2025-06-20 18:39:26 INFO Initializing first population
2025-06-20 18:39:26 INFO Initializing population from 4 seed files...
2025-06-20 18:39:26 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:39:41 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.6858
2025-06-20 18:39:41 INFO FeHistory: [ 1.25924073e+06  1.24829710e+06  1.16254437e+06 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 18:39:41 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:39:41 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:39:41 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:39:57 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.7083
2025-06-20 18:39:57 INFO FeHistory: [221911.2624215  207806.45859007 257880.30674801 ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 18:39:57 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:39:57 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:39:57 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:40:13 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.7082
2025-06-20 18:40:13 INFO FeHistory: [212706.54801727 230433.5780639  237190.10873776 ...   -656.78899799
   -656.78899799   -656.78899799]
2025-06-20 18:40:13 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:40:13 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:40:13 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:40:26 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:40:59 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0628
2025-06-20 18:40:59 INFO FeHistory: [160.99918153 197.29802648 179.04070224 ... -80.30515009 -80.30515009
 -80.30515009]
2025-06-20 18:40:59 INFO Expected Optimum FE: -100
2025-06-20 18:40:59 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:40:59 INFO Unimodal AOCC mean: 0.6858
2025-06-20 18:40:59 INFO Multimodal (single component) AOCC mean: 0.7083
2025-06-20 18:40:59 INFO Multimodal (multiple components) AOCC mean: 0.0628
2025-06-20 18:40:59 INFO AOCC mean: 0.5413
2025-06-20 18:40:59 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:41:14 ERROR Can not run the algorithm
2025-06-20 18:41:14 INFO Run function 4 complete. FEHistory len: 148981, AOCC: 0.2291
2025-06-20 18:41:14 INFO FeHistory: [ 1.01484380e+06  1.01213033e+06  1.01484380e+06 ... -3.82577797e+02
  8.77779289e+02 -3.82577797e+02]
2025-06-20 18:41:14 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:41:14 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-20 18:41:14 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:41:29 ERROR Can not run the algorithm
2025-06-20 18:41:30 INFO Run function 7 complete. FEHistory len: 149001, AOCC: 0.3414
2025-06-20 18:41:30 INFO FeHistory: [167958.61294561 169483.23284528 167958.61294561 ...   -912.85255877
   -833.09819259   -912.85255877]
2025-06-20 18:41:30 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:41:30 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-20 18:41:30 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:41:44 ERROR Can not run the algorithm
2025-06-20 18:41:45 INFO Run function 8 complete. FEHistory len: 149021, AOCC: 0.3624
2025-06-20 18:41:45 INFO FeHistory: [129174.21764147 131936.044824   129174.21764147 ...   -656.78579265
   -652.24716162   -656.78579265]
2025-06-20 18:41:45 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:41:45 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-20 18:41:45 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:41:59 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:42:32 ERROR Can not run the algorithm
2025-06-20 18:42:32 INFO Run function 24 complete. FEHistory len: 149121, AOCC: 0.0000
2025-06-20 18:42:32 INFO FeHistory: [198.8419616  189.03423037 198.8419616  ... 114.13664904 129.66714218
 114.13664904]
2025-06-20 18:42:32 INFO Expected Optimum FE: -100
2025-06-20 18:42:32 INFO Unimodal AOCC mean: 0.2291
2025-06-20 18:42:32 INFO Multimodal (single component) AOCC mean: 0.3519
2025-06-20 18:42:32 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:42:32 INFO AOCC mean: 0.2332
2025-06-20 18:42:32 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:42:47 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.5139
2025-06-20 18:42:47 INFO FeHistory: [ 5.85394063e+05  1.03228556e+06  9.87943511e+05 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 18:42:47 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:42:47 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 18:42:47 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:43:02 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.4424
2025-06-20 18:43:02 INFO FeHistory: [142482.48048457 172392.19619908 212427.89673585 ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 18:43:02 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:43:02 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 18:43:02 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:43:17 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.4489
2025-06-20 18:43:17 INFO FeHistory: [190328.83124694 170805.29511114 154461.52899101 ...   -656.78899799
   -656.78899799   -656.78899799]
2025-06-20 18:43:17 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:43:17 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 18:43:17 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:43:32 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:44:02 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0964
2025-06-20 18:44:02 INFO FeHistory: [183.60043531 167.36925069 163.91082017 ... -98.13858302 -98.13086079
 -98.15702118]
2025-06-20 18:44:02 INFO Expected Optimum FE: -100
2025-06-20 18:44:02 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 18:44:02 INFO Unimodal AOCC mean: 0.5139
2025-06-20 18:44:02 INFO Multimodal (single component) AOCC mean: 0.4456
2025-06-20 18:44:02 INFO Multimodal (multiple components) AOCC mean: 0.0964
2025-06-20 18:44:02 INFO AOCC mean: 0.3754
2025-06-20 18:44:02 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:44:21 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.6397
2025-06-20 18:44:21 INFO FeHistory: [ 9.39913006e+05  6.06099601e+05  1.02308194e+06 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 18:44:21 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:44:21 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:44:21 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:44:40 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.6838
2025-06-20 18:44:40 INFO FeHistory: [311811.08095075 284797.83340093 155663.66644179 ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 18:44:40 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:44:40 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:44:40 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:44:59 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.6801
2025-06-20 18:44:59 INFO FeHistory: [232670.62472936 171365.94603827 213105.64603773 ...   -656.78899799
   -656.78899799   -656.78899799]
2025-06-20 18:44:59 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:44:59 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:44:59 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:45:02 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:45:50 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0332
2025-06-20 18:45:50 INFO FeHistory: [206.117641   189.13775587 198.46266513 ... -59.2995626  -59.56301536
 -61.08577044]
2025-06-20 18:45:50 INFO Expected Optimum FE: -100
2025-06-20 18:45:50 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 18:45:50 INFO Unimodal AOCC mean: 0.6397
2025-06-20 18:45:50 INFO Multimodal (single component) AOCC mean: 0.6820
2025-06-20 18:45:50 INFO Multimodal (multiple components) AOCC mean: 0.0332
2025-06-20 18:45:50 INFO AOCC mean: 0.5092
2025-06-20 18:45:50 INFO Started evolutionary loop, best so far: 0.5412867233158046
2025-06-20 18:45:50 INFO --- Performing Long-Term Reflection at Generation 1 ---
2025-06-20 18:45:54 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) AdaptiveMultimodalOptimizerImproved, we see that AdaptiveGaussianSamplingEA uses a more sophisticated population initialization based on an adaptive Gaussian distribution centered around a random point, leading to better exploration of the search space.  AdaptiveMultimodalOptimizerImproved relies on a simpler uniform random initialization and local search, which can get stuck in local optima more easily in high-dimensional spaces.  (second best) AdaptiveGaussianMutationDE vs (second worst) EnhancedArchiveGuidedDE: AdaptiveGaussianMutationDE incorporates adaptive Gaussian mutation within a DE framework, providing a balance between exploration and exploitation.  EnhancedArchiveGuidedDE uses an archive to guide the search but lacks the adaptive mutation strategy for escaping local optima effectively. Comparing (1st) vs (2nd), we see that AdaptiveGaussianSamplingEA's direct Gaussian sampling of the initial population is more efficient than AdaptiveGaussianMutationDE's iterative approach using differential evolution, resulting in faster convergence. (3rd) vs (4th): EnhancedArchiveGuidedDE benefits from the archive for maintaining diversity and focusing on promising regions, offering a clear advantage over AdaptiveMultimodalOptimizerImproved's simplistic tabu search and simulated annealing which struggle with high dimensionality. Comparing (second worst) vs (worst), we see that EnhancedArchiveGuidedDE's use of an archive and differential evolution provides more structured exploration than AdaptiveMultimodalOptimizerImproved's random perturbations and local search, hence a better average performance. Overall: The top two algorithms demonstrate that adaptive Gaussian sampling and mutation strategies are highly effective for high-dimensional problems, while simpler methods based on local search or random perturbations are less robust and efficient.  The archive used in EnhancedArchiveGuidedDE seems helpful in maintaining diversity but lacks the sophisticated adaptation found in the Gaussian-based approaches.

**Experience:**

Adaptive Gaussian sampling and mutation are crucial for high-dimensional optimization.  Maintaining population diversity through archive mechanisms or other strategies is also beneficial, particularly for multimodal functions.  Careful consideration of the balance between exploration and exploitation is essential for efficient algorithm design.

2025-06-20 18:45:54 INFO Generating offspring via Crossover...
2025-06-20 18:46:03 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:46:19 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.3508
2025-06-20 18:46:19 INFO FeHistory: [ 1.06044850e+06  1.17509974e+06  1.19735400e+06 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 18:46:19 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:46:19 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
from scipy.stats import cauchy

# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling, archive-based diversity, and differential evolution for multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            #Selection
            parents = self._tournament_selection(population, fitness_values)
            
            #Recombination (Differential Evolution)
            offspring = self._differential_evolution(parents)

            #Mutation (Adaptive Gaussian)
            offspring = self._adaptive_gaussian_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Update Archive
            self._update_archive(offspring, offspring_fitness)

            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population


    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _differential_evolution(self, parents):
        offspring = []
        F = 0.8 #Scaling factor
        for i in range(len(parents)):
            a, b, c = np.random.choice(len(parents), 3, replace=False)
            while a == i or b == i or c == i:
                a, b, c = np.random.choice(len(parents), 3, replace=False)
            
            offspring_i = parents[i] + F*(parents[a] - parents[b]) + F*(parents[c]-parents[i])
            offspring.append(np.clip(offspring_i, self.lower_bounds, self.upper_bounds))
        return np.array(offspring)


    def _adaptive_gaussian_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _update_archive(self, offspring, offspring_fitness):
        for i, solution in enumerate(offspring):
          fitness = offspring_fitness[i]
          self.archive.append((solution, fitness))
        self.archive = sorted(self.archive, key=lambda item: item[1])[:self.archive_size]
        
    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:46:19 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:46:36 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.3932
2025-06-20 18:46:36 INFO FeHistory: [230629.40081674 391697.76788499 235668.33640286 ...   -912.85737396
   -912.85737395   -912.85737394]
2025-06-20 18:46:36 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:46:36 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
from scipy.stats import cauchy

# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling, archive-based diversity, and differential evolution for multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            #Selection
            parents = self._tournament_selection(population, fitness_values)
            
            #Recombination (Differential Evolution)
            offspring = self._differential_evolution(parents)

            #Mutation (Adaptive Gaussian)
            offspring = self._adaptive_gaussian_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Update Archive
            self._update_archive(offspring, offspring_fitness)

            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population


    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _differential_evolution(self, parents):
        offspring = []
        F = 0.8 #Scaling factor
        for i in range(len(parents)):
            a, b, c = np.random.choice(len(parents), 3, replace=False)
            while a == i or b == i or c == i:
                a, b, c = np.random.choice(len(parents), 3, replace=False)
            
            offspring_i = parents[i] + F*(parents[a] - parents[b]) + F*(parents[c]-parents[i])
            offspring.append(np.clip(offspring_i, self.lower_bounds, self.upper_bounds))
        return np.array(offspring)


    def _adaptive_gaussian_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _update_archive(self, offspring, offspring_fitness):
        for i, solution in enumerate(offspring):
          fitness = offspring_fitness[i]
          self.archive.append((solution, fitness))
        self.archive = sorted(self.archive, key=lambda item: item[1])[:self.archive_size]
        
    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:46:36 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:46:52 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.3947
2025-06-20 18:46:52 INFO FeHistory: [194420.62944729 167710.92509271 152196.24172826 ...   -656.78899795
   -656.78899797   -656.78899797]
2025-06-20 18:46:52 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:46:52 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
from scipy.stats import cauchy

# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling, archive-based diversity, and differential evolution for multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            #Selection
            parents = self._tournament_selection(population, fitness_values)
            
            #Recombination (Differential Evolution)
            offspring = self._differential_evolution(parents)

            #Mutation (Adaptive Gaussian)
            offspring = self._adaptive_gaussian_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Update Archive
            self._update_archive(offspring, offspring_fitness)

            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population


    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _differential_evolution(self, parents):
        offspring = []
        F = 0.8 #Scaling factor
        for i in range(len(parents)):
            a, b, c = np.random.choice(len(parents), 3, replace=False)
            while a == i or b == i or c == i:
                a, b, c = np.random.choice(len(parents), 3, replace=False)
            
            offspring_i = parents[i] + F*(parents[a] - parents[b]) + F*(parents[c]-parents[i])
            offspring.append(np.clip(offspring_i, self.lower_bounds, self.upper_bounds))
        return np.array(offspring)


    def _adaptive_gaussian_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _update_archive(self, offspring, offspring_fitness):
        for i, solution in enumerate(offspring):
          fitness = offspring_fitness[i]
          self.archive.append((solution, fitness))
        self.archive = sorted(self.archive, key=lambda item: item[1])[:self.archive_size]
        
    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:46:52 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:47:03 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:47:37 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0485
2025-06-20 18:47:37 INFO FeHistory: [174.41004173 220.15194067 197.77582552 ... -77.03123827 -77.03123829
 -77.03123822]
2025-06-20 18:47:37 INFO Expected Optimum FE: -100
2025-06-20 18:47:37 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
from scipy.stats import cauchy

# Name: AdaptiveGaussianArchiveEA
# Description: Combines adaptive Gaussian sampling, archive-based diversity, and differential evolution for multimodal optimization.
# Code:
class AdaptiveGaussianArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            #Selection
            parents = self._tournament_selection(population, fitness_values)
            
            #Recombination (Differential Evolution)
            offspring = self._differential_evolution(parents)

            #Mutation (Adaptive Gaussian)
            offspring = self._adaptive_gaussian_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Update Archive
            self._update_archive(offspring, offspring_fitness)

            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population


    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _differential_evolution(self, parents):
        offspring = []
        F = 0.8 #Scaling factor
        for i in range(len(parents)):
            a, b, c = np.random.choice(len(parents), 3, replace=False)
            while a == i or b == i or c == i:
                a, b, c = np.random.choice(len(parents), 3, replace=False)
            
            offspring_i = parents[i] + F*(parents[a] - parents[b]) + F*(parents[c]-parents[i])
            offspring.append(np.clip(offspring_i, self.lower_bounds, self.upper_bounds))
        return np.array(offspring)


    def _adaptive_gaussian_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _update_archive(self, offspring, offspring_fitness):
        for i, solution in enumerate(offspring):
          fitness = offspring_fitness[i]
          self.archive.append((solution, fitness))
        self.archive = sorted(self.archive, key=lambda item: item[1])[:self.archive_size]
        
    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 18:47:37 INFO Unimodal AOCC mean: 0.3508
2025-06-20 18:47:37 INFO Multimodal (single component) AOCC mean: 0.3939
2025-06-20 18:47:37 INFO Multimodal (multiple components) AOCC mean: 0.0485
2025-06-20 18:47:37 INFO AOCC mean: 0.2968
2025-06-20 18:47:45 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:48:04 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.6415
2025-06-20 18:48:04 INFO FeHistory: [ 1.45847046e+06  1.32120977e+06  2.14783850e+06 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 18:48:04 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:48:04 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:48:04 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:48:23 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.6827
2025-06-20 18:48:23 INFO FeHistory: [276108.19463562 212546.86806156 169755.09307064 ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 18:48:23 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:48:23 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:48:23 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:48:42 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.6812
2025-06-20 18:48:42 INFO FeHistory: [296915.72249296 160046.15743641 378190.99870375 ...   -656.78899799
   -656.78899799   -656.78899799]
2025-06-20 18:48:42 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:48:42 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:48:42 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:48:45 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:49:32 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0315
2025-06-20 18:49:32 INFO FeHistory: [198.30460488 205.6178041  206.39675466 ... -61.32625601 -61.32625185
 -60.52904443]
2025-06-20 18:49:32 INFO Expected Optimum FE: -100
2025-06-20 18:49:32 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 18:49:32 INFO Unimodal AOCC mean: 0.6415
2025-06-20 18:49:32 INFO Multimodal (single component) AOCC mean: 0.6819
2025-06-20 18:49:32 INFO Multimodal (multiple components) AOCC mean: 0.0315
2025-06-20 18:49:32 INFO AOCC mean: 0.5092
2025-06-20 18:49:32 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

### Worse code
AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) AdaptiveMultimodalOptimizerImproved, we see that AdaptiveGaussianSamplingEA uses a more sophisticated population initialization based on an adaptive Gaussian distribution centered around a random point, leading to better exploration of the search space.  AdaptiveMultimodalOptimizerImproved relies on a simpler uniform random initialization and local search, which can get stuck in local optima more easily in high-dimensional spaces.  (second best) AdaptiveGaussianMutationDE vs (second worst) EnhancedArchiveGuidedDE: AdaptiveGaussianMutationDE incorporates adaptive Gaussian mutation within a DE framework, providing a balance between exploration and exploitation.  EnhancedArchiveGuidedDE uses an archive to guide the search but lacks the adaptive mutation strategy for escaping local optima effectively. Comparing (1st) vs (2nd), we see that AdaptiveGaussianSamplingEA's direct Gaussian sampling of the initial population is more efficient than AdaptiveGaussianMutationDE's iterative approach using differential evolution, resulting in faster convergence. (3rd) vs (4th): EnhancedArchiveGuidedDE benefits from the archive for maintaining diversity and focusing on promising regions, offering a clear advantage over AdaptiveMultimodalOptimizerImproved's simplistic tabu search and simulated annealing which struggle with high dimensionality. Comparing (second worst) vs (worst), we see that EnhancedArchiveGuidedDE's use of an archive and differential evolution provides more structured exploration than AdaptiveMultimodalOptimizerImproved's random perturbations and local search, hence a better average performance. Overall: The top two algorithms demonstrate that adaptive Gaussian sampling and mutation strategies are highly effective for high-dimensional problems, while simpler methods based on local search or random perturbations are less robust and efficient.  The archive used in EnhancedArchiveGuidedDE seems helpful in maintaining diversity but lacks the sophisticated adaptation found in the Gaussian-based approaches.
- Adaptive Gaussian sampling and mutation are crucial for high-dimensional optimization.  Maintaining population diversity through archive mechanisms or other strategies is also beneficial, particularly for multimodal functions.  Careful consideration of the balance between exploration and exploitation is essential for efficient algorithm design.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:49:32 INFO Generation 2, best so far: 0.5412867233158046
2025-06-20 18:49:32 INFO --- Performing Long-Term Reflection at Generation 2 ---
2025-06-20 18:49:37 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) AdaptiveGaussianArchiveEA, we see that the top performer uses a simpler, more focused approach with adaptive Gaussian sampling for initialization and mutation, along with tournament selection and a straightforward recombination method.  The worst performer, conversely, combines multiple techniques (adaptive Gaussian sampling, archive-based diversity, and differential evolution), leading to increased complexity and potentially less efficient exploration of the search space.  AdaptiveGaussianSamplingEA directly adapts the Gaussian width based on performance, while AdaptiveGaussianArchiveEA only decays its sigma parameter.

(second best) AdaptiveGaussianSamplingDE vs (second worst) EnhancedArchiveGuidedDE: AdaptiveGaussianSamplingDE uses a simpler DE approach with adaptive Gaussian perturbation for mutation, focusing on efficient exploration.  EnhancedArchiveGuidedDE uses an archive to maintain diversity and incorporates a pbest mechanism but has a more complex offspring generation process, potentially slowing down convergence. AdaptiveGaussianSamplingDE also incorporates the Gaussian mutation directly into the DE framework, enhancing exploration and escaping local optima, unlike EnhancedArchiveGuidedDE's simpler approach.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that while both utilize adaptive Gaussian sampling, the EA approach (AdaptiveGaussianSamplingEA) with its tournament selection and simpler recombination strategy demonstrates slightly superior performance.  AdaptiveGaussianSamplingDE, incorporating Differential Evolution, introduces more complexity without a significant performance gain in this specific case.


(3rd) AdaptiveGaussianMutationDE vs (4th) EnhancedArchiveGuidedDE: Both algorithms use DE, but AdaptiveGaussianMutationDE adds adaptive Gaussian mutation, providing a mechanism to escape local optima more effectively than the archive-based approach of EnhancedArchiveGuidedDE. EnhancedArchiveGuidedDE attempts to improve diversity using an archive, but this strategy is less effective compared to the adaptive Gaussian mutation.

Comparing (second worst) EnhancedArchiveGuidedDE vs (worst) AdaptiveGaussianArchiveEA:  EnhancedArchiveGuidedDE shows better performance because its archive mechanism, while complex, aids in maintaining diversity in the search space. AdaptiveGaussianArchiveEA's combination of numerous techniques (differential evolution, adaptive Gaussian mutation, and archive) appears less effective.  The complexity added by multiple techniques negatively impacts the algorithm's efficiency in this scenario.

Overall:  Simpler, focused algorithms with effective mechanisms for adaptive exploration and exploitation, such as those using adaptive Gaussian sampling and direct mutation control, generally outperformed more complex hybrids combining several techniques.


**Experience:**

Combining multiple sophisticated techniques doesn't guarantee better performance.  A well-tuned, simpler algorithm focused on effective exploration and exploitation of the search space can outperform more complex methods.  The adaptive scaling and mutation strategies are key to achieving good convergence in high-dimensional problems.

2025-06-20 18:49:37 INFO Generating offspring via Crossover...
2025-06-20 18:49:46 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:50:04 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.3643
2025-06-20 18:50:04 INFO FeHistory: [ 9.46551897e+05  7.21865364e+05  5.94278105e+05 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 18:50:04 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:50:04 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithTournamentSelection
import numpy as np

class AdaptiveGaussianDEwithTournamentSelection:
    """
    Combines adaptive Gaussian mutation with Differential Evolution and Tournament Selection for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds) # Initial Gaussian width
        self.sigma_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0] if self.dim > 0 else float('inf')
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring = self._adaptive_gaussian_mutation(offspring) # Add Gaussian mutation

            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._tournament_selection(population, fitness_values, offspring, offspring_fitness)

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)


    def _differential_evolution(self, population, fitness_values):
        offspring = np.copy(population)
        for i in range(self.population_size):
            # Select three distinct vectors from the population (excluding the current one)
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            
            # Create a mutant vector using Differential Evolution
            mutant = population[a] + self.F * (population[b] - population[c])
            
            # Perform binomial crossover
            jrand = np.random.randint(0, self.dim)
            for j in range(self.dim):
                if np.random.rand() < self.CR or j == jrand:
                    offspring[i, j] = mutant[j]
                    
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _adaptive_gaussian_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        tournament_size = 5
        next_gen = np.zeros_like(population)
        next_fit = np.zeros_like(fitness_values)
        for i in range(self.population_size):
            tournament = np.random.choice(len(combined_pop), tournament_size, replace=False)
            winner_index = tournament[np.argmin(combined_fit[tournament])]
            next_gen[i] = combined_pop[winner_index]
            next_fit[i] = combined_fit[winner_index]
            
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 18:50:04 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:50:22 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.3982
2025-06-20 18:50:22 INFO FeHistory: [177353.00488162 275009.62574075 212597.28774173 ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 18:50:22 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:50:22 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithTournamentSelection
import numpy as np

class AdaptiveGaussianDEwithTournamentSelection:
    """
    Combines adaptive Gaussian mutation with Differential Evolution and Tournament Selection for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds) # Initial Gaussian width
        self.sigma_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0] if self.dim > 0 else float('inf')
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring = self._adaptive_gaussian_mutation(offspring) # Add Gaussian mutation

            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._tournament_selection(population, fitness_values, offspring, offspring_fitness)

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)


    def _differential_evolution(self, population, fitness_values):
        offspring = np.copy(population)
        for i in range(self.population_size):
            # Select three distinct vectors from the population (excluding the current one)
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            
            # Create a mutant vector using Differential Evolution
            mutant = population[a] + self.F * (population[b] - population[c])
            
            # Perform binomial crossover
            jrand = np.random.randint(0, self.dim)
            for j in range(self.dim):
                if np.random.rand() < self.CR or j == jrand:
                    offspring[i, j] = mutant[j]
                    
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _adaptive_gaussian_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        tournament_size = 5
        next_gen = np.zeros_like(population)
        next_fit = np.zeros_like(fitness_values)
        for i in range(self.population_size):
            tournament = np.random.choice(len(combined_pop), tournament_size, replace=False)
            winner_index = tournament[np.argmin(combined_fit[tournament])]
            next_gen[i] = combined_pop[winner_index]
            next_fit[i] = combined_fit[winner_index]
            
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 18:50:22 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:50:41 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0640
2025-06-20 18:50:41 INFO FeHistory: [149510.15758226 222447.27859054 293419.83880554 ...   -644.0162886
   -644.01628723   -644.01629062]
2025-06-20 18:50:41 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:50:41 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithTournamentSelection
import numpy as np

class AdaptiveGaussianDEwithTournamentSelection:
    """
    Combines adaptive Gaussian mutation with Differential Evolution and Tournament Selection for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds) # Initial Gaussian width
        self.sigma_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0] if self.dim > 0 else float('inf')
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring = self._adaptive_gaussian_mutation(offspring) # Add Gaussian mutation

            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._tournament_selection(population, fitness_values, offspring, offspring_fitness)

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)


    def _differential_evolution(self, population, fitness_values):
        offspring = np.copy(population)
        for i in range(self.population_size):
            # Select three distinct vectors from the population (excluding the current one)
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            
            # Create a mutant vector using Differential Evolution
            mutant = population[a] + self.F * (population[b] - population[c])
            
            # Perform binomial crossover
            jrand = np.random.randint(0, self.dim)
            for j in range(self.dim):
                if np.random.rand() < self.CR or j == jrand:
                    offspring[i, j] = mutant[j]
                    
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _adaptive_gaussian_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        tournament_size = 5
        next_gen = np.zeros_like(population)
        next_fit = np.zeros_like(fitness_values)
        for i in range(self.population_size):
            tournament = np.random.choice(len(combined_pop), tournament_size, replace=False)
            winner_index = tournament[np.argmin(combined_fit[tournament])]
            next_gen[i] = combined_pop[winner_index]
            next_fit[i] = combined_fit[winner_index]
            
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 18:50:41 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:50:46 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:51:28 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0114
2025-06-20 18:51:28 INFO FeHistory: [167.11653757 242.15579533 205.98192245 ... -28.06650976 -28.06649383
 -28.0664993 ]
2025-06-20 18:51:28 INFO Expected Optimum FE: -100
2025-06-20 18:51:28 INFO Unimodal AOCC mean: 0.3643
2025-06-20 18:51:28 INFO Multimodal (single component) AOCC mean: 0.2311
2025-06-20 18:51:28 INFO Multimodal (multiple components) AOCC mean: 0.0114
2025-06-20 18:51:28 INFO AOCC mean: 0.2094
2025-06-20 18:51:35 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:51:35 ERROR Can not run the algorithm
2025-06-20 18:51:35 INFO Run function 4 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 18:51:35 INFO FeHistory: []
2025-06-20 18:51:35 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:51:35 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:51:35 ERROR Can not run the algorithm
2025-06-20 18:51:35 INFO Run function 7 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 18:51:35 INFO FeHistory: []
2025-06-20 18:51:35 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:51:35 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:51:35 ERROR Can not run the algorithm
2025-06-20 18:51:35 INFO Run function 8 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 18:51:35 INFO FeHistory: []
2025-06-20 18:51:35 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:51:35 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:51:35 ERROR Can not run the algorithm
2025-06-20 18:51:35 INFO Run function 24 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 18:51:35 INFO FeHistory: []
2025-06-20 18:51:35 INFO Expected Optimum FE: -100
2025-06-20 18:51:35 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:51:35 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:51:35 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:51:35 INFO AOCC mean: 0.0000
2025-06-20 18:51:35 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) AdaptiveGaussianArchiveEA, we see that the top performer uses a simpler, more focused approach with adaptive Gaussian sampling for initialization and mutation, along with tournament selection and a straightforward recombination method.  The worst performer, conversely, combines multiple techniques (adaptive Gaussian sampling, archive-based diversity, and differential evolution), leading to increased complexity and potentially less efficient exploration of the search space.  AdaptiveGaussianSamplingEA directly adapts the Gaussian width based on performance, while AdaptiveGaussianArchiveEA only decays its sigma parameter.

(second best) AdaptiveGaussianSamplingDE vs (second worst) EnhancedArchiveGuidedDE: AdaptiveGaussianSamplingDE uses a simpler DE approach with adaptive Gaussian perturbation for mutation, focusing on efficient exploration.  EnhancedArchiveGuidedDE uses an archive to maintain diversity and incorporates a pbest mechanism but has a more complex offspring generation process, potentially slowing down convergence. AdaptiveGaussianSamplingDE also incorporates the Gaussian mutation directly into the DE framework, enhancing exploration and escaping local optima, unlike EnhancedArchiveGuidedDE's simpler approach.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that while both utilize adaptive Gaussian sampling, the EA approach (AdaptiveGaussianSamplingEA) with its tournament selection and simpler recombination strategy demonstrates slightly superior performance.  AdaptiveGaussianSamplingDE, incorporating Differential Evolution, introduces more complexity without a significant performance gain in this specific case.


(3rd) AdaptiveGaussianMutationDE vs (4th) EnhancedArchiveGuidedDE: Both algorithms use DE, but AdaptiveGaussianMutationDE adds adaptive Gaussian mutation, providing a mechanism to escape local optima more effectively than the archive-based approach of EnhancedArchiveGuidedDE. EnhancedArchiveGuidedDE attempts to improve diversity using an archive, but this strategy is less effective compared to the adaptive Gaussian mutation.

Comparing (second worst) EnhancedArchiveGuidedDE vs (worst) AdaptiveGaussianArchiveEA:  EnhancedArchiveGuidedDE shows better performance because its archive mechanism, while complex, aids in maintaining diversity in the search space. AdaptiveGaussianArchiveEA's combination of numerous techniques (differential evolution, adaptive Gaussian mutation, and archive) appears less effective.  The complexity added by multiple techniques negatively impacts the algorithm's efficiency in this scenario.

Overall:  Simpler, focused algorithms with effective mechanisms for adaptive exploration and exploitation, such as those using adaptive Gaussian sampling and direct mutation control, generally outperformed more complex hybrids combining several techniques.
- Combining multiple sophisticated techniques doesn't guarantee better performance.  A well-tuned, simpler algorithm focused on effective exploration and exploitation of the search space can outperform more complex methods.  The adaptive scaling and mutation strategies are key to achieving good convergence in high-dimensional problems.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:51:35 INFO Generation 3, best so far: 0.5412867233158046
2025-06-20 18:51:35 INFO --- Performing Long-Term Reflection at Generation 3 ---
2025-06-20 18:51:39 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) AdaptiveGaussianArchiveEA, we see that AdaptiveGaussianSamplingEA uses a more refined adaptive Gaussian sampling for initialization, focusing on a single Gaussian distribution centered around a random point, and incorporating adaptive mutation within its evolutionary process.  AdaptiveGaussianArchiveEA, while employing adaptive Gaussian mutation, combines it with differential evolution and an archive, potentially leading to increased computational overhead without a commensurate improvement in performance.  (second best) AdaptiveGaussianSamplingDE vs (second worst) EnhancedArchiveGuidedDE shows that integrating differential evolution (DE) with adaptive Gaussian sampling in AdaptiveGaussianSamplingDE improves exploration compared to EnhancedArchiveGuidedDE's reliance on a simple uniform random initialization and archive-based guidance. AdaptiveGaussianSamplingEA uses a more sophisticated tournament selection, which might enhance the exploitation and convergence.  Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA is simpler and more direct, solely relying on Gaussian perturbation for its exploration strategy, whereas AdaptiveGaussianSamplingDE incorporates DE, potentially adding complexity without a clear advantage. (3rd) AdaptiveGaussianMutationDE vs (4th) EnhancedArchiveGuidedDE shows that AdaptiveGaussianMutationDE employs a more effective adaptive Gaussian perturbation as a mutation operator rather than EnhancedArchiveGuidedDE, whose mutation is based on DE. Comparing (second worst) EnhancedArchiveGuidedDE vs (worst) AdaptiveGaussianArchiveEA, we observe that EnhancedArchiveGuidedDE uses a simpler DE approach without sophisticated archive management; AdaptiveGaussianArchiveEA overcomplicates the process with both DE and an archive, which could impede efficiency. Overall:  Adaptive Gaussian sampling as initialization significantly improves performance.  Simpler algorithms incorporating adaptive mutation and selection mechanisms outperform more complex hybrids.

**Experience:**

Prioritizing simplicity and targeted adaptive mechanisms, like adaptive Gaussian sampling and mutation, is crucial.  Overly complex hybrids can hinder efficiency without substantial performance gains.  Careful selection and recombination strategies should be implemented to enhance convergence and exploration.

2025-06-20 18:51:39 INFO Generating offspring via Crossover...
2025-06-20 18:51:47 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:52:03 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:52:03 INFO FeHistory: [1028814.33473425 1270337.74848227  920791.74729407 ...    2774.54826143
    4404.73801584    6390.89816016]
2025-06-20 18:52:03 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:52:03 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:52:19 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:52:19 INFO FeHistory: [138099.28168836 344408.03660887 222954.66416273 ...   3982.35207836
   5219.35850249   2394.30574406]
2025-06-20 18:52:19 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:52:19 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:52:35 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:52:35 INFO FeHistory: [285046.21340748 192584.87947953 323239.56305366 ...   6517.51702962
  10671.26108782   8086.87684544]
2025-06-20 18:52:35 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:52:35 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:52:47 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:53:21 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:53:21 INFO FeHistory: [180.46719986 174.64269841 185.37341576 ...  26.56900685  51.30675397
  40.17017642]
2025-06-20 18:53:21 INFO Expected Optimum FE: -100
2025-06-20 18:53:21 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:53:21 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:53:21 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:53:21 INFO AOCC mean: 0.0000
2025-06-20 18:53:28 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:53:42 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:53:42 INFO FeHistory: [1173560.24392539 1248862.44585159 1504156.706077   ...  300195.47577056
  300195.47565069  300195.47505368]
2025-06-20 18:53:42 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:53:42 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:53:57 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:53:57 INFO FeHistory: [212050.08506221 244546.32993006 214823.7961598  ...  89501.72703709
  89501.7261709   89501.72616701]
2025-06-20 18:53:57 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:53:57 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:54:11 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:54:11 INFO FeHistory: [280920.63610647 313914.24389011 190076.58681438 ...  63150.61179971
  63150.61179971  63150.61179971]
2025-06-20 18:54:11 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:54:11 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:54:28 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:54:55 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:54:55 INFO FeHistory: [250.2403137  220.42535177 219.34516142 ... 111.40329794 111.40329794
 111.40329794]
2025-06-20 18:54:55 INFO Expected Optimum FE: -100
2025-06-20 18:54:55 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:54:55 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:54:55 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:54:55 INFO AOCC mean: 0.0000
2025-06-20 18:54:55 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) AdaptiveGaussianArchiveEA, we see that AdaptiveGaussianSamplingEA uses a more refined adaptive Gaussian sampling for initialization, focusing on a single Gaussian distribution centered around a random point, and incorporating adaptive mutation within its evolutionary process.  AdaptiveGaussianArchiveEA, while employing adaptive Gaussian mutation, combines it with differential evolution and an archive, potentially leading to increased computational overhead without a commensurate improvement in performance.  (second best) AdaptiveGaussianSamplingDE vs (second worst) EnhancedArchiveGuidedDE shows that integrating differential evolution (DE) with adaptive Gaussian sampling in AdaptiveGaussianSamplingDE improves exploration compared to EnhancedArchiveGuidedDE's reliance on a simple uniform random initialization and archive-based guidance. AdaptiveGaussianSamplingEA uses a more sophisticated tournament selection, which might enhance the exploitation and convergence.  Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA is simpler and more direct, solely relying on Gaussian perturbation for its exploration strategy, whereas AdaptiveGaussianSamplingDE incorporates DE, potentially adding complexity without a clear advantage. (3rd) AdaptiveGaussianMutationDE vs (4th) EnhancedArchiveGuidedDE shows that AdaptiveGaussianMutationDE employs a more effective adaptive Gaussian perturbation as a mutation operator rather than EnhancedArchiveGuidedDE, whose mutation is based on DE. Comparing (second worst) EnhancedArchiveGuidedDE vs (worst) AdaptiveGaussianArchiveEA, we observe that EnhancedArchiveGuidedDE uses a simpler DE approach without sophisticated archive management; AdaptiveGaussianArchiveEA overcomplicates the process with both DE and an archive, which could impede efficiency. Overall:  Adaptive Gaussian sampling as initialization significantly improves performance.  Simpler algorithms incorporating adaptive mutation and selection mechanisms outperform more complex hybrids.
- Prioritizing simplicity and targeted adaptive mechanisms, like adaptive Gaussian sampling and mutation, is crucial.  Overly complex hybrids can hinder efficiency without substantial performance gains.  Careful selection and recombination strategies should be implemented to enhance convergence and exploration.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:54:55 INFO Generation 4, best so far: 0.5412867233158046
2025-06-20 18:54:55 INFO --- Performing Long-Term Reflection at Generation 4 ---
2025-06-20 18:54:58 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) AdaptiveGaussianArchiveEA, we see that AdaptiveGaussianSamplingEA uses a more sophisticated adaptive Gaussian mutation and recombination, leading to better exploration and exploitation.  AdaptiveGaussianArchiveEA, while incorporating an archive, has a less effective mutation strategy and simpler selection. (second best) AdaptiveGaussianSamplingDE vs (second worst) EnhancedArchiveGuidedDE: AdaptiveGaussianSamplingDE benefits from the adaptive Gaussian sampling initialization and incorporates Differential Evolution, a robust search method. EnhancedArchiveGuidedDE relies on a simpler initialization and uses a less refined archive management strategy. Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA, although simpler, demonstrates superior performance possibly due to the more direct adaptation of the Gaussian parameters. (3rd) AdaptiveGaussianMutationDE vs (4th) EnhancedArchiveGuidedDE:  AdaptiveGaussianMutationDE uses adaptive Gaussian perturbation within a DE framework, offering a more focused exploration than EnhancedArchiveGuidedDE’s archive-based approach.  Comparing (second worst) EnhancedArchiveGuidedDE vs (worst) AdaptiveGaussianArchiveEA, we see that EnhancedArchiveGuidedDE's archive mechanism, while less sophisticated, provides a more structured approach to diversity maintenance compared to AdaptiveGaussianArchiveEA's less selective archiving. Overall: The top-performing algorithms effectively combine adaptive Gaussian sampling or mutation with robust search mechanisms like Differential Evolution, showing that adaptive sampling for initialization and incorporating Gaussian perturbations into the search process are key factors.

**Experience:**

Adaptive Gaussian sampling for initialization is crucial for high-dimensional problems.  Combining adaptive mutation/sampling with robust search methods like DE significantly improves performance.  Careful archive management is also vital for maintaining diversity, but not always the dominant factor.

2025-06-20 18:54:58 INFO Generating offspring via Crossover...
2025-06-20 18:55:07 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:55:07 ERROR Can not run the algorithm
2025-06-20 18:55:07 INFO Run function 4 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:55:07 INFO FeHistory: [1168141.62340317 1049892.88414947 1047959.75529136  713899.76415579
  931929.15177375  805079.56563974  834694.9730428   979787.92606251
  815746.17170289  916872.40592952  749654.59140488 1237282.36181228
 1108947.44485741  780283.24764528  946261.87005445 1058726.93244375
 1023095.99886453 1229090.83555124  997189.12401009  833073.75234667
  819836.29660113  755565.84095543  935855.10386169  883074.06820047
  782898.32027499  809646.75620216  860281.9526628   747586.11294304
 1071066.65125274 1039367.07907103  624944.87937717 1105055.28531645
  929511.25004952  965986.09223897  944978.84412222  913435.23406878
  699525.70773763  691328.41191361  823380.12712721 1025488.68076555
  653460.17329708  869551.54463463  583873.31376797  878056.08509049
  804421.26717021 1028345.09608362  915435.5165876   969533.05999854
  902071.54520975  826533.62386556  732029.29160298 1256434.27467626
 1019437.55249751  910938.49493897 1057803.01494972  761496.12486331
  980267.46945693 1119854.09157262  865191.99922417  963111.90879023
  790305.31486595  889142.68063327  851570.02969349  747914.35929945
  938071.70936039  993147.85507557  908459.06027591  757091.21063372
  940816.81272385  937625.36944749 1279187.71638404  846072.64757334
  886393.99097853  538992.11051864 1020359.41298057  812467.13121309
  795834.49224145  737924.06627911  898141.15071718  889871.54252184
 1058192.77274453  999568.25418406  727886.44088493  852110.15628505
  959448.23039258  859218.35414246  815585.54346455  978244.06112006
 1036695.83900289  907946.81689784  872524.49334899  606366.13120038
  665719.5862883   959925.02753588 1078355.85917926  747666.26910793
  959015.46898613  822628.46284825  922524.40118756 1055807.81066127]
2025-06-20 18:55:07 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:55:07 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:55:07 ERROR Can not run the algorithm
2025-06-20 18:55:07 INFO Run function 7 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:55:07 INFO FeHistory: [313179.40488367 257005.25572235 240560.34567105 277206.11524394
 195242.68257823 287960.51338062 262883.54630147 271092.35851154
 280162.61544108 320800.38999459 302945.80531884 196271.4620348
 222742.7286516  161566.442153   189894.42079355 215164.55921123
 274121.89765258 201158.65292422 252079.77141215 245850.19780539
 202479.27348186 205891.90633346 247482.15643369 243375.23345659
 207306.33036472 193209.46210829 209745.91453622 266636.74937217
 184376.20979794 280178.77680219 247992.04730701 279983.25864391
 228113.87290223 281107.29115011 176713.75274433 274542.74763345
 268832.19263487 305988.85282696 282622.31877469 261204.79007025
 222196.16156401 270869.76693561 231386.45359378 226149.24868767
 133042.25956502 274005.18637957 260404.64683695 213030.44325237
 203493.63262004 288314.49813779 251520.67625669 296353.16361858
 275142.65172071 206206.19226352 226177.0850431  241604.5928002
 233467.35526296 272480.24701614 172771.9136531  298162.41983428
 223436.10761062 282174.49299951 341500.47433444 279390.61085653
 230164.51762734 271548.16284415 270286.46331268 149359.01443924
 270048.58643361 248556.26837078 242241.25172655 206208.48300581
 226740.78425245 215981.36641473 191361.93338224 266909.99505083
 266925.11440394 257628.74812633 219683.93577125 250784.61877995
 242317.76853338 209049.71420207 223549.88031056 169532.6590232
 275761.06236857 271197.64157531 226000.59137352 186489.22001283
 268073.21222267 243750.92169697 217377.15892324 292484.39092147
 245623.25178859 269968.46623923 203685.02090309 297988.28193402
 185325.13705461 230312.52446605 249342.23834287 270636.79569431]
2025-06-20 18:55:07 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:55:07 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:55:07 ERROR Can not run the algorithm
2025-06-20 18:55:08 INFO Run function 8 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:55:08 INFO FeHistory: [224566.48198998 227267.42462123 181842.94130735 195161.28516563
 206812.05587269 192328.31681019 316858.33329371 262092.25243883
 168300.45242109 162428.51052607 133813.3972041  170572.51892122
 183135.30884088 233415.74607845 168170.07785811 210732.4561233
 274554.82868982 199929.00998601 226470.18423226 125014.53418653
 314845.45308023 207617.60938136 217203.63163167 202253.0560217
 152932.68660336 258225.40671066 222034.1252277  230880.0541852
 247793.79339078 296946.19514498 176923.18017875 190190.12186905
 156447.99759877 179503.24721435 212576.01291357 171662.52605951
 240071.95585435 136476.2013761  266558.2505995  182668.92800088
 131891.6716501  221997.95492319 199767.62328198 191015.72475334
 188399.05073832 221829.09929407 198404.45563613 197222.40786675
 187602.37702261 187227.62500876 241933.9433025  118112.24100878
 144873.49638748 170376.14053986 240050.43157205 206818.42611786
 194849.55552999 243826.55287245 166812.71063855 168761.07818408
 242196.42978096 205607.83565954 179455.40636004 150224.48061335
 186066.0321139  251874.12730574 234453.08384673 190841.54161423
 196781.24713788 212486.325086   179951.81900177 228127.57122907
 165838.73315607 194973.32527744 178703.85569405 293850.13272519
 276762.66712651 220404.06945427 189646.84521402 190801.94776139
 220859.75818044 154704.69348871 198123.36894981 145695.10070294
 236856.59826899 139315.53251959 183818.78353979 183198.62624395
 219123.05248797 175453.42229608 235843.56855727 220165.86100654
 202740.08134139 261974.92252145 224111.39621633 276674.70884362
 158509.97360024 187050.91702413 220302.87257943 172474.46532541]
2025-06-20 18:55:08 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:55:08 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:55:08 ERROR Can not run the algorithm
2025-06-20 18:55:08 INFO Run function 24 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:55:08 INFO FeHistory: [181.86982711 149.20419797 166.50361947 175.03781791 169.71573044
 170.18633605 182.6053704  176.94277346 177.2937514  144.57881105
 204.66717115 168.32861796 181.12918785 177.55626174 153.11793452
 194.53291033 180.50052846 204.79598967 154.09754205 168.32615871
 194.09568811 151.68695899 181.30879554 222.59634016 149.18786073
 167.39473538 197.67087496 156.76661639 159.73305535 132.19075242
 161.88894865 134.15017545 171.39791913 180.94021163 201.26035335
 199.14358369 157.25351262 182.39877229 171.66337004 153.76039077
 178.93886179 173.51289292 148.32312355 178.80460853 179.04583882
 181.10589322 177.75476789 154.90643161 180.51289694 168.08442191
 161.68423969 197.60404819 179.75433408 187.08093894 161.68260106
 163.73128697 180.71274264 199.5593596  191.71975026 179.3464875
 153.816294   190.83302055 178.16706259 178.25113967 205.18381922
 209.19515265 180.03276525 171.50467416 173.76416286 203.66372852
 167.85972114 182.18923448 152.64287114 167.38167192 175.1445099
 175.36138921 174.56099234 177.6029539  152.38041614 186.82985613
 156.95830573 142.5744492  162.34928619 153.9920712  145.98284837
 154.91430852 172.80148386 172.10864765 174.89828372 168.87278202
 166.94867578 166.53246375 162.24531058 158.0210945  178.19454387
 152.51971781 211.17459217 180.29767587 174.4426196  162.37575476]
2025-06-20 18:55:08 INFO Expected Optimum FE: -100
2025-06-20 18:55:08 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:55:08 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:55:08 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:55:08 INFO AOCC mean: 0.0000
2025-06-20 18:55:17 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:55:36 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.6413
2025-06-20 18:55:36 INFO FeHistory: [ 1.17090617e+06  1.67213143e+06  1.21146461e+06 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 18:55:36 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:55:36 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianDEwithArchive
# Description: Combines adaptive Gaussian mutation with Differential Evolution and an archive for multimodal optimization.
class AdaptiveGaussianDEwithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianDEwithArchive."""
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50 #Added Archive
        self.archive = []
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.population = None
        self.fitness_values = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                    self._update_archive(trial, trial_fitness) #Update archive
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, solution, fitness): #Archive Update
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_index = np.argmax([f for _, f in self.archive])
            if fitness < self.archive[worst_index][1]:
                self.archive[worst_index] = (solution, fitness)

2025-06-20 18:55:36 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:55:56 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.6817
2025-06-20 18:55:56 INFO FeHistory: [249872.11589934 244577.23370106 166151.44183213 ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 18:55:56 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:55:56 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianDEwithArchive
# Description: Combines adaptive Gaussian mutation with Differential Evolution and an archive for multimodal optimization.
class AdaptiveGaussianDEwithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianDEwithArchive."""
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50 #Added Archive
        self.archive = []
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.population = None
        self.fitness_values = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                    self._update_archive(trial, trial_fitness) #Update archive
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, solution, fitness): #Archive Update
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_index = np.argmax([f for _, f in self.archive])
            if fitness < self.archive[worst_index][1]:
                self.archive[worst_index] = (solution, fitness)

2025-06-20 18:55:56 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:56:16 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.6818
2025-06-20 18:56:16 INFO FeHistory: [204636.47591706 426818.45696455 190773.74261118 ...   -656.78899799
   -656.78899799   -656.78899799]
2025-06-20 18:56:16 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:56:16 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianDEwithArchive
# Description: Combines adaptive Gaussian mutation with Differential Evolution and an archive for multimodal optimization.
class AdaptiveGaussianDEwithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianDEwithArchive."""
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50 #Added Archive
        self.archive = []
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.population = None
        self.fitness_values = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                    self._update_archive(trial, trial_fitness) #Update archive
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, solution, fitness): #Archive Update
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_index = np.argmax([f for _, f in self.archive])
            if fitness < self.archive[worst_index][1]:
                self.archive[worst_index] = (solution, fitness)

2025-06-20 18:56:16 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:56:17 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:57:08 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0273
2025-06-20 18:57:08 INFO FeHistory: [208.75313833 167.78271619 196.01221961 ... -53.03680472 -53.28910344
 -50.6368831 ]
2025-06-20 18:57:08 INFO Expected Optimum FE: -100
2025-06-20 18:57:08 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianDEwithArchive
# Description: Combines adaptive Gaussian mutation with Differential Evolution and an archive for multimodal optimization.
class AdaptiveGaussianDEwithArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianDEwithArchive."""
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50 #Added Archive
        self.archive = []
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.population = None
        self.fitness_values = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                    self._update_archive(trial, trial_fitness) #Update archive
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, solution, fitness): #Archive Update
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_index = np.argmax([f for _, f in self.archive])
            if fitness < self.archive[worst_index][1]:
                self.archive[worst_index] = (solution, fitness)

2025-06-20 18:57:08 INFO Unimodal AOCC mean: 0.6413
2025-06-20 18:57:08 INFO Multimodal (single component) AOCC mean: 0.6818
2025-06-20 18:57:08 INFO Multimodal (multiple components) AOCC mean: 0.0273
2025-06-20 18:57:08 INFO AOCC mean: 0.5080
2025-06-20 18:57:08 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) AdaptiveGaussianArchiveEA, we see that AdaptiveGaussianSamplingEA uses a more sophisticated adaptive Gaussian mutation and recombination, leading to better exploration and exploitation.  AdaptiveGaussianArchiveEA, while incorporating an archive, has a less effective mutation strategy and simpler selection. (second best) AdaptiveGaussianSamplingDE vs (second worst) EnhancedArchiveGuidedDE: AdaptiveGaussianSamplingDE benefits from the adaptive Gaussian sampling initialization and incorporates Differential Evolution, a robust search method. EnhancedArchiveGuidedDE relies on a simpler initialization and uses a less refined archive management strategy. Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA, although simpler, demonstrates superior performance possibly due to the more direct adaptation of the Gaussian parameters. (3rd) AdaptiveGaussianMutationDE vs (4th) EnhancedArchiveGuidedDE:  AdaptiveGaussianMutationDE uses adaptive Gaussian perturbation within a DE framework, offering a more focused exploration than EnhancedArchiveGuidedDE’s archive-based approach.  Comparing (second worst) EnhancedArchiveGuidedDE vs (worst) AdaptiveGaussianArchiveEA, we see that EnhancedArchiveGuidedDE's archive mechanism, while less sophisticated, provides a more structured approach to diversity maintenance compared to AdaptiveGaussianArchiveEA's less selective archiving. Overall: The top-performing algorithms effectively combine adaptive Gaussian sampling or mutation with robust search mechanisms like Differential Evolution, showing that adaptive sampling for initialization and incorporating Gaussian perturbations into the search process are key factors.
- Adaptive Gaussian sampling for initialization is crucial for high-dimensional problems.  Combining adaptive mutation/sampling with robust search methods like DE significantly improves performance.  Careful archive management is also vital for maintaining diversity, but not always the dominant factor.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:57:08 INFO Generation 5, best so far: 0.5412867233158046
2025-06-20 18:57:08 INFO --- Performing Long-Term Reflection at Generation 5 ---
2025-06-20 18:57:21 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that `AdaptiveGaussianSamplingEA` uses adaptive Gaussian sampling for initialization, focusing on a localized exploration around a randomly selected point, while `EnhancedArchiveGuidedDE` uses simple uniform random initialization, which is less effective in high-dimensional spaces.  `AdaptiveGaussianSamplingEA` also incorporates adaptive mutation and sigma decay, allowing it to dynamically adjust its search strategy based on the optimization progress, whereas `EnhancedArchiveGuidedDE` uses a static scaling factor.

(second best) `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`: `AdaptiveGaussianSamplingDE` combines adaptive Gaussian sampling initialization with Differential Evolution, a robust optimization technique, while `AdaptiveGaussianDEwithArchive` adds an archive to potentially find multiple optima. The archive's effectiveness depends on the problem and adds complexity without a guaranteed improvement as seen in AOCC scores.

Comparing (1st) vs (2nd), we see that `AdaptiveGaussianSamplingEA` uses a simpler evolutionary strategy, while `AdaptiveGaussianSamplingDE` integrates Differential Evolution for a potentially more robust exploration, though the simpler EA may be more efficient given the similar AOCC scores.

(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`: Both use DE but differ in initialization (`uniform` vs `AdaptiveGaussian`).  `AdaptiveGaussianDEwithArchive` includes an archive, but again this doesn't guarantee performance improvement, given that it ranks lower than the simpler `AdaptiveGaussianMutationDE`.

Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that `AdaptiveGaussianDEwithArchive` at least uses an adaptive Gaussian perturbation in the DE mutation and an adaptive scaling factor.  Both use an archive, but `AdaptiveGaussianDEwithArchive` employs a more refined archive update strategy.

Overall: Adaptive Gaussian sampling initialization consistently shows a significant advantage over uniform random initialization.  Adaptive mutation and sigma decay mechanisms enhance exploration and exploitation.  While Differential Evolution offers robustness, its added complexity does not always translate to better performance in this specific benchmark, relative to simpler EA strategies with adaptive Gaussian components.  Archives require careful design to be effective; their indiscriminate use can even hinder performance.


**Experience:**

The key to effective population initialization in high-dimensional spaces lies in smart, localized exploration rather than broad, uniform sampling.  Adaptive mechanisms are crucial for dynamic adjustment of exploration and exploitation balance during the optimization process. Simple evolutionary strategies with well-tuned adaptive elements can often outperform more complex ones.

2025-06-20 18:57:21 INFO Generating offspring via Crossover...
2025-06-20 18:57:30 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:57:30 ERROR Can not run the algorithm
2025-06-20 18:57:30 INFO Run function 4 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 18:57:30 INFO FeHistory: [ 733283.63893732  959639.33686691  650204.97332355  928001.47807394
 1310008.59812773  622608.01313689  884036.1669924  1135586.31740479
 1009914.31217176  708269.90800568  785193.67769925 1026881.2946208
 1028372.32909918 1286704.02668329 1056453.25875747  713810.35942664
  954251.77444436  972826.23725753  685342.8631322   791358.4282385
 1064823.72112483  902436.60536322 1247495.72774751 1222727.87782334
  862293.00784234 1127576.23738033 1152364.35749052 1158236.04927271
 1006239.22082129  803256.83174604  980595.19280692 1242245.69780308
  890874.51096631 1361372.82107126 1298619.75797516  847867.91664602
  761928.65834973 1112095.24637627  823533.0787594   743573.68558749
  913956.76649768  951397.29533055  976390.76262465  720161.11274209
  923365.33779515 1111780.99824921  894721.77815618  943395.30245118
 1135023.45825714 1126855.48093488 1062285.50999475  698244.21614957
  907928.2760455   947678.62468249  702385.66028272 1113775.33567411
  958221.34284714  809551.06491345  672495.94342298  849029.97282295
  691787.52625606 1401249.80348588 1473428.54048407  561016.61000564
  895346.50193252  940017.96638342  992321.88992923  729856.66159371
 1031500.20122084  942165.11246178  778615.87518368 1015874.96420961
 1150073.13697267 1222623.27082959  984364.94375331  742396.31045591
  531880.15205826 1184979.49099304 1283219.07724371  899754.11940805
  872685.316912    602985.18465602  936097.32154354  658257.9421457
 1105192.56498346  927946.92062392 1248907.36561084  923626.61454576
  796677.29229144  778482.40154882  836977.86971575  984948.03974568
  790175.1967149   972308.99595286 1242579.22658189  936546.13388054
  596535.6578214  1053870.82037802  845561.94171724 1206646.46738818
  841727.22762364]
2025-06-20 18:57:30 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:57:30 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:57:30 ERROR Can not run the algorithm
2025-06-20 18:57:31 INFO Run function 7 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 18:57:31 INFO FeHistory: [172693.36564241 219731.71088638 198265.85301639 219004.35620257
 151373.68378238 255144.43292563 236006.58488042 239987.88264664
 219608.13642106 296268.68020539 370669.14120382 307352.74685313
 315092.53019617 288427.47494027 253180.81382773 185012.79877642
 257976.23284598 128957.17234173 260614.45373544 225325.38545273
 170165.15257203 224566.2330335  233338.87093635 245219.74728182
 177964.37677168 255609.01514449 285911.04426204 225373.67981131
 279992.46599354 281659.73534535 142063.24192018 193175.99501384
 334828.53110207 168810.46997219 315034.21914249 321312.44592307
 340237.11159614 309030.74275806 264142.39885784 320311.85970434
 284688.90118975 218605.83666843 321735.34758863 218086.03481559
 252032.71960199 227718.36909709 281785.60363163 214718.17180133
 287682.6840266  251582.39446879 181708.10633438 264790.28041324
 311627.01766259 309143.07731909 244013.94664628 425423.37803349
 374312.64178759 306614.6897188  269340.93378262 238784.89616289
 213772.13983259 177537.68809333 272907.71923549 180988.33907533
 233821.58889763 341981.40564338 152643.66781365 195996.39412661
 198252.62599264 167266.61402772 219621.98294455 243462.38882186
 365696.87698248 325963.37363661 170603.18520781 141325.23584786
 174702.03862302 233966.72540868 207584.10950563 202022.02181767
 160492.21982928 278644.66601691 201703.21826384 284065.19764565
 268092.50794792 200944.56167499 290669.15455075 184354.22714004
 290905.25463667 159353.90264779 192113.24578751 231722.2870135
 264535.09121904 215696.35687964 276650.68258288 240988.31253708
 178886.03717401 245974.68788966 282569.02318906 243832.53397862
 292233.57048902]
2025-06-20 18:57:31 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:57:31 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:57:31 ERROR Can not run the algorithm
2025-06-20 18:57:31 INFO Run function 8 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 18:57:31 INFO FeHistory: [ 94593.89900531 122876.46750549 224401.08115103 210396.34621283
 260378.04259334 177516.22630958 281487.39842977 179775.89447893
 173866.36456492 151888.78476261 215661.69693842 288357.1643335
 228034.86538978 332449.47860356 138475.10167281 258379.49559215
 148357.25022551 153400.26080822 283971.44385183 115692.82787268
 212361.85002077 212400.65081625 229878.46476435 281980.34173446
 140667.72442703 336039.74574926 239619.0212513  214273.31932
 241807.1554448  247942.20180304 229546.78436261 200180.72397658
 380759.58014315 197635.74973912 220442.55125634 182525.59569401
 216627.36325343 223880.17412689 228774.97384815 231968.36393734
 184555.02697112 250161.37029223 222402.91186828 205712.65726034
 200457.48291872 167703.34037046 188012.42020334 217024.29531188
 250100.04563589 258909.12610419 273226.48889476 218119.12605705
 238637.30222368 199286.089974   200509.44274401 171497.95724911
 279221.81302383 200857.605211   270260.05648301 319202.69445251
 218579.71082508 235863.69358866 233595.62017503 203887.16721163
 156307.80890003 230312.01546192 194887.35241189 158537.21736914
 259837.63015597 168872.82325832 217667.49427851 249144.54097971
 140460.37530848 248903.0900038  247576.86280546 225330.77270326
 207705.18451063 196174.58343708 334535.35777421 164436.6978712
 242715.61283529 202969.87852777 158356.17020267 231635.82942891
 222822.45134597 173774.11356273 243225.80448772 318174.87444817
 181914.17904754 219344.73053971 182770.59180234 279083.5445197
 179032.62350018 149334.61734968 229688.44358635 169668.90792337
 262893.62304157 316215.22678197 324381.13209853 224895.16754952
 256610.8845658 ]
2025-06-20 18:57:31 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:57:31 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:57:31 ERROR Can not run the algorithm
2025-06-20 18:57:32 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 18:57:32 INFO FeHistory: [182.7732484  171.58459656 178.48611177 165.30814896 210.19180305
 179.88173099 174.32445774 171.55773486 196.39522447 207.02447599
 177.60942736 202.15197327 165.39116779 194.01365502 142.89464524
 192.41113542 180.81724416 165.52850402 179.85615841 172.73102948
 211.92614898 192.70616916 175.79523178 196.35115559 167.6570905
 199.79656604 192.49799614 179.97371753 161.09527515 195.12154114
 176.36945375 179.36786448 177.13734041 146.49089552 205.39875139
 193.8735527  169.32032049 178.23991915 163.4023058  212.31412683
 187.57724063 169.84705209 212.80388719 180.21595526 169.92258354
 199.21411022 171.06263649 183.8860129  167.90910855 171.49698573
 161.06750993 149.9794597  186.99385427 164.1181151  209.72822809
 160.20599348 197.59749765 182.04838405 201.46753542 152.03187532
 210.02441241 192.83981129 178.24307547 175.59544271 166.86115281
 203.97869847 236.88044357 172.23434422 177.73420185 169.25654977
 200.73904241 160.39721239 182.90623395 173.82677974 168.47835008
 184.09917781 183.8712462  145.79846657 209.55236272 172.16968873
 206.35360635 197.96398208 184.9664149  200.83766411 179.98241836
 181.60833738 184.86887657 174.99416174 172.75745606 174.00627685
 200.07532212 210.01225277 196.7675142  182.62289302 165.02281058
 205.17543077 204.0060617  185.76541528 161.17221709 169.37686441
 181.52683481]
2025-06-20 18:57:32 INFO Expected Optimum FE: -100
2025-06-20 18:57:32 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:57:32 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:57:32 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:57:32 INFO AOCC mean: 0.0000
2025-06-20 18:57:36 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:57:53 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:57:53 INFO FeHistory: [682053.26171208 687808.85142537 834543.24627495 ...   2467.63090244
   2467.63090244   2467.63090244]
2025-06-20 18:57:53 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:57:53 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:58:10 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:58:10 INFO FeHistory: [175276.41985343 238913.8188097  257881.85470112 ...  25181.47397068
  25181.47397068  25181.47397068]
2025-06-20 18:58:10 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:58:10 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:58:27 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:58:27 INFO FeHistory: [121363.6914001  194442.99778349 192136.33776348 ...  30020.40779328
  30020.40779328  30020.40779328]
2025-06-20 18:58:27 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:58:27 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:58:36 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 18:59:16 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 18:59:16 INFO FeHistory: [174.34269558 244.25871832 198.56440201 ...  77.26572563  77.26572563
  77.26572563]
2025-06-20 18:59:16 INFO Expected Optimum FE: -100
2025-06-20 18:59:16 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:59:16 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:59:16 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:59:16 INFO AOCC mean: 0.0000
2025-06-20 18:59:16 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that `AdaptiveGaussianSamplingEA` uses adaptive Gaussian sampling for initialization, focusing on a localized exploration around a randomly selected point, while `EnhancedArchiveGuidedDE` uses simple uniform random initialization, which is less effective in high-dimensional spaces.  `AdaptiveGaussianSamplingEA` also incorporates adaptive mutation and sigma decay, allowing it to dynamically adjust its search strategy based on the optimization progress, whereas `EnhancedArchiveGuidedDE` uses a static scaling factor.

(second best) `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`: `AdaptiveGaussianSamplingDE` combines adaptive Gaussian sampling initialization with Differential Evolution, a robust optimization technique, while `AdaptiveGaussianDEwithArchive` adds an archive to potentially find multiple optima. The archive's effectiveness depends on the problem and adds complexity without a guaranteed improvement as seen in AOCC scores.

Comparing (1st) vs (2nd), we see that `AdaptiveGaussianSamplingEA` uses a simpler evolutionary strategy, while `AdaptiveGaussianSamplingDE` integrates Differential Evolution for a potentially more robust exploration, though the simpler EA may be more efficient given the similar AOCC scores.

(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`: Both use DE but differ in initialization (`uniform` vs `AdaptiveGaussian`).  `AdaptiveGaussianDEwithArchive` includes an archive, but again this doesn't guarantee performance improvement, given that it ranks lower than the simpler `AdaptiveGaussianMutationDE`.

Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that `AdaptiveGaussianDEwithArchive` at least uses an adaptive Gaussian perturbation in the DE mutation and an adaptive scaling factor.  Both use an archive, but `AdaptiveGaussianDEwithArchive` employs a more refined archive update strategy.

Overall: Adaptive Gaussian sampling initialization consistently shows a significant advantage over uniform random initialization.  Adaptive mutation and sigma decay mechanisms enhance exploration and exploitation.  While Differential Evolution offers robustness, its added complexity does not always translate to better performance in this specific benchmark, relative to simpler EA strategies with adaptive Gaussian components.  Archives require careful design to be effective; their indiscriminate use can even hinder performance.
- The key to effective population initialization in high-dimensional spaces lies in smart, localized exploration rather than broad, uniform sampling.  Adaptive mechanisms are crucial for dynamic adjustment of exploration and exploitation balance during the optimization process. Simple evolutionary strategies with well-tuned adaptive elements can often outperform more complex ones.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:59:16 INFO Generation 6, best so far: 0.5412867233158046
2025-06-20 18:59:16 INFO --- Performing Long-Term Reflection at Generation 6 ---
2025-06-20 18:59:20 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for both initialization and mutation, leading to a more efficient exploration of the search space.  EnhancedArchiveGuidedDE relies on a simple uniform random initialization and lacks the adaptive mechanisms for better convergence. (second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive show that incorporating Differential Evolution (DE) improves the search process, as seen in AdaptiveGaussianSamplingDE's better performance. AdaptiveGaussianDEwithArchive's archive, while intending to aid multimodal optimization, doesn't significantly improve performance compared to the simpler DE approach. Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that while AdaptiveGaussianSamplingDE introduces DE, the adaptive Gaussian sampling in AdaptiveGaussianSamplingEA might offer a more focused and effective exploration in some landscapes. (3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive reveals that the archive in AdaptiveGaussianDEwithArchive doesn't provide a substantial performance boost, possibly due to overhead in managing and selecting from it. Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive utilizes adaptive Gaussian mutation and DE for better search than the basic uniform initialization and DE strategy in EnhancedArchiveGuidedDE. Overall: The adaptive Gaussian sampling and mutation strategies appear to be crucial for better performance on the GNBG benchmark, surpassing the benefits of simple archiving mechanisms or even the inclusion of DE in some cases.  The adaptive scaling of mutation strength is also a factor, enabling efficient exploration initially and targeted exploitation later.


**Experience:**

Adaptive initialization and mutation strategies are paramount for high-dimensional problems like GNBG.  Balancing exploration and exploitation using adaptive parameters is key to superior performance.  Overly complex mechanisms like large archives may hinder efficiency without offering substantial benefits.

2025-06-20 18:59:20 INFO Generating offspring via Crossover...
2025-06-20 18:59:29 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:59:29 ERROR Can not run the algorithm
2025-06-20 18:59:29 INFO Run function 4 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:59:29 INFO FeHistory: [ 927545.76056647 1042980.51618527 1187767.60095611 1138853.33372419
  938108.24624727 1195220.212513   1011738.27057426 1340507.05410405
  875654.62365708 1113599.28828355 1183112.30050495 1147059.36657819
 1257946.86805189  962804.39678707 1154663.72264218 1078751.48249211
  997473.35308146 1017097.82606657  878007.40392105 1163920.54786565
 1054760.68045623 1116507.38173735 1341079.24324242 1372463.01853386
  824464.56850896 1091517.10660657 1210882.83882067  969337.67049564
 1086264.53548334 1000031.57109687 1074184.08124224 1112486.28319448
 1044553.89618103 1539735.09169955 1109689.83355112 1065481.90196747
 1056763.71835351  805084.32811926  829189.41507305  976240.7895387
  787170.72112353  996344.71346594 1224298.38990097 1167039.10996053
  876432.53593327 1102912.04272888  761247.68462339 1165565.07593861
  961621.12806071 1125764.60699219 1033138.92850648 1316732.85091564
 1198478.0874262  1151000.19584485  809774.61325101 1197678.414923
 1121238.67556608 1074138.39229743 1117288.83846859 1185993.15230023
  746153.15896874 1259605.97411857 1176301.40670334 1021552.16610193
  941517.10897691  995262.97949187  974836.06525331 1006629.04134916
  927194.84039427  903008.55680841 1006044.83659124 1315292.45711758
  971307.34469043 1112963.93991387 1309251.87601714 1091986.6975337
  878460.46056543  844296.28426206 1015133.69281999 1067415.3684286
  862894.78423932 1183390.78877516 1235318.40010218 1194139.30415319
 1029917.46429969  946612.9838541   884721.48324046  908451.31269899
 1188578.21906698 1096921.46966046 1513113.08366384  995571.0290435
 1138204.76297753 1188031.90602806 1226502.55685826 1189563.80834569
  813613.77328596 1146508.5697012  1068241.12213173 1090640.93645813]
2025-06-20 18:59:29 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:59:29 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:59:29 ERROR Can not run the algorithm
2025-06-20 18:59:30 INFO Run function 7 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:59:30 INFO FeHistory: [159495.8329464  207107.15586744 194482.66535995 169592.63855281
 148872.45295208 171466.20844116 195360.63153514 193836.83867077
 275410.73646605 203418.31257428 229667.18637435 171121.89394633
 197262.31131791 174745.50101716 174364.33566256 258903.15501377
 242482.21887341 209746.96537692 174008.90948661 152671.72703791
 203083.26925707 190920.40285676 219835.57923353 209571.71772349
 247191.09556032 160665.61355619 232778.8162233  203495.93113869
 191808.50278182 170366.91619005 130654.96522222 140474.63966584
 211717.00043427 182677.18268248 187998.33087084 130846.80698116
 314739.48419587 201595.76243786 157925.37804859 174458.84411207
 152263.6999215  200302.52829178 202810.59937143 175854.2246252
 125631.12141403 117858.21886487 194785.7891305  206658.22245928
 206846.67368379 226772.35044645 179261.12241368 243673.9365379
 221811.01328498 179784.75339698 216104.0042083  203513.87945574
 187081.50277443 195086.70773469 148451.62246796 227684.71068957
 131298.78647285 152296.19396681 225201.63669664 185761.45868255
 226689.91899202 171218.14121101 214984.88306695 144619.53337173
 211955.19977193 161033.49824422 172815.04799188 149568.78678727
 164996.09725939 165301.37038458 178139.48569979 206867.1246282
 167767.66695136 201079.54813105 149103.12493762 182566.91723454
 202713.58239491 214804.34548542 125763.09561827 191870.63429155
 206641.18529247 181869.87608411 161271.90305126 196409.99205463
 249294.82500701 205783.79074466 161155.87700036 197193.98061274
 185908.45412064 178448.03148001 202109.89354106 244548.20102536
 177993.77939706 222331.38671846 220419.83269278 209028.30607744]
2025-06-20 18:59:30 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:59:30 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:59:30 ERROR Can not run the algorithm
2025-06-20 18:59:30 INFO Run function 8 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:59:30 INFO FeHistory: [307608.77479904 208204.83448111 184477.86763826 305271.17174497
 310149.58618746 247091.77114541 222024.26340158 215980.8181986
 295345.7360302  271654.73772561 242174.28093632 211065.56389829
 243215.45683485 199348.63981578 235213.8792437  160833.67540354
 171988.65543678 215271.41004125 283858.23325186 241448.0464125
 224227.97449864 324651.86359149 383156.22956985 256848.17612345
 158324.84237282 159651.88207807 241252.36019316 188755.1727349
 201198.92567576 266492.88395674 202414.04298181 186444.91936363
 267287.95409342 230075.10052923 273751.91368976 257234.60936093
 179425.49702403 203425.06185051 182357.4566202  229218.24611294
 179280.90628629 161085.84451453 215963.47430272 259977.4644854
 226422.36269514 265832.70220574 188863.33415335 245409.21812561
 229872.03747609 266474.10126531 285509.88895069 270876.32801083
 193648.46997321 297166.98615432 346824.47929468 182945.90535925
 234094.77605473 245591.16165196 322957.26963772 170665.76709869
 288690.44550958 269211.24526822 264750.62544683 223359.47767709
 191020.42261925 254820.34269294 260897.0425118  229694.66069393
 221204.10958076 208712.53848827 258319.3487449  258683.50669867
 264052.54152757 266663.41783955 323726.17067955 259452.74913022
 221161.28794079 289594.6836987  259692.93729283 266980.70168719
 251015.34920538 225370.37325664 196486.73631992 245642.82872013
 269111.4267779  333337.48388545 192559.30736951 247099.19466668
 261954.53174449 264115.84723171 212620.6585489  234018.87344128
 229173.01527213 253537.89573277 146726.0410239  236508.65764326
 185730.12063438 293072.77486416 228596.94955832 165946.80113388]
2025-06-20 18:59:30 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:59:30 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:59:30 ERROR Can not run the algorithm
2025-06-20 18:59:30 INFO Run function 24 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:59:30 INFO FeHistory: [187.27822709 207.26363295 209.34153409 201.25268207 185.10951553
 227.75778239 195.90277265 200.84056488 191.22161565 211.6413767
 201.53416893 179.71641184 184.06510984 196.55149895 211.61512837
 213.12218727 197.74953398 230.01876493 195.23750305 196.28735714
 223.49562    199.23832849 209.52905272 173.06131078 204.79347647
 185.82179303 210.97630211 186.24233966 206.29717196 202.39747945
 178.25682139 222.68639574 227.67765861 192.31533761 219.63948979
 219.16043852 227.1024276  195.52819557 203.29373994 173.87139317
 187.66753051 206.25652876 179.11202742 208.04074705 223.03736407
 215.92535111 199.15683908 178.198741   173.75968854 210.41978864
 188.12390191 202.82183924 215.72936172 176.49092461 222.22378297
 216.40216816 218.11128229 192.70759698 209.62466114 203.17816178
 207.13410044 211.72644971 165.97314162 204.30670078 213.52860196
 196.90910898 214.09582614 211.08054907 228.59276965 225.28318901
 209.75197092 208.55054828 164.13966077 190.49757815 192.0512376
 202.22473384 186.1041898  206.15222413 215.51542539 203.03959395
 214.54117334 228.9500851  192.50897663 217.94888806 219.27500598
 178.06157162 205.90186935 189.08414025 194.94578049 218.02540533
 197.20545721 179.39629357 185.0309485  231.05494505 183.92299795
 222.2598205  228.20606764 176.14843126 235.1534437  193.70106549]
2025-06-20 18:59:30 INFO Expected Optimum FE: -100
2025-06-20 18:59:30 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:59:30 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:59:30 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:59:30 INFO AOCC mean: 0.0000
2025-06-20 18:59:40 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:59:40 ERROR Can not run the algorithm
2025-06-20 18:59:41 INFO Run function 4 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-20 18:59:41 INFO FeHistory: [1211628.6708707  1315290.83194755 1844470.64862022  911398.66815789
 1034533.34975959 1093636.8021991  1518108.13802903 1665714.16164198
 1217915.8588434  1584898.20445564 1182792.63690704  899164.694326
 1802632.75863013 1831530.170707   1690790.36720737 1695553.27585085
  964266.94169143  838621.59845904 1316035.70475853 1889396.37879952
 1129586.67269411 1485457.46180033 1707013.5178654  1149444.81135272
 1405893.78894065 1543253.19603141 1173805.59371913 1216863.91497042
 1437241.26392289 1165110.16266162 1538146.37081972 1670459.6648013
 1062951.38211742 1355608.38026392 1173107.58879135 1710163.94707973
 1630266.2916773   950811.29475071 1163919.6905645  1211543.34003
 1659176.49390308 1985120.50982906  983241.92642772 1441325.91291782
 1533351.73586002 1130916.89946358 1591559.48824046 1627480.24365955
 1186567.31870275 1115367.50004754  940912.3208625  1271576.62531428
 1810444.33169064 1762278.14413619 1008225.51843572 1367133.35754619
 2140948.15086705 1511637.81409546 1636144.94147775 1621431.73613894
 1615463.87631758 1412738.45076513 1352175.62511294 2065519.90778448
  774583.79529593  981091.90549862 1544593.42577107 1211851.19043414
  968491.09918539  994768.08245945 1215490.68237609 1526903.0863537
 1367188.62025475 1077249.4411187  1080368.88394782  914855.84244984
 1435170.6819282  1141459.74580713 1171416.4286996  1180524.98692675
 2221484.05210181 1371291.19066889 1171560.41194048 1000121.66223981
 1502561.98285244 1015050.13876071  957581.50319149 1362871.79186392
 1214119.81801975 1369448.31433902 1317006.12548936 1595962.07730285
 1139891.3935991  1232026.30203461 1148428.05450091 1736181.33167156
 1525339.30651532 1733148.45541767 1168214.84667891 1695781.96965225
 1330574.03806785 1418979.05037436 1216440.4593223  1209305.04769048
 1263983.33680047 1061450.9718654  1306770.0195946  1396521.02476518
 1422471.65476098 1172105.61577216 1092700.94007866 1360444.53618767
 1351016.32874856 1592679.55080617 1335519.44639472 1057342.50280853
  600332.27884672  921643.77598734 1306966.34043925 1315574.9432683
  994712.21599731  992541.09983086 1240707.11139984 1258481.31119782
 1537747.08804746 1094081.06506884 1042108.26825556 1983931.42124848
 1611558.02169484 1295586.11614708 1003649.71144131 1331771.05256109
 1115322.77173778 1062369.40830162 1121749.99051327 1108315.30183545
 1285903.69836214 1052161.68047797 1201304.02704914 1337294.05351576
 1561098.66044088 1346119.95171049 1153008.35663878 1841359.55066598
 1816884.54469689  927658.44037717 1242490.87171483  957640.84275213
 1328013.91430047 1364304.93527555  948163.16649828 1045839.25944048
 1088453.4111522  1683829.61525955 1247649.70097457 1239875.56210578
 1736030.43227506 1157750.72220423 1425550.56005366 1495942.12735105
 1668905.09187898 1390064.98289714 1569176.73554818 1282115.12370013
  777838.37358043  924257.20439406  745690.85008999 1080531.70147334
  945531.91581908 1131110.35348972 1178706.51424358 1298527.4091881
 1439632.12713227 1071441.6110763   891054.32440299  908026.82219758
 1465633.01480575 1107401.5631238  1212284.82287008 1283163.18473905
 2241680.65738898 1235543.44883804  828481.56376055 1214846.72555782
 1025381.29684124  854974.0134162  1064630.26364718 1794574.10624754
  998483.99295982 2017661.43487781 1440291.75669315 1582504.88746948
 1065411.17374137 1119793.50515489  789465.37414735 1627575.45102843
 1278228.88274722 1738636.63842368 1351544.72815463 1892509.43970642]
2025-06-20 18:59:41 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:59:41 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:59:41 ERROR Can not run the algorithm
2025-06-20 18:59:41 INFO Run function 7 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-20 18:59:41 INFO FeHistory: [229142.99094364 169603.95547423 308306.36326467 157486.49717032
 228008.01315895 196189.40072844 204055.23268439 148567.63752835
 241933.15723579 322640.18356987 210871.49591158 271940.51813891
 248941.1660755  193160.17002437 302699.78416979 245390.1597272
 450240.53817867 300354.01003199 154872.33347965 124003.70885498
 259268.5381668  251335.99132253 233835.77947923 182668.28156544
 199829.63620364 153050.95950767 203179.80699275 229912.30227374
 195716.70847229 150547.50936851 219306.74421653 182152.14109613
 273873.49125588 195747.91515295 175065.14702821 218145.88838028
 329055.00379197 212384.61545804 140339.95716842 263161.1171197
 264104.08492001 240785.43053064 186956.55864734 148337.62612814
 225558.08549752 199226.42404872 246836.66537836 248333.22678066
 303884.62594739 166480.46103969 194493.43839525 268154.78366798
 223873.26758933 267520.28954446 173363.47432561 284323.11457402
 421061.84787088 293558.49696527 296454.69420683 283239.29452171
 165532.03533088 323489.29129475 256424.9421142  252987.29064413
 311630.88320737 191176.31507314 284460.60217941 219906.30462351
 192565.39065862 290245.50575663 220061.36350524 225920.50076987
 179007.30512508 174438.71346472 217321.13022458 146721.1385268
 211373.03434475 258856.56217004 145143.44983565 194204.03522569
 249704.71100554 201004.54407627 367601.414414   225291.0863844
 316235.82542781 253557.20956963 249388.76810782 166862.87994703
 254800.84950155 236235.5204903  258294.76601689 178614.13746349
 167994.3076753  167367.96671943 177545.60844674 299542.85422202
 147592.55078659 409946.60874384 266059.64153149 230586.81946576
 306225.82945753 235372.24754481 360628.916634   143634.92628409
 215355.28727397 211786.04986351 272175.5293294  145649.86026221
 176553.93553854 350076.4666256  306421.8865097  335617.26884014
 136850.02580487 254706.8569385  353887.61801129 270447.16717393
 428043.37630056 217644.9742752  185257.96219564 134293.12368485
 198725.37490119 255099.8354954  317539.56368036 204799.32180834
 151721.97406281 152757.93453212 278324.59291058 235273.12383118
 212164.53743513 120418.98523471 301502.10758606 246189.50029933
 293083.68372163 156346.50380906 178137.67518622 292194.81846096
 384041.56073914 226409.28918329 272357.39655188 330929.55521773
 250785.08392844 235036.21235412 194258.2240246  235572.15424307
 249194.78185588 235697.39829353 402596.10041029 284846.09384856
 339341.66206342 262373.60177802 202248.65951689 223537.44126181
 211786.45016165 292002.47091333 214976.15838902 339652.7538473
 278759.53013729 341482.13323269 267699.14970825 241764.19509312
 175366.8475925  154753.53991142 275481.61553724 184481.41019371
 337640.81953664 214115.79402576 265730.5264725  208016.51259689
 236954.55044966 263599.37634933 292432.26785256 253385.21783673
 160677.92709263 231112.32521345 200179.76530767 185310.61641359
 278632.41647092 307470.59964324 246831.24442259 309585.3045137
 292006.03343288 159561.6728332  245969.03996305 308618.2825114
 341239.54939677 220909.97155623 174043.82649884 224693.14958721
 309143.63972789 356782.03346843 272976.99814695 177754.83954536
 306394.9954911  253577.29775242 273915.35805282 251711.96634661
 190155.08581404 366783.86099791 245525.27509433 204195.52348527]
2025-06-20 18:59:41 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:59:41 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:59:41 ERROR Can not run the algorithm
2025-06-20 18:59:41 INFO Run function 8 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-20 18:59:41 INFO FeHistory: [313527.14673006 386133.01321982 301760.18954346 250260.53243275
 244965.57939382 245799.25779807 244162.96138648 280011.70923684
 336657.37874727 215243.45308262 237615.64905499 337842.62298715
 233800.76156333 240499.97116672 400515.9012666  243410.3163384
 423077.41677611 359494.64997261 391264.37613106 321204.32111742
 275362.94551976 226064.72219673 261634.36227233 345843.19678869
 331531.897121   210272.41106622 247440.26047048 266760.87963198
 432308.90550708 302076.78469081 339226.58215737 316484.96066785
 352493.42837701 438291.00885368 425414.49675815 340924.57438388
 205305.5308135  325584.61160099 269420.27899769 455012.80046083
 245759.03160471 317002.65377723 211643.62604281 334871.41624511
 353381.01153171 230102.26750835 373223.35695643 295347.7895879
 405470.06681906 182165.76030424 244983.28746186 337394.96283448
 341259.24987305 303955.50253157 402925.11861376 310030.00689194
 173591.80783916 423049.69719811 265214.52875701 303063.9924041
 379270.43439008 433029.42155745 291681.7929753  293756.63198916
 243205.3961615  493246.40249565 157658.37554153 162589.23144025
 301034.82492973 363302.29362578 265977.0004666  298645.7751431
 379416.78614967 230320.31285478 323448.70656969 336464.73081535
 414361.81294791 500136.41337499 308313.96353228 308705.80676997
 232502.81252594 265391.68839043 173324.03207253 279010.03657481
 267460.02653671 187845.25216893 285173.70149305 211874.41600986
 214655.54794461 229692.44942802 269180.3221115  241993.51555991
 386850.42686875 287801.3370299  318924.44349589 317865.98788308
 201465.23301438 211823.50289086 270829.19758036 246172.70753187
 306074.73675778 188737.0300279  346898.21833627 293388.96222894
 159381.98769789 293406.38674284 231042.24654911 220898.18681304
 227402.23883141 341445.10518187 297539.27795879 311178.80473202
 206253.32418836 210589.4974256  284532.63478135 285842.3062772
 335306.23119681 395122.51909299 300411.77489173 239519.18326051
 309637.87125184 248247.27463433 168141.63980652 296467.18733664
 338609.08943841 174667.10299646 242694.4202188  253397.86654304
 330869.85583749 289976.59189315 246329.53119566 270795.47233656
 370402.27694021 185227.8538829  263339.53838949 302457.64196906
 239001.27247175 263010.26074901 287072.6926447  229351.75811926
 340066.17014171 343938.71701585 224401.82631994 273951.36472273
 259955.45787497 247888.38860468 403991.70271089 265010.59033714
 379070.60599751 261924.97631449 176450.53903296 416584.68947875
 227838.66363637 210807.29817527 335105.07831637 357493.29413802
 193446.77356217 387669.45399166 262853.04156269 329064.05165489
 255337.11781116 393685.49197277 365375.4497284  245512.94206723
 220145.5352095  354528.74089284 214871.09708174 248028.11646893
 323796.65733343 264268.05447245 321517.81240967 354638.30025779
 316578.66113648 370123.88773702 266201.74362606 258232.94339378
 356613.88608441 335768.19251387 239733.27020326 372129.5291047
 236990.46206566 296752.07763031 125331.00423604 251475.72693517
 333950.21724496 273220.76262443 264919.02719056 361351.97798224
 326970.53071872 269539.94667758 305065.97314158 245439.3773777
 348853.18667026 277700.7722437  174445.00501286 297502.95058312
 208812.15183192 211740.35988448 314159.45814321 229348.7761555 ]
2025-06-20 18:59:41 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:59:41 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:59:42 ERROR Can not run the algorithm
2025-06-20 18:59:42 INFO Run function 24 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-20 18:59:42 INFO FeHistory: [173.75599743 234.86582653 195.88878571 210.96922966 201.37225345
 206.20159909 201.51799674 198.71455098 175.28148923 210.80273596
 195.71862246 164.17875074 191.62426312 205.28174907 221.35944185
 210.61126337 222.01314221 213.71525483 218.81294191 210.39613919
 199.16541911 224.08681155 208.69294075 228.45321698 185.82103378
 235.22019171 199.67298131 164.14773054 169.74024762 197.4801992
 217.89038157 189.54246948 205.23751469 195.26985388 223.82280588
 200.07319086 210.81373123 210.14426715 239.40206026 217.59777761
 214.1173566  193.86154947 184.48003486 206.85404575 211.36521114
 222.70564846 241.51400299 211.80875831 206.90146433 209.83509428
 204.15844598 202.36831657 193.19149846 176.46244653 221.10080222
 224.48037022 213.74614904 224.10899478 204.34299455 204.04209501
 207.43348573 174.01285109 217.70066    219.76191648 218.21079173
 187.06691534 167.65778575 197.97676513 224.80665307 218.6691442
 191.69758138 226.77210665 215.28821545 205.81247633 222.02925152
 203.69789984 230.69835809 194.34886481 212.6056109  228.31945818
 172.22267803 187.21061003 173.99410686 202.68167773 232.23206174
 229.58253433 203.65324328 196.58451738 209.3492753  206.00139847
 197.45539342 205.27277675 189.39485773 219.10331433 203.23478442
 197.1618546  209.5483538  218.96776568 228.32322694 204.86116616
 185.20207168 224.78690627 244.95978127 202.62435415 190.68076097
 181.4905156  212.16991599 179.01263796 204.02249788 221.97123432
 193.0018615  215.01201207 186.70752147 208.4003588  198.48182659
 216.17128735 207.3224786  212.98307524 218.41070243 186.41599461
 202.68795565 227.19139995 224.79152565 221.88696402 208.34154212
 212.51706388 220.52775039 182.91067159 208.27488042 210.3215008
 183.10852744 194.92903505 193.75415454 215.59492576 230.06844267
 205.06077712 198.70912824 238.1695691  192.49267658 239.81953619
 224.30511333 192.34049679 218.02944094 184.80250537 206.0332023
 195.33619017 198.27677386 205.8533331  202.81144976 186.82483575
 191.54284795 187.23201836 181.72441394 200.52926169 228.51480018
 216.66915038 229.11414644 167.98420778 249.09806754 197.42044274
 207.82904683 206.78424797 215.96718248 223.1593491  222.00889111
 202.50910195 193.66720853 223.58065332 212.94191186 189.5595862
 229.70510511 197.59323716 246.89924356 223.0684445  196.39465599
 216.45750736 193.03818372 188.02211553 211.19427508 230.80538293
 188.68091819 190.20002027 197.83988808 210.34342537 218.27342661
 180.56961173 196.62437033 203.61268941 180.52671583 173.21449681
 211.23733321 212.15663894 201.40024323 180.3034706  228.45811152
 205.91870478 195.22938805 191.66484277 210.30994491 201.03662699]
2025-06-20 18:59:42 INFO Expected Optimum FE: -100
2025-06-20 18:59:42 INFO Unimodal AOCC mean: 0.0000
2025-06-20 18:59:42 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:59:42 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 18:59:42 INFO AOCC mean: 0.0000
2025-06-20 18:59:42 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for both initialization and mutation, leading to a more efficient exploration of the search space.  EnhancedArchiveGuidedDE relies on a simple uniform random initialization and lacks the adaptive mechanisms for better convergence. (second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive show that incorporating Differential Evolution (DE) improves the search process, as seen in AdaptiveGaussianSamplingDE's better performance. AdaptiveGaussianDEwithArchive's archive, while intending to aid multimodal optimization, doesn't significantly improve performance compared to the simpler DE approach. Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that while AdaptiveGaussianSamplingDE introduces DE, the adaptive Gaussian sampling in AdaptiveGaussianSamplingEA might offer a more focused and effective exploration in some landscapes. (3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive reveals that the archive in AdaptiveGaussianDEwithArchive doesn't provide a substantial performance boost, possibly due to overhead in managing and selecting from it. Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive utilizes adaptive Gaussian mutation and DE for better search than the basic uniform initialization and DE strategy in EnhancedArchiveGuidedDE. Overall: The adaptive Gaussian sampling and mutation strategies appear to be crucial for better performance on the GNBG benchmark, surpassing the benefits of simple archiving mechanisms or even the inclusion of DE in some cases.  The adaptive scaling of mutation strength is also a factor, enabling efficient exploration initially and targeted exploitation later.
- Adaptive initialization and mutation strategies are paramount for high-dimensional problems like GNBG.  Balancing exploration and exploitation using adaptive parameters is key to superior performance.  Overly complex mechanisms like large archives may hinder efficiency without offering substantial benefits.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:59:42 INFO Generation 7, best so far: 0.5412867233158046
2025-06-20 18:59:42 INFO --- Performing Long-Term Reflection at Generation 7 ---
2025-06-20 18:59:47 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses an adaptive Gaussian sampling for initialization, focusing on a localized exploration around a randomly chosen point, while EnhancedArchiveGuidedDE uses a simple uniform random initialization.  AdaptiveGaussianSamplingEA also incorporates adaptive mutation and selection mechanisms. EnhancedArchiveGuidedDE relies on an archive but lacks the adaptive strategies of the top performer.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE combines adaptive Gaussian sampling with Differential Evolution (DE), leveraging the exploration capabilities of Gaussian sampling and the exploitation power of DE.  AdaptiveGaussianDEwithArchive also uses DE and adaptive Gaussian mutation but adds an archive. While an archive can help with multimodal problems, the added complexity may not outweigh the benefit of simpler adaptive strategies.  AdaptiveGaussianSamplingDE's clearer focus on adaptive sampling seems more effective.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA uses a simpler EA framework, whereas AdaptiveGaussianSamplingDE integrates DE. This added complexity may not significantly improve performance in this case, as suggested by the AOCC scores, possibly due to the overhead of DE operations.  AdaptiveGaussianSamplingEA’s direct approach to adaptive Gaussian sampling is more efficient.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE and incorporate adaptive Gaussian mutation; however, AdaptiveGaussianDEwithArchive includes an archive.  The archive's performance impact appears negligible in this case and may even negatively affect performance considering its AOCC score.  The simpler approach of AdaptiveGaussianMutationDE performs slightly better.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive utilizes Differential Evolution and an adaptive Gaussian mutation, while EnhancedArchiveGuidedDE relies solely on DE and a simple uniform random initialization. AdaptiveGaussianDEwithArchive shows a slightly better approach by using an adaptive mutation and the archive to maintain diversity, but its approach seems less effective than AdaptiveGaussianSamplingEA which uses a cleaner implementation.  The simple uniform random initialization and the lack of adaptive components in EnhancedArchiveGuidedDE contribute to its lower performance.

Overall: The top-performing algorithms consistently utilize adaptive Gaussian sampling or mutation for initialization and incorporate adaptive mechanisms throughout the optimization process.  The addition of an archive or more complex operators (DE) does not automatically lead to better performance. A simpler, well-tuned adaptive strategy appears more effective for this specific problem.


**Experience:**

Focusing on adaptive initialization strategies, specifically Gaussian sampling tailored to the search space, proves crucial.  Simpler algorithms with well-tuned adaptive mechanisms often outperform more complex ones, emphasizing the importance of efficient implementation.

2025-06-20 18:59:47 INFO Generating offspring via Crossover...
2025-06-20 18:59:56 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:00:14 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:00:14 INFO FeHistory: [1036622.30155381 1304784.13470907 1573691.38545651 ...   13752.69909174
    7968.68484316   11217.94086739]
2025-06-20 19:00:14 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:00:14 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:00:32 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:00:32 INFO FeHistory: [237144.03712234 286602.56790732 228145.618858   ...   4717.68930812
   3750.55962076   3230.5917701 ]
2025-06-20 19:00:32 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:00:32 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:00:51 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:00:51 INFO FeHistory: [167864.63490061 224570.07556189 149098.01767572 ...   3501.5411636
   3746.66160491   5400.1954089 ]
2025-06-20 19:00:51 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:00:51 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:00:56 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:01:38 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0001
2025-06-20 19:01:38 INFO FeHistory: [188.39830299 179.34186118 179.69813211 ...  23.3228476   21.85237799
  40.22414729]
2025-06-20 19:01:38 INFO Expected Optimum FE: -100
2025-06-20 19:01:38 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:01:38 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:01:38 INFO Multimodal (multiple components) AOCC mean: 0.0001
2025-06-20 19:01:38 INFO AOCC mean: 0.0000
2025-06-20 19:01:45 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:02:02 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:02:02 INFO FeHistory: [ 717698.02857533 1474321.17705616 1724593.86488927 ... 1003646.09182647
  688937.35895457 1116119.26151059]
2025-06-20 19:02:02 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:02:02 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:02:20 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:02:20 INFO FeHistory: [213999.25963027 380464.27613963 305565.63663085 ... 252420.60718759
 145250.52126971 403135.75880866]
2025-06-20 19:02:20 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:02:20 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:02:37 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:02:37 INFO FeHistory: [295249.13332722 208040.9141244  288840.22127553 ...  74322.1315429
 141040.84828262  88966.44091816]
2025-06-20 19:02:37 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:02:38 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:02:45 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:03:28 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:03:28 INFO FeHistory: [226.01311813 189.03946864 243.44431802 ... 124.97871921 234.22870881
 137.01766249]
2025-06-20 19:03:28 INFO Expected Optimum FE: -100
2025-06-20 19:03:28 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:03:28 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:03:28 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:03:28 INFO AOCC mean: 0.0000
2025-06-20 19:03:28 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses an adaptive Gaussian sampling for initialization, focusing on a localized exploration around a randomly chosen point, while EnhancedArchiveGuidedDE uses a simple uniform random initialization.  AdaptiveGaussianSamplingEA also incorporates adaptive mutation and selection mechanisms. EnhancedArchiveGuidedDE relies on an archive but lacks the adaptive strategies of the top performer.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE combines adaptive Gaussian sampling with Differential Evolution (DE), leveraging the exploration capabilities of Gaussian sampling and the exploitation power of DE.  AdaptiveGaussianDEwithArchive also uses DE and adaptive Gaussian mutation but adds an archive. While an archive can help with multimodal problems, the added complexity may not outweigh the benefit of simpler adaptive strategies.  AdaptiveGaussianSamplingDE's clearer focus on adaptive sampling seems more effective.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA uses a simpler EA framework, whereas AdaptiveGaussianSamplingDE integrates DE. This added complexity may not significantly improve performance in this case, as suggested by the AOCC scores, possibly due to the overhead of DE operations.  AdaptiveGaussianSamplingEA’s direct approach to adaptive Gaussian sampling is more efficient.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE and incorporate adaptive Gaussian mutation; however, AdaptiveGaussianDEwithArchive includes an archive.  The archive's performance impact appears negligible in this case and may even negatively affect performance considering its AOCC score.  The simpler approach of AdaptiveGaussianMutationDE performs slightly better.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive utilizes Differential Evolution and an adaptive Gaussian mutation, while EnhancedArchiveGuidedDE relies solely on DE and a simple uniform random initialization. AdaptiveGaussianDEwithArchive shows a slightly better approach by using an adaptive mutation and the archive to maintain diversity, but its approach seems less effective than AdaptiveGaussianSamplingEA which uses a cleaner implementation.  The simple uniform random initialization and the lack of adaptive components in EnhancedArchiveGuidedDE contribute to its lower performance.

Overall: The top-performing algorithms consistently utilize adaptive Gaussian sampling or mutation for initialization and incorporate adaptive mechanisms throughout the optimization process.  The addition of an archive or more complex operators (DE) does not automatically lead to better performance. A simpler, well-tuned adaptive strategy appears more effective for this specific problem.
- Focusing on adaptive initialization strategies, specifically Gaussian sampling tailored to the search space, proves crucial.  Simpler algorithms with well-tuned adaptive mechanisms often outperform more complex ones, emphasizing the importance of efficient implementation.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:03:28 INFO Generation 8, best so far: 0.5412867233158046
2025-06-20 19:03:28 INFO --- Performing Long-Term Reflection at Generation 8 ---
2025-06-20 19:03:33 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization, leading to a more focused initial population near a potential optimum, unlike EnhancedArchiveGuidedDE's purely uniform random initialization.  AdaptiveGaussianSamplingEA also incorporates adaptive mutation and recombination operators, which help fine-tune the exploration-exploitation balance.  EnhancedArchiveGuidedDE relies on a potentially less effective archive-guided DE strategy.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE leverages the efficiency of Differential Evolution, while AdaptiveGaussianDEwithArchive adds an archive mechanism that might increase computational cost without a significant gain in performance for this specific benchmark as indicated by their respective AOCC scores.  The archive mechanism's added overhead may not justify the marginal performance improvement.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that while both use adaptive Gaussian sampling, AdaptiveGaussianSamplingEA employs a simpler EA structure, which might be more robust for the GNBG function.  AdaptiveGaussianSamplingDE's DE framework adds complexity, but in this case doesn't yield superior results.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive:  Both use DE and adaptive Gaussian mutation.  However, AdaptiveGaussianDEwithArchive's added archive, similar to the previous comparison, doesn't seem to improve performance, suggesting that for this problem the cost of maintaining the archive outweighs the benefit of diversifying the search.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive employs a more sophisticated mutation strategy and initialization, while EnhancedArchiveGuidedDE relies on a purely uniform random initial population.  The AdaptiveGaussianDEwithArchive also benefits from incorporating the DE framework, while EnhancedArchiveGuidedDE utilizes a simpler evolutionary approach.

Overall: The best-performing algorithms consistently utilize adaptive Gaussian sampling for initialization and incorporate adaptive mutation strategies.  The addition of an archive or complex mechanisms like differential evolution doesn't automatically lead to better performance, particularly for this benchmark function.  Simpler, more focused strategies seem to be more effective.


**Experience:**

Adaptive Gaussian sampling proves crucial for high-dimensional problems with wide bounds.  Simpler EA structures, carefully tuned mutation, and recombination operators are more effective than complex hybrid methods in certain cases.  Algorithm design should prioritize the right balance between exploration and exploitation rather than adding unnecessary complexity.

2025-06-20 19:03:33 INFO Generating offspring via Crossover...
2025-06-20 19:03:42 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:03:57 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:03:57 INFO FeHistory: [726867.7007206  737226.73639007 848017.12959706 ...  54422.93761771
  54422.93761771  54422.93761771]
2025-06-20 19:03:57 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:03:57 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:04:12 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:04:12 INFO FeHistory: [206765.44961627 242401.83693877 246054.10880992 ...  43353.75060525
  43353.75060525  43353.75060525]
2025-06-20 19:04:12 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:04:12 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:04:27 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:04:27 INFO FeHistory: [120226.04622832 161613.17750437 180474.59883127 ...  25189.66459999
  25189.66459999  25189.66459999]
2025-06-20 19:04:27 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:04:27 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:04:42 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:05:06 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:05:06 INFO FeHistory: [210.01771172 202.09833145 227.75917966 ... 147.88784417 147.88784417
 147.88784417]
2025-06-20 19:05:06 INFO Expected Optimum FE: -100
2025-06-20 19:05:06 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:05:06 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:05:06 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:05:06 INFO AOCC mean: 0.0000
2025-06-20 19:05:15 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:05:27 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:05:27 INFO FeHistory: [2052706.82078673 1909686.37722606 2576590.76695811 ...   49448.10695391
   49193.72526859   49466.77037272]
2025-06-20 19:05:27 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:05:27 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:05:39 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:05:39 INFO FeHistory: [278792.59923331 222097.1345913  295394.50248356 ...  83725.63531805
  83723.64629306  83692.71992208]
2025-06-20 19:05:39 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:05:39 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:05:51 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:05:51 INFO FeHistory: [299882.61047577 191966.82730155 262905.3263224  ...  52857.33059149
  52947.87707798  53424.07490321]
2025-06-20 19:05:51 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:05:51 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:06:15 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:14:55 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:14:55 INFO FeHistory: [173.77580349 190.12411555 208.58621135 ... 116.0290719  115.49435305
 115.23668607]
2025-06-20 19:14:55 INFO Expected Optimum FE: -100
2025-06-20 19:14:55 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:14:55 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:14:55 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:14:55 INFO AOCC mean: 0.0000
2025-06-20 19:14:55 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization, leading to a more focused initial population near a potential optimum, unlike EnhancedArchiveGuidedDE's purely uniform random initialization.  AdaptiveGaussianSamplingEA also incorporates adaptive mutation and recombination operators, which help fine-tune the exploration-exploitation balance.  EnhancedArchiveGuidedDE relies on a potentially less effective archive-guided DE strategy.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE leverages the efficiency of Differential Evolution, while AdaptiveGaussianDEwithArchive adds an archive mechanism that might increase computational cost without a significant gain in performance for this specific benchmark as indicated by their respective AOCC scores.  The archive mechanism's added overhead may not justify the marginal performance improvement.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that while both use adaptive Gaussian sampling, AdaptiveGaussianSamplingEA employs a simpler EA structure, which might be more robust for the GNBG function.  AdaptiveGaussianSamplingDE's DE framework adds complexity, but in this case doesn't yield superior results.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive:  Both use DE and adaptive Gaussian mutation.  However, AdaptiveGaussianDEwithArchive's added archive, similar to the previous comparison, doesn't seem to improve performance, suggesting that for this problem the cost of maintaining the archive outweighs the benefit of diversifying the search.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive employs a more sophisticated mutation strategy and initialization, while EnhancedArchiveGuidedDE relies on a purely uniform random initial population.  The AdaptiveGaussianDEwithArchive also benefits from incorporating the DE framework, while EnhancedArchiveGuidedDE utilizes a simpler evolutionary approach.

Overall: The best-performing algorithms consistently utilize adaptive Gaussian sampling for initialization and incorporate adaptive mutation strategies.  The addition of an archive or complex mechanisms like differential evolution doesn't automatically lead to better performance, particularly for this benchmark function.  Simpler, more focused strategies seem to be more effective.
- Adaptive Gaussian sampling proves crucial for high-dimensional problems with wide bounds.  Simpler EA structures, carefully tuned mutation, and recombination operators are more effective than complex hybrid methods in certain cases.  Algorithm design should prioritize the right balance between exploration and exploitation rather than adding unnecessary complexity.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:14:55 INFO Generation 9, best so far: 0.5412867233158046
2025-06-20 19:14:55 INFO --- Performing Long-Term Reflection at Generation 9 ---
2025-06-20 19:15:00 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that adaptive Gaussian sampling for initialization and mutation, combined with a tournament selection strategy, significantly outperforms a simple uniform random initialization and a more complex archive-guided DE approach.  `AdaptiveGaussianSamplingEA` leverages a decaying Gaussian width to explore the search space effectively, while `EnhancedArchiveGuidedDE` relies on a static population size and simpler DE mutation without adaptive exploration.  (second best) `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`, shows that incorporating an archive doesn't consistently improve performance compared to a simpler DE-based approach with adaptive Gaussian perturbation.  Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`, we see that the EA-based approach with tournament selection and gaussian recombination is slightly better than a pure DE approach, suggesting a balance between exploration and exploitation. (3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`, demonstrates again that an archive doesn't necessarily enhance the search, especially given a well-tuned adaptive Gaussian mutation. Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, highlights how a carefully designed adaptive mutation strategy (as seen in `AdaptiveGaussianDEwithArchive`) is more effective than relying solely on archive-guided pbest selection. Overall: Adaptive Gaussian sampling and mutation strategies that incorporate adaptive scaling factors or decay rates consistently outperform simpler initialization and mutation methods. The addition of an archive or specific selection mechanisms does not necessarily improve results in this particular benchmark and high-dimensional space, and can sometimes harm performance by adding complexity without proportional improvements.


**Experience:**

Adaptive Gaussian sampling proves crucial for effective high-dimensional initialization.  Focusing on adaptive mutation and selection mechanisms, rather than complex archiving schemes, yields better results in high-dimensional spaces.  Simple, well-tuned methods often outperform more sophisticated but less carefully implemented ones.

2025-06-20 19:15:00 INFO Generating offspring via Crossover...
2025-06-20 19:15:08 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:15:26 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:15:26 INFO FeHistory: [1071629.80957857  835029.06501833 1195685.86634471 ...   33087.33273562
   21045.25790495   30271.04371213]
2025-06-20 19:15:26 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:15:26 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:15:44 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:15:44 INFO FeHistory: [208786.03152526 222146.10891439 250125.16189683 ...  17162.95162053
  11036.19177437  12014.41475298]
2025-06-20 19:15:44 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:15:44 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:16:02 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:16:02 INFO FeHistory: [223437.41318107 212273.86202589 222080.99437233 ...  33333.46893863
  24318.08776184  17848.13334587]
2025-06-20 19:16:02 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:16:02 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:16:08 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:16:48 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:16:48 INFO FeHistory: [170.17929311 169.87759909 171.85239092 ...  54.39761796  88.3267501
  67.27545466]
2025-06-20 19:16:48 INFO Expected Optimum FE: -100
2025-06-20 19:16:48 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:16:48 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:16:48 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:16:48 INFO AOCC mean: 0.0000
2025-06-20 19:16:56 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:16:56 ERROR Can not run the algorithm
2025-06-20 19:16:56 INFO Run function 4 complete. FEHistory len: 104, AOCC: 0.0000
2025-06-20 19:16:56 INFO FeHistory: [ 980684.82697302  875472.65630318  788220.58597683 1124298.88750692
  922097.15569928 1017517.5286017   939878.856388    822011.94907952
 1055557.31677292  754104.31599826  727320.21267343 1116876.69911829
 1377501.59816422 1315113.16124355  833619.01310371  971238.7675822
  927419.08854351  759146.6847918  1154039.47081634  827912.61167464
  795600.13612152 1074118.384336   1082627.56960184 1077346.32220292
 1519398.98387041 1561067.96342784 1251184.01875673 1015524.2848733
  586030.50745155  580038.43345409 1058572.59338775 1137055.74947764
  901732.5622591   903467.93626426 1188474.00462787 1016642.32517245
  976141.69447099  910171.87359813 1011240.33102382  608124.28301683
  935293.93288895  832000.97113096  664390.68577428  971550.77008584
  937082.02235151  673419.11071208 1105011.01118769 1148677.64632939
 1292021.81269947 1215815.51843641  778982.00244858 1178993.80417744
  800748.01650025  614231.10684309 1192269.45410962  700333.27374938
 1052808.4317042  1097237.74548225 1474019.54886064 1039488.59832892
 1040125.46965043  694601.7114093   923490.59508591 1044879.96337638
  872613.77414861  842909.64123744 1332523.38633718 1212316.16874766
 1264456.98586361  889214.57679933  867715.52377195  955757.18613135
 1120419.10645363 1045765.70538831 1138660.09255749 1100695.62723988
  974245.60897545 1034389.94173651  781697.45413566  673826.84733403
  899786.46997151  825442.3246694  1090067.83672977 1323870.6596756
  471736.56223617 1186922.53865158  743635.04954047  922172.73112347
  945284.14223469 1140146.74857948  763336.99016976  764851.3075067
 1030588.3649464   942732.18745251  896285.49229119 1119601.01601547
 1487952.80718581 1061531.5452627   994690.1936905   910982.53365835
  787597.35780359  520604.46213584  429812.62339404  754356.27732703]
2025-06-20 19:16:56 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:16:56 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:16:56 ERROR Can not run the algorithm
2025-06-20 19:16:56 INFO Run function 7 complete. FEHistory len: 118, AOCC: 0.0000
2025-06-20 19:16:56 INFO FeHistory: [208381.27402795 272038.64463193 368640.08030525 211992.89129606
 391276.41425006 256098.26486187 227390.31842064 390079.31857118
 328067.14679834 455465.8687452  228691.2251114  268702.58961297
 183892.50007928 354379.94367319 212398.76497682 271169.72715956
 209707.43962364 229452.59230881 204145.06205831 290339.64879314
 266039.15045954 215353.84092841 350180.50688806 306785.22220491
 366310.45677012 348888.54179292 257463.65384102 266964.15075737
 296335.01986847 173314.23400066 264279.8039024  206787.92477688
 334268.50257708 301315.82118316 261484.79448993 288844.73163657
 195464.49922638 367848.38806337 419367.19331618 366143.43283506
 284394.42187769 445407.26944762 263176.7746715  243679.44676891
 336002.54221139 182629.83096694 339519.33496779 241607.93780816
 226731.44765004 186706.27252203 342762.26412783 297167.39300529
 372497.615331   327986.48414893 202362.98968086 265951.01520947
 280504.37710686 199378.06719137 353049.71279111 243201.9012119
 319282.89351162 269848.93044467 366905.73526693 243730.19460198
 234192.79538762 267663.81514055 312785.84332197 316495.07547714
 282274.60570071 265519.06968567 267486.4890909  263813.30884053
 315906.95329766 275170.12701593 250929.49535109 249466.12268118
 301330.57539173 244526.38358641 334366.82652744 198986.62200671
 240171.6546193  267186.61808986 252755.37683469 282422.22386111
 319870.12298554 258680.515513   320300.41468682 155335.85319249
 259834.49580649 169581.56882737 278288.7861112  259484.24784662
 354645.46557055 317179.90551682 298962.21196299 252817.04009372
 347570.66372336 215898.56773041 304289.62073274 125189.52532399
 292561.72827697 203083.46276854 244153.00994507 227733.33903217
 177871.41830878 155249.42235457 326924.98201078 240970.39401904
 218690.68965478 184952.99675088 198030.0915094  168007.94475273
 272312.87730831 166299.44195392 230346.60806362 173064.5910068
 125059.89372396 265277.81460992]
2025-06-20 19:16:56 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:16:56 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:16:56 ERROR Can not run the algorithm
2025-06-20 19:16:57 INFO Run function 8 complete. FEHistory len: 106, AOCC: 0.0000
2025-06-20 19:16:57 INFO FeHistory: [209784.66766092 225078.22515862 212997.27883252 263826.08181464
 286244.58344025 376920.70990979 234244.61241965 329471.15528793
 262613.84842713 152854.3028755  211156.427514   307308.51404997
 224627.99986218 258141.21887982 245798.33212215 297130.92792814
 231816.78507034 249725.66084577 199547.26679568 232655.44574038
 154667.14394488 162313.62566507 188391.31325005 203454.26090388
 353984.59842578 208892.03762522 164868.09522477 319389.72633309
 223178.15020398 159863.3651826  281513.63401317 378904.44938806
 272309.36032922 285197.47381512 247138.17854746 381429.24595152
 260568.69470122 200249.43922272 187044.65768081 151694.56239276
 277373.95773297 195506.47938237 181238.03382728 379236.09214713
 239031.36326523 314950.50222197 215418.07212633 247220.31731351
 149495.09111659 349460.48550758 229009.1170919  200654.96921949
 180693.82899133 176954.74867717 304896.29831725 284536.11486061
 224550.88800626 190654.40356809 297034.7289258  226615.02817344
 246653.84671096 227758.77958205 198322.84357966 286593.96327699
 177083.77452022 475315.93923145 247480.41084181 310839.34098787
 237874.32645196 279216.9773435  211997.9780862  239127.16384115
 261645.67905865 377125.2546072  163384.60346517 265227.85155853
 302220.8804307  284899.04986084 153275.35802718 283921.43590383
 289497.60963937 336725.0206353  276304.06589728 149550.96029943
 213889.13480346 186669.99997149 286005.06146314 340497.88166172
 243655.07892094 271992.44834379 295222.11552748 373107.09847308
 214468.32007402 262986.88099943 222649.52084502 345709.14541909
 134437.71908816 252777.4900563  319450.72764857 254709.40721817
 229354.14833772 145976.43999976 191553.89988494 126816.27044976
 101359.31663294 212067.90552602]
2025-06-20 19:16:57 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:16:57 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:16:57 ERROR Can not run the algorithm
2025-06-20 19:16:57 INFO Run function 24 complete. FEHistory len: 170, AOCC: 0.0000
2025-06-20 19:16:57 INFO FeHistory: [135.33703695 209.04408086 210.65316211 184.2139931  245.34175609
 220.31307187 210.53438283 219.6422828  244.59842756 204.2404288
 217.36148787 204.77727973 230.40697667 177.80151376 180.39388051
 254.96169619 197.70395076 197.38591083 205.31364533 216.94325504
 172.7851332  246.18469853 205.31635075 239.26574011 221.97192255
 210.62269647 186.15264961 202.37095621 228.34858371 198.23744981
 211.56499263 225.0588391  203.40817157 184.68507228 214.436824
 225.53184626 205.49450497 189.97250056 231.71692038 224.50531433
 194.94064819 230.6389845  241.15850312 212.31506728 195.06494802
 232.94406283 172.01024348 270.23065788 257.77857931 233.76287312
 226.59203462 222.22750248 195.30197049 170.80830418 182.63734178
 205.53318903 195.9735032  212.79048251 214.24780135 261.48921619
 236.33506216 223.34833006 191.39726645 225.82456477 210.96788098
 229.05428062 257.12969502 257.44139328 204.65346387 236.67745648
 172.11292546 182.40263996 192.69411567 219.30186929 206.52876634
 204.60684598 260.01945782 214.55363912 199.88713309 206.08883894
 198.22671225 192.76550282 215.77157822 250.7481621  196.04664682
 214.09490622 196.8394198  190.61387815 230.18200658 188.75766317
 190.64887474 199.19721184 230.26514083 205.44920137 191.05274169
 246.89940369 223.18325661 180.7481927  206.85451572 227.05396246
 183.16755049 151.43832466 194.84813676 181.62344433 196.51073764
 178.08716717 186.57140557 191.13449964 192.08669629 161.81108858
 164.90509886 206.598637   195.16786786 175.0359401  179.23114095
 186.13172463 222.92301896 187.22974422 216.10772817 204.899413
 182.34738158 191.0813831  212.79662574 201.22175167 220.76919614
 159.05325357 201.60359972 200.25522539 226.91500999 176.75360722
 181.30342055 186.17129002 170.44101937 200.8649213  183.71937959
 186.93433681 214.74563907 183.5122197  186.47338351 217.04567751
 205.88547694 200.17501377 200.9484395  194.89652488 167.41923185
 170.3344419  192.59515533 216.56826017 208.26942479 200.86287487
 169.72247724 197.59195246 222.18152077 193.90302207 162.33520775
 199.93927255 182.32182303 197.51918492 168.19764538 193.89059714
 216.07284579 199.92194244 157.87576167 184.58998696 174.4209606
 197.72143023 190.7074763  184.01902761 131.93063011 175.98575359]
2025-06-20 19:16:57 INFO Expected Optimum FE: -100
2025-06-20 19:16:57 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:16:57 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:16:57 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:16:57 INFO AOCC mean: 0.0000
2025-06-20 19:16:57 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that adaptive Gaussian sampling for initialization and mutation, combined with a tournament selection strategy, significantly outperforms a simple uniform random initialization and a more complex archive-guided DE approach.  `AdaptiveGaussianSamplingEA` leverages a decaying Gaussian width to explore the search space effectively, while `EnhancedArchiveGuidedDE` relies on a static population size and simpler DE mutation without adaptive exploration.  (second best) `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`, shows that incorporating an archive doesn't consistently improve performance compared to a simpler DE-based approach with adaptive Gaussian perturbation.  Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`, we see that the EA-based approach with tournament selection and gaussian recombination is slightly better than a pure DE approach, suggesting a balance between exploration and exploitation. (3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`, demonstrates again that an archive doesn't necessarily enhance the search, especially given a well-tuned adaptive Gaussian mutation. Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, highlights how a carefully designed adaptive mutation strategy (as seen in `AdaptiveGaussianDEwithArchive`) is more effective than relying solely on archive-guided pbest selection. Overall: Adaptive Gaussian sampling and mutation strategies that incorporate adaptive scaling factors or decay rates consistently outperform simpler initialization and mutation methods. The addition of an archive or specific selection mechanisms does not necessarily improve results in this particular benchmark and high-dimensional space, and can sometimes harm performance by adding complexity without proportional improvements.
- Adaptive Gaussian sampling proves crucial for effective high-dimensional initialization.  Focusing on adaptive mutation and selection mechanisms, rather than complex archiving schemes, yields better results in high-dimensional spaces.  Simple, well-tuned methods often outperform more sophisticated but less carefully implemented ones.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:16:57 INFO Generation 10, best so far: 0.5412867233158046
2025-06-20 19:16:57 INFO --- Performing Long-Term Reflection at Generation 10 ---
2025-06-20 19:17:02 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses an adaptive Gaussian sampling for initialization, creating a population centered around a randomly chosen point, leading to better exploration of the search space, while EnhancedArchiveGuidedDE uses simple uniform random initialization, which is less effective in high-dimensional spaces.  AdaptiveGaussianSamplingEA also incorporates adaptive mutation and sigma decay for finer-grained adjustments to the search process. EnhancedArchiveGuidedDE uses a simpler DE strategy and archive management that is less effective.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE uses DE, a powerful global optimization method, combined with adaptive Gaussian sampling, leading to a good balance of exploration and exploitation. AdaptiveGaussianDEwithArchive also uses DE but adds an archive, which can be beneficial for multimodal problems, however it adds overhead and does not show improvements.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA is simpler and directly uses Gaussian perturbation for recombination and adaptive mutation. AdaptiveGaussianSamplingDE uses differential evolution which is more computationally expensive.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE, but AdaptiveGaussianMutationDE employs simpler selection and archive handling than AdaptiveGaussianDEwithArchive.  The archive in AdaptiveGaussianDEwithArchive doesn't yield the expected benefits in this specific context.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive at least incorporates an adaptive Gaussian mutation and uses DE.  EnhancedArchiveGuidedDE employs a less sophisticated DE approach with basic archive management, and shows the least effective performance. Overall: The top two heuristics showcase the value of adaptive Gaussian sampling in initialization and the use of well-structured DE for a 30D high-bound search space. Simple uniform initialization and less sophisticated DE and archive mechanisms prove inferior.  Adaptive Gaussian perturbation offers a more efficient and effective way to explore the search space than simple random search.


**Experience:**

Adaptive Gaussian sampling for initialization is crucial for high-dimensional problems with wide bounds.  Careful algorithm design and parameter tuning are essential for superior performance.  Simpler, well-tuned algorithms often outperform more complex ones in such scenarios.

2025-06-20 19:17:02 INFO Generating offspring via Crossover...
2025-06-20 19:17:11 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:17:27 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:17:27 INFO FeHistory: [ 818421.03913909 1861920.10715287 1096835.98056337 ...   64008.11229134
   64008.11229134   64008.11229134]
2025-06-20 19:17:27 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:17:27 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:17:42 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:17:42 INFO FeHistory: [228127.51058839 154400.72313461 246815.09577687 ...  33663.22255811
  33663.22255811  33663.22255811]
2025-06-20 19:17:42 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:17:42 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:17:57 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:17:57 INFO FeHistory: [141602.86506842 286288.66772702 173696.66686996 ...  35845.94218929
  35845.94218929  35845.94218929]
2025-06-20 19:17:57 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:17:57 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:18:11 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:18:38 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:18:38 INFO FeHistory: [214.04840071 202.30086088 227.04360282 ... 157.07944266 157.07944266
 157.07944266]
2025-06-20 19:18:38 INFO Expected Optimum FE: -100
2025-06-20 19:18:38 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:18:38 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:18:38 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:18:38 INFO AOCC mean: 0.0000
2025-06-20 19:18:46 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:18:46 ERROR Can not run the algorithm
2025-06-20 19:18:46 INFO Run function 4 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 19:18:46 INFO FeHistory: [ 901772.63443687  728536.23752545  565643.74117891 1065175.00559277
  946024.53743977  843963.74460335  570025.007931   1180288.02419527
 1204318.09003765  839513.44448541  684019.18005215  691896.13417184
 1065138.45764529 1291451.35878187  847568.9731342  1134815.36478716
  740929.96606452 1003213.72009308  937344.91737867  730828.98920993
  874507.03881926  898174.43197947  774581.35648219  694942.27662435
  962984.40645635 1375286.77759622  878542.59125741  785322.49274935
  697155.6462334  1067958.65710027 1024650.54637335  851341.59012101
 1144713.68754563  739423.09726552  774633.39373051 1469487.64276281
  614483.62072915 1289133.41014453 1011540.17462242  893518.23618476
  451058.25617924  830388.85975533  563120.1021402   928386.79585859
 1453443.27935562  729117.50704267 1234526.41709354  560023.29439041
  727857.68751171 1120425.31053157  914770.75878701  950316.37453902
  990901.39477846  823519.34694167  897338.76378914  958643.090202
  664170.78141458  501220.67421209 1078948.08169801  748412.67282996
  412311.71382748  934272.08660571  750764.32830234 1079959.38804427
  817749.58815784 1000559.39179032  610260.13758356  847157.19763893
  831851.4682262  1361170.84836401  794819.46593802 1313740.41633769
 1059327.42901443  543708.6866804   877710.50122999  665669.80977221
  708959.71792857 1205242.97673232  561546.4292651   926155.25680887
  669923.1470035   736882.38247521 1092407.38775933  782028.15155303
  988017.0425938   874365.20840593  719820.86520056  797086.45287576
  951531.54158567  805547.10577324  895105.70662302  952184.71829939
  924386.83952155  883400.6823583  1124755.21139076 1059160.1279295
  821254.69086827  962515.70749957  811681.50434811  871756.82178558]
2025-06-20 19:18:46 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:18:46 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:18:46 ERROR Can not run the algorithm
2025-06-20 19:18:47 INFO Run function 7 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 19:18:47 INFO FeHistory: [415435.67729244 183800.01671104 370843.66239424 352125.41033035
 283925.52819414 218468.73065093 250881.32694072 356668.73277015
 364615.13221675 408495.82020818 297275.78446475 280368.94674814
 393698.57910653 329833.60135046 279757.1002645  359728.84955657
 405052.97115375 316648.69743856 184114.40000859 390335.04938457
 378034.82904398 239526.50606532 254776.02704945 374870.57459017
 467070.58138117 247647.60412269 292563.8129747  253833.20386057
 316387.62956386 477003.38851301 151838.85721104 239994.69439105
 247837.8326145  538363.54424201 263429.24175429 289171.83711365
 294942.60543279 347042.06563101 233370.27804612 342109.17811849
 373489.20536006 483829.0879729  191831.30689499 348368.25119534
 399647.93494614 301540.95752907 334217.14515519 196623.46184816
 309550.33712284 310720.39380049 393673.0917686  326051.67860746
 211410.29474584 303269.4848342  186523.96049674 347277.71067135
 295702.8035208  456358.07881303 296957.54274234 452011.68019518
 330637.87853948 346208.17878761 310050.60508952 330603.0893844
 281146.51085982 277055.54657384 257671.36836055 245593.99670107
 301591.49677311 202648.65942353 270543.6106798  270402.8983391
 241845.39730833 350541.87461395 268583.30879465 417498.20950491
 291867.43906829 206755.54920533 308635.6643895  215767.58691991
 285049.36843193 259498.42800297 208675.29049658 323952.60577622
 253787.62816167 491322.82655249 296567.0471376  184876.95714912
 405755.28329642 159729.3646817  296786.61966797 283373.61492543
 283920.57759758 252894.82323327 370639.34889873 302541.60837843
 265351.69021152 401601.84048908 274629.52763705 338373.77194362]
2025-06-20 19:18:47 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:18:47 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:18:47 ERROR Can not run the algorithm
2025-06-20 19:18:47 INFO Run function 8 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:18:47 INFO FeHistory: [183044.81622643 396543.37414387 373055.50548752 339922.65421453
 340949.74134065 314487.6462856  382925.62442257 429091.72198862
 357742.50710974 215441.05856341 416901.23414809 332460.46100416
 292057.04956264 386462.41420959 237975.33300983 521651.39125984
 455857.801597   297565.49765948 444597.03710599 475903.22151075
 318319.18888847 305745.23708442 302530.53490079 333978.218507
 342305.35577483 323181.60052844 590906.96130532 492624.66515107
 428952.99195578 299808.0437687  308192.57496878 452487.55970508
 299071.82008636 237130.48293966 356206.76903416 299524.89154574
 439534.44599554 292787.62181099 440044.19570418 326894.12306298
 413970.66279456 385731.49615957 464224.13016175 313239.03567734
 428096.04801627 302163.34014208 375495.42492763 314455.75396802
 468797.16016083 263767.80124451 316648.75932576 343428.74895108
 538585.35148611 378804.73723321 563341.04831435 285071.69284921
 267149.56990463 311857.87013756 492204.49841588 439151.69502811
 282199.10619019 410495.20130234 358940.75024949 333576.87364949
 284174.98925738 298657.00789852 230794.37001619 345810.36504839
 426990.18542291 380669.81810081 339452.78460381 426896.31020915
 501766.52011623 253402.14475865 506510.27309829 302788.06344473
 341356.08350427 308264.79215202 355281.34110592 295515.42166114
 394221.68839951 440625.87003007 322676.82998032 391036.58692785
 366939.35168247 424023.09419588 430446.28312049 339988.5744758
 385952.71962728 360379.68500563 504385.80033058 311310.54877758
 403590.92589529 525926.54177577 268980.54527496 390413.41687164
 368192.56199365 462332.84904631 411475.71172144 381521.97100106
 308387.61947243]
2025-06-20 19:18:47 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:18:47 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:18:49 ERROR Can not run the algorithm
2025-06-20 19:18:49 INFO Run function 24 complete. FEHistory len: 5901, AOCC: 0.0000
2025-06-20 19:18:49 INFO FeHistory: [222.43790564 217.73035211 226.23485396 ... 115.16517641  72.59371936
 157.12072248]
2025-06-20 19:18:49 INFO Expected Optimum FE: -100
2025-06-20 19:18:49 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:18:49 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:18:49 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:18:49 INFO AOCC mean: 0.0000
2025-06-20 19:18:49 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses an adaptive Gaussian sampling for initialization, creating a population centered around a randomly chosen point, leading to better exploration of the search space, while EnhancedArchiveGuidedDE uses simple uniform random initialization, which is less effective in high-dimensional spaces.  AdaptiveGaussianSamplingEA also incorporates adaptive mutation and sigma decay for finer-grained adjustments to the search process. EnhancedArchiveGuidedDE uses a simpler DE strategy and archive management that is less effective.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE uses DE, a powerful global optimization method, combined with adaptive Gaussian sampling, leading to a good balance of exploration and exploitation. AdaptiveGaussianDEwithArchive also uses DE but adds an archive, which can be beneficial for multimodal problems, however it adds overhead and does not show improvements.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA is simpler and directly uses Gaussian perturbation for recombination and adaptive mutation. AdaptiveGaussianSamplingDE uses differential evolution which is more computationally expensive.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE, but AdaptiveGaussianMutationDE employs simpler selection and archive handling than AdaptiveGaussianDEwithArchive.  The archive in AdaptiveGaussianDEwithArchive doesn't yield the expected benefits in this specific context.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive at least incorporates an adaptive Gaussian mutation and uses DE.  EnhancedArchiveGuidedDE employs a less sophisticated DE approach with basic archive management, and shows the least effective performance. Overall: The top two heuristics showcase the value of adaptive Gaussian sampling in initialization and the use of well-structured DE for a 30D high-bound search space. Simple uniform initialization and less sophisticated DE and archive mechanisms prove inferior.  Adaptive Gaussian perturbation offers a more efficient and effective way to explore the search space than simple random search.
- Adaptive Gaussian sampling for initialization is crucial for high-dimensional problems with wide bounds.  Careful algorithm design and parameter tuning are essential for superior performance.  Simpler, well-tuned algorithms often outperform more complex ones in such scenarios.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:18:49 INFO Generation 11, best so far: 0.5412867233158046
2025-06-20 19:18:49 INFO --- Performing Long-Term Reflection at Generation 11 ---
2025-06-20 19:18:54 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for both initialization and mutation, leading to a more focused exploration of the search space, while EnhancedArchiveGuidedDE relies on uniform random initialization and a less sophisticated archive update strategy.  AdaptiveGaussianSamplingEA's adaptive sigma reduces exploration over time, leading to exploitation and better convergence.  EnhancedArchiveGuidedDE uses a simpler DE strategy.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE incorporates adaptive Gaussian perturbation within its DE scheme, enhancing its ability to escape local optima.  AdaptiveGaussianDEwithArchive adds an archive but lacks the adaptive Gaussian perturbation within the main DE loop, impacting exploration. The archive in AdaptiveGaussianDEwithArchive is less sophisticated than in EnhancedArchiveGuidedDE.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA employs a simpler EA structure, focusing on adaptive Gaussian mutation and selection, leading to cleaner code and potentially better performance in the given benchmark. AdaptiveGaussianSamplingDE uses DE which is less efficient in this case.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE but AdaptiveGaussianMutationDE's adaptive Gaussian mutation provides a more robust escape from local optima, while AdaptiveGaussianDEwithArchive's archive, while aiming for multimodal optimization, doesn't significantly improve performance in this specific benchmark. AdaptiveGaussianDEwithArchive adds complexity without commensurate performance gains.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive incorporates an archive and adaptive Gaussian perturbation within its DE framework, demonstrating a more refined approach compared to EnhancedArchiveGuidedDE which only uses a simple archive. However both perform comparatively less well than the first two approaches. Overall: Adaptive Gaussian sampling initialization and adaptive mutation strategies consistently outperform simple uniform random initialization and static mutation scales in this high-dimensional search space, indicating the importance of adaptive mechanisms for efficient exploration and exploitation.  The addition of Differential Evolution does not always improve performance, especially without careful integration of adaptive sampling techniques.


**Experience:**

Adaptive mechanisms for controlling exploration and exploitation are crucial in high-dimensional spaces.  Combining adaptive sampling with efficient search operators like DE requires careful design to avoid redundancy and ensure synergistic interaction.  A simple and well-tuned adaptive algorithm can outperform complex algorithms with multiple components.

2025-06-20 19:18:54 INFO Generating offspring via Crossover...
2025-06-20 19:19:03 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:19:03 ERROR Can not run the algorithm
2025-06-20 19:19:03 INFO Run function 4 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-20 19:19:03 INFO FeHistory: [ 829387.19249827  894111.52393744 1096614.64013135 1005860.4955828
  846926.55433049  701513.92635749  913256.7670456   960735.96185055
  773297.37752041  941452.69755718 1022820.69717354 1049301.93662533
  819334.99026653 1042829.48500523  971313.83734146  916006.51653118
  657514.87044303  702793.92537878  696005.07504546  769867.8410911
  697385.16814103  757286.34719211  479062.28062707 1067307.08640413
  843122.20958316 1233695.60016281 1031465.430906    583364.89634626
  986494.92450737  726240.78867163  884445.73596581  854426.13150608
  817301.0862461  1149868.87370996  805329.6205099   791213.0504467
  805129.14627561 1051889.51606542  853736.48200842  718999.98383512
  873120.95967923  890894.4505368   715942.51600936 1011052.26177581
  875860.40066675  779442.39818453  861062.32340174  944536.53984049
 1116586.40511881 1225853.9721841   592585.20476565  821974.60417756
 1068685.41398236 1165465.72495992  962329.46514802 1147725.85015391
 1033433.64327708  814018.40210693  708932.78809796  840758.52919557
 1117815.87579088  703524.71043451 1082839.30797826 1027291.3518463
  841111.13601514 1028111.605107    905099.18631503  924469.22096577
  871168.94229063 1023134.41833728  780346.51422831  782430.53794752
  817447.2548671   958497.41915694  698970.30928864  859575.25781083
  798269.82938032  638331.42714011  802030.0674521   947060.18015005
  892400.07218358  737127.60976048  665171.59640895  764996.08904426
  727247.03165988  897898.12946867  862417.59246472  935406.04078884
 1008097.22861564  772178.38486173 1004419.06568241 1222978.68593569
  891113.28136415  656703.73028416  933418.68052303  574995.02711744
  855772.7846431   978502.2316802   817762.47176816  982543.22923901
 1028980.42786875 1264836.76095678  669017.77829223  881392.11418561
  677742.52790106  858679.29094391 1473004.74077727 1096271.83060351
 1113384.29534063 1037099.24941756 1645846.46170629  894361.70982671
 1283681.26078268 1187840.27342145 1042764.01477524  938963.43571763
  974549.99737793  936460.54739109  826862.541506    881992.12901669
  947921.21568815  979019.75315413  964862.51531268  779990.21462273
  949530.33221518 1261357.97537568  577461.94246611  909067.55749885
  933438.56747884  949468.51933499  602628.19466916  797843.05018262
 1506927.71675046 1433437.9140982   773208.75570915  836073.40544801
 1110838.86296356  861561.50853305 1089818.73424203  983209.37808413
  932102.29002899 1123550.49977677 1072764.29541162  995671.3762146
  751942.88300866  978107.30364284  783877.91245884  693292.97099142
 1365734.31971618 1099391.9236741   726270.17800937  867387.71283386
  796141.44194085  937639.6996023  1098644.86423617 1086435.87037304
  906886.5627592  1042013.70541037  591672.94413135 1294043.72095737
  972561.83420653 1121724.00599393 1130919.23545704 1068076.9609472
 1098290.77535132 1037858.23774741  884376.0077887   799830.89179833
 1005810.32189045 1036513.36439264 1018264.08177198  611909.49070973
 1118328.20861691  972223.52090273  966794.01028202  771574.93748844
 1082459.53853309 1133926.01186002 1018150.7990288  1029415.09914065
  766296.77340805  884954.91784316  896450.25317349 1035522.81068069
  656341.05398719  918489.20882782  777695.46847047  737362.95792191
 1366760.24621624  951862.94893786  942839.26407786  925504.93588297
 1048451.39054954  846588.04294805  979467.8301018  1057536.64293642
  915886.12056595  827160.99559629 1018668.45219352 1096239.62039556
  919931.03185771]
2025-06-20 19:19:03 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:19:03 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:19:03 ERROR Can not run the algorithm
2025-06-20 19:19:04 INFO Run function 7 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-20 19:19:04 INFO FeHistory: [175876.54285841 227583.29139356 313630.14269956 324014.88347522
 407246.98342443 306799.4365341  272513.84151361 347024.72646097
 334690.80179856 376937.26346746 331113.31624622 212316.09708276
 328986.11857449 240899.42767754 240413.12067328 288683.8584183
 291677.95974512 282182.89233901 235138.21909546 298139.27440879
 288095.88628496 273890.99964591 299063.8981571  234101.18427313
 310278.60537009 309763.94328546 213382.87584716 277796.6937621
 285576.73302717 322027.31563637 354252.87573175 275303.50239772
 250380.3119624  368755.97704248 220371.34326834 314165.26846753
 200670.83423505 247790.14026025 289500.30756211 308270.91703424
 164468.56548377 310025.57266595 299054.2939129  309616.08343641
 217386.04669975 258146.71963913 326684.33918384 274077.14403456
 373973.00159582 295974.81337311 378025.94485549 300147.61948942
 255146.33326973 253333.04401825 264550.61286909 343912.87560311
 194218.19083839 301208.39278331 250940.70719856 272367.5757041
 270154.57762665 268559.30293779 271602.57717055 320864.26334612
 300166.79009392 325318.65216763 215490.17509667 368362.3348128
 300468.28342369 363891.83586964 285606.7399929  303993.25272533
 391242.50419695 331500.75892392 283553.35648753 295350.19839657
 286213.58591908 275532.47493155 236107.88614608 335753.78352531
 191488.29033804 323841.54292663 296209.04299001 134623.29824147
 252987.90844801 357738.33748713 248515.04235904 260647.44075562
 189779.40444318 244328.23152601 274462.39865882 301263.54747153
 287320.07158038 311284.40290315 286721.33753615 286769.70735083
 303628.08699077 269678.43751379 325248.34905137 225462.77461079
 268445.73233986 236647.5000152  282643.85933688 302921.87990299
 142569.6076632  374648.93753102 264073.29079587 288446.98232355
 270580.85953435 367175.67856203 237806.63308974 265923.57214096
 247629.6856703  261785.83805823 266280.94388722 269632.28018595
 215080.20209698 327924.27097521 300283.46756179 302382.97112057
 311250.9009306  232058.51917316 219592.27097008 244824.39965267
 325481.49673622 227758.93966365 278413.26658562 325985.15415419
 323179.81719358 327635.95850914 316180.06038207 287489.31440526
 231106.25379575 307097.88916191 374455.83289893 283229.00490389
 292149.31586421 236272.37139418 256912.43476339 359394.48722573
 338587.9413623  284811.09914324 391049.98853342 311481.07471777
 241560.25068854 385181.35800761 225322.49383709 263056.71042601
 313782.26054027 345214.34623862 181596.29157655 252017.18183201
 235064.24883433 358352.440287   296567.57735355 364442.94062365
 328398.93786719 235711.62654213 304500.21340501 299530.61216725
 252949.71694849 345851.491533   208618.56504226 312615.00976667
 288088.60361585 225548.89946501 255378.03095586 394286.43164217
 366439.02423751 269140.81905994 294581.66763111 319976.66493764
 281812.54555217 384692.44872519 310950.21921326 343271.65322854
 377185.84256403 352298.00600649 221304.42046994 368642.12524334
 232512.13429288 195710.48540658 383467.62638709 191830.51882308
 327853.57943707 272014.14889769 341103.78956206 375560.87056147
 301946.41014008 168640.03575655 340547.21293935 364363.42339061
 260839.00493916 283871.89800774 333097.38794097 276310.25438969
 304329.91492671 221053.12358442 285177.35997163 307974.15029934
 295728.83393191]
2025-06-20 19:19:04 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:19:04 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:19:04 ERROR Can not run the algorithm
2025-06-20 19:19:04 INFO Run function 8 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-20 19:19:04 INFO FeHistory: [189676.43606943 303197.2736458  304294.50125667 300656.23608471
 177110.02283148 275912.39352351 203056.61771847 212904.80451641
 360114.84470189 267540.8598994  196782.00205565 271612.10593961
 236356.56692822 224491.04078113 191593.60048279 285612.84825318
 247829.54320999 301984.28157573 344415.38759469 188501.73642549
 262592.00741589 204677.90994174 214765.24283818 151871.73016188
 310974.3487535  247729.44377465 262862.58263025 142631.75619117
 252667.85308312 275233.88525602 279531.33249311 245386.14482419
 274677.37643653 277875.67980502 331079.12160132 249648.66917855
 219893.46503989 284579.83636544 338270.51415903 279269.94372243
 254917.90074477 239343.97225737 202410.19595184 224310.00024622
 385823.51788373 277583.16216666 228506.98963075 225932.27064628
 187112.23529898 262505.33065367 237863.07268183 285071.161387
 182115.62433661 311502.98771565 269851.83812856 395662.29112504
 227658.12756562 273131.82109576 321911.10951788 229193.27240334
 282417.81363675 233138.69772796 307686.85200698 194599.95224491
 330222.45855283 260914.54342345 256773.8346796  243306.86338306
 285397.57358002 400399.85744818 287668.9457536  252670.82577702
 178296.01285897 257609.21873728 277572.88803129 255837.97818668
 299487.21582229 221194.33740894 283833.52956348 224223.35619908
 244328.80097411 149271.56857104 171914.39268676 259571.31218354
 347134.10431071 190556.13231337 262288.04195343 340793.46411087
 277223.99401563 349867.06196035 281973.92524472 331774.36583808
 280706.82795042 353794.41050916 261663.30341185 243768.14379881
 275248.15773686 226214.4480827  247116.75012947 173236.70690761
 229816.3449274  256383.11326816 221674.55146242 166529.10300728
 224828.62082007 292151.45231487 236093.36372853 165634.89708352
 278108.51905972 293149.41705065 286947.32051344 255569.38749894
 283187.79724142 222303.5913646  219611.08145923 394260.6403769
 276143.35964627 208358.52683464 276657.45344157 286461.54349223
 252625.84465502 231621.20811174 246368.72287447 181998.05945838
 225109.3478062  202404.80395803 314678.29873467 268396.23135696
 292232.41089725 263664.79869274 259423.74509374 273526.76171134
 307036.98809101 260406.43608937 258154.46519441 213335.8119319
 387093.15654209 299970.94627425 260367.66842565 250728.09705519
 333776.84710537 253371.34831684 272005.00107247 185647.34042893
 200764.34749678 257484.07506763 284244.9761603  212255.86580455
 227281.62674535 312873.34804968 197949.89749667 200656.60580736
 274455.73159844 248720.78179957 192788.75362814 373217.96314422
 173974.08490681 229536.60487608 279929.16814787 277602.6492594
 286445.74787322 171818.67214221 216456.14103387 232285.52400817
 291131.93419668 275846.28536266 171589.79200343 225165.33492344
 271234.35715847 203364.73031333 283294.50363621 238737.1461637
 221320.10113235 248347.20346527 248745.12287464 255578.1923523
 192296.29330099 242452.18266918 309494.69901109 385986.77341592
 262453.19474155 273532.07128411 279286.73272597 309763.24189826
 251713.90057929 284418.18472321 268227.20818463 197449.10885805
 183924.90298614 194297.77673462 295091.52985533 237502.8780494
 359182.35178145 207303.31271017 240876.18386028 310313.47600128
 334961.53802057 346098.08958472 322244.05786985 224879.69782889
 240810.91828474]
2025-06-20 19:19:04 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:19:04 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:19:04 ERROR Can not run the algorithm
2025-06-20 19:19:05 INFO Run function 24 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-20 19:19:05 INFO FeHistory: [169.77249698 188.40444784 200.3580464  189.86336007 222.11691765
 165.08420618 217.1868587  206.24017649 189.82486332 173.87202005
 150.94642065 201.36971687 152.30346082 192.74413592 172.01075772
 194.93325568 170.45599039 189.03461803 153.88150316 172.20942411
 194.67637403 203.52935448 205.17972028 213.10299844 201.52448323
 194.66899002 213.09116097 220.57044407 210.1937655  170.85668997
 193.82797797 189.36760733 213.32085132 194.093718   189.10213658
 216.42979502 206.41220463 216.47961843 187.35023351 219.62355816
 204.56761096 199.99733154 212.86351945 213.39121885 208.99301967
 191.30189381 215.52114743 177.37108482 216.22459456 185.57826758
 184.67405132 184.35117897 192.80004782 194.98848132 200.95822145
 187.86049635 221.02176749 170.80511974 167.34527154 165.74013748
 197.00963386 163.32850995 179.12084252 201.57170839 216.64048863
 221.62772536 192.02858826 217.0766485  218.94909101 215.13588404
 206.16967326 207.30416516 197.52013588 190.72167193 224.22623447
 159.52496421 203.35752497 206.07473221 186.56130986 196.1930256
 199.21078661 183.21057762 184.38414106 216.6177939  212.06104765
 184.88743664 192.57722759 199.1741652  199.40265745 201.02925189
 188.31009784 149.48729323 181.8363922  196.88929874 209.62848329
 210.69095953 196.66528722 210.96279667 174.19607857 185.4724785
 185.83995869 215.53180206 189.42336769 171.64279867 223.75742491
 186.68509137 189.61958338 176.54724234 179.98703839 190.83514746
 185.28733483 197.50682543 167.98825461 212.60084465 216.45016496
 185.50991934 228.06757702 223.73137144 176.78417417 183.45467714
 214.29258836 195.78840336 191.70476979 195.53859872 186.92678892
 192.16326172 170.9663677  205.76941007 199.26781645 182.01107539
 232.31168221 187.72645328 212.26849252 192.97448249 181.72572236
 210.23993181 213.39205741 183.70074645 187.65664128 164.64295162
 166.73868557 199.96989601 198.0604433  199.29033178 206.99603207
 210.02436274 187.11359239 190.36665549 191.64636529 179.80535207
 211.62812372 180.1700647  200.07690247 173.13550625 215.07870609
 235.64405841 190.50122357 183.78758742 225.11424839 156.52984895
 208.86196322 195.12092382 206.77727908 222.84953019 177.37166195
 189.32572935 209.08414808 200.78199609 228.51253757 189.88083428
 215.70209946 153.47926605 186.10645017 181.75896341 204.64744961
 219.25099141 184.49166527 206.48214498 166.76216175 215.08495199
 201.62794171 163.48646104 213.01013166 181.3893912  192.97558248
 205.70626801 157.6639362  156.96810355 190.65235938 221.52936933
 187.87789429 188.94999342 172.79742539 174.18560388 178.10527817
 208.83957387 217.02940945 190.55217832 191.57893136 167.62573937
 220.78590137]
2025-06-20 19:19:05 INFO Expected Optimum FE: -100
2025-06-20 19:19:05 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:19:05 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:19:05 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:19:05 INFO AOCC mean: 0.0000
2025-06-20 19:19:11 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:19:26 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:19:26 INFO FeHistory: [ 894632.52296022  479572.40851238 1101269.54883223 ...  220805.75066628
  274237.11523467  324333.18239238]
2025-06-20 19:19:26 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:19:26 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:19:40 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:19:40 INFO FeHistory: [323981.66015384 548870.30733286 360194.68346514 ...  78982.85419822
  66107.49482702  94321.57066355]
2025-06-20 19:19:40 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:19:40 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:19:55 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:19:55 INFO FeHistory: [228944.39028698 294259.71586372 354721.81934469 ... 123640.96068426
  76379.85308844  70602.38987159]
2025-06-20 19:19:55 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:19:55 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:20:11 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:20:37 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:20:37 INFO FeHistory: [196.82633831 187.80526381 216.72943358 ... 127.02097257 105.55433824
 145.01044804]
2025-06-20 19:20:37 INFO Expected Optimum FE: -100
2025-06-20 19:20:37 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:20:37 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:20:37 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:20:37 INFO AOCC mean: 0.0000
2025-06-20 19:20:37 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for both initialization and mutation, leading to a more focused exploration of the search space, while EnhancedArchiveGuidedDE relies on uniform random initialization and a less sophisticated archive update strategy.  AdaptiveGaussianSamplingEA's adaptive sigma reduces exploration over time, leading to exploitation and better convergence.  EnhancedArchiveGuidedDE uses a simpler DE strategy.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE incorporates adaptive Gaussian perturbation within its DE scheme, enhancing its ability to escape local optima.  AdaptiveGaussianDEwithArchive adds an archive but lacks the adaptive Gaussian perturbation within the main DE loop, impacting exploration. The archive in AdaptiveGaussianDEwithArchive is less sophisticated than in EnhancedArchiveGuidedDE.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA employs a simpler EA structure, focusing on adaptive Gaussian mutation and selection, leading to cleaner code and potentially better performance in the given benchmark. AdaptiveGaussianSamplingDE uses DE which is less efficient in this case.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE but AdaptiveGaussianMutationDE's adaptive Gaussian mutation provides a more robust escape from local optima, while AdaptiveGaussianDEwithArchive's archive, while aiming for multimodal optimization, doesn't significantly improve performance in this specific benchmark. AdaptiveGaussianDEwithArchive adds complexity without commensurate performance gains.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive incorporates an archive and adaptive Gaussian perturbation within its DE framework, demonstrating a more refined approach compared to EnhancedArchiveGuidedDE which only uses a simple archive. However both perform comparatively less well than the first two approaches. Overall: Adaptive Gaussian sampling initialization and adaptive mutation strategies consistently outperform simple uniform random initialization and static mutation scales in this high-dimensional search space, indicating the importance of adaptive mechanisms for efficient exploration and exploitation.  The addition of Differential Evolution does not always improve performance, especially without careful integration of adaptive sampling techniques.
- Adaptive mechanisms for controlling exploration and exploitation are crucial in high-dimensional spaces.  Combining adaptive sampling with efficient search operators like DE requires careful design to avoid redundancy and ensure synergistic interaction.  A simple and well-tuned adaptive algorithm can outperform complex algorithms with multiple components.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:20:37 INFO Generation 12, best so far: 0.5412867233158046
2025-06-20 19:20:37 INFO --- Performing Long-Term Reflection at Generation 12 ---
2025-06-20 19:20:41 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that adaptive Gaussian sampling for initialization and a decaying sigma value significantly improves performance compared to simple uniform random initialization.  `AdaptiveGaussianSamplingEA` also incorporates adaptive mutation and tournament selection, leading to a more robust search.  `EnhancedArchiveGuidedDE` uses a large population size (10*dim) and a relatively simple DE strategy with an archive that may not be effectively utilized.

(second best) `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`:  `AdaptiveGaussianSamplingDE` uses adaptive Gaussian perturbation within its DE framework, improving exploration.  `AdaptiveGaussianDEwithArchive` adds an archive, but its implementation doesn't clearly outperform the simpler `AdaptiveGaussianSamplingDE`, suggesting that the added complexity doesn't provide sufficient benefit. The archive may be too small or not effectively used to guide the search.

Comparing (1st) vs (2nd), we see that a dedicated EA framework (with selection and recombination steps like in `AdaptiveGaussianSamplingEA`) outperforms a DE variant (`AdaptiveGaussianSamplingDE`), even with similar Gaussian sampling initialization. The structured approach helps maintain diversity and avoid premature convergence.

(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`: Both utilize Differential Evolution and adaptive Gaussian mutation.  The addition of an archive in `AdaptiveGaussianDEwithArchive` provides only a marginal improvement over `AdaptiveGaussianMutationDE`, indicating that the archive implementation's effectiveness is questionable.

Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we observe that while both employ archives, `AdaptiveGaussianDEwithArchive` uses an adaptive Gaussian mutation strategy that improves performance, while `EnhancedArchiveGuidedDE` uses a basic DE strategy and a potentially less effective archive update mechanism.

Overall: Adaptive Gaussian sampling for initialization is crucial for high-dimensional problems.  Adaptive mutation strategies are beneficial for escaping local optima. Well-structured EA frameworks (with explicit selection and recombination) tend to outperform simpler DE variants for this benchmark. Archive mechanisms require careful design to provide a clear performance boost; poorly implemented archives may offer no benefit or even hinder performance.


**Experience:**

The key to effective initialization in high-dimensional spaces is a balanced approach that combines exploration (covering a broad area of the search space) and exploitation (focusing on promising regions). Adaptive Gaussian sampling effectively achieves this balance. Archive mechanisms require careful consideration of their size and update strategy to improve the algorithm's effectiveness, otherwise they are just an overhead.

2025-06-20 19:20:41 INFO Generating offspring via Crossover...
2025-06-20 19:20:51 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:21:07 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:21:07 INFO FeHistory: [ 635162.23786153 1057617.28317912  979815.23157465 ...    4440.00365074
    5862.8067676     5442.30550837]
2025-06-20 19:21:07 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:21:07 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:21:24 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:21:24 INFO FeHistory: [140815.51768131 238983.24411385 228731.95267312 ...   2754.90546454
   1922.46764609   1885.3634961 ]
2025-06-20 19:21:24 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:21:24 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:21:40 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:21:40 INFO FeHistory: [248713.39588641  83925.92861694 160542.32761401 ...   4239.43613465
   7762.54547695   6537.60180978]
2025-06-20 19:21:40 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:21:40 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:21:51 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:22:22 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0003
2025-06-20 19:22:22 INFO FeHistory: [182.56192278 191.45568078 193.38786532 ...  49.68764669  34.59874412
  39.74172434]
2025-06-20 19:22:22 INFO Expected Optimum FE: -100
2025-06-20 19:22:22 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:22:22 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:22:22 INFO Multimodal (multiple components) AOCC mean: 0.0003
2025-06-20 19:22:22 INFO AOCC mean: 0.0001
2025-06-20 19:22:30 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:22:43 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:22:43 INFO FeHistory: [1859307.22487079 1159037.33618694 1934364.31260032 ...  598709.09293375
  598709.09309094  598709.09161359]
2025-06-20 19:22:43 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:22:43 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:22:56 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:22:56 INFO FeHistory: [249572.72508376 235328.96591929 203385.00181532 ...  61821.26668122
  61821.26668122  61821.26668122]
2025-06-20 19:22:56 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:22:56 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:23:09 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:23:09 INFO FeHistory: [377432.12585183 372928.71944926 383340.80184714 ...  58298.89105241
  58298.89105241  58298.89105241]
2025-06-20 19:23:09 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:23:09 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:23:30 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:23:46 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:23:46 INFO FeHistory: [228.89968324 216.97547171 208.09596092 ... 123.26695225 123.26695226
 123.26695224]
2025-06-20 19:23:46 INFO Expected Optimum FE: -100
2025-06-20 19:23:46 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:23:46 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:23:46 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:23:46 INFO AOCC mean: 0.0000
2025-06-20 19:23:46 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that adaptive Gaussian sampling for initialization and a decaying sigma value significantly improves performance compared to simple uniform random initialization.  `AdaptiveGaussianSamplingEA` also incorporates adaptive mutation and tournament selection, leading to a more robust search.  `EnhancedArchiveGuidedDE` uses a large population size (10*dim) and a relatively simple DE strategy with an archive that may not be effectively utilized.

(second best) `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`:  `AdaptiveGaussianSamplingDE` uses adaptive Gaussian perturbation within its DE framework, improving exploration.  `AdaptiveGaussianDEwithArchive` adds an archive, but its implementation doesn't clearly outperform the simpler `AdaptiveGaussianSamplingDE`, suggesting that the added complexity doesn't provide sufficient benefit. The archive may be too small or not effectively used to guide the search.

Comparing (1st) vs (2nd), we see that a dedicated EA framework (with selection and recombination steps like in `AdaptiveGaussianSamplingEA`) outperforms a DE variant (`AdaptiveGaussianSamplingDE`), even with similar Gaussian sampling initialization. The structured approach helps maintain diversity and avoid premature convergence.

(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`: Both utilize Differential Evolution and adaptive Gaussian mutation.  The addition of an archive in `AdaptiveGaussianDEwithArchive` provides only a marginal improvement over `AdaptiveGaussianMutationDE`, indicating that the archive implementation's effectiveness is questionable.

Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we observe that while both employ archives, `AdaptiveGaussianDEwithArchive` uses an adaptive Gaussian mutation strategy that improves performance, while `EnhancedArchiveGuidedDE` uses a basic DE strategy and a potentially less effective archive update mechanism.

Overall: Adaptive Gaussian sampling for initialization is crucial for high-dimensional problems.  Adaptive mutation strategies are beneficial for escaping local optima. Well-structured EA frameworks (with explicit selection and recombination) tend to outperform simpler DE variants for this benchmark. Archive mechanisms require careful design to provide a clear performance boost; poorly implemented archives may offer no benefit or even hinder performance.
- The key to effective initialization in high-dimensional spaces is a balanced approach that combines exploration (covering a broad area of the search space) and exploitation (focusing on promising regions). Adaptive Gaussian sampling effectively achieves this balance. Archive mechanisms require careful consideration of their size and update strategy to improve the algorithm's effectiveness, otherwise they are just an overhead.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:23:46 INFO Generation 13, best so far: 0.5412867233158046
2025-06-20 19:23:46 INFO --- Performing Long-Term Reflection at Generation 13 ---
2025-06-20 19:23:51 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best uses adaptive Gaussian sampling for both initialization and mutation, leading to a more effective exploration of the search space, while the worst relies on purely uniform random initialization and a less sophisticated mutation strategy.  `AdaptiveGaussianSamplingEA` also incorporates a decaying sigma value, which helps fine-tune the search as the algorithm progresses.  `EnhancedArchiveGuidedDE`'s archive is less effectively managed and doesn't demonstrably improve performance.

`(second best)` `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`:  `AdaptiveGaussianSamplingDE` leverages Differential Evolution (DE), a robust optimization technique known for its efficiency in high-dimensional spaces.  `AdaptiveGaussianDEwithArchive` adds an archive, intending to improve exploration of the solution space and potentially handle multimodality, but this improvement is not reflected in the AOCC scores.  The additional complexity introduced by the archive does not outweigh the benefit of simple DE.

Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`, we see that the top performer uses a more refined selection mechanism (tournament selection) and a more structured recombination strategy compared to the second best's simpler approach.  While both leverage adaptive Gaussian sampling for initialization, EA's careful adaptation and selection provide a higher likelihood of converging to better solutions.

(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`: Both utilize DE and adaptive Gaussian mutation, but `AdaptiveGaussianMutationDE` avoids the overhead and potential inefficiency of the archive.  The adaptive mutation, in combination with DE, proves more effective than adding an archive that doesn't significantly contribute to improved performance.

Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that even though both employ archives, `AdaptiveGaussianDEwithArchive`’s adaptive Gaussian perturbation during DE mutation provides a better mechanism to explore diverse regions of the search space compared to `EnhancedArchiveGuidedDE`'s less refined mutation and archive update.

Overall: The consistent use of adaptive Gaussian sampling and a well-structured EA or DE framework, along with a carefully tuned mutation strategy, contributes significantly to better performance. Archives, while conceptually appealing, may add unnecessary complexity if not effectively integrated.


**Experience:**

Effective population initialization requires strategies beyond simple random sampling. Adaptive methods, combined with robust search algorithms (like DE or EA) and careful tuning of parameters, are crucial for efficiently exploring high-dimensional search spaces.  Adding complexity like archives should be carefully justified based on demonstrable performance improvements.

2025-06-20 19:23:51 INFO Generating offspring via Crossover...
2025-06-20 19:23:59 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:24:14 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:24:14 INFO FeHistory: [ 935392.03457157  871065.82379505 1025745.03116289 ...    9371.61388338
    5829.55855397   10184.19082514]
2025-06-20 19:24:14 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:24:14 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:24:29 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:24:29 INFO FeHistory: [213111.47423683 322675.0996214  259030.74244968 ...   6181.27465762
   1702.83755689   4347.30996375]
2025-06-20 19:24:29 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:24:29 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:24:44 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:24:44 INFO FeHistory: [151864.197314   232855.54253398 151035.71602123 ...   8962.04466293
  13441.78582618  13739.76016878]
2025-06-20 19:24:44 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:24:44 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:24:59 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:25:24 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:25:24 INFO FeHistory: [199.57262496 212.58814527 212.59132706 ...  40.59723483  47.86551487
  51.30444599]
2025-06-20 19:25:24 INFO Expected Optimum FE: -100
2025-06-20 19:25:24 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:25:24 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:25:24 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:25:24 INFO AOCC mean: 0.0000
2025-06-20 19:25:31 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:25:31 ERROR Can not run the algorithm
2025-06-20 19:25:31 INFO Run function 4 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:25:31 INFO FeHistory: [1440868.87976783 1074142.04752935 1604756.62506013  924488.44788066
 1362213.01364824 1346122.65414241 1092744.51859177 1359413.72112286
  958112.16908269  634824.80849392 1015145.78821624 1501389.23604674
  926782.95490516 1183843.12217796 1519876.07683382 1233733.91720823
 1630286.73585454 1450255.94921975 1173892.44460899 1464966.18252477
 1351768.26234477 1564389.4267862   886482.4180171  1346454.64744114
 1178873.28979093 1529374.89707099 1544707.0149763  1342535.57349981
  954083.03554492  838551.13778237 1223160.68826579 1053564.22821609
 1070328.05381193 1172472.81893855 1107284.07135648 1223245.80598164
  978243.10594876 1045786.26145894 1412276.68978705 1363002.59800105
 1115528.82037481 1271642.01781789 1406393.4273131  1133567.22241732
 1626747.24618205 1158188.27366897 1460483.87449944 1504318.34064248
 1173005.01087581 1675225.58939459 1195228.40194573  979513.84618058
 1393356.85955116  993023.02448123 1305650.97251622 1656650.27580621
 1317653.26388955 1230712.32301512 1116568.54391354  572803.88446905
 1216360.79544695 1461018.02265102 1054644.60650095 1116143.52632159
  933784.0458304  1053829.36066659 1009228.40223979 1246941.21575064
 1612482.92106408 1371442.20686871 1417061.842227   1163929.88429262
 1462326.0964032  1492477.24205481 1491890.06620301  977207.73735948
 1346444.47896481 1769033.11371558  891059.0833013  1606738.96189749
 1123843.39059488 1616522.72895195 1435405.9533179  1197231.50955319
 1308283.3253169  1139586.61072177 1104667.14966649 1575749.82245279
 1201603.66913773 1003154.00164188 1349388.53233213 1280714.13306142
  918546.45780087 1104322.72281836 1353872.63585134 1003181.29068045
 1302936.21845683 1608437.50757289  947853.06769215 1218415.62557961
  944599.46999051]
2025-06-20 19:25:31 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:25:31 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:25:31 ERROR Can not run the algorithm
2025-06-20 19:25:32 INFO Run function 7 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:25:32 INFO FeHistory: [211360.9875523  333829.4471451  318774.44170689 183041.96679143
 328805.17618786 213481.97968285 230197.58585939 237826.68756982
 155386.60693857 216741.82466464 115131.9060795  238096.95246066
 130562.28397174 344461.89830748 194879.23762998 280368.55920896
 312094.25492361 175683.7858522  310000.41049032 230152.54808887
 179934.2068756  325243.59357895 227006.94163278 197925.17404856
 299253.39800344 255831.48996458 240311.3730484  230148.05997419
 275944.57607276 311154.82816647 133433.6579275  172061.22920869
 263511.35257201 402222.61521631 172486.04334991 236154.69368612
 244127.95163168 262686.92745791 300367.49069178 352237.06037284
 265275.49224711 319614.85126783 175605.69587965 282140.10916288
 251634.49147702 286630.92635251 302647.37716763 235262.30913957
 327180.76832477 404455.92853554 318401.63421274 254455.71847542
 333203.07692591 333689.08201834 251536.2660289  349595.19767151
 287994.0075975  316004.28070214 255451.39187748 338296.0171541
 315070.3845578  249993.99227778 218131.29618418 286695.20810158
 243355.71906481 213917.30319765 215503.97710072 380193.0259799
 308835.39565449 189446.73266065 275918.04351071 244115.12043991
 260062.04078675 196246.69319887 198953.49969056 193963.12681385
 325091.79602335 346680.68407169 296942.1944185  155958.43973834
 296321.96536956 308288.1220508  207178.81927831 336083.11991788
 302149.98677352 227347.63670657 141207.50595274 207505.44870177
 313442.10941771 252269.96842768 186474.43038544 249635.61510404
 180275.57438037 280204.36225989 241451.07595179 280889.16408356
 148785.80990144 319334.94701504 209488.11737842 257804.00361177
 156688.03406925]
2025-06-20 19:25:32 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:25:32 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:25:32 ERROR Can not run the algorithm
2025-06-20 19:25:32 INFO Run function 8 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-20 19:25:32 INFO FeHistory: [250281.0214355  220407.59781174 411422.38688985 280013.24964816
 270992.72928192 306001.26818437 244840.36277036 407505.98932207
 236053.31156724 393599.19554382 318913.91415176 268139.62819206
 253305.03445918 302239.20215257 299709.10044018 363335.34850265
 241499.95120711 427922.05209352 344795.78574202 407687.96893784
 338180.19016091 455767.46396293 225729.53971326 423151.20236013
 293936.57769786 396196.2023663  389801.39029585 262385.5504442
 269753.10131245 386498.31626493 223347.5498495  329686.61102319
 208027.42104352 346420.77755777 308393.91252791 374181.82394048
 281869.9188228  406392.65274402 370146.99319489 481482.19586064
 369505.34226729 267702.16526248 265378.31378245 334353.71026346
 336188.09741395 326033.04221913 244861.22848318 442062.09322261
 412281.61726655 279435.01835186 373135.95100191 324207.21149058
 310079.0865908  370979.54458407 421762.25991127 302755.36023015
 245386.67873416 432949.88757512 486834.39316751 497401.6768149
 325638.4659925  354998.0168946  345815.09748148 265831.88967798
 342078.17482866 417868.06912988 274461.94992536 201768.69526877
 424353.03049834 329146.89995221 332051.93323425 350119.33576069
 315283.875258   343700.67718888 258888.28048623 202692.14179342
 261309.2563207  308182.23242619 291323.02000001 286380.18904055
 282385.35114863 250716.44367264 189920.69511183 301777.31599945
 330567.58545078 325005.69464659 292462.42309381 298821.42503441
 296562.8693882  433767.70864677 269265.94035995 400125.6047255
 680308.73480763 328329.27624578 514069.58910477 260051.00846153
 525002.49554551 270069.38432523 255849.61686835 244324.06853917
 182481.15647099 213893.39399127 234200.9801043  215548.98718417
 201962.65837603 214017.2658177  170802.69140797 229188.75786218
 248580.24804895 187874.22741885 222648.25097189 188551.46247657
 191067.12067982 191690.03181469 264404.01579664 262467.22660897
 194826.13453419 228690.41614542 162363.29808372 316454.32373483
 176789.06730165 184397.75841886 185895.3226729  180265.68710308
 240798.56573334 259248.63487707 254835.2627221  225265.37011275
 232213.4088711  251325.70776452 169696.13370885 175967.55229197
 177096.98695395 243468.65553352 205350.56931126 249572.71790073
 282825.12880736 169395.68249433 175905.24825585 254423.3436045
 221367.69438985 259562.69128757 186370.62183126 175808.09076184
 269330.36118659 265292.97100268 197725.29627998 218285.90263771
 213678.69963276 218211.66947112 244531.95092154 190130.10307412
 249051.78928381 229717.2587296  224006.87842721 184758.39372396
 202612.06602487 226594.16167535 234075.56419289 272951.14582191
 298648.82286177 209927.88685178 218277.13029549 238466.92586894
 264880.93284183 270656.71710846 242671.60907667 202747.04458522
 242811.52081377 176731.67936746 248679.99817561 188803.12461904
 309703.72542923 204079.7936828  215116.22437962 249549.63719243
 208611.44387194 315205.7673941  182954.04036608 184387.06771111
 389471.54616179 243954.36377123 290611.55293624 218579.64541842
 175656.19344976 202874.40933832 245429.37867444 184476.27800257
 199908.14004423 223134.04508945 185292.64696503 371073.44602527
 250350.43893892 292510.93089965 201098.25957967 267166.32040013
 204263.96073267 240710.38469813 179706.2864407  271880.49783611
 183925.181472  ]
2025-06-20 19:25:32 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:25:32 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:25:32 ERROR Can not run the algorithm
2025-06-20 19:25:33 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:25:33 INFO FeHistory: [219.52474296 213.71801925 206.52215875 229.89501145 225.60152511
 160.73433779 212.12182146 189.41841797 197.24904455 230.52006928
 223.43344365 217.79002234 197.19923699 228.77149167 171.48544915
 213.03060889 179.83250353 205.82958542 232.56923813 249.65785269
 200.29011024 214.26501014 175.45196966 255.29933448 191.94903198
 205.33005424 209.81416679 208.89752775 191.58287607 215.6120893
 227.3800179  225.68966735 176.35900263 220.68136825 225.1822622
 221.52369661 194.22564752 222.17001934 186.65170296 220.2758726
 193.67575622 227.76482307 200.12804838 184.63912922 228.12349864
 196.73656119 219.59990054 213.34767445 230.80077028 200.47038505
 231.88593295 211.07569025 203.32661517 202.48172614 217.02798964
 227.32096839 216.7930581  240.12706029 155.22384708 215.48053123
 185.31377532 185.68224266 208.40724146 172.43752733 184.16247382
 195.46038816 209.48329861 176.89821121 224.50915521 186.69161142
 187.88908555 240.94971241 209.78184937 211.73856281 203.20357078
 189.68044443 151.61779238 212.06366505 197.54139636 198.54691147
 229.10622177 217.83835753 212.97100617 216.1922584  206.8040938
 219.74054875 231.34215571 227.26666353 194.39952979 263.13685229
 195.25844781 229.72309676 207.6100403  206.13066604 167.59166991
 211.29273137 207.06930014 254.42760484 180.62301845 171.0238911
 171.08205717]
2025-06-20 19:25:33 INFO Expected Optimum FE: -100
2025-06-20 19:25:33 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:25:33 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:25:33 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:25:33 INFO AOCC mean: 0.0000
2025-06-20 19:25:33 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best uses adaptive Gaussian sampling for both initialization and mutation, leading to a more effective exploration of the search space, while the worst relies on purely uniform random initialization and a less sophisticated mutation strategy.  `AdaptiveGaussianSamplingEA` also incorporates a decaying sigma value, which helps fine-tune the search as the algorithm progresses.  `EnhancedArchiveGuidedDE`'s archive is less effectively managed and doesn't demonstrably improve performance.

`(second best)` `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`:  `AdaptiveGaussianSamplingDE` leverages Differential Evolution (DE), a robust optimization technique known for its efficiency in high-dimensional spaces.  `AdaptiveGaussianDEwithArchive` adds an archive, intending to improve exploration of the solution space and potentially handle multimodality, but this improvement is not reflected in the AOCC scores.  The additional complexity introduced by the archive does not outweigh the benefit of simple DE.

Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`, we see that the top performer uses a more refined selection mechanism (tournament selection) and a more structured recombination strategy compared to the second best's simpler approach.  While both leverage adaptive Gaussian sampling for initialization, EA's careful adaptation and selection provide a higher likelihood of converging to better solutions.

(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`: Both utilize DE and adaptive Gaussian mutation, but `AdaptiveGaussianMutationDE` avoids the overhead and potential inefficiency of the archive.  The adaptive mutation, in combination with DE, proves more effective than adding an archive that doesn't significantly contribute to improved performance.

Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that even though both employ archives, `AdaptiveGaussianDEwithArchive`’s adaptive Gaussian perturbation during DE mutation provides a better mechanism to explore diverse regions of the search space compared to `EnhancedArchiveGuidedDE`'s less refined mutation and archive update.

Overall: The consistent use of adaptive Gaussian sampling and a well-structured EA or DE framework, along with a carefully tuned mutation strategy, contributes significantly to better performance. Archives, while conceptually appealing, may add unnecessary complexity if not effectively integrated.
- Effective population initialization requires strategies beyond simple random sampling. Adaptive methods, combined with robust search algorithms (like DE or EA) and careful tuning of parameters, are crucial for efficiently exploring high-dimensional search spaces.  Adding complexity like archives should be carefully justified based on demonstrable performance improvements.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:25:33 INFO Generation 14, best so far: 0.5412867233158046
2025-06-20 19:25:33 INFO --- Performing Long-Term Reflection at Generation 14 ---
2025-06-20 19:25:37 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization and mutation, leading to a more efficient exploration of the search space, while EnhancedArchiveGuidedDE relies on simple uniform random initialization and lacks adaptive mechanisms for parameter tuning.  AdaptiveGaussianSamplingEA also incorporates a more sophisticated selection mechanism (tournament selection) compared to EnhancedArchiveGuidedDE's simpler selection based on sorting fitness values.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE integrates adaptive Gaussian sampling and Differential Evolution, leveraging the strengths of both. In contrast, AdaptiveGaussianDEwithArchive adds an archive, which while potentially beneficial for multimodal functions, adds overhead and doesn't significantly improve the AOCC score in this particular benchmark. AdaptiveGaussianSamplingDE also exhibits a more efficient mutation strategy.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA uses a simpler EA framework with Gaussian recombination, resulting in slightly better performance, while AdaptiveGaussianSamplingDE incorporates DE, which may be more effective in higher dimensions or more complex landscapes, although it shows slightly lower performance in this specific benchmark.


(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE, but AdaptiveGaussianMutationDE uses a simpler design without an archive, which contributes to its slightly better AOCC score and likely reduced computational overhead. The adaptive Gaussian mutation in AdaptiveGaussianMutationDE seems to be more effective than just using the archive in AdaptiveGaussianDEwithArchive


Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive utilizes Differential Evolution and an adaptive Gaussian mutation, offering a more structured search strategy compared to EnhancedArchiveGuidedDE's simpler DE approach and basic uniform random initialization. The archive in AdaptiveGaussianDEwithArchive, while not exceptionally helpful in this instance, suggests a potential direction for improvement in handling multimodality.

Overall: The best performing heuristics prioritize adaptive mechanisms in both initialization and mutation, combining them with either a simple EA or DE structure. Adding an archive does not consistently improve performance in this case, suggesting that more focused exploration strategies are preferable.


**Experience:**

Adaptive initialization and mutation are crucial for high-dimensional optimization.  Simple DE variants may be outperformed by simpler EAs when coupled with effective adaptive sampling. Archive-based methods may prove more beneficial in multimodal landscapes, but require careful design to avoid performance overhead.

2025-06-20 19:25:37 INFO Generating offspring via Crossover...
2025-06-20 19:25:47 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:26:01 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:26:01 INFO FeHistory: [864881.92754028 883909.23845347 588089.72187456 ...   9505.45485152
   7744.3902227   10918.52293816]
2025-06-20 19:26:01 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:26:01 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:26:15 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:26:15 INFO FeHistory: [284667.15675676 213301.20685639 332914.40294476 ...   4802.14878894
   4690.84785629   8676.5142687 ]
2025-06-20 19:26:15 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:26:15 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:26:29 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:26:29 INFO FeHistory: [200557.79646574 349254.03420689 299271.19849887 ...  13721.52981449
   5441.74000748   9031.40252838]
2025-06-20 19:26:29 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:26:29 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:26:47 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:27:09 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:27:09 INFO FeHistory: [159.21348854 158.85405187 183.33283804 ...  32.15303252  66.16971186
  45.47602268]
2025-06-20 19:27:09 INFO Expected Optimum FE: -100
2025-06-20 19:27:09 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:27:09 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:27:09 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:27:09 INFO AOCC mean: 0.0000
2025-06-20 19:27:17 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:27:29 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:27:29 INFO FeHistory: [1789345.89467662 1286391.50713764 1791945.45241033 ...  420272.01096263
  420272.00936266  420272.0086431 ]
2025-06-20 19:27:29 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:27:29 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:27:40 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:27:40 INFO FeHistory: [230383.02680018 395175.23199791 269543.86047256 ...  86244.08777291
  86244.08801531  86244.08777291]
2025-06-20 19:27:40 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:27:40 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:27:52 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:27:52 INFO FeHistory: [255883.58887639 179388.83668631 203643.65752735 ...  58115.28098504
  58115.28060092  58115.28085947]
2025-06-20 19:27:52 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:27:52 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:28:17 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:28:28 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:28:28 INFO FeHistory: [202.63995352 218.37405841 211.94815631 ... 116.04952674 116.04952674
 116.04952674]
2025-06-20 19:28:28 INFO Expected Optimum FE: -100
2025-06-20 19:28:28 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:28:28 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:28:28 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:28:28 INFO AOCC mean: 0.0000
2025-06-20 19:28:28 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization and mutation, leading to a more efficient exploration of the search space, while EnhancedArchiveGuidedDE relies on simple uniform random initialization and lacks adaptive mechanisms for parameter tuning.  AdaptiveGaussianSamplingEA also incorporates a more sophisticated selection mechanism (tournament selection) compared to EnhancedArchiveGuidedDE's simpler selection based on sorting fitness values.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE integrates adaptive Gaussian sampling and Differential Evolution, leveraging the strengths of both. In contrast, AdaptiveGaussianDEwithArchive adds an archive, which while potentially beneficial for multimodal functions, adds overhead and doesn't significantly improve the AOCC score in this particular benchmark. AdaptiveGaussianSamplingDE also exhibits a more efficient mutation strategy.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA uses a simpler EA framework with Gaussian recombination, resulting in slightly better performance, while AdaptiveGaussianSamplingDE incorporates DE, which may be more effective in higher dimensions or more complex landscapes, although it shows slightly lower performance in this specific benchmark.


(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE, but AdaptiveGaussianMutationDE uses a simpler design without an archive, which contributes to its slightly better AOCC score and likely reduced computational overhead. The adaptive Gaussian mutation in AdaptiveGaussianMutationDE seems to be more effective than just using the archive in AdaptiveGaussianDEwithArchive


Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive utilizes Differential Evolution and an adaptive Gaussian mutation, offering a more structured search strategy compared to EnhancedArchiveGuidedDE's simpler DE approach and basic uniform random initialization. The archive in AdaptiveGaussianDEwithArchive, while not exceptionally helpful in this instance, suggests a potential direction for improvement in handling multimodality.

Overall: The best performing heuristics prioritize adaptive mechanisms in both initialization and mutation, combining them with either a simple EA or DE structure. Adding an archive does not consistently improve performance in this case, suggesting that more focused exploration strategies are preferable.
- Adaptive initialization and mutation are crucial for high-dimensional optimization.  Simple DE variants may be outperformed by simpler EAs when coupled with effective adaptive sampling. Archive-based methods may prove more beneficial in multimodal landscapes, but require careful design to avoid performance overhead.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:28:28 INFO Generation 15, best so far: 0.5412867233158046
2025-06-20 19:28:28 INFO --- Performing Long-Term Reflection at Generation 15 ---
2025-06-20 19:28:33 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best uses adaptive Gaussian sampling for initialization and mutation, leading to more efficient exploration of the search space, while the worst uses simple uniform random initialization and a less sophisticated mutation strategy.  `AdaptiveGaussianSamplingEA` also incorporates adaptive sigma decay, dynamically adjusting the search intensity. `EnhancedArchiveGuidedDE`'s archive, while intended for multimodal optimization, doesn't significantly improve performance in this single-objective scenario and adds computational overhead.

`(second best)` `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`:  `AdaptiveGaussianSamplingDE` leverages the efficient initialization of the best algorithm and integrates it with the robust Differential Evolution (DE) framework, which efficiently handles high-dimensional spaces. `AdaptiveGaussianDEwithArchive` attempts to improve upon DE by adding an archive, but this addition doesn't yield a performance boost and adds complexity. The archive’s diversity maintenance strategy is less effective than the adaptive Gaussian approach.

Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`: Both utilize adaptive Gaussian sampling for initialization, but `AdaptiveGaussianSamplingEA` employs a more focused approach with Gaussian recombination and adaptive mutation directly within the EA framework.  `AdaptiveGaussianSamplingDE` combines Gaussian perturbation with the DE strategy, introducing extra complexity that may not always be beneficial.  While DE is robust, the adaptive Gaussian method's direct control over mutation proves superior.


(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`:  Both algorithms use DE but differ in their initialization and mutation strategies.  `AdaptiveGaussianMutationDE`'s uniform initialization is less effective than the Gaussian initialization of the top performers. `AdaptiveGaussianDEwithArchive` adds an archive to improve exploration, yet lacks fine-grained control over the mutation process, and the archive's impact is marginal in this case.


Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`: Both use archives, but `AdaptiveGaussianDEwithArchive` benefits from DE’s efficient exploration compared to `EnhancedArchiveGuidedDE`’s less refined DE variant and uniform initialization.  The archive in `AdaptiveGaussianDEwithArchive` is not sophisticated enough for significant improvement, but it performs better than `EnhancedArchiveGuidedDE`'s archive maintenance approach.


Overall: Adaptive Gaussian sampling for initialization and mutation, combined with carefully tuned adaptive parameters, significantly outperform simple uniform initialization and less sophisticated mutation methods. The integration of DE, while sometimes beneficial, isn't universally superior to a well-designed EA utilizing the adaptive Gaussian strategy. Archives can be beneficial, but their impact depends critically on implementation and the problem landscape.


**Experience:**

Focusing on adaptive Gaussian sampling for initialization and controlled, adaptive mutation proves crucial for high-dimensional optimization.  Simple uniform random initialization is insufficient; sophisticated archive mechanisms require careful design for demonstrable benefit.

2025-06-20 19:28:33 INFO Generating offspring via Crossover...
2025-06-20 19:28:41 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:28:41 ERROR Can not run the algorithm
2025-06-20 19:28:42 INFO Run function 4 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:28:42 INFO FeHistory: [ 741584.33319926  688302.10134012  836901.581635    918193.77672443
  899463.66492635 1064040.13946467 1039405.85367192 1199843.00563082
  977447.8714218   965946.68948782 1332945.16327653 1203315.54294745
  991682.30642156  960508.32984137 1042261.59515073  927603.24982945
 1211944.90888553 1168428.5543523  1291923.0023839  1300511.13471727
 1058425.48266232 1013806.49841732 1034663.2327575   932111.29481039
  844515.28634408  806202.73434996  908834.15923852  872995.15687528
  883103.38974226 1199481.22596759 1249598.24828056  916404.95809872
 1025363.50983295  869560.1439558  1177734.4651593   934564.58930778
 1067436.63786843  826354.7753479  1103828.70624022 1224290.00905149
 1161039.95551026  965459.61634038 1194199.39188071 1143868.92218144
  930600.20018658 1121600.04237301 1002422.37050495  957883.61242125
 1285291.08551331 1020356.31344443 1132166.33616063  989261.0317389
 1137243.06169169  843265.42475721  983537.77474934 1077334.89006728
 1020495.63068328  985652.33230373  997205.88349623  804242.92128029
 1166842.83706967 1523663.05480709 1003363.62197088 1015448.37558167
  857687.37656148 1247930.74465253 1078288.98258722 1101070.2808069
  989754.12265151 1081423.6647556  1226718.14822493  964067.31454173
 1070700.99370083 1039216.94591375 1014342.18448312 1096166.20460209
 1217660.39058045 1048337.68206339 1502671.97227015 1155438.58944253
 1015684.63614394 1018350.68647897 1184874.15861969  913530.47233323
  970642.10284042 1142991.48053039 1137743.60004255 1188915.87230652
 1013724.45869699 1146531.01666971 1282873.5325552  1420101.21443126
  760763.53884246 1078125.22267628 1296730.70106833  824980.29466037
 1116302.70939845 1176642.61453498 1437354.37613239 1158531.47773763
  798250.44894114]
2025-06-20 19:28:42 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:28:42 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:28:42 ERROR Can not run the algorithm
2025-06-20 19:28:42 INFO Run function 7 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:28:42 INFO FeHistory: [231698.37389337 279737.04901376 244635.55462805 234796.31994181
 290059.47157913 260346.76788043 231708.42676129 215762.60756083
 260787.56974381 219460.61584145 219709.04217985 201033.34425971
 240197.70270517 267949.80687055 191018.82895758 315225.09555997
 165883.89027747 236388.43708632 222106.2377215  302948.76942879
 204535.23657486 244438.98369584 282276.55909883 248274.56327584
 285234.65003044 203130.61539819 234577.18323678 232000.78134366
 363889.02178198 188086.30248843 177615.22808755 140556.87014513
 144735.86557911 279490.13477377 242203.20866461 182991.85750664
 191491.51713705 253011.79715711 247479.99663295 220025.40537683
 169009.63267919 228783.26054885 227877.40700281 279008.78267979
 258468.16768342 328837.95025692 189672.05974809 133531.09416412
 154083.23333584 223727.59163962 299302.69499168 197659.04347933
 202895.88199965 258467.19009573 203493.30801839 269589.66931105
 245334.26442847 262767.34770372 191593.29771574 207710.41395629
 294051.12719096 176749.19248241 268276.59467039 206417.61806127
 248504.96884438 210387.04323674 173239.35658095 232774.10911289
 186204.30002361 230951.12783538 281746.47801639 255460.76761348
 198804.98955308 148142.37918366 256829.62258107 207674.02518703
 233077.51991163 230779.5101168  250098.77937867 249816.8089666
 297409.69401996 282262.34282594 137241.74930768 245134.61575777
 190105.84670571 137833.37297198 228685.70767181 246422.7342101
 283965.26874079 221275.51864267 217109.40866341 255427.3424995
 196914.52233133 212964.50472935 294502.53818093 132122.09349876
 224823.57071772 188361.83592266 194044.62674084 218198.74144148
 278435.55578343]
2025-06-20 19:28:42 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:28:42 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:28:42 ERROR Can not run the algorithm
2025-06-20 19:28:42 INFO Run function 8 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:28:42 INFO FeHistory: [187164.16050768 206609.59343198 180620.37803372 201426.86536179
 249262.94045365 264093.82631555 206527.8327033  238444.51465939
 270503.35597843 255159.56560157 263654.59962427 211857.01748589
 230991.13263236 287813.30260214 266001.05517568 213196.91050369
 232130.01544652 225474.88060067 246591.75167664 291655.52487334
 204557.78260253 204158.37092459 229709.1007937  208206.71617681
 183481.00942707 278228.55860251 212118.14562228 246132.70998786
 240161.3584239  250910.66294408 167123.68850335 166562.86700303
 285874.82731075 232408.37498768 314729.52607226 167677.00247097
 243753.64150726 240382.69620555 161782.03725835 221200.77318354
 224113.20729678 297702.45308026 265465.74836979 211840.243044
 248752.04534361 233474.01451882 205981.62356685 325016.67199753
 160692.04191004 245177.52607293 188612.88795915 299091.49969785
 250750.38365464 234658.02244004 335289.30011402 229482.4710896
 201400.41876126 167558.87044618 218981.25476502 214301.74187839
 276951.41642008 288584.52049894 216753.37792231 138471.80629837
 273538.84588095 228359.84024515 151862.76622778 270379.70151779
 185042.75984894 181782.12836177 213160.69052897 247274.08222576
 220191.27610442 213212.74453479 148369.19792043 272756.10339469
 242628.4797329  187751.74157004 249139.65351603 254915.16850554
 310515.98185096 299065.26813155 228912.42391858 249892.53156414
 213837.11089692 267868.60757545 169682.28843967 330425.60948141
 170189.07813787 247019.84463162 199542.47551365 247535.5838096
 169615.14134353 264870.14370272 294455.87596485 190110.4874774
 257630.43235122 260266.34228654 193122.59357546 207894.19604453
 184776.05854463]
2025-06-20 19:28:42 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:28:42 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:28:43 ERROR Can not run the algorithm
2025-06-20 19:28:43 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:28:43 INFO FeHistory: [203.13311267 182.10991181 181.54519226 211.74707239 203.22823926
 202.34440052 218.78927852 221.56664962 189.5277817  200.32196911
 163.22307051 160.87942453 186.30437761 187.3824316  165.78933699
 188.31305323 206.10224946 202.03686637 173.27607518 175.59160067
 178.37199559 196.08258264 180.37114451 190.07789743 185.28060169
 164.96829307 190.46029732 223.23322522 189.48199076 176.66458202
 182.20375324 162.30744623 178.81493605 194.78551669 197.49062119
 199.74190477 192.95947245 205.30871231 199.73669059 203.24348393
 179.26910812 161.35072072 187.96461274 192.36897481 199.95480583
 196.19255911 172.25526087 190.2763064  169.41937396 177.43672097
 166.33116341 179.07571347 177.17230609 187.02500115 194.45926395
 176.70186335 186.70429202 206.28611079 182.85015807 206.28518122
 201.32484564 200.78060492 149.83752036 177.19651797 200.61438916
 216.09913306 195.31371003 208.84560743 190.17270579 187.17441197
 165.94499196 167.50470789 209.24904142 224.6579917  177.95768009
 199.54705104 172.15956111 158.94072596 195.3799088  209.59513918
 205.2730939  171.94412743 207.4463772  189.32151777 161.47369703
 176.11377846 193.07076401 190.25395893 203.83115529 208.9396573
 173.28322903 169.17194023 176.9657256  175.19129183 186.31213367
 178.62833192 175.16238931 195.60392607 174.72890141 184.68171403
 191.8192317 ]
2025-06-20 19:28:43 INFO Expected Optimum FE: -100
2025-06-20 19:28:43 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:28:43 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:28:43 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:28:43 INFO AOCC mean: 0.0000
2025-06-20 19:28:50 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:29:05 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:29:05 INFO FeHistory: [1664608.71583055  736495.41771348 1186181.11414485 ...  698511.12853867
  681698.91653371  830749.06541219]
2025-06-20 19:29:05 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:29:05 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:29:22 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:29:22 INFO FeHistory: [405236.16202527 309017.54469609 406421.7493601  ... 235447.82221171
 137559.63217394 204912.63450927]
2025-06-20 19:29:22 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:29:22 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:29:38 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:29:38 INFO FeHistory: [228535.1223425  182553.7264836  203156.34380615 ...  93026.2315467
  59064.89775616  84882.83119951]
2025-06-20 19:29:38 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:29:38 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:29:50 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:30:22 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:30:22 INFO FeHistory: [209.91486526 215.20718685 209.52101896 ... 137.71497719 150.16206297
 142.55058851]
2025-06-20 19:30:22 INFO Expected Optimum FE: -100
2025-06-20 19:30:22 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:30:22 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:30:22 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:30:22 INFO AOCC mean: 0.0000
2025-06-20 19:30:22 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best uses adaptive Gaussian sampling for initialization and mutation, leading to more efficient exploration of the search space, while the worst uses simple uniform random initialization and a less sophisticated mutation strategy.  `AdaptiveGaussianSamplingEA` also incorporates adaptive sigma decay, dynamically adjusting the search intensity. `EnhancedArchiveGuidedDE`'s archive, while intended for multimodal optimization, doesn't significantly improve performance in this single-objective scenario and adds computational overhead.

`(second best)` `AdaptiveGaussianSamplingDE` vs (second worst) `AdaptiveGaussianDEwithArchive`:  `AdaptiveGaussianSamplingDE` leverages the efficient initialization of the best algorithm and integrates it with the robust Differential Evolution (DE) framework, which efficiently handles high-dimensional spaces. `AdaptiveGaussianDEwithArchive` attempts to improve upon DE by adding an archive, but this addition doesn't yield a performance boost and adds complexity. The archive’s diversity maintenance strategy is less effective than the adaptive Gaussian approach.

Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`: Both utilize adaptive Gaussian sampling for initialization, but `AdaptiveGaussianSamplingEA` employs a more focused approach with Gaussian recombination and adaptive mutation directly within the EA framework.  `AdaptiveGaussianSamplingDE` combines Gaussian perturbation with the DE strategy, introducing extra complexity that may not always be beneficial.  While DE is robust, the adaptive Gaussian method's direct control over mutation proves superior.


(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`:  Both algorithms use DE but differ in their initialization and mutation strategies.  `AdaptiveGaussianMutationDE`'s uniform initialization is less effective than the Gaussian initialization of the top performers. `AdaptiveGaussianDEwithArchive` adds an archive to improve exploration, yet lacks fine-grained control over the mutation process, and the archive's impact is marginal in this case.


Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`: Both use archives, but `AdaptiveGaussianDEwithArchive` benefits from DE’s efficient exploration compared to `EnhancedArchiveGuidedDE`’s less refined DE variant and uniform initialization.  The archive in `AdaptiveGaussianDEwithArchive` is not sophisticated enough for significant improvement, but it performs better than `EnhancedArchiveGuidedDE`'s archive maintenance approach.


Overall: Adaptive Gaussian sampling for initialization and mutation, combined with carefully tuned adaptive parameters, significantly outperform simple uniform initialization and less sophisticated mutation methods. The integration of DE, while sometimes beneficial, isn't universally superior to a well-designed EA utilizing the adaptive Gaussian strategy. Archives can be beneficial, but their impact depends critically on implementation and the problem landscape.
- Focusing on adaptive Gaussian sampling for initialization and controlled, adaptive mutation proves crucial for high-dimensional optimization.  Simple uniform random initialization is insufficient; sophisticated archive mechanisms require careful design for demonstrable benefit.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:30:22 INFO Generation 16, best so far: 0.5412867233158046
2025-06-20 19:30:22 INFO --- Performing Long-Term Reflection at Generation 16 ---
2025-06-20 19:30:27 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization, which centers the initial population around a randomly selected point, leading to potentially faster convergence. EnhancedArchiveGuidedDE uses a simple uniform random initialization, which can be less effective in high-dimensional spaces. AdaptiveGaussianSamplingEA also incorporates adaptive mutation and sigma decay, which helps to fine-tune the search process.  EnhancedArchiveGuidedDE lacks such sophisticated mechanisms.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE incorporates Differential Evolution, a powerful search strategy, while AdaptiveGaussianDEwithArchive adds an archive, which can help in multimodal optimization but adds complexity without significant improvement based on the AOCC scores. The Adaptive Gaussian perturbation in both is a valuable addition compared to purely random initialization.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA uses a simpler evolutionary strategy (tournament selection, Gaussian recombination, adaptive mutation), leading to potentially easier implementation and analysis. AdaptiveGaussianSamplingDE uses Differential Evolution, which is more complex but can be more efficient in some cases. The AOCC scores show that the simpler approach is slightly superior in this specific benchmark.


(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both employ Differential Evolution and adaptive Gaussian mutation.  The difference lies in the archive used by AdaptiveGaussianDEwithArchive, which, based on the AOCC score, doesn't significantly improve performance and adds computational overhead.


Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive utilizes a more sophisticated initialization (Adaptive Gaussian Sampling) and incorporates Differential Evolution, giving it a significant advantage over EnhancedArchiveGuidedDE's simple uniform random initialization and less sophisticated DE.


Overall: The top-performing algorithms consistently use adaptive Gaussian sampling for initialization, demonstrating its effectiveness in high-dimensional problems.  The addition of sophisticated search operators (e.g., Differential Evolution) can improve performance, but only if implemented carefully and efficiently.  Adding complexity (like an archive) without clear performance gains may be detrimental.


**Experience:**

Adaptive Gaussian sampling for initialization significantly improves performance in high-dimensional search spaces.  Sophistication should be carefully balanced with computational cost; simpler, well-tuned algorithms can outperform more complex ones.  Thorough benchmarking is crucial for evaluating the effectiveness of different initialization and search strategies.

2025-06-20 19:30:27 INFO Generating offspring via Crossover...
2025-06-20 19:30:35 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:30:50 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:30:50 INFO FeHistory: [980821.27766363 746706.87598623 565277.37329493 ...  49476.1697081
  49476.1697081   49476.1697081 ]
2025-06-20 19:30:50 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:30:50 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:31:05 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:31:05 INFO FeHistory: [418645.51801371 142929.78426046 140069.25322264 ...  41206.37102886
  41206.37102886  41206.37102886]
2025-06-20 19:31:05 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:31:05 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:31:20 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:31:20 INFO FeHistory: [146174.34973489 120856.65559859 177168.65241339 ...  51448.27073106
  51448.27073106  51448.27073106]
2025-06-20 19:31:20 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:31:20 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:31:35 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:32:00 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:32:00 INFO FeHistory: [178.2545943  171.93783191 189.9698128  ... 135.0353368  135.0353368
 135.0353368 ]
2025-06-20 19:32:00 INFO Expected Optimum FE: -100
2025-06-20 19:32:00 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:32:00 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:32:00 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:32:00 INFO AOCC mean: 0.0000
2025-06-20 19:32:08 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:32:08 ERROR Can not run the algorithm
2025-06-20 19:32:09 INFO Run function 4 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:32:09 INFO FeHistory: [1318412.80934142 1266798.17696697 1230370.82960931 1342383.58149086
 1859403.9089471  1581166.08165869 1115421.69196638 1396035.81397836
  822000.31086537 1562044.19101056 1921855.22910556 1320545.24074374
 1488820.05992668 1413511.94170393 1051562.68711029 1422641.3254036
 1078364.8167042  1135584.2620062  1207143.31199334 1314964.83871264
 1609908.20134879 1574013.04397773 1406908.8927839   976187.44665927
 1067998.53764321 1879270.96799854 1423757.35886445 1214553.74384988
 1613251.09913063 1472877.39179976  980745.69159622 1249155.82581352
 1434293.45404792 1130195.71395097 1526435.05985288 1649334.68744711
 2099427.44682814 1379726.87033519 1622919.75536659 1491956.41126369
 1550639.94131003 1555468.63777933  888499.23436544  988479.37407422
 1139707.45537482 1811869.73107929 1584105.64871758 1327552.34908723
 1921233.43140872  943026.5231272  1390685.75606495  880359.54476349
 1324107.67225675 1632398.04384031 1327864.65148069 1499317.01193051
 2011825.33215573 1809428.62411545 1047419.33707045 1555805.66389386
  946570.60996581  939853.06898834 1021733.8537921  1533804.57412686
 1612617.18590831 1912486.10241984 1137366.24101908  941073.56938972
 1405459.62576428 1210240.47368331 1044567.19036342 1218999.45327399
 1192014.99263033 1461931.5617409  1496896.76160628 1075807.84186376
  937363.76564573 1616985.99629076 1458389.88452998 1233160.47863549
 1001783.3547462  1187820.34727413 1286882.38876311  690632.29782458
 1331941.90515254 1311952.0027799  1214915.35355427 1720342.35734617
 1277706.58679827 1472419.18121928 1276320.93386718 1520685.87743558
 1512862.31109384 1408378.46763161 1413854.04978194 1738794.66341945
 1315188.97845979 1122094.06420377 1283023.79242799 1045315.8262322
 1456237.58003508]
2025-06-20 19:32:09 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:32:09 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:32:09 ERROR Can not run the algorithm
2025-06-20 19:32:09 INFO Run function 7 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:32:09 INFO FeHistory: [416380.81491595 372913.913581   262550.10681006 329158.66222498
 259096.5548029  323672.57471283 287678.03804913 404740.36990797
 346829.64751933 264258.97084394 304265.39298051 309934.82476887
 371072.14678235 275925.20411547 270155.21431146 337010.16457407
 423886.78872915 430581.55596687 387970.86220393 178241.49581059
 282455.75344812 314112.78307997 383432.16241745 360089.85082767
 298122.10169994 353700.76249622 177675.02267739 403095.14186559
 247120.11646498 253650.10664168 335220.9081685  312709.47808386
 231024.40999623 288897.35470384 411342.76943994 345794.06071103
 327345.2435894  414363.46139949 372917.03325808 248075.27633886
 331283.64128034 352678.72353239 239466.82094615 403061.08744059
 298881.93043678 246231.17463447 266133.50880834 276030.79572219
 269475.65805921 374650.96178984 279229.94156039 459997.70964626
 434364.29553512 281796.06637083 359233.19522742 361387.34425544
 304574.72838782 441139.18647412 329969.7660929  333764.94371936
 226228.74857808 281668.93286397 234365.00716082 439884.46174
 315890.52838053 320065.9403461  277363.61641843 448572.93317426
 246834.47477075 225239.71946786 472059.8620658  408545.91080237
 249812.36393406 305379.24082274 314688.02575368 273830.49513176
 327078.03906634 251718.71208599 314194.55077886 238288.41809565
 180669.63509558 420544.98854876 264596.94127768 327376.53617071
 228883.61959612 265529.35255642 280877.62971488 430984.75733841
 183504.06684819 378254.40242698 338745.29043954 350250.62216865
 368721.41158667 134778.29419259 262082.04600191 323902.66031025
 270815.26772198 323448.12252088 340936.14993475 447929.40236493
 217983.47055386]
2025-06-20 19:32:09 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:32:09 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:32:09 ERROR Can not run the algorithm
2025-06-20 19:32:09 INFO Run function 8 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:32:09 INFO FeHistory: [263314.27699867 213105.05315029 251887.60165746 372832.26615808
 242927.16568271 228915.82001378 381332.50409819 311682.1623318
 204365.80692397 356961.49842749 242720.95843728 210563.5027199
 336607.7087009  314907.2872006  407199.99777375 202597.14782095
 231470.94727807 287013.56367508 420460.02141375 193567.47068499
 259477.93204631 373893.95092182 343096.55811005 194206.74481088
 224248.36412886 307766.41940881 232148.72620192 323258.31069579
 293676.20557647 297307.67006308 349884.72905887 287228.70945692
 294566.91870195 340658.07771874 327735.37109446 215692.81460273
 323483.4447471  242801.95585108 270863.1724316  298450.87779544
 265279.74009202 292329.68888726 303221.44980956 321781.61763305
 279695.16289112 408615.55331429 300341.99266988 146442.88153672
 329737.03691571 353429.69679146 208954.91729334 181748.19492746
 196667.41990658 232357.39504502 246502.93230788 298450.70447851
 258580.14097709 238268.62187239 262045.27483822 223187.12884931
 327727.24243585 395936.24163329 406859.33833713 207628.05012553
 172532.79687814 277649.07775745 353718.75415746 363681.64832265
 216919.00096371 257442.97456648 307373.03534411 377202.26203014
 273129.44615321 454007.4269465  336361.48861764 414062.72973812
 341086.0693369  255371.53827535 289111.59358866 279921.23574958
 429117.78643284 246732.10190629 354319.26801772 179574.12106872
 196612.72911538 289228.55986438 333680.21355647 301006.70356508
 242926.10826157 337544.63998034 421080.12909545 277228.55641613
 227004.83085916 315415.41806776 282701.37923134 282448.81779134
 357788.92615678 188589.83942048 255090.28669468 284707.0549406
 337556.11246372]
2025-06-20 19:32:09 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:32:09 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:32:09 ERROR Can not run the algorithm
2025-06-20 19:32:10 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:32:10 INFO FeHistory: [179.49194634 212.43526816 209.96934028 231.40942806 231.2137788
 223.28306712 202.88359318 143.60083262 227.33215578 229.68661501
 226.10846316 158.28388119 172.81864979 177.04429928 276.53729207
 201.01869609 199.16459654 221.13850566 270.12625636 197.35854614
 213.72021171 191.67519173 201.51130229 179.83876514 222.10591097
 158.91729067 202.51695019 178.19702487 218.36254494 192.28712645
 219.87574865 220.41815958 192.48705327 180.86246142 191.3737216
 192.43095813 154.32614927 188.18864021 180.58887393 184.34165769
 223.60010858 205.76229487 170.01543962 202.72633061 212.35135603
 205.05546457 251.43682061 204.7760807  183.92214478 247.34460348
 217.43463574 180.94197031 164.00469048 230.59419015 192.88330782
 190.58329191 190.36667569 187.9970859  203.68119534 259.37177419
 197.68510719 195.54821504 217.18227058 237.6329986  208.42228538
 208.14721304 222.83177138 228.65667161 183.11838653 181.86661517
 217.48055131 220.72402277 204.61698841 207.55395006 214.89629797
 204.13376188 222.06168108 224.88765884 170.70298468 208.1093332
 216.05507738 205.43797133 182.67221482 194.72421359 217.33477914
 227.22181784 189.32217981 237.54114214 205.34435168 229.09632553
 183.16083265 201.45667667 160.09598613 194.78429856 236.28641439
 190.42788251 190.92480655 169.45682031 214.76855385 192.66662778
 203.2149018 ]
2025-06-20 19:32:10 INFO Expected Optimum FE: -100
2025-06-20 19:32:10 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:32:10 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:32:10 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:32:10 INFO AOCC mean: 0.0000
2025-06-20 19:32:10 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization, which centers the initial population around a randomly selected point, leading to potentially faster convergence. EnhancedArchiveGuidedDE uses a simple uniform random initialization, which can be less effective in high-dimensional spaces. AdaptiveGaussianSamplingEA also incorporates adaptive mutation and sigma decay, which helps to fine-tune the search process.  EnhancedArchiveGuidedDE lacks such sophisticated mechanisms.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE incorporates Differential Evolution, a powerful search strategy, while AdaptiveGaussianDEwithArchive adds an archive, which can help in multimodal optimization but adds complexity without significant improvement based on the AOCC scores. The Adaptive Gaussian perturbation in both is a valuable addition compared to purely random initialization.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA uses a simpler evolutionary strategy (tournament selection, Gaussian recombination, adaptive mutation), leading to potentially easier implementation and analysis. AdaptiveGaussianSamplingDE uses Differential Evolution, which is more complex but can be more efficient in some cases. The AOCC scores show that the simpler approach is slightly superior in this specific benchmark.


(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both employ Differential Evolution and adaptive Gaussian mutation.  The difference lies in the archive used by AdaptiveGaussianDEwithArchive, which, based on the AOCC score, doesn't significantly improve performance and adds computational overhead.


Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive utilizes a more sophisticated initialization (Adaptive Gaussian Sampling) and incorporates Differential Evolution, giving it a significant advantage over EnhancedArchiveGuidedDE's simple uniform random initialization and less sophisticated DE.


Overall: The top-performing algorithms consistently use adaptive Gaussian sampling for initialization, demonstrating its effectiveness in high-dimensional problems.  The addition of sophisticated search operators (e.g., Differential Evolution) can improve performance, but only if implemented carefully and efficiently.  Adding complexity (like an archive) without clear performance gains may be detrimental.
- Adaptive Gaussian sampling for initialization significantly improves performance in high-dimensional search spaces.  Sophistication should be carefully balanced with computational cost; simpler, well-tuned algorithms can outperform more complex ones.  Thorough benchmarking is crucial for evaluating the effectiveness of different initialization and search strategies.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:32:10 INFO Generation 17, best so far: 0.5412867233158046
2025-06-20 19:32:10 INFO --- Performing Long-Term Reflection at Generation 17 ---
2025-06-20 19:32:14 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses an adaptive Gaussian sampling for initialization, focusing on exploration around an initial random point, while EnhancedArchiveGuidedDE uses simple uniform random initialization, which is less effective in high-dimensional spaces.  AdaptiveGaussianSamplingEA also incorporates adaptive mutation and recombination strategies, leading to better exploitation and escaping local optima. EnhancedArchiveGuidedDE, while using an archive, lacks the sophisticated adaptive mechanisms.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE combines adaptive Gaussian sampling with Differential Evolution, a robust optimization method.  AdaptiveGaussianDEwithArchive also uses DE but adds an archive, which while potentially beneficial for multimodal problems, adds complexity without a demonstrably superior outcome based on the AOCC scores.  The adaptive Gaussian perturbation in AdaptiveGaussianSamplingDE is more directly integrated into the DE process.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA is a simpler algorithm, relying on Gaussian mutation and recombination. AdaptiveGaussianSamplingDE integrates the adaptive Gaussian approach into a more complex DE framework.  The simpler approach of AdaptiveGaussianSamplingEA surprisingly outperforms the more complex one.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use Differential Evolution and adaptive Gaussian mutation. AdaptiveGaussianDEwithArchive incorporates an archive, intended to maintain diversity and find multiple optima, but the AOCC score indicates that the added complexity does not significantly improve performance in this context.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive at least employs Differential Evolution and an adaptive Gaussian mutation, giving it a more structured search than the simple random initialization and DE crossover in EnhancedArchiveGuidedDE. The addition of the archive in AdaptiveGaussianDEwithArchive doesn't make it significantly better than EnhancedArchiveGuidedDE. Overall:  Adaptive Gaussian sampling initialization and adaptive mutation strategies are strongly correlated with better performance. Simple uniform random initialization is significantly less effective.  Adding an archive doesn't automatically improve results.  Simpler algorithms, appropriately designed, can outperform more complex ones.


**Experience:**

The results highlight the importance of carefully considering initialization strategies and adaptive mechanisms in high-dimensional optimization. Simple enhancements may not always lead to better performance; focusing on effective exploration and exploitation strategies is crucial.  Careful algorithm design is more effective than simply adding complexity.

2025-06-20 19:32:14 INFO Generating offspring via Crossover...
2025-06-20 19:32:24 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:32:39 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:32:39 INFO FeHistory: [1518762.27512365  608660.12816519  969798.09860734 ...   11320.99038841
   12494.80431685   13748.35754537]
2025-06-20 19:32:39 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:32:39 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:32:55 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:32:55 INFO FeHistory: [147044.42873213 134322.10707399 162747.12733715 ...   5962.53551102
   4351.78544294   5177.47651403]
2025-06-20 19:32:55 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:32:55 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:33:10 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:33:10 INFO FeHistory: [275040.82048009 133353.37676496 190565.08195437 ...  10541.0604988
  10451.88012383  16262.80447231]
2025-06-20 19:33:10 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:33:10 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:33:24 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:33:51 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:33:51 INFO FeHistory: [212.48340645 204.25614178 232.97200061 ...  37.26047606  20.80685265
  38.86368041]
2025-06-20 19:33:51 INFO Expected Optimum FE: -100
2025-06-20 19:33:51 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:33:51 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:33:51 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:33:51 INFO AOCC mean: 0.0000
2025-06-20 19:34:00 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:34:00 ERROR Can not run the algorithm
2025-06-20 19:34:00 INFO Run function 4 complete. FEHistory len: 182, AOCC: 0.0000
2025-06-20 19:34:00 INFO FeHistory: [1265051.78799025 1139651.9476579  1146589.35483674  984877.12968172
 1017516.3924973  1476272.9877054  1213931.78870263 1221892.56380576
 1200259.44222014 1462490.33969394 1332082.86277087 1718718.39253809
 1069606.00077408  930483.09344142 1227389.75488291 1410037.20491898
 1325491.90510432 1681500.2913059  1765646.09194491  988068.76124127
 1518947.08598277 1071764.14102346 1625622.18080263 1410609.42208264
 1313747.12895563 1524246.6555477  1097184.88808809  947136.85881298
 1592353.31662992 1101597.41197701 1137494.49952613 1242372.50728784
  989958.04795741 1501764.46414406 1200023.7883916  1383042.96705142
 1511387.87985297 1870395.36543865 1178799.7420118  1413633.47094682
 1403942.18738659 1607295.67586214 1665617.37453576 1720524.06467274
 1338766.87286659 1382024.14194469 1059228.33295939 1635458.92385961
 1315661.22323181 1546892.23646735 1226137.23057285 1341099.10504298
 1529814.13537002 1925757.11260535 1443613.87195154 1500833.23279445
 1224233.06317373 1168506.38863659  817373.40057632 1586921.76871855
  994075.0456114  1361111.05301075 1167854.01164012 1493247.05523096
 1232823.31011197 1211955.54246698 1722313.03765509 1035556.6602657
 1266357.22845145 1249921.61160393 1567132.02832053 1360049.59849682
 1506601.91387833 1441309.69805862 1515903.84720528 1362337.1639675
 1445456.86352575 1437437.26560917 1569926.69658053 1166281.30711913
 1452156.00129364 1205513.69583585 1366885.9025448   731863.8360037
 1356138.517677   1239811.7186944  1393170.75393735 1459247.08793264
 1648127.54624172 1432433.88015954 1162754.10503889 1758856.45893365
 1149183.26175376 1091208.61434288 1443449.10387383 1082221.29523655
 1662537.70309233 1184696.05956178 1855069.46183118  882918.10436182
 1170539.84721348 1456237.34664014 1208532.88944975 1034697.48653034
 1142648.17942275 1248490.92939315 1140423.02253178 1018899.98152802
 1065667.51280172 1111213.43996947 1334008.04604598 1546979.03417373
 1278831.42863778 1025477.83164465 1304011.46073593 1229893.56521855
 1381836.52605014 1392075.75078301 1396452.18180716 1326687.81226972
 1398476.658272   1320040.92884821 1426448.00890838 1652398.20986597
 1113958.77299658 1523765.96557183 1402253.9028732  1182718.46166808
 1373676.4418716   920884.54621454 1257748.19684426 1255896.50186494
 1570692.23428955 1695754.09547243 1250049.63594645 1359298.09022286
 1199643.48101661 1755579.22721996 1031212.99074619 1406112.13998884
  918602.68484654 1358096.08241731 1435964.29239077  923463.36817056
 1176650.54813355 1116254.10236374 1547120.97621418 1717299.70558035
 1837850.13720509 1472524.51441243  723811.63991535  954834.83861627
 1117397.31130722 1806812.56970531 1191991.48829794 1547196.90029614
  935360.09024407 1608137.25891159  929986.37949032 1328908.07371788
 1142483.83788337 1311859.16593629 1057657.63832706 1626208.83828653
 1054177.03741051 1460969.791307   1549615.00679209  921163.72468954
 1163609.46663364 1193946.24021659 1342617.43194429 1122288.76815066
 1415312.04246079 2011807.3118706  1978029.45283515 1299899.66411924
 1444100.09793056 1184470.03226298 1339167.42818498 1579460.66615774
 1351673.82163853  839469.42032637]
2025-06-20 19:34:00 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:34:00 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:34:00 ERROR Can not run the algorithm
2025-06-20 19:34:01 INFO Run function 7 complete. FEHistory len: 214, AOCC: 0.0000
2025-06-20 19:34:01 INFO FeHistory: [378408.68026805 392313.10975893 368760.54775235 289892.68018756
 321595.79038972 291160.94695727 313910.71138108 267832.35261898
 263322.733621   251878.33875063 296482.4511266  253155.92986025
 667596.93962626 411843.69201086 474977.33843429 324177.00930542
 283898.78008192 420299.28312101 388598.29858673 308354.98315119
 240760.12983132 296688.43341587 383435.85137859 257932.20741789
 265153.89153004 427416.21376522 312794.82666815 300288.33205348
 289669.72588413 360098.21465569 324336.39482416 263079.93741294
 286265.90148554 350320.66184638 259212.69646597 207861.21027526
 248009.97389757 507130.92335349 370177.39322075 206512.83383537
 304258.95164308 234884.32269604 322965.26734211 314258.58697672
 259281.24802507 273108.99866144 335664.99580789 238044.15345148
 258295.43912801 334925.32227281 222356.22717449 270574.27723125
 363855.62677148 390088.46117932 310449.37407589 197553.26409784
 316278.61735731 259884.92045932 268392.860473   340049.43316216
 354389.99286524 299018.66169671 400476.46544438 243418.74935616
 310046.86146896 337715.73162634 285765.54332233 249581.1272691
 417634.9758953  266858.34931321 279485.19737201 188551.9845581
 366294.69576173 371320.49969106 287701.18555426 246684.42727336
 330184.66519168 433745.97490544 355240.61004044 264355.70791098
 249739.11618419 260147.4248163  534010.67063563 259001.78834138
 438041.151922   226431.15374269 289760.89662889 173703.85221774
 348444.20084666 257509.65026572 326435.89535109 252512.74256537
 324160.10254371 224182.58005646 387238.74342137 575025.18807873
 300190.12975896 421303.93523942 233658.16456618 355144.80851731
 427230.74200351 295864.46982766 385171.28115976 163440.964111
 220850.24352956 166310.98107693 276261.68060929 310676.85237861
 239116.70078051 315682.66340566 268407.40724621 298368.75120152
 566373.30470383 425016.12320191 303178.98717035 367483.25334842
 373288.34540156 306076.5039383  441336.91246376 328497.26721258
 381772.48185742 317051.54956207 277135.15633844 287112.93037064
 320288.25849831 348103.23270918 327758.05433573 228421.31887046
 257484.75713051 306944.63451857 429776.27612048 375649.61750928
 336543.74373563 408418.32847229 319989.70586302 225228.91963035
 313760.30026116 445688.02892607 357960.15301991 267332.34084931
 276563.48443845 232627.34968551 508561.23705408 292114.86142525
 239970.8972674  310400.69919996 351179.30876571 287371.46734801
 321719.127793   244865.94667718 236337.90559567 339733.96379773
 307468.02799264 262469.40860818 216302.52478694 342400.87172016
 220153.3849573  311640.04314981 313521.7612028  356482.88933449
 341348.62711438 262333.93990564 328727.2088331  280816.86162414
 367604.39467504 356344.57057362 273212.01892993 258332.43944211
 324998.31083145 369915.63099637 333241.65038975 182398.37116499
 252495.50227672 356543.35193795 374179.25517343 262183.67319789
 358358.22563613 357416.57493683 371109.60607782 167672.78769778
 268090.25932633 194090.2869575  457297.39094325 348525.99849439
 449676.23816175 278469.54568587 336177.64850742 213656.30975453
 320232.71262884 367093.00333703 390232.39568047 311537.64078102
 274761.59401441 242862.79572955 289505.37508544 410779.91958759
 368221.40182818 350339.89476149 325619.92684621 247173.43724747
 352881.71805388 291597.90421954 411820.98302156 272587.89016259
 240176.16062637 153373.24173494 234144.19087426 296541.84092921
 368626.22259953 261894.78741763 225644.04673072 331459.84968048
 459629.79013454 350421.30361101]
2025-06-20 19:34:01 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:34:01 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:34:01 ERROR Can not run the algorithm
2025-06-20 19:34:01 INFO Run function 8 complete. FEHistory len: 171, AOCC: 0.0000
2025-06-20 19:34:01 INFO FeHistory: [358627.46056052 338351.10533529 215640.35247131 250154.15076463
 253200.35818749 313905.68907324 224185.9943493  245086.93363387
 211601.84900905 258292.57181811 168498.75984419 227376.15401549
 411853.34722486 205029.6128289  257361.77544027 184162.64737321
 268274.6194848  309475.73271245 272357.22366009 209597.61430223
 269902.56573048 296664.5673008  297283.28647304 298492.09417637
 206066.27110261 312028.30346607 333501.04152688 290601.72045062
 146269.57618401 301852.3839032  282449.0021264  428894.04737898
 252221.67650393 268238.51098078 267044.09978885 340221.43698157
 439994.01284832 248865.10046124 196205.26929932 447674.33248389
 316720.75036346 346596.56426219 208819.63885705 335932.73993747
 315768.04334483 300595.98537602 317630.14829457 207193.90404671
 333473.0048767  310253.2460149  324928.4492468  218318.68760557
 306692.1352119  276765.19989991 328803.78767658 120692.97342219
 296503.2524635  196852.7638949  215107.27641881 309032.08984455
 265330.40743447 250784.47558689 186691.17990882 249496.53111813
 373632.29703134 364142.0506725  263886.06010447 308189.84090151
 270203.51195491 266315.73123995 253534.15234622 345943.86039083
 239363.26578252 327096.98242269 344574.55517985 357842.27373828
 234776.37844878 256195.65550156 237522.87818875 348490.10709882
 336781.34606016 307777.84457722 390999.07193941 214575.56930544
 227146.7481608  285541.03047917 307653.82400397 235144.05548576
 196248.380718   408841.25685688 408298.77618323 259457.78692682
 338630.35671509 220551.4464888  309551.7427965  242395.90050558
 253099.30653649 216844.19610572 272850.68520331 195873.96517761
 130378.70957378 231532.18936339 234091.82010317 222530.94925586
 200740.40095383 243969.7948398  192399.32145002 203110.3175901
 248179.61810636 214193.71672601 180382.43276022 174402.54572875
 352590.45874954 250923.11664951 207862.06657054 186878.7234412
 152815.43466196 274661.76020619 232832.11432549 268520.82771337
 347418.81223043 196476.32326521 312390.46473568 258252.83165522
 196047.3003378  261623.78111215 277956.16048826 169065.11640754
 156488.8065002  227322.26722092 263015.11580566 337363.06585417
 162522.416866   224815.10648172 314164.38494119 149312.05975648
 277214.85764823 245787.88673372 284752.7265379  417946.39339229
 292954.72833132 347830.0551824  185924.69624384 333345.46312025
 266942.11989527 419918.04844567 241883.30143952 161458.86253574
 397998.37781337 222170.25113844 270149.47450233 228885.25222651
 256402.12648671 204266.4676506  266694.04483822 185214.92396706
 315676.20085134 242343.41065495 224617.06077934 255815.63737457
 193417.96140879 242969.05719666 192708.76860568 201919.84158864
 197543.2594571  282097.84343971 221686.34844361 178158.22727991
 188529.10348126 263862.67371264 228996.76429978]
2025-06-20 19:34:01 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:34:01 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:34:01 ERROR Can not run the algorithm
2025-06-20 19:34:02 INFO Run function 24 complete. FEHistory len: 220, AOCC: 0.0000
2025-06-20 19:34:02 INFO FeHistory: [208.03231968 182.16819859 188.74727235 210.12380788 193.88021629
 201.34315203 174.10562998 213.41358922 216.8018676  193.39460233
 236.31694479 242.08693818 180.92613875 260.30757908 182.22897867
 242.95298106 233.48670619 197.15225694 202.81365154 209.69446556
 198.9783926  235.01502622 238.71886552 210.45787779 230.24278079
 175.08894367 187.76365115 229.88192326 226.52191521 235.97182567
 251.07458286 207.16984619 213.37621157 169.8371014  183.10882261
 183.58099768 234.08080312 205.46334813 240.19287347 237.93542058
 159.2208279  196.2351771  225.22254264 231.97216256 207.67549913
 173.30225267 212.74494268 197.33106443 176.31510462 213.46662711
 196.81511839 185.42900045 214.28991256 228.1735825  224.01755507
 227.4411616  220.40059605 191.43596684 210.07937379 191.27925149
 205.89294188 194.25973957 191.44501163 223.46202748 205.86166713
 194.11053459 247.36392977 185.04941356 233.9867355  176.00596382
 152.73200573 205.36230885 198.47480729 227.17361213 212.69346352
 171.01797921 226.67407388 237.73361635 191.79607593 191.07097886
 197.99843639 243.98405236 190.05323607 183.75800886 220.8578149
 213.715157   231.65886951 193.92959221 222.42965419 210.66004472
 196.34024993 225.50346275 218.72876585 190.39574868 204.82783874
 204.37137022 230.67088717 167.39223122 199.31252295 218.08797156
 220.47055994 230.46074142 182.43242844 200.36756775 195.18487191
 224.66277246 191.62697806 237.76609112 177.09418469 210.28922162
 199.05112115 236.0113081  177.20509753 253.35606672 193.64437261
 210.46204849 220.76632478 215.63494842 171.5809313  227.75884437
 173.02451115 211.24190113 214.14781421 216.56756941 197.61516472
 242.57428664 210.97610911 214.84510851 256.23110566 225.68811683
 229.70047913 208.3768738  213.13335136 195.09038111 210.23009547
 189.79392805 194.42752549 181.82361438 204.09058292 246.74893767
 203.63374416 194.62415233 211.8527965  221.79388894 192.65831662
 206.03520142 207.37882956 213.01559504 202.46914905 199.99325423
 220.98275795 203.39556011 221.80372587 235.69160039 207.74746574
 165.36084981 194.13410087 178.8991136  208.14981973 210.80392537
 190.25896957 193.5554787  211.62271879 228.49877891 165.76428766
 185.5435036  214.25836466 204.85860566 234.52748961 210.98809092
 189.02141289 244.11048411 210.37389647 181.31142265 172.99922605
 203.51327967 198.90352154 240.27672461 180.53441909 209.03545421
 192.02825118 213.32752694 195.39745195 185.06868014 230.45626107
 224.78728353 216.33281174 196.2061324  203.92671965 220.20171403
 220.83978558 241.50039224 232.14134274 171.44694171 209.30959804
 219.63122916 232.11541795 206.81449731 222.31289281 223.67087235
 186.28048818 215.64330108 209.21134416 227.89955187 203.95253296
 237.4404404  214.73429238 178.17238713 216.19775829 223.67722902
 230.13375587 216.08203404 214.94424333 258.53178711 209.60782206
 190.21309608 225.35483754 214.07617187 189.69168842 199.56909832]
2025-06-20 19:34:02 INFO Expected Optimum FE: -100
2025-06-20 19:34:02 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:34:02 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:34:02 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:34:02 INFO AOCC mean: 0.0000
2025-06-20 19:34:02 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses an adaptive Gaussian sampling for initialization, focusing on exploration around an initial random point, while EnhancedArchiveGuidedDE uses simple uniform random initialization, which is less effective in high-dimensional spaces.  AdaptiveGaussianSamplingEA also incorporates adaptive mutation and recombination strategies, leading to better exploitation and escaping local optima. EnhancedArchiveGuidedDE, while using an archive, lacks the sophisticated adaptive mechanisms.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE combines adaptive Gaussian sampling with Differential Evolution, a robust optimization method.  AdaptiveGaussianDEwithArchive also uses DE but adds an archive, which while potentially beneficial for multimodal problems, adds complexity without a demonstrably superior outcome based on the AOCC scores.  The adaptive Gaussian perturbation in AdaptiveGaussianSamplingDE is more directly integrated into the DE process.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA is a simpler algorithm, relying on Gaussian mutation and recombination. AdaptiveGaussianSamplingDE integrates the adaptive Gaussian approach into a more complex DE framework.  The simpler approach of AdaptiveGaussianSamplingEA surprisingly outperforms the more complex one.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use Differential Evolution and adaptive Gaussian mutation. AdaptiveGaussianDEwithArchive incorporates an archive, intended to maintain diversity and find multiple optima, but the AOCC score indicates that the added complexity does not significantly improve performance in this context.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive at least employs Differential Evolution and an adaptive Gaussian mutation, giving it a more structured search than the simple random initialization and DE crossover in EnhancedArchiveGuidedDE. The addition of the archive in AdaptiveGaussianDEwithArchive doesn't make it significantly better than EnhancedArchiveGuidedDE. Overall:  Adaptive Gaussian sampling initialization and adaptive mutation strategies are strongly correlated with better performance. Simple uniform random initialization is significantly less effective.  Adding an archive doesn't automatically improve results.  Simpler algorithms, appropriately designed, can outperform more complex ones.
- The results highlight the importance of carefully considering initialization strategies and adaptive mechanisms in high-dimensional optimization. Simple enhancements may not always lead to better performance; focusing on effective exploration and exploitation strategies is crucial.  Careful algorithm design is more effective than simply adding complexity.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:34:02 INFO Generation 18, best so far: 0.5412867233158046
2025-06-20 19:34:02 INFO --- Performing Long-Term Reflection at Generation 18 ---
2025-06-20 19:34:06 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best uses adaptive Gaussian sampling for initialization and incorporates adaptive mutation and recombination, leading to more efficient exploration and exploitation of the search space.  The worst uses a simple uniform random initialization and lacks adaptive mechanisms, resulting in less effective exploration, particularly in high-dimensional spaces.  `AdaptiveGaussianSamplingEA` also dynamically adjusts the mutation strength based on the optimization progress.

`(second best) AdaptiveGaussianSamplingDE` vs `(second worst) AdaptiveGaussianDEwithArchive`:  `AdaptiveGaussianSamplingDE` effectively combines adaptive Gaussian sampling with Differential Evolution (DE), benefiting from DE's ability to handle multimodal functions while retaining the advantages of the adaptive Gaussian approach. In contrast, `AdaptiveGaussianDEwithArchive` adds an archive mechanism, but its added complexity doesn't demonstrably improve performance compared to the simpler DE-based approach. The archive might be beneficial for multimodal functions but may not be necessary in this case.

Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`, we see that the best-performing algorithm uses a more sophisticated selection mechanism (tournament selection) and explicitly manages recombination.  `AdaptiveGaussianSamplingDE` uses a simpler selection method within the DE framework and implicitly combines recombination and mutation. The tournament selection in `AdaptiveGaussianSamplingEA` can better guide the search towards better solutions.

(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`: Both algorithms use Differential Evolution, but `AdaptiveGaussianMutationDE` employs an adaptive Gaussian perturbation in the mutation step, contributing to better exploration. The archive used in `AdaptiveGaussianDEwithArchive`, despite being intended to improve exploration, does not significantly improve the overall AOCC score.

Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that `AdaptiveGaussianDEwithArchive` uses a more informed initialization through adaptive Gaussian sampling compared to the uniform random initialization of `EnhancedArchiveGuidedDE`.  The archive mechanism in `AdaptiveGaussianDEwithArchive` seems less effective than simply using adaptive Gaussian perturbations.

Overall: The top-performing algorithms leverage adaptive Gaussian sampling for initialization and incorporate adaptive mutation or recombination strategies.  The use of DE in combination with adaptive mechanisms proves highly efficient. Simple archives do not consistently improve performance.  Adaptive mechanisms for parameter tuning (like mutation scale) are key.


**Experience:**

Focusing on adaptive initialization techniques, particularly Gaussian sampling centered around strategically chosen points, and incorporating adaptive parameter tuning within the optimization algorithm proved crucial for achieving high performance on the GNBG benchmark.  Simple archive mechanisms did not yield significant improvements.

2025-06-20 19:34:06 INFO Generating offspring via Crossover...
2025-06-20 19:34:16 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:34:31 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:34:31 INFO FeHistory: [1063015.98436039 1153695.34095545 1017078.62629847 ...   72495.8617181
   72495.8617181    72495.8617181 ]
2025-06-20 19:34:31 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:34:31 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:34:47 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:34:47 INFO FeHistory: [389988.61910352 384718.85866081 359510.89368114 ...  66333.57818783
  66333.57818783  66333.57818783]
2025-06-20 19:34:47 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:34:47 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:35:02 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:35:02 INFO FeHistory: [279124.16578286 260169.24573487 282736.36778349 ...  83846.15045147
  83846.15045147  83846.15045147]
2025-06-20 19:35:02 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:35:02 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:35:16 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:35:42 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:35:42 INFO FeHistory: [198.02377052 206.97497854 190.46833251 ... 156.23067716 156.23067716
 156.23067716]
2025-06-20 19:35:42 INFO Expected Optimum FE: -100
2025-06-20 19:35:42 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:35:42 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:35:42 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:35:42 INFO AOCC mean: 0.0000
2025-06-20 19:35:50 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:35:50 ERROR Can not run the algorithm
2025-06-20 19:35:50 INFO Run function 4 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:35:50 INFO FeHistory: [1504843.77610979 1853281.79503816 2296716.30995684 1147444.10520037
 1131563.44494802 1421038.46936451 1795388.6759459  1278266.84559079
 1376282.42360519 2009901.27953974 1663600.98197872 1878101.6489862
 1928769.34088419 1474584.41586818 1180504.81928055 1593183.70921271
 2348291.11870314 2266474.38741594 1929105.50722929 1788939.87844211
 1770770.10198143 1613522.92708402 2094815.40626875 1413817.89156877
 1481679.84741269 1710877.75238357 1570163.00171143 1820640.1845648
 1995684.11884704 1234807.5801882  1458849.91172299 1529689.55847165
 1772263.09552959 1422425.30457033 2019142.65725551 2750569.2514011
 1277604.15073815 1721881.85455822 1973020.48671562 2083332.91024177
 2186572.01207255 1684699.05121554 2052168.1500536  1396540.71838977
 2195762.38362212 1429470.52842656 2127957.86464428 1647649.63100878
 1390995.60106116 1609378.70870228 1976888.5172729  1191917.58295919
 1996150.21915648 1727735.71119179 1544079.43328721 1768342.67348213
 1793169.58874402 1577556.56281374 1997352.1544129  1625435.96865911
 1234038.95339316 2115177.07341599 1716407.81782925 1754144.55902217
 1628170.27811927 2437001.68532407 2064447.56807485 1731052.70758603
 2100319.46724133 1420618.73967713 1713398.02311455 1618809.74389584
 1476813.54630308 2124417.98180459 2276757.09997293 1525536.27041083
 2028835.40187473 1435581.92002822 1800584.41504447 2158548.64088824
 1379341.02301323 1704813.24984771 1272706.08064948 1979130.20398208
 1211547.00166642 1448619.60099336 1890911.08023922 2072550.89232487
 1534013.25560196 1983524.56515741 1745005.46713466 2211107.83637693
 1633064.44208446 1689254.0379463  2045756.04812234 1261740.53508797
 1904967.4523112  1703634.03652453 2474081.67795369 1489377.21350676
 1193410.33891787]
2025-06-20 19:35:50 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:35:51 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:35:51 ERROR Can not run the algorithm
2025-06-20 19:35:51 INFO Run function 7 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:35:51 INFO FeHistory: [402290.82196333 175642.951297   299294.31593627 238936.86819888
 359198.65851497 341011.38436672 318314.94577805 219211.86609567
 220104.23076316 277547.79831816 264579.09724259 192975.143417
 280784.81214688 229228.99729975 299640.31023691 335530.02440663
 331127.26046874 210816.52375008 285870.0461947  353929.64540209
 252141.64521233 457779.61592656 255007.78966343 229952.69912348
 312462.92911841 186632.27480817 440905.69668876 338327.08376431
 165617.95048287 347880.9584764  356642.52228477 294716.74131473
 277571.16271367 193957.33390773 367051.22731168 317315.20507913
 347206.31107299 368808.16942888 322808.7218233  123239.17593148
 286560.98873502 369655.91793712 275362.54769971 211778.53701197
 215164.64775552 414441.45213682 327704.486432   290711.8316344
 336393.84700597 327163.80541437 388796.33204189 206903.02856316
 266246.53442642 230090.37636349 368876.11727521 358324.2413893
 470475.79619589 345732.55336234 409698.37236633 262644.95991152
 195064.77515775 193910.62559501 334126.98967724 335929.92820385
 330823.07868589 238311.39151182 266993.78952301 348507.35121863
 209907.82445613 201580.94388846 345853.0820422  403698.74663059
 287343.07500011 308279.60286894 268564.37616199 308702.26099486
 239418.62150921 279243.44838421 354371.41091319 364609.36860759
 174803.36461108 350848.31103891 227590.76986599 396562.53213469
 314038.94862325 314596.2734565  347222.46220824 250953.59819756
 262379.93038978 284335.24269607 254577.77580031 230910.95866018
 299553.19570447 285728.12907649 273508.49512413 277677.13282128
 433879.66687789 239050.99882692 411148.23792287 221542.12731165
 207699.96118241]
2025-06-20 19:35:51 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:35:51 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:35:51 ERROR Can not run the algorithm
2025-06-20 19:35:51 INFO Run function 8 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:35:51 INFO FeHistory: [374152.51029809 133866.21037112 182576.53083564 224672.90307206
 247999.03845569 166082.8389628  273406.13602251 252549.3268626
 435804.3110114  251619.09123096 223734.73533732 362933.27731565
 214397.69529432 382221.200956   283307.21684796 184045.64977528
 245298.26141175 215868.82618    348751.75672403 365010.05897664
 278886.36745537 257396.89741565 392879.85372621 301356.99014736
 462723.34121066 197960.71532052 153150.4059174  200165.26079957
 282934.50253715 237954.58907381 228688.62728359 276587.53762246
 215790.29993572 243249.08907409 311815.36567475 153033.04998875
 193327.35734578 254137.35043218 178897.7973455  325843.78713835
 207683.18865333 257224.54424171 186328.68965138 258584.94896276
 201957.15346407 280978.75133124 237120.38438618 245190.36282593
 328185.29288578 265169.42422649 212871.8887162  191374.14831247
 219369.10434831 132623.37801616 224935.27023573 237630.07603808
 150625.683986   188261.78501091 301254.08392172 277805.28617759
 167920.97288256 247323.36485988 206664.83938605 268009.31817434
 247417.63040281 272792.383514   212897.13283986 193673.43371667
 225226.54265973 258104.94367146 225787.22596261 302315.89362906
 233144.82976685 314827.40524649 248382.80943476 198504.22147262
 261070.19773342 174920.51731942 189044.98340016 276871.47797854
 326094.09996217 292159.94761694 382997.60802488 323777.05638255
 318657.07227441 362226.30283721 207279.22518327 335032.05394537
 171804.81932872 249061.18899324 217241.39995317 199535.79237948
 180456.27837704 271835.3137228  208374.1847222  183271.23391425
 221173.40263154 303590.06327519 251450.05804579 131894.85116359
 139330.16986838]
2025-06-20 19:35:51 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:35:51 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:35:51 ERROR Can not run the algorithm
2025-06-20 19:35:52 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:35:52 INFO FeHistory: [199.85584468 205.18600026 225.38151253 212.61357298 268.85747498
 231.13536616 231.45179593 219.72072522 186.16635495 204.09355241
 197.90544383 251.35040287 244.96122475 217.91423763 239.0001906
 262.09654438 269.0617058  240.94945571 243.72840747 270.16972168
 223.09857275 202.88986567 264.29105996 266.95168911 257.15895789
 238.63938874 241.89289248 212.02599549 238.9456037  216.7738378
 277.37772661 210.77100336 257.42500138 248.31017994 257.78510675
 223.82642222 189.63699231 198.49570347 205.14668026 222.71368333
 257.09144373 242.75251808 259.47395339 215.12740464 267.0805095
 186.0661013  189.1304718  242.49568398 236.44524942 215.04419399
 259.65632936 252.07787438 211.92775789 227.02544361 267.54576168
 206.15968031 276.9874637  210.2527128  247.67794708 183.45592422
 189.02487692 197.31843648 245.01156413 220.81307524 251.23215206
 253.15865904 195.45692997 259.67236317 274.27817083 233.04640848
 201.82379934 244.68497013 206.85768127 230.72674582 234.79891979
 260.3219355  234.08380537 260.24504645 219.85865379 225.82101481
 206.67516678 270.24314792 250.98369603 215.01688535 231.51567623
 243.71789678 250.11494986 292.141239   223.05759414 171.43863989
 177.9808205  226.20507337 212.55688819 245.27938619 225.59407914
 201.29735257 270.12636368 158.66395418 168.21064794 190.64088244
 193.37942946]
2025-06-20 19:35:52 INFO Expected Optimum FE: -100
2025-06-20 19:35:52 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:35:52 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:35:52 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:35:52 INFO AOCC mean: 0.0000
2025-06-20 19:35:52 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best uses adaptive Gaussian sampling for initialization and incorporates adaptive mutation and recombination, leading to more efficient exploration and exploitation of the search space.  The worst uses a simple uniform random initialization and lacks adaptive mechanisms, resulting in less effective exploration, particularly in high-dimensional spaces.  `AdaptiveGaussianSamplingEA` also dynamically adjusts the mutation strength based on the optimization progress.

`(second best) AdaptiveGaussianSamplingDE` vs `(second worst) AdaptiveGaussianDEwithArchive`:  `AdaptiveGaussianSamplingDE` effectively combines adaptive Gaussian sampling with Differential Evolution (DE), benefiting from DE's ability to handle multimodal functions while retaining the advantages of the adaptive Gaussian approach. In contrast, `AdaptiveGaussianDEwithArchive` adds an archive mechanism, but its added complexity doesn't demonstrably improve performance compared to the simpler DE-based approach. The archive might be beneficial for multimodal functions but may not be necessary in this case.

Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`, we see that the best-performing algorithm uses a more sophisticated selection mechanism (tournament selection) and explicitly manages recombination.  `AdaptiveGaussianSamplingDE` uses a simpler selection method within the DE framework and implicitly combines recombination and mutation. The tournament selection in `AdaptiveGaussianSamplingEA` can better guide the search towards better solutions.

(3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveGaussianDEwithArchive`: Both algorithms use Differential Evolution, but `AdaptiveGaussianMutationDE` employs an adaptive Gaussian perturbation in the mutation step, contributing to better exploration. The archive used in `AdaptiveGaussianDEwithArchive`, despite being intended to improve exploration, does not significantly improve the overall AOCC score.

Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that `AdaptiveGaussianDEwithArchive` uses a more informed initialization through adaptive Gaussian sampling compared to the uniform random initialization of `EnhancedArchiveGuidedDE`.  The archive mechanism in `AdaptiveGaussianDEwithArchive` seems less effective than simply using adaptive Gaussian perturbations.

Overall: The top-performing algorithms leverage adaptive Gaussian sampling for initialization and incorporate adaptive mutation or recombination strategies.  The use of DE in combination with adaptive mechanisms proves highly efficient. Simple archives do not consistently improve performance.  Adaptive mechanisms for parameter tuning (like mutation scale) are key.
- Focusing on adaptive initialization techniques, particularly Gaussian sampling centered around strategically chosen points, and incorporating adaptive parameter tuning within the optimization algorithm proved crucial for achieving high performance on the GNBG benchmark.  Simple archive mechanisms did not yield significant improvements.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:35:52 INFO Generation 19, best so far: 0.5412867233158046
2025-06-20 19:35:52 INFO --- Performing Long-Term Reflection at Generation 19 ---
2025-06-20 19:35:56 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best utilizes adaptive Gaussian sampling for both initialization and mutation, leading to a more effective exploration of the search space, especially in high dimensions.  `EnhancedArchiveGuidedDE`, on the other hand, relies on a uniform random initialization and lacks adaptive mechanisms for adjusting exploration/exploitation balance. (second best) `AdaptiveGaussianSamplingDE` and (second worst) `AdaptiveGaussianDEwithArchive` both employ Differential Evolution, but `AdaptiveGaussianSamplingDE` includes a more sophisticated Gaussian perturbation for mutation which aids its better performance. Comparing (1st) vs (2nd), we see that while both use adaptive Gaussian sampling, the EA approach in `AdaptiveGaussianSamplingEA` shows better convergence properties for the GNBG benchmark compared to the DE framework of `AdaptiveGaussianSamplingDE`.  (3rd) `AdaptiveGaussianMutationDE` and (4th) `AdaptiveGaussianDEwithArchive` differ primarily in the archive mechanism. The addition of an archive in `AdaptiveGaussianDEwithArchive` provides no significant performance improvement compared to a more straightforward adaptive Gaussian mutation in `AdaptiveGaussianMutationDE`. Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that adaptive Gaussian mutation enhances exploration in both. However, `AdaptiveGaussianDEwithArchive` incorporates DE, providing a structured search that is more effective than uniform random initialization employed in `EnhancedArchiveGuidedDE`. Overall: Adaptive Gaussian sampling for initialization and adaptive mutation strategies significantly improve performance, while archive mechanisms and a simple DE framework don't necessarily provide substantial improvements for this specific benchmark.

**Experience:**

Combining adaptive sampling with well-structured evolutionary operators (like DE or EA) is crucial for high-dimensional optimization.  Adaptive mechanisms for controlling exploration-exploitation trade-offs are essential, and careful evaluation of the archive's impact is needed.  Simple random initializations are suboptimal for complex landscapes.

2025-06-20 19:35:56 INFO Generating offspring via Crossover...
2025-06-20 19:36:05 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:36:05 ERROR Can not run the algorithm
2025-06-20 19:36:06 INFO Run function 4 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:36:06 INFO FeHistory: [ 688531.66315228 1147293.78750837  912323.58498426  941177.36879508
  965019.80544175 1219337.10096482  948972.00045684 1334193.63080466
  930182.80201543 1141651.58421981 1215560.02759999  903187.88098579
  918082.71751203 1214361.04674266 1277591.86920416 1041718.26372936
 1117206.84151512  964876.94661328  985668.71787827 1385102.70006324
  948115.0176338  1012254.30477081  942327.84392213  893128.1526751
 1147092.52336999 1046477.22159511 1009899.00000244 1360897.7578891
 1070902.1646085   916117.75665895 1117441.70591856  793642.24780024
 1154802.54096487  823908.70307798 1494480.79436686  795448.19556805
 1534397.01755442  906290.3120169   896136.25716493 1222250.06304193
 1551172.67164962  949379.26466712 1050520.70322979 1076148.85528913
 1293067.07731869 1152589.35536135 1266120.83461847 1227794.26217621
 1060263.70885488 1099600.17158582 1124739.00940945 1040240.0380759
  934749.95011084 1373063.36928801 1073015.0112261  1183747.02598908
 1242477.42555124  913199.0993764  1185155.19091321 1010549.15868342
  850352.98971982 1167168.78506892 1034844.98416782 1088258.21791839
  876569.49070394 1155087.96233836 1192640.11602291  977020.27588937
 1105569.24046177  953580.38726621  827699.61768478 1180939.72142018
 1313379.41010733 1450938.45004854  970060.69334233 1272619.40703512
  964310.48544637 1253257.30024781 1470876.65808705 1080018.47257579
 1271045.87671773 1214383.89700514  864364.38153847  923654.05317782
 1052830.13005244 1194801.53908842 1524244.70875048 1312116.2507049
 1550017.4675328  1153894.32595163  634126.22120129 1167727.39340408
 1129720.56521837 1218835.81384014  945118.79683713 1131245.37636824
 1018162.07633151 1120831.19651605 1422307.89040426  739680.93218881
  752874.21010061]
2025-06-20 19:36:06 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:36:06 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:36:06 ERROR Can not run the algorithm
2025-06-20 19:36:06 INFO Run function 7 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:36:06 INFO FeHistory: [175356.83402132 276031.93770633 215035.58035853 225047.5431807
 289514.58215298 205239.79007116 302371.12524422 249346.21825108
 285351.73360837 196718.07134338 300710.21656287 258985.14815333
 170645.55888909 307378.0891694  244729.63763328 138014.22176429
 273663.15794548 252853.01246536 304918.99737634 252417.65933555
 267041.40772195 198853.77905824 224186.23934257 225189.67378261
 284776.41509908 284865.09745384 201760.63962079 253825.5935897
 282214.73803012 237185.3043108  331802.15628656 196412.02317888
 204534.61059551 176629.36893842 178946.94865393 307044.66320315
 244715.4630159  221641.1551834  282379.68372577 259620.82436222
 253658.30049169 220156.22610469 248406.55018659 253401.54603184
 189562.54287641 178282.32493158 270688.06808501 270001.84821372
 305975.8260042  189230.78345018 292797.79736792 258319.60019195
 199726.5427246  200931.9551976  296294.9596579  257130.7049214
 303895.35495044 272014.60364755 297325.73183259 218600.30176356
 263699.45955294 216691.77822303 204008.26231818 281562.7611481
 193627.51065582 213783.45681811 232834.1121239  208474.60254584
 236841.46492741 306830.92961885 305440.94344401 175394.9987563
 169841.48365642 278000.8691197  282790.14902554 268717.22013596
 240702.5426325  181638.60225967 170206.90937905 297596.33263914
 304665.11724853 188311.91023486 250535.55422895 261614.29399054
 200005.17389206 203694.76138577 268306.91265139 211065.29760573
 265808.59587478 181278.01348791 292281.12684089 218706.03990044
 189078.82598493 316982.24446516 281356.08428278 250222.46519272
 207333.34247185 315413.96374464 233409.70690702 333042.22204224
 237583.14558997]
2025-06-20 19:36:06 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:36:06 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:36:06 ERROR Can not run the algorithm
2025-06-20 19:36:07 INFO Run function 8 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:36:07 INFO FeHistory: [203272.52517492 125256.42437044 148073.4688789  169916.09233205
 242829.68576654 226891.74049065 162945.09902219 120988.56997632
 120064.38880919 153268.86275191 146847.9288966  175772.79853891
 155803.26801466 120432.56072208 181733.64904514 136038.75187874
 112505.89015662 179474.47075086 169880.22849189 154399.27985128
 135067.42591676 150781.32213357 126314.93146774 173972.85557342
 139770.85298884 106433.18176242 152325.84491922 167736.14323262
 155498.29535694 161601.45564021 204526.95639472 184722.73022981
 143271.86441816 102879.94563196 116328.47659836 169293.91677412
 119215.68075297 115061.70231547 157273.00002945 207207.82815559
 177032.65489246 141974.34222827 111715.08036154 106350.24720487
 133059.23013955 163470.94751045 167421.73683029 118755.8807988
 128655.02744162 137048.08344292 177109.57778963 113303.27999813
 136608.33983794 110778.7797893  142334.24286732 138334.6500816
 151949.79833282 142045.58121338 169540.13777359  95345.77277518
 168134.89907543 204459.15331522 168931.88213199 161752.54333951
 177932.56443194 197228.35860565 152447.82211084 136045.52846297
 158218.60917769 136669.69150844 189135.13916792 146124.1831268
 152558.93167895 143223.57127476 174220.09657053 204055.52306068
 126410.28356713  94769.98808449 170369.37674934 150371.89995445
 172508.94366358 164527.79290786 117019.84271023 158062.82004399
 148753.03451207 138198.27710745 113566.54940185 160847.55356236
 146161.79791451 128624.25747636 205253.04143394 184450.58162378
 136219.06892115 131221.33591464 180234.37469393 226465.46214023
 102194.23454452 108166.48989108  57973.12799446 130159.97846726
 142076.74488112]
2025-06-20 19:36:07 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:36:07 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:36:07 ERROR Can not run the algorithm
2025-06-20 19:36:07 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:36:07 INFO FeHistory: [219.00930082 200.41954036 197.06529022 197.35762869 242.49548005
 183.83287899 209.09045821 198.08985068 222.28487488 186.5050225
 176.61877533 186.99285649 201.78997125 164.26662121 190.17798899
 206.77419056 179.40079011 185.54422854 205.18338267 149.48524751
 204.69529184 185.76802674 180.45772514 214.05368317 180.83254597
 177.76512453 167.35444313 206.65068692 179.93037265 231.53053474
 169.5520919  209.14959357 182.33634464 169.53207773 184.54005288
 160.68807674 195.5725725  207.56518627 205.84931078 170.70133093
 162.95821096 199.07888719 207.94002877 208.24364343 180.60524067
 193.72664793 208.65655471 212.32996086 192.0409962  228.40897407
 196.9193321  184.60939512 197.91016218 179.78948138 173.76590958
 175.44267745 206.20065651 186.29071963 216.41506334 183.91481845
 178.33516558 202.75239075 203.6751871  188.02344415 199.09464954
 179.26373406 188.36106486 209.15876311 182.95163067 208.093773
 209.87699527 208.10901288 202.27025643 196.11204885 167.04108042
 180.83020216 204.18836542 199.52670847 206.3803684  202.31570661
 216.21358245 181.7659769  217.76158785 202.25219323 192.60677995
 201.39424546 208.43158104 186.12355119 196.12845223 201.65144352
 210.9677311  202.29868972 173.83978351 200.42168893 202.71881669
 210.6824243  198.73954611 210.41824159 241.83868897 205.00787536
 201.32694607]
2025-06-20 19:36:07 INFO Expected Optimum FE: -100
2025-06-20 19:36:07 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:36:07 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:36:07 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:36:07 INFO AOCC mean: 0.0000
2025-06-20 19:36:14 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:36:26 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:36:26 INFO FeHistory: [ 754333.8936961   921697.4068749  1821655.8432066  ...  287481.3314534
  287481.33121367  287481.33125228]
2025-06-20 19:36:26 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:36:26 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:36:37 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:36:37 INFO FeHistory: [243437.74086969 197465.64124252 282686.2936487  ...  75096.81932158
  75096.81932158  75096.81932158]
2025-06-20 19:36:37 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:36:37 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:36:49 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:36:49 INFO FeHistory: [178308.38233605 313007.6370315  219232.32384049 ...  65570.84266704
  65570.84269769  65570.84266018]
2025-06-20 19:36:49 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:36:49 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:37:14 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:37:26 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:37:26 INFO FeHistory: [227.60438033 226.71196469 237.00291719 ... 114.7045654  114.7045654
 114.7045654 ]
2025-06-20 19:37:26 INFO Expected Optimum FE: -100
2025-06-20 19:37:26 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:37:26 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:37:26 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:37:26 INFO AOCC mean: 0.0000
2025-06-20 19:37:26 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best utilizes adaptive Gaussian sampling for both initialization and mutation, leading to a more effective exploration of the search space, especially in high dimensions.  `EnhancedArchiveGuidedDE`, on the other hand, relies on a uniform random initialization and lacks adaptive mechanisms for adjusting exploration/exploitation balance. (second best) `AdaptiveGaussianSamplingDE` and (second worst) `AdaptiveGaussianDEwithArchive` both employ Differential Evolution, but `AdaptiveGaussianSamplingDE` includes a more sophisticated Gaussian perturbation for mutation which aids its better performance. Comparing (1st) vs (2nd), we see that while both use adaptive Gaussian sampling, the EA approach in `AdaptiveGaussianSamplingEA` shows better convergence properties for the GNBG benchmark compared to the DE framework of `AdaptiveGaussianSamplingDE`.  (3rd) `AdaptiveGaussianMutationDE` and (4th) `AdaptiveGaussianDEwithArchive` differ primarily in the archive mechanism. The addition of an archive in `AdaptiveGaussianDEwithArchive` provides no significant performance improvement compared to a more straightforward adaptive Gaussian mutation in `AdaptiveGaussianMutationDE`. Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that adaptive Gaussian mutation enhances exploration in both. However, `AdaptiveGaussianDEwithArchive` incorporates DE, providing a structured search that is more effective than uniform random initialization employed in `EnhancedArchiveGuidedDE`. Overall: Adaptive Gaussian sampling for initialization and adaptive mutation strategies significantly improve performance, while archive mechanisms and a simple DE framework don't necessarily provide substantial improvements for this specific benchmark.
- Combining adaptive sampling with well-structured evolutionary operators (like DE or EA) is crucial for high-dimensional optimization.  Adaptive mechanisms for controlling exploration-exploitation trade-offs are essential, and careful evaluation of the archive's impact is needed.  Simple random initializations are suboptimal for complex landscapes.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:37:26 INFO Generation 20, best so far: 0.5412867233158046
2025-06-20 19:37:26 INFO --- Performing Long-Term Reflection at Generation 20 ---
2025-06-20 19:37:31 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization and mutation, leading to a more effective exploration of the search space, unlike the uniform random initialization in EnhancedArchiveGuidedDE. AdaptiveGaussianSamplingEA also incorporates a sigma decay mechanism to refine the search as the algorithm progresses, which is absent in EnhancedArchiveGuidedDE.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE utilizes Differential Evolution, a powerful search method, while AdaptiveGaussianDEwithArchive adds an archive which doesn't show significant improvement in this benchmark, potentially due to increased overhead or less efficient archive management.  AdaptiveGaussianSamplingDE’s simpler approach is more effective in this specific case.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA's simpler EA structure provides better results than incorporating DE.  While DE is powerful, the additional complexity might hinder performance on this specific function.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE and adaptive Gaussian mutation.  AdaptiveGaussianDEwithArchive includes an archive, intended to improve multimodal optimization. However, AdaptiveGaussianMutationDE demonstrates that the additional archive complexity does not lead to a substantial advantage.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive employs a more sophisticated initialization method (Adaptive Gaussian) than the simple uniform random initialization in EnhancedArchiveGuidedDE. While both use archives, the less complex archive management in AdaptiveGaussianDEwithArchive is beneficial here.  

Overall: The best performing algorithms consistently employ adaptive Gaussian sampling for initialization and, in most cases, a simpler, more focused approach outperforms adding features like an archive or Differential Evolution, highlighting the importance of effective initialization and targeted search strategies for the GNBG benchmark.


**Experience:**

Effective population initialization is crucial, especially in high-dimensional spaces. Simple random sampling is often insufficient; adaptive methods like Gaussian sampling, centered around an initial informed guess, are more effective.  Overly complex algorithms with many features don't always translate to better performance; simplicity and focus are sometimes superior.

2025-06-20 19:37:31 INFO Generating offspring via Crossover...
2025-06-20 19:37:39 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:37:53 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:37:53 INFO FeHistory: [ 779245.07579292  958787.6637433  1027431.7709781  ...    3782.40724571
    4553.43727538    4392.0324672 ]
2025-06-20 19:37:53 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:37:53 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:38:07 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:38:07 INFO FeHistory: [203032.7657947  233590.49041179 279951.28337555 ...   5672.88869046
   5536.16287137   7260.0130164 ]
2025-06-20 19:38:07 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:38:07 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:38:22 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:38:22 INFO FeHistory: [212583.57597702 136001.14197828 230029.88689471 ...   7985.70010758
   5590.34628764   4765.54419473]
2025-06-20 19:38:22 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:38:22 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:38:39 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:39:02 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:39:02 INFO FeHistory: [206.40059588 192.37855957 201.69518443 ...  58.72118525  91.43216987
 101.44931945]
2025-06-20 19:39:02 INFO Expected Optimum FE: -100
2025-06-20 19:39:02 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:39:02 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:39:02 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:39:02 INFO AOCC mean: 0.0000
2025-06-20 19:39:09 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:39:25 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0014
2025-06-20 19:39:25 INFO FeHistory: [873875.95463674 860304.44322123 897150.87733891 ...   3170.36964533
   2080.52776893   1006.78075999]
2025-06-20 19:39:25 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:39:25 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:39:42 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:39:42 INFO FeHistory: [307741.21344833 330353.21792577 294722.41356667 ...  45052.58171937
  17004.47789162  27916.81356177]
2025-06-20 19:39:42 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:39:42 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:39:58 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:39:58 INFO FeHistory: [381354.09596792 321368.99922765 301856.6098752  ...  58936.97784587
  61376.10712627  52107.27939419]
2025-06-20 19:39:58 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:39:58 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:40:09 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:40:41 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:40:41 INFO FeHistory: [156.74887017 174.610072   182.1060262  ...  74.08859399 100.73640729
  99.61615167]
2025-06-20 19:40:41 INFO Expected Optimum FE: -100
2025-06-20 19:40:41 INFO Unimodal AOCC mean: 0.0014
2025-06-20 19:40:41 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:40:41 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:40:41 INFO AOCC mean: 0.0004
2025-06-20 19:40:41 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization and mutation, leading to a more effective exploration of the search space, unlike the uniform random initialization in EnhancedArchiveGuidedDE. AdaptiveGaussianSamplingEA also incorporates a sigma decay mechanism to refine the search as the algorithm progresses, which is absent in EnhancedArchiveGuidedDE.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE utilizes Differential Evolution, a powerful search method, while AdaptiveGaussianDEwithArchive adds an archive which doesn't show significant improvement in this benchmark, potentially due to increased overhead or less efficient archive management.  AdaptiveGaussianSamplingDE’s simpler approach is more effective in this specific case.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA's simpler EA structure provides better results than incorporating DE.  While DE is powerful, the additional complexity might hinder performance on this specific function.

(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE and adaptive Gaussian mutation.  AdaptiveGaussianDEwithArchive includes an archive, intended to improve multimodal optimization. However, AdaptiveGaussianMutationDE demonstrates that the additional archive complexity does not lead to a substantial advantage.

Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive employs a more sophisticated initialization method (Adaptive Gaussian) than the simple uniform random initialization in EnhancedArchiveGuidedDE. While both use archives, the less complex archive management in AdaptiveGaussianDEwithArchive is beneficial here.  

Overall: The best performing algorithms consistently employ adaptive Gaussian sampling for initialization and, in most cases, a simpler, more focused approach outperforms adding features like an archive or Differential Evolution, highlighting the importance of effective initialization and targeted search strategies for the GNBG benchmark.
- Effective population initialization is crucial, especially in high-dimensional spaces. Simple random sampling is often insufficient; adaptive methods like Gaussian sampling, centered around an initial informed guess, are more effective.  Overly complex algorithms with many features don't always translate to better performance; simplicity and focus are sometimes superior.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:40:41 INFO Generation 21, best so far: 0.5412867233158046
2025-06-20 19:40:41 INFO --- Performing Long-Term Reflection at Generation 21 ---
2025-06-20 19:40:46 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses an adaptive Gaussian sampling for initialization, centered around a random point, leading to a more informed starting population compared to EnhancedArchiveGuidedDE's purely random uniform initialization. AdaptiveGaussianSamplingEA also incorporates adaptive mutation and sigma decay to further refine the search, whereas EnhancedArchiveGuidedDE relies on a simpler DE strategy.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE uses adaptive Gaussian perturbation within its DE framework, adding exploration capabilities.  AdaptiveGaussianDEwithArchive also uses adaptive Gaussian perturbation but introduces an archive, potentially slowing down convergence and possibly getting stuck in local optima if the archive is not managed well.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA is a simpler EA with Gaussian recombination, while AdaptiveGaussianSamplingDE integrates DE, which offers a potentially more efficient exploration-exploitation balance but adds complexity.


(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE and adaptive Gaussian mutation. AdaptiveGaussianDEwithArchive adds an archive, which can be beneficial for multimodal problems but may slow down convergence.


Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive uses a more sophisticated initialization and mutation strategy compared to the purely random uniform initialization in EnhancedArchiveGuidedDE. It also incorporates Differential Evolution which is generally more effective than the simpler DE used in EnhancedArchiveGuidedDE. Overall: The best-performing algorithms leverage adaptive Gaussian sampling for initialization and incorporate adaptive mechanisms for mutation and scaling factors.  Simpler algorithms with purely random initialization underperform. Archives can be helpful but require careful management.


**Experience:**

Adaptive initialization techniques combined with adaptive parameter adjustments (mutation rate, scaling factor) significantly improve performance.  Careful consideration of the balance between exploration and exploitation is crucial for high-dimensional problems.  Archives can be useful but add complexity, requiring effective management to avoid hindering convergence.

2025-06-20 19:40:46 INFO Generating offspring via Crossover...
2025-06-20 19:40:55 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:41:10 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:41:10 INFO FeHistory: [1089695.71092928  630067.32319401 1099396.35649298 ...   12910.0193775
   11606.81074724   10766.91758369]
2025-06-20 19:41:10 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:41:10 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:41:25 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:41:25 INFO FeHistory: [174750.94880404 273602.70924521 244873.52020848 ...   3232.08230065
   3937.27092104   6952.79606911]
2025-06-20 19:41:25 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:41:25 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:41:41 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:41:41 INFO FeHistory: [313281.57998175 180820.566349   103765.33034112 ...  11541.29095301
  14560.8311173   10394.78428073]
2025-06-20 19:41:41 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:41:41 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:41:55 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:42:21 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 19:42:21 INFO FeHistory: [173.8631569  177.23345788 208.50676679 ...  53.54915038  55.86371876
  27.52788939]
2025-06-20 19:42:21 INFO Expected Optimum FE: -100
2025-06-20 19:42:21 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:42:21 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:42:21 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:42:21 INFO AOCC mean: 0.0000
2025-06-20 19:42:30 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:42:30 ERROR Can not run the algorithm
2025-06-20 19:42:30 INFO Run function 4 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:42:30 INFO FeHistory: [1202674.5073637  1265883.95215704 2181036.33534317 1234235.68896074
  944418.13009296 1592363.41464854 1368675.98033931 1362428.07358616
 1314943.14882214 1114061.20477374 1136885.18038694  729195.4407873
 1116591.77522275 1209699.38044606 1066728.4135747  1139942.32456733
 1152634.92477067 1466762.20572604 1029988.05736132 1367668.12546914
  959627.79287865 1652780.92647    1092186.69451794  910661.69972552
 1148156.82853459  924860.32121381 1197275.92183496 1689352.65587447
 1002979.0105041  1514523.94039029 1656649.12167115 1621665.97903954
 1411416.86922759 1387974.93564885  920543.90197383 1512342.11723012
 1269826.91194793 1098568.85181019 1500138.05344122 1035488.61088674
 1352086.44881597  925851.06380723  824778.76125212 1372964.68970252
 1225133.49325765 1019150.20954336 1325094.46236455 1335297.32743784
 1333065.17671494 1233974.42923063 1296538.47786892  870272.65103384
  993563.33193137 1019851.36312118  923003.42110497  980373.34531247
 1700615.51615586 1059322.89549591  938578.97651103 1386722.18632779
 1116339.89671095 1196996.95238783 1226332.48617093 1092571.39206701
 1539813.7219664  1164770.32231963 1604720.06455668 1342977.73976154
 1442647.06967358 1119369.53527944 1234321.48755045 1339125.07122086
 1743337.12931022  926038.19440694 1014678.81410487 1047694.91761767
 1040173.77423036 1322499.65316407 1469264.26369782  989273.21257869
 1166049.75637988 1217123.79437809 1199699.14993327 1327694.01030566
  920253.4211098  1100459.61088015 1045315.26498911  972213.30891429
 1303399.74510283  927330.74856478 1312091.64516189 1425528.87275711
 1205034.48665108 1567648.28809315 1050307.88028257 1281436.92913545
 1744941.45020483 1406674.16415993 1219537.88325903 1308688.18989509
 1084271.19903205]
2025-06-20 19:42:30 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:42:30 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:42:31 ERROR Can not run the algorithm
2025-06-20 19:42:31 INFO Run function 7 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:42:31 INFO FeHistory: [252926.68746565 280745.70515771 314460.50378265 236680.91332302
 269795.40673574 172725.82778943 287096.13530169 243377.56770668
 225074.29291252 123696.94832186 394663.53207392 349056.63217776
 156147.29086469 144160.73687498 268149.41412897 156143.79723753
 209516.07487179 185202.59772016 339502.58439417 236026.12577744
 217329.15650661 172462.34521805 221114.51564117 167359.72833684
 222552.16123458 216882.53399566 234406.56404912 231543.23869332
 125209.56601189 347033.18374379 197655.5441503  254854.03244618
 315274.37103647 236902.29188283 169477.96373839 240244.69692914
 353141.90205869 243195.46634271 222352.38357888 264719.65830151
 259393.33737981 287742.87720064 204237.29911429 293730.20712714
 203208.51710889 273503.88111846 270988.65600837 228663.14205296
 279530.47894451 323339.04159922 447762.08674772 217495.39963292
 212580.15415931 275840.34067654 188096.52320032 329568.90940665
 255281.28584149 221391.09745781 192369.4475443  270242.49718874
 261420.017653   283662.79229164 346819.29529642 280036.83004408
 283084.02952476 222108.47214623 245110.30535095 221199.96972216
 162874.23600586 301562.47828969 243568.2312208  276004.9095646
 205133.7274158  226154.04831228 283176.75138121 287150.09020737
 205758.54250702 282908.9965083  252051.25546225 219718.45048541
 171179.08211815 307935.85787666 227331.1302875  186734.21033491
 163759.10735425 262248.09318485 260767.31270848 345603.79495007
 298420.7482413  338625.04832309 264129.08130917 223825.77361467
 243685.06536083 250388.88145684 372839.58040456 194239.59876456
 412293.01827124 343583.11751611 181462.13363954 332156.21124798
 198667.22873872]
2025-06-20 19:42:31 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:42:31 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:42:31 ERROR Can not run the algorithm
2025-06-20 19:42:31 INFO Run function 8 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:42:31 INFO FeHistory: [268521.06453379 176676.2532606  156252.43089239 291491.33292065
 274000.97629071 192673.25800911 260472.71287501 259040.29524947
 328128.46407325 223695.95642471 222358.4195057  276885.78873242
 273259.4064858  157988.7632266  166090.15715044 249728.00952929
 259142.61263333 226949.49957251 228853.712382   244456.86167053
 264417.37618504 259907.55694163 265371.694159   209691.93397257
 316395.31292532 308831.15706254 303581.01694127 199458.3364389
 230620.72367617 212643.45475834 254610.65156545 192396.02250405
 194244.22728499 233716.70557542 270498.04833207 208842.48219675
 281193.73323089 229415.76055112 297120.58591415 244259.61040212
 169588.47880238 200897.55490568 283923.26040445 213281.06861013
 189141.86614006 308808.95537861 211164.74170405 227670.69143464
 240075.09820028 198557.17514924 213099.01460536 134341.28015822
 314609.88752784 178822.94924027 181693.19921165 355560.4235287
 170107.90421497 156951.77981374 285498.23087137 245335.45505406
 258664.36141358 262008.78785605 189024.04812801 110451.18087539
 352873.89218749 160602.17465425 278336.49342014 174069.76786724
 277456.06451125 212007.24312974 195239.7097932  236285.73958732
 217254.4164288  221842.55544874 143943.53102141 252846.43022603
 399280.38901361 411596.94674719 256271.15356094 222452.75873061
 233730.2839605  250050.25018152 296598.73658685 316384.25420598
 136059.33700791 132327.6444047  223079.92148457 245357.61313014
 233674.96362866 242853.28833376 254526.5444562  252909.57119211
 256618.82575938 253177.46264116 150414.82539035 308065.78794267
 235916.51302418 451914.90372574 319977.84140889 196654.0141792
 259017.10224514]
2025-06-20 19:42:31 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:42:31 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:42:31 ERROR Can not run the algorithm
2025-06-20 19:42:32 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 19:42:32 INFO FeHistory: [212.01774291 153.63170721 200.20121365 175.59794183 226.5322029
 206.15419297 225.25853291 207.26502467 237.97081968 187.30233447
 196.86317933 166.68278722 222.63219065 200.72500873 184.58705044
 206.44553166 198.8249622  200.53800862 203.87790908 185.4406959
 205.29228449 167.88752419 209.4552121  189.87321164 197.5456329
 192.76429858 187.30908951 205.70586549 213.48329878 209.32134629
 188.88614801 213.76346412 190.41451356 187.02254247 188.68992172
 174.46788265 210.26462502 232.61037579 212.69391873 172.11188584
 195.83798044 216.98395689 222.4426721  202.85594332 191.12643899
 222.02973682 195.67011512 212.78553746 225.31330878 186.77747575
 175.46449801 194.71620029 178.6711624  221.86995731 187.57983796
 200.50419294 226.52319707 180.75519112 171.49261957 207.36916394
 177.94779066 193.35442557 192.84728623 180.36213991 208.01152202
 200.25793171 199.53199618 200.22022861 214.91528831 190.95458448
 209.44532918 222.40828633 172.80610577 193.44817818 200.85018487
 218.46412795 180.92076131 178.42516166 199.65380188 191.17028412
 187.53412057 149.78832229 165.32684107 189.22061801 173.32707639
 185.03211989 191.64131901 180.76408471 158.40142901 196.57786235
 165.24883194 160.75566652 183.60020507 176.49642045 197.13480051
 206.42788594 197.90116756 195.61831143 203.49276814 205.61175256
 175.71514967]
2025-06-20 19:42:32 INFO Expected Optimum FE: -100
2025-06-20 19:42:32 INFO Unimodal AOCC mean: 0.0000
2025-06-20 19:42:32 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 19:42:32 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 19:42:32 INFO AOCC mean: 0.0000
2025-06-20 19:42:32 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses an adaptive Gaussian sampling for initialization, centered around a random point, leading to a more informed starting population compared to EnhancedArchiveGuidedDE's purely random uniform initialization. AdaptiveGaussianSamplingEA also incorporates adaptive mutation and sigma decay to further refine the search, whereas EnhancedArchiveGuidedDE relies on a simpler DE strategy.

(second best) AdaptiveGaussianSamplingDE vs (second worst) AdaptiveGaussianDEwithArchive: AdaptiveGaussianSamplingDE uses adaptive Gaussian perturbation within its DE framework, adding exploration capabilities.  AdaptiveGaussianDEwithArchive also uses adaptive Gaussian perturbation but introduces an archive, potentially slowing down convergence and possibly getting stuck in local optima if the archive is not managed well.


Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianSamplingDE, we see that AdaptiveGaussianSamplingEA is a simpler EA with Gaussian recombination, while AdaptiveGaussianSamplingDE integrates DE, which offers a potentially more efficient exploration-exploitation balance but adds complexity.


(3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveGaussianDEwithArchive: Both use DE and adaptive Gaussian mutation. AdaptiveGaussianDEwithArchive adds an archive, which can be beneficial for multimodal problems but may slow down convergence.


Comparing (second worst) AdaptiveGaussianDEwithArchive vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithArchive uses a more sophisticated initialization and mutation strategy compared to the purely random uniform initialization in EnhancedArchiveGuidedDE. It also incorporates Differential Evolution which is generally more effective than the simpler DE used in EnhancedArchiveGuidedDE. Overall: The best-performing algorithms leverage adaptive Gaussian sampling for initialization and incorporate adaptive mechanisms for mutation and scaling factors.  Simpler algorithms with purely random initialization underperform. Archives can be helpful but require careful management.
- Adaptive initialization techniques combined with adaptive parameter adjustments (mutation rate, scaling factor) significantly improve performance.  Careful consideration of the balance between exploration and exploitation is crucial for high-dimensional problems.  Archives can be useful but add complexity, requiring effective management to avoid hindering convergence.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:42:32 INFO Generation 22, best so far: 0.5412867233158046
2025-06-20 19:42:32 INFO --- Performing Long-Term Reflection at Generation 22 ---
2025-06-20 19:42:36 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best utilizes adaptive Gaussian sampling for both initialization and mutation, leading to a more effective exploration of the search space,  while the worst uses a simple uniform random initialization and a less sophisticated mutation strategy. `AdaptiveGaussianSamplingEA` also incorporates tournament selection and adaptive sigma decay, enhancing convergence.  `(second best)` `AdaptiveGaussianSamplingDE` and `(second worst)` `AdaptiveGaussianDEwithArchive` both use Differential Evolution (DE) but differ in the adaptive Gaussian component.  `AdaptiveGaussianSamplingDE` incorporates it into mutation, while `AdaptiveGaussianDEwithArchive` uses it alongside an archive. The adaptive Gaussian component likely helps escape local optima, and the archive in `AdaptiveGaussianDEwithArchive` may be less effective than the other approaches in this high-dimensional search space. Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`, we see that `AdaptiveGaussianSamplingEA` uses a simpler, more direct approach, which possibly results in better performance. The Gaussian recombination in `AdaptiveGaussianSamplingEA` can potentially allow for a more precise exploitation of the search space. `(3rd)` `AdaptiveGaussianMutationDE` vs `(4th)` `AdaptiveGaussianDEwithArchive`, shows both utilize DE and adaptive Gaussian mutation. However, `AdaptiveGaussianDEwithArchive` attempts to improve exploration by using an archive, which might add unnecessary overhead in this case. Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that both use archives, but `AdaptiveGaussianDEwithArchive` integrates adaptive Gaussian mutation more effectively. `EnhancedArchiveGuidedDE`'s simple random initialization and less targeted archive updating could explain its inferior performance.  Overall: Adaptive Gaussian sampling for initialization and mutation consistently shows superior performance compared to simple uniform random initialization.  The use of sophisticated selection methods (like tournament selection) also appears to be beneficial.  Archives, while potentially helpful, didn't consistently provide performance improvements in this context.  Simpler, more focused algorithms generally outperformed the more complex ones.


**Experience:**

Prioritizing adaptive sampling for initialization and incorporating mechanisms for escaping local optima is crucial for high-dimensional optimization. Simpler, well-tuned algorithms often outperform more complex ones, highlighting the need to carefully evaluate the tradeoff between exploration and exploitation.  Careful consideration of selection methods is paramount.

2025-06-20 19:42:36 INFO Generating offspring via Crossover...
2025-06-20 19:42:47 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:43:47 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:45:48 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.6856
2025-06-20 19:45:48 INFO FeHistory: [ 5.24492881e+05  1.22562860e+06  1.12017367e+06 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 19:45:48 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:45:48 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #added archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population,fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population,offspring)),np.concatenate((fitness_values,offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        # Simple archive update: add solutions if not already present (within a tolerance)
        combined = np.column_stack((population, fitness_values)) # Combine population and fitness for easy comparison
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6): # Check for similarity within tolerance
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1]) #Sort by fitness.
        return np.array(new_archive[:self.archive_size]) #Keep only top archive_size individuals
2025-06-20 19:45:48 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:48:48 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.7085
2025-06-20 19:48:48 INFO FeHistory: [217495.81049133 244630.65025532 303215.93175189 ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 19:48:48 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:48:48 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #added archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population,fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population,offspring)),np.concatenate((fitness_values,offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        # Simple archive update: add solutions if not already present (within a tolerance)
        combined = np.column_stack((population, fitness_values)) # Combine population and fitness for easy comparison
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6): # Check for similarity within tolerance
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1]) #Sort by fitness.
        return np.array(new_archive[:self.archive_size]) #Keep only top archive_size individuals
2025-06-20 19:48:48 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:52:07 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.7085
2025-06-20 19:52:07 INFO FeHistory: [191291.9294303  122311.83817798 196750.2898763  ...   -656.78899799
   -656.78899799   -656.78899799]
2025-06-20 19:52:07 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:52:07 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #added archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population,fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population,offspring)),np.concatenate((fitness_values,offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        # Simple archive update: add solutions if not already present (within a tolerance)
        combined = np.column_stack((population, fitness_values)) # Combine population and fitness for easy comparison
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6): # Check for similarity within tolerance
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1]) #Sort by fitness.
        return np.array(new_archive[:self.archive_size]) #Keep only top archive_size individuals
2025-06-20 19:52:07 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:55:34 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0409
2025-06-20 19:55:34 INFO FeHistory: [201.7642251  210.58434027 178.19731896 ... -64.65784441 -64.65784441
 -64.65784441]
2025-06-20 19:55:34 INFO Expected Optimum FE: -100
2025-06-20 19:55:34 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #added archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population,fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population,offspring)),np.concatenate((fitness_values,offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        # Simple archive update: add solutions if not already present (within a tolerance)
        combined = np.column_stack((population, fitness_values)) # Combine population and fitness for easy comparison
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6): # Check for similarity within tolerance
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1]) #Sort by fitness.
        return np.array(new_archive[:self.archive_size]) #Keep only top archive_size individuals
2025-06-20 19:55:34 INFO Unimodal AOCC mean: 0.6856
2025-06-20 19:55:34 INFO Multimodal (single component) AOCC mean: 0.7085
2025-06-20 19:55:34 INFO Multimodal (multiple components) AOCC mean: 0.0409
2025-06-20 19:55:34 INFO AOCC mean: 0.5359
2025-06-20 19:55:42 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:55:53 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.3244
2025-06-20 19:55:53 INFO FeHistory: [ 1.81765356e+06  1.09076216e+06  1.46694246e+06 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 19:55:53 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:55:53 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEA
# Description: Combines adaptive Gaussian sampling for initialization with an elitist evolutionary strategy.

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingEA."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = (self.upper_bounds - self.lower_bounds) / 4  # Initial step size
        self.sigma_decay = 0.99  # Decay rate for sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = self._evaluate_population(objective_function, offspring)
            self.population, self.fitness_values = self._selection(offspring, offspring_fitness)
            best_solution, best_fitness = self._find_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        """Adaptive Gaussian sampling initialization."""
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=self.sigma, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function, population=None):
        """Evaluates the fitness of a population."""
        if population is None:
            population = self.population
        population_reshaped = population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += len(fitness)
        return fitness

    def _generate_offspring(self):
        """Generates offspring using Gaussian mutation."""
        offspring = self.population + np.random.normal(0, self.sigma, size=(self.population_size, self.dim))
        offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)
        return offspring


    def _selection(self, offspring, offspring_fitness):
        """Elitist selection."""
        combined_population = np.concatenate((self.population, offspring))
        combined_fitness = np.concatenate((self.fitness_values, offspring_fitness))
        indices = np.argsort(combined_fitness)
        selected_indices = indices[:self.population_size]
        return combined_population[selected_indices], combined_fitness[selected_indices]


    def _find_best(self, population, fitness_values):
        """Finds the best solution in the population."""
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 19:55:53 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:56:05 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.3624
2025-06-20 19:56:05 INFO FeHistory: [235390.51388171 156996.51419764 167983.39349013 ...   -912.85737394
   -912.85737393   -912.85737392]
2025-06-20 19:56:05 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:56:05 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEA
# Description: Combines adaptive Gaussian sampling for initialization with an elitist evolutionary strategy.

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingEA."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = (self.upper_bounds - self.lower_bounds) / 4  # Initial step size
        self.sigma_decay = 0.99  # Decay rate for sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = self._evaluate_population(objective_function, offspring)
            self.population, self.fitness_values = self._selection(offspring, offspring_fitness)
            best_solution, best_fitness = self._find_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        """Adaptive Gaussian sampling initialization."""
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=self.sigma, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function, population=None):
        """Evaluates the fitness of a population."""
        if population is None:
            population = self.population
        population_reshaped = population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += len(fitness)
        return fitness

    def _generate_offspring(self):
        """Generates offspring using Gaussian mutation."""
        offspring = self.population + np.random.normal(0, self.sigma, size=(self.population_size, self.dim))
        offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)
        return offspring


    def _selection(self, offspring, offspring_fitness):
        """Elitist selection."""
        combined_population = np.concatenate((self.population, offspring))
        combined_fitness = np.concatenate((self.fitness_values, offspring_fitness))
        indices = np.argsort(combined_fitness)
        selected_indices = indices[:self.population_size]
        return combined_population[selected_indices], combined_fitness[selected_indices]


    def _find_best(self, population, fitness_values):
        """Finds the best solution in the population."""
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 19:56:05 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:56:17 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.3652
2025-06-20 19:56:17 INFO FeHistory: [202343.20089093 344750.65747692 261375.52365817 ...   -656.78899794
   -656.78899795   -656.78899796]
2025-06-20 19:56:17 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:56:17 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEA
# Description: Combines adaptive Gaussian sampling for initialization with an elitist evolutionary strategy.

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingEA."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = (self.upper_bounds - self.lower_bounds) / 4  # Initial step size
        self.sigma_decay = 0.99  # Decay rate for sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = self._evaluate_population(objective_function, offspring)
            self.population, self.fitness_values = self._selection(offspring, offspring_fitness)
            best_solution, best_fitness = self._find_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        """Adaptive Gaussian sampling initialization."""
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=self.sigma, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function, population=None):
        """Evaluates the fitness of a population."""
        if population is None:
            population = self.population
        population_reshaped = population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += len(fitness)
        return fitness

    def _generate_offspring(self):
        """Generates offspring using Gaussian mutation."""
        offspring = self.population + np.random.normal(0, self.sigma, size=(self.population_size, self.dim))
        offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)
        return offspring


    def _selection(self, offspring, offspring_fitness):
        """Elitist selection."""
        combined_population = np.concatenate((self.population, offspring))
        combined_fitness = np.concatenate((self.fitness_values, offspring_fitness))
        indices = np.argsort(combined_fitness)
        selected_indices = indices[:self.population_size]
        return combined_population[selected_indices], combined_fitness[selected_indices]


    def _find_best(self, population, fitness_values):
        """Finds the best solution in the population."""
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 19:56:17 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:56:42 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:56:53 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0324
2025-06-20 19:56:53 INFO FeHistory: [245.02193076 230.77314748 227.21628621 ... -62.53281968 -62.53281968
 -62.53281957]
2025-06-20 19:56:53 INFO Expected Optimum FE: -100
2025-06-20 19:56:53 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEA
# Description: Combines adaptive Gaussian sampling for initialization with an elitist evolutionary strategy.

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingEA."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = (self.upper_bounds - self.lower_bounds) / 4  # Initial step size
        self.sigma_decay = 0.99  # Decay rate for sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = self._evaluate_population(objective_function, offspring)
            self.population, self.fitness_values = self._selection(offspring, offspring_fitness)
            best_solution, best_fitness = self._find_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        """Adaptive Gaussian sampling initialization."""
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=self.sigma, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function, population=None):
        """Evaluates the fitness of a population."""
        if population is None:
            population = self.population
        population_reshaped = population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += len(fitness)
        return fitness

    def _generate_offspring(self):
        """Generates offspring using Gaussian mutation."""
        offspring = self.population + np.random.normal(0, self.sigma, size=(self.population_size, self.dim))
        offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)
        return offspring


    def _selection(self, offspring, offspring_fitness):
        """Elitist selection."""
        combined_population = np.concatenate((self.population, offspring))
        combined_fitness = np.concatenate((self.fitness_values, offspring_fitness))
        indices = np.argsort(combined_fitness)
        selected_indices = indices[:self.population_size]
        return combined_population[selected_indices], combined_fitness[selected_indices]


    def _find_best(self, population, fitness_values):
        """Finds the best solution in the population."""
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 19:56:53 INFO Unimodal AOCC mean: 0.3244
2025-06-20 19:56:53 INFO Multimodal (single component) AOCC mean: 0.3638
2025-06-20 19:56:53 INFO Multimodal (multiple components) AOCC mean: 0.0324
2025-06-20 19:56:53 INFO AOCC mean: 0.2711
2025-06-20 19:56:53 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Worse code
AdaptiveGaussianSamplingDE
import numpy as np
import random

# Name: AdaptiveGaussianSamplingDE
# Description: Combines adaptive Gaussian sampling for initialization with Differential Evolution for search.

class AdaptiveGaussianSamplingDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianSamplingDE with budget, dimensionality, and bounds."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization process and returns the best solution found."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)  #Adaptive Gaussian perturbation
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        initial_point = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return np.random.normal(loc=initial_point, scale=(self.upper_bounds - self.lower_bounds) / 4, size=(self.population_size, self.dim))


    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]


### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best utilizes adaptive Gaussian sampling for both initialization and mutation, leading to a more effective exploration of the search space,  while the worst uses a simple uniform random initialization and a less sophisticated mutation strategy. `AdaptiveGaussianSamplingEA` also incorporates tournament selection and adaptive sigma decay, enhancing convergence.  `(second best)` `AdaptiveGaussianSamplingDE` and `(second worst)` `AdaptiveGaussianDEwithArchive` both use Differential Evolution (DE) but differ in the adaptive Gaussian component.  `AdaptiveGaussianSamplingDE` incorporates it into mutation, while `AdaptiveGaussianDEwithArchive` uses it alongside an archive. The adaptive Gaussian component likely helps escape local optima, and the archive in `AdaptiveGaussianDEwithArchive` may be less effective than the other approaches in this high-dimensional search space. Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingDE`, we see that `AdaptiveGaussianSamplingEA` uses a simpler, more direct approach, which possibly results in better performance. The Gaussian recombination in `AdaptiveGaussianSamplingEA` can potentially allow for a more precise exploitation of the search space. `(3rd)` `AdaptiveGaussianMutationDE` vs `(4th)` `AdaptiveGaussianDEwithArchive`, shows both utilize DE and adaptive Gaussian mutation. However, `AdaptiveGaussianDEwithArchive` attempts to improve exploration by using an archive, which might add unnecessary overhead in this case. Comparing (second worst) `AdaptiveGaussianDEwithArchive` vs (worst) `EnhancedArchiveGuidedDE`, we see that both use archives, but `AdaptiveGaussianDEwithArchive` integrates adaptive Gaussian mutation more effectively. `EnhancedArchiveGuidedDE`'s simple random initialization and less targeted archive updating could explain its inferior performance.  Overall: Adaptive Gaussian sampling for initialization and mutation consistently shows superior performance compared to simple uniform random initialization.  The use of sophisticated selection methods (like tournament selection) also appears to be beneficial.  Archives, while potentially helpful, didn't consistently provide performance improvements in this context.  Simpler, more focused algorithms generally outperformed the more complex ones.
- Prioritizing adaptive sampling for initialization and incorporating mechanisms for escaping local optima is crucial for high-dimensional optimization. Simpler, well-tuned algorithms often outperform more complex ones, highlighting the need to carefully evaluate the tradeoff between exploration and exploitation.  Careful consideration of selection methods is paramount.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 19:56:53 INFO Generation 23, best so far: 0.5412867233158046
2025-06-20 19:56:53 INFO --- Performing Long-Term Reflection at Generation 23 ---
2025-06-20 19:56:58 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `AdaptiveGaussianDEwithArchive`, we see that the best utilizes adaptive Gaussian sampling for both initialization and recombination, leading to more effective exploration of the search space.  The worst uses uniform random initialization and incorporates an archive which doesn't significantly improve performance in this case, potentially adding computational overhead without sufficient benefit. (second best) `AdaptiveGaussianSamplingEAwithArchive` vs (second worst) `AdaptiveGaussianMutationDE`, shows that adding an archive to the adaptive Gaussian sampling approach provides a slight improvement, suggesting that storing and re-utilizing promising solutions can be beneficial, while `AdaptiveGaussianMutationDE` lacks this feature and only uses adaptive Gaussian perturbation in mutation. Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingEAwithArchive`, we see that while the archive helps slightly, the simpler approach without the archive still performs better, suggesting that the added complexity doesn't outweigh the performance gains for this specific problem. (3rd) `AdaptiveGaussianSamplingDE` vs (4th) `AdaptiveGaussianMutationDE`, demonstrates that DE, even with an adaptive Gaussian mutation, doesn't perform as well as the adaptive Gaussian sampling method. The difference might stem from DE's reliance on vector differences that may not be effective in every high-dimensional landscape. Comparing (second worst) `AdaptiveGaussianMutationDE` vs (worst) `AdaptiveGaussianDEwithArchive`, we observe that although both utilize Differential Evolution, adding the archive in the worst performing method is not very helpful in improving its performance compared to the other which lacks it. Overall: Adaptive Gaussian sampling for initialization and recombination proves superior to DE and the inclusion of an archive, while showing only minor performance gains, suggests it is not a critical component for this specific problem.


**Experience:**

Careful consideration of initialization strategies is crucial for high-dimensional problems.  Adaptive methods, informed by problem characteristics, are vital for superior EA performance.  Simplicity and efficiency should be prioritized over unnecessary complexity.

2025-06-20 19:56:58 INFO Generating offspring via Crossover...
2025-06-20 19:57:08 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:57:26 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.3738
2025-06-20 19:57:26 INFO FeHistory: [ 4.61359835e+05  6.58575680e+05  6.58445740e+05 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 19:57:26 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:57:26 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithClusteringEA
import numpy as np

# Name: AdaptiveGaussianSamplingWithClusteringEA
# Description: Combines adaptive Gaussian sampling with clustering to escape local optima in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithClusteringEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.cluster_threshold = 0.1 #parameter to adjust clustering sensitivity


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            #Clustering solutions
            clusters = self._cluster_population(population, fitness_values)

            #Adaptive Gaussian Sampling for each cluster
            offspring = self._adaptive_gaussian_sampling_per_cluster(clusters)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Selection
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _cluster_population(self, population, fitness_values):
      # Simple clustering based on distance and fitness similarity.  Can be replaced with k-means or DBSCAN for more robust clustering.
        clusters = []
        for i in range(len(population)):
            assigned = False
            for j, cluster in enumerate(clusters):
                if np.linalg.norm(population[i] - cluster[0]) < self.cluster_threshold and abs(fitness_values[i] - cluster[1]) < self.cluster_threshold:
                    clusters[j].append(population[i])
                    clusters[j].append(fitness_values[i])
                    assigned = True
                    break
            if not assigned:
                clusters.append([population[i], fitness_values[i]])
        return clusters


    def _adaptive_gaussian_sampling_per_cluster(self, clusters):
        offspring = []
        for cluster in clusters:
            center = np.mean(np.array(cluster[::2]), axis=0) #Average of solutions in cluster
            num_offspring = len(cluster) //2 # Generate offspring proportional to cluster size
            new_offspring = np.random.normal(center, self.sigma, size=(num_offspring, self.dim))
            offspring.extend(np.clip(new_offspring, self.lower_bounds, self.upper_bounds))
        return np.array(offspring)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 19:57:26 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:57:45 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.4155
2025-06-20 19:57:45 INFO FeHistory: [259687.79031742 157117.46395771 268911.07286623 ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 19:57:45 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:57:45 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithClusteringEA
import numpy as np

# Name: AdaptiveGaussianSamplingWithClusteringEA
# Description: Combines adaptive Gaussian sampling with clustering to escape local optima in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithClusteringEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.cluster_threshold = 0.1 #parameter to adjust clustering sensitivity


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            #Clustering solutions
            clusters = self._cluster_population(population, fitness_values)

            #Adaptive Gaussian Sampling for each cluster
            offspring = self._adaptive_gaussian_sampling_per_cluster(clusters)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Selection
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _cluster_population(self, population, fitness_values):
      # Simple clustering based on distance and fitness similarity.  Can be replaced with k-means or DBSCAN for more robust clustering.
        clusters = []
        for i in range(len(population)):
            assigned = False
            for j, cluster in enumerate(clusters):
                if np.linalg.norm(population[i] - cluster[0]) < self.cluster_threshold and abs(fitness_values[i] - cluster[1]) < self.cluster_threshold:
                    clusters[j].append(population[i])
                    clusters[j].append(fitness_values[i])
                    assigned = True
                    break
            if not assigned:
                clusters.append([population[i], fitness_values[i]])
        return clusters


    def _adaptive_gaussian_sampling_per_cluster(self, clusters):
        offspring = []
        for cluster in clusters:
            center = np.mean(np.array(cluster[::2]), axis=0) #Average of solutions in cluster
            num_offspring = len(cluster) //2 # Generate offspring proportional to cluster size
            new_offspring = np.random.normal(center, self.sigma, size=(num_offspring, self.dim))
            offspring.extend(np.clip(new_offspring, self.lower_bounds, self.upper_bounds))
        return np.array(offspring)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 19:57:45 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:58:05 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.4174
2025-06-20 19:58:05 INFO FeHistory: [106060.9205013  207768.15069975 179396.30874289 ...   -656.78899799
   -656.78899799   -656.78899799]
2025-06-20 19:58:05 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:58:05 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithClusteringEA
import numpy as np

# Name: AdaptiveGaussianSamplingWithClusteringEA
# Description: Combines adaptive Gaussian sampling with clustering to escape local optima in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithClusteringEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.cluster_threshold = 0.1 #parameter to adjust clustering sensitivity


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            #Clustering solutions
            clusters = self._cluster_population(population, fitness_values)

            #Adaptive Gaussian Sampling for each cluster
            offspring = self._adaptive_gaussian_sampling_per_cluster(clusters)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Selection
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _cluster_population(self, population, fitness_values):
      # Simple clustering based on distance and fitness similarity.  Can be replaced with k-means or DBSCAN for more robust clustering.
        clusters = []
        for i in range(len(population)):
            assigned = False
            for j, cluster in enumerate(clusters):
                if np.linalg.norm(population[i] - cluster[0]) < self.cluster_threshold and abs(fitness_values[i] - cluster[1]) < self.cluster_threshold:
                    clusters[j].append(population[i])
                    clusters[j].append(fitness_values[i])
                    assigned = True
                    break
            if not assigned:
                clusters.append([population[i], fitness_values[i]])
        return clusters


    def _adaptive_gaussian_sampling_per_cluster(self, clusters):
        offspring = []
        for cluster in clusters:
            center = np.mean(np.array(cluster[::2]), axis=0) #Average of solutions in cluster
            num_offspring = len(cluster) //2 # Generate offspring proportional to cluster size
            new_offspring = np.random.normal(center, self.sigma, size=(num_offspring, self.dim))
            offspring.extend(np.clip(new_offspring, self.lower_bounds, self.upper_bounds))
        return np.array(offspring)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 19:58:05 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:58:08 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 19:58:49 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0470
2025-06-20 19:58:49 INFO FeHistory: [201.70877877 224.942971   236.95260973 ... -76.44375426 -76.44375427
 -76.44375406]
2025-06-20 19:58:49 INFO Expected Optimum FE: -100
2025-06-20 19:58:49 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithClusteringEA
import numpy as np

# Name: AdaptiveGaussianSamplingWithClusteringEA
# Description: Combines adaptive Gaussian sampling with clustering to escape local optima in multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingWithClusteringEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.cluster_threshold = 0.1 #parameter to adjust clustering sensitivity


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            #Clustering solutions
            clusters = self._cluster_population(population, fitness_values)

            #Adaptive Gaussian Sampling for each cluster
            offspring = self._adaptive_gaussian_sampling_per_cluster(clusters)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Selection
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _cluster_population(self, population, fitness_values):
      # Simple clustering based on distance and fitness similarity.  Can be replaced with k-means or DBSCAN for more robust clustering.
        clusters = []
        for i in range(len(population)):
            assigned = False
            for j, cluster in enumerate(clusters):
                if np.linalg.norm(population[i] - cluster[0]) < self.cluster_threshold and abs(fitness_values[i] - cluster[1]) < self.cluster_threshold:
                    clusters[j].append(population[i])
                    clusters[j].append(fitness_values[i])
                    assigned = True
                    break
            if not assigned:
                clusters.append([population[i], fitness_values[i]])
        return clusters


    def _adaptive_gaussian_sampling_per_cluster(self, clusters):
        offspring = []
        for cluster in clusters:
            center = np.mean(np.array(cluster[::2]), axis=0) #Average of solutions in cluster
            num_offspring = len(cluster) //2 # Generate offspring proportional to cluster size
            new_offspring = np.random.normal(center, self.sigma, size=(num_offspring, self.dim))
            offspring.extend(np.clip(new_offspring, self.lower_bounds, self.upper_bounds))
        return np.array(offspring)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 19:58:49 INFO Unimodal AOCC mean: 0.3738
2025-06-20 19:58:49 INFO Multimodal (single component) AOCC mean: 0.4165
2025-06-20 19:58:49 INFO Multimodal (multiple components) AOCC mean: 0.0470
2025-06-20 19:58:49 INFO AOCC mean: 0.3134
2025-06-20 19:58:57 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 19:59:12 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.3908
2025-06-20 19:59:12 INFO FeHistory: [ 9.04209539e+05  1.17224286e+06  1.18351658e+06 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 19:59:12 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 19:59:12 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithClustering
import numpy as np
from sklearn.cluster import KMeans

class AdaptiveGaussianSamplingEAwithClustering:
    """
    Combines adaptive Gaussian sampling with clustering for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.cluster_count = 5 # Number of clusters

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            clusters = self._cluster_population(population, fitness_values)
            offspring = self._generate_offspring(clusters)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _cluster_population(self, population, fitness_values):
      kmeans = KMeans(n_clusters=self.cluster_count, random_state=0)
      kmeans.fit(population)
      return kmeans.cluster_centers_

    def _generate_offspring(self, cluster_centers):
        offspring = []
        for center in cluster_centers:
            offspring.extend([center + np.random.normal(0, self.sigma, self.dim) for _ in range(self.population_size // self.cluster_count)])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 19:59:12 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:59:29 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.4386
2025-06-20 19:59:29 INFO FeHistory: [260686.6769861  193711.06869612 219756.60210071 ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 19:59:29 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 19:59:29 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithClustering
import numpy as np
from sklearn.cluster import KMeans

class AdaptiveGaussianSamplingEAwithClustering:
    """
    Combines adaptive Gaussian sampling with clustering for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.cluster_count = 5 # Number of clusters

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            clusters = self._cluster_population(population, fitness_values)
            offspring = self._generate_offspring(clusters)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _cluster_population(self, population, fitness_values):
      kmeans = KMeans(n_clusters=self.cluster_count, random_state=0)
      kmeans.fit(population)
      return kmeans.cluster_centers_

    def _generate_offspring(self, cluster_centers):
        offspring = []
        for center in cluster_centers:
            offspring.extend([center + np.random.normal(0, self.sigma, self.dim) for _ in range(self.population_size // self.cluster_count)])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 19:59:29 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 19:59:44 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.4381
2025-06-20 19:59:44 INFO FeHistory: [140658.34155886 130995.70828952 203066.76552666 ...   -656.78899799
   -656.78899799   -656.78899799]
2025-06-20 19:59:44 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 19:59:44 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithClustering
import numpy as np
from sklearn.cluster import KMeans

class AdaptiveGaussianSamplingEAwithClustering:
    """
    Combines adaptive Gaussian sampling with clustering for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.cluster_count = 5 # Number of clusters

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            clusters = self._cluster_population(population, fitness_values)
            offspring = self._generate_offspring(clusters)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _cluster_population(self, population, fitness_values):
      kmeans = KMeans(n_clusters=self.cluster_count, random_state=0)
      kmeans.fit(population)
      return kmeans.cluster_centers_

    def _generate_offspring(self, cluster_centers):
        offspring = []
        for center in cluster_centers:
            offspring.extend([center + np.random.normal(0, self.sigma, self.dim) for _ in range(self.population_size // self.cluster_count)])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 19:59:44 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 19:59:57 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 20:00:25 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0347
2025-06-20 20:00:25 INFO FeHistory: [213.68504655 202.04363012 201.26811681 ... -61.47106351 -61.47106355
 -61.47106351]
2025-06-20 20:00:25 INFO Expected Optimum FE: -100
2025-06-20 20:00:25 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithClustering
import numpy as np
from sklearn.cluster import KMeans

class AdaptiveGaussianSamplingEAwithClustering:
    """
    Combines adaptive Gaussian sampling with clustering for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.cluster_count = 5 # Number of clusters

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            clusters = self._cluster_population(population, fitness_values)
            offspring = self._generate_offspring(clusters)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _cluster_population(self, population, fitness_values):
      kmeans = KMeans(n_clusters=self.cluster_count, random_state=0)
      kmeans.fit(population)
      return kmeans.cluster_centers_

    def _generate_offspring(self, cluster_centers):
        offspring = []
        for center in cluster_centers:
            offspring.extend([center + np.random.normal(0, self.sigma, self.dim) for _ in range(self.population_size // self.cluster_count)])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

2025-06-20 20:00:25 INFO Unimodal AOCC mean: 0.3908
2025-06-20 20:00:25 INFO Multimodal (single component) AOCC mean: 0.4384
2025-06-20 20:00:25 INFO Multimodal (multiple components) AOCC mean: 0.0347
2025-06-20 20:00:25 INFO AOCC mean: 0.3256
2025-06-20 20:00:25 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingEAwithArchive
import numpy as np

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #added archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population,fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population,offspring)),np.concatenate((fitness_values,offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        # Simple archive update: add solutions if not already present (within a tolerance)
        combined = np.column_stack((population, fitness_values)) # Combine population and fitness for easy comparison
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6): # Check for similarity within tolerance
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1]) #Sort by fitness.
        return np.array(new_archive[:self.archive_size]) #Keep only top archive_size individuals

### Worse code
AdaptiveGaussianSamplingEAwithArchive
import numpy as np

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #added archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population,fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population,offspring)),np.concatenate((fitness_values,offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        # Simple archive update: add solutions if not already present (within a tolerance)
        combined = np.column_stack((population, fitness_values)) # Combine population and fitness for easy comparison
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6): # Check for similarity within tolerance
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1]) #Sort by fitness.
        return np.array(new_archive[:self.archive_size]) #Keep only top archive_size individuals

### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `AdaptiveGaussianDEwithArchive`, we see that the best utilizes adaptive Gaussian sampling for both initialization and recombination, leading to more effective exploration of the search space.  The worst uses uniform random initialization and incorporates an archive which doesn't significantly improve performance in this case, potentially adding computational overhead without sufficient benefit. (second best) `AdaptiveGaussianSamplingEAwithArchive` vs (second worst) `AdaptiveGaussianMutationDE`, shows that adding an archive to the adaptive Gaussian sampling approach provides a slight improvement, suggesting that storing and re-utilizing promising solutions can be beneficial, while `AdaptiveGaussianMutationDE` lacks this feature and only uses adaptive Gaussian perturbation in mutation. Comparing (1st) `AdaptiveGaussianSamplingEA` vs (2nd) `AdaptiveGaussianSamplingEAwithArchive`, we see that while the archive helps slightly, the simpler approach without the archive still performs better, suggesting that the added complexity doesn't outweigh the performance gains for this specific problem. (3rd) `AdaptiveGaussianSamplingDE` vs (4th) `AdaptiveGaussianMutationDE`, demonstrates that DE, even with an adaptive Gaussian mutation, doesn't perform as well as the adaptive Gaussian sampling method. The difference might stem from DE's reliance on vector differences that may not be effective in every high-dimensional landscape. Comparing (second worst) `AdaptiveGaussianMutationDE` vs (worst) `AdaptiveGaussianDEwithArchive`, we observe that although both utilize Differential Evolution, adding the archive in the worst performing method is not very helpful in improving its performance compared to the other which lacks it. Overall: Adaptive Gaussian sampling for initialization and recombination proves superior to DE and the inclusion of an archive, while showing only minor performance gains, suggests it is not a critical component for this specific problem.
- Careful consideration of initialization strategies is crucial for high-dimensional problems.  Adaptive methods, informed by problem characteristics, are vital for superior EA performance.  Simplicity and efficiency should be prioritized over unnecessary complexity.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 20:00:25 INFO Generation 24, best so far: 0.5412867233158046
2025-06-20 20:00:25 INFO --- Performing Long-Term Reflection at Generation 24 ---
2025-06-20 20:00:30 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `AdaptiveGaussianDEwithArchive`, we see that incorporating an archive (as in the worst-performing algorithm) doesn't necessarily improve performance in this specific case.  The best-performing algorithm uses a simpler, more focused approach to population initialization and adaptation. `AdaptiveGaussianSamplingEA` directly leverages Gaussian sampling centered around a random point, adapting the spread (sigma) over generations. This adaptive strategy proves more effective. `AdaptiveGaussianDEwithArchive` combines DE, known for its exploitation abilities, with archive and adaptive Gaussian mutation. However, this combination leads to suboptimal performance compared to a simpler adaptive Gaussian approach.

(second best) `AdaptiveGaussianSamplingEAwithArchive` vs (second worst) `AdaptiveGaussianMutationDE`:  The addition of an archive in `AdaptiveGaussianSamplingEAwithArchive` shows a small performance improvement over a pure DE approach (`AdaptiveGaussianMutationDE`).  The archive helps to preserve diversity, which can be beneficial, but the improvement is marginal. `AdaptiveGaussianMutationDE` lacks the adaptive sampling of the population's center, relying solely on uniform random initialization. The adaptive Gaussian perturbation only affects the mutation step, not the initial population's diversity.

Comparing (1st) vs (2nd), we see that the adaptive Gaussian sampling strategy is highly effective, and the addition of an archive offers only a modest performance gain. The archive might be more beneficial in more complex and multimodal landscapes.

(3rd) vs (4th) `AdaptiveGaussianSamplingDE` vs `AdaptiveGaussianMutationDE`: Both use DE, but `AdaptiveGaussianSamplingDE` starts with an adaptively sampled initial population, resulting in slightly better performance. `AdaptiveGaussianMutationDE` only applies adaptive Gaussian perturbation during mutation. This difference in the application of the adaptive mechanism directly impacts performance.

Comparing (second worst) vs (worst), we see that while both utilize archives, the combination of DE and the adaptive Gaussian mutation in `AdaptiveGaussianDEwithArchive` does not compensate for its less effective initialization strategy compared to `AdaptiveGaussianSamplingDE`.  Both have archives, highlighting the marginal benefit in this particular context.

Overall: The adaptive Gaussian sampling for initialization proves crucial, outperforming DE-based methods with or without an archive in this GNBG benchmark scenario.  An archive seems to provide a marginal benefit, but only when paired with already effective sampling mechanisms.


**Experience:**

Adaptive initialization strategies significantly impact performance. Simple, focused approaches, like adaptive Gaussian sampling centered at a random point, can outperform more complex combinations of algorithms.  The usefulness of archives depends heavily on the algorithm and problem landscape.

2025-06-20 20:00:30 INFO Generating offspring via Crossover...
2025-06-20 20:00:40 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 20:00:55 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 20:00:55 INFO FeHistory: [ 710888.33791338 1053967.10818399  993603.00198018 ...    9690.47372591
    9710.35737816   11646.39594122]
2025-06-20 20:00:55 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 20:00:55 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 20:01:10 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 20:01:10 INFO FeHistory: [224108.95489253 229074.57002928 191499.83127181 ...   1591.16463788
   4804.4458703    4009.91894947]
2025-06-20 20:01:10 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 20:01:10 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 20:01:26 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 20:01:26 INFO FeHistory: [212266.80348852 329257.03656287 244008.96631437 ...   6512.19419704
   3832.16831352   5661.71920153]
2025-06-20 20:01:26 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 20:01:26 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 20:01:40 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 20:02:07 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 20:02:07 INFO FeHistory: [188.93331802 195.86866453 242.30058461 ...  40.14341987  29.3767824
  54.32001623]
2025-06-20 20:02:07 INFO Expected Optimum FE: -100
2025-06-20 20:02:07 INFO Unimodal AOCC mean: 0.0000
2025-06-20 20:02:07 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 20:02:07 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 20:02:07 INFO AOCC mean: 0.0000
2025-06-20 20:02:17 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 20:03:17 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 20:05:19 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.6853
2025-06-20 20:05:19 INFO FeHistory: [ 6.47586135e+05  9.29517499e+05  9.10136464e+05 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 20:05:19 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 20:05:19 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for robust multimodal optimization.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        # Adaptive Gaussian Sampling:  Initial population centered around a random point.
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6):
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-20 20:05:19 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 20:08:17 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.7091
2025-06-20 20:08:17 INFO FeHistory: [222069.13861902 278774.14177191 254948.7276422  ...   -912.85737397
   -912.85737397   -912.85737397]
2025-06-20 20:08:17 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 20:08:17 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for robust multimodal optimization.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        # Adaptive Gaussian Sampling:  Initial population centered around a random point.
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6):
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-20 20:08:17 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 20:11:35 INFO Run function 8 complete. FEHistory len: 150000, AOCC: 0.7089
2025-06-20 20:11:35 INFO FeHistory: [174986.63197075 228624.48135387 211219.24711588 ...   -656.78899799
   -656.78899799   -656.78899799]
2025-06-20 20:11:35 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 20:11:35 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for robust multimodal optimization.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        # Adaptive Gaussian Sampling:  Initial population centered around a random point.
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6):
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-20 20:11:35 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 20:15:02 INFO Run function 24 complete. FEHistory len: 150000, AOCC: 0.0408
2025-06-20 20:15:02 INFO FeHistory: [151.8680078  183.04274176 167.11664688 ... -64.53591676 -64.53591676
 -64.53591676]
2025-06-20 20:15:02 INFO Expected Optimum FE: -100
2025-06-20 20:15:02 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveEA
import numpy as np

# Name: AdaptiveGaussianSamplingWithArchiveEA
# Description: Combines adaptive Gaussian sampling with an archive for robust multimodal optimization.
# Code:
class AdaptiveGaussianSamplingWithArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        # Adaptive Gaussian Sampling:  Initial population centered around a random point.
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6):
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-20 20:15:02 INFO Unimodal AOCC mean: 0.6853
2025-06-20 20:15:02 INFO Multimodal (single component) AOCC mean: 0.7090
2025-06-20 20:15:02 INFO Multimodal (multiple components) AOCC mean: 0.0408
2025-06-20 20:15:02 INFO AOCC mean: 0.5360
2025-06-20 20:15:02 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingEAwithArchive
import numpy as np

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #added archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population,fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population,offspring)),np.concatenate((fitness_values,offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        # Simple archive update: add solutions if not already present (within a tolerance)
        combined = np.column_stack((population, fitness_values)) # Combine population and fitness for easy comparison
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6): # Check for similarity within tolerance
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1]) #Sort by fitness.
        return np.array(new_archive[:self.archive_size]) #Keep only top archive_size individuals

### Worse code
AdaptiveGaussianSamplingEAwithArchive
import numpy as np

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #added archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population,fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population,offspring)),np.concatenate((fitness_values,offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        # Simple archive update: add solutions if not already present (within a tolerance)
        combined = np.column_stack((population, fitness_values)) # Combine population and fitness for easy comparison
        new_archive = []
        for sol in combined:
            already_present = False
            for archived_sol in self.archive:
                if np.allclose(sol[:-1], archived_sol[:-1], atol=1e-6): # Check for similarity within tolerance
                    already_present = True
                    break
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1]) #Sort by fitness.
        return np.array(new_archive[:self.archive_size]) #Keep only top archive_size individuals

### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `AdaptiveGaussianDEwithArchive`, we see that incorporating an archive (as in the worst-performing algorithm) doesn't necessarily improve performance in this specific case.  The best-performing algorithm uses a simpler, more focused approach to population initialization and adaptation. `AdaptiveGaussianSamplingEA` directly leverages Gaussian sampling centered around a random point, adapting the spread (sigma) over generations. This adaptive strategy proves more effective. `AdaptiveGaussianDEwithArchive` combines DE, known for its exploitation abilities, with archive and adaptive Gaussian mutation. However, this combination leads to suboptimal performance compared to a simpler adaptive Gaussian approach.

(second best) `AdaptiveGaussianSamplingEAwithArchive` vs (second worst) `AdaptiveGaussianMutationDE`:  The addition of an archive in `AdaptiveGaussianSamplingEAwithArchive` shows a small performance improvement over a pure DE approach (`AdaptiveGaussianMutationDE`).  The archive helps to preserve diversity, which can be beneficial, but the improvement is marginal. `AdaptiveGaussianMutationDE` lacks the adaptive sampling of the population's center, relying solely on uniform random initialization. The adaptive Gaussian perturbation only affects the mutation step, not the initial population's diversity.

Comparing (1st) vs (2nd), we see that the adaptive Gaussian sampling strategy is highly effective, and the addition of an archive offers only a modest performance gain. The archive might be more beneficial in more complex and multimodal landscapes.

(3rd) vs (4th) `AdaptiveGaussianSamplingDE` vs `AdaptiveGaussianMutationDE`: Both use DE, but `AdaptiveGaussianSamplingDE` starts with an adaptively sampled initial population, resulting in slightly better performance. `AdaptiveGaussianMutationDE` only applies adaptive Gaussian perturbation during mutation. This difference in the application of the adaptive mechanism directly impacts performance.

Comparing (second worst) vs (worst), we see that while both utilize archives, the combination of DE and the adaptive Gaussian mutation in `AdaptiveGaussianDEwithArchive` does not compensate for its less effective initialization strategy compared to `AdaptiveGaussianSamplingDE`.  Both have archives, highlighting the marginal benefit in this particular context.

Overall: The adaptive Gaussian sampling for initialization proves crucial, outperforming DE-based methods with or without an archive in this GNBG benchmark scenario.  An archive seems to provide a marginal benefit, but only when paired with already effective sampling mechanisms.
- Adaptive initialization strategies significantly impact performance. Simple, focused approaches, like adaptive Gaussian sampling centered at a random point, can outperform more complex combinations of algorithms.  The usefulness of archives depends heavily on the algorithm and problem landscape.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 20:15:02 INFO Generation 25, best so far: 0.5412867233158046
2025-06-20 20:15:02 INFO --- Performing Long-Term Reflection at Generation 25 ---
2025-06-20 20:15:07 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEA` vs (worst) `AdaptiveGaussianMutationDE`, we see that adaptive Gaussian sampling for initialization (centering the initial population around a random point and then sampling from a Gaussian distribution) significantly outperforms simple uniform random initialization.  The adaptive nature of the Gaussian width (sigma) in the top-ranked algorithms allows for a more focused search as the algorithm progresses. The worst-performing algorithm uses simple uniform random initialization and lacks the adaptive Gaussian sampling.

`(second best)` `AdaptiveGaussianSamplingWithArchiveEA` vs `(second worst)` `AdaptiveGaussianSamplingDE`: The addition of an archive in the second-best algorithm improves the ability to find multiple optima, which is crucial for multimodal functions like GNBG.  The second-worst algorithm uses Differential Evolution, which might struggle in high-dimensional spaces and lacks the archive's ability to store diverse solutions.

Comparing (1st) vs (2nd), we see that while both use adaptive Gaussian sampling, the archive in `AdaptiveGaussianSamplingWithArchiveEA` provides a slight performance boost, suggesting that preserving diverse promising solutions is beneficial.

(3rd) `AdaptiveGaussianSamplingEAwithArchive` vs (4th) `AdaptiveGaussianSamplingDE`: Both use archives, but the former uses a more refined Gaussian sampling approach that, combined with the better selection scheme, yields better results. The DE approach in the 4th ranked algorithm may struggle with the exploration/exploitation balance in a high-dimensional search space.

Comparing (second worst) vs (worst), we see that `AdaptiveGaussianSamplingDE` improves upon `AdaptiveGaussianMutationDE` by introducing a more sophisticated mutation strategy (Differential Evolution) but still doesn't match the performance of the top-ranked algorithms due to lack of the adaptive Gaussian initialization and the archive.  Overall: Adaptive Gaussian sampling for initialization, adaptive sigma adjustment, and the inclusion of an archive are all key factors contributing to the superior performance of the top-ranked algorithms.  Simple random initialization and mutation strategies are less effective, especially in high-dimensional search spaces.

**Experience:**

Adaptive Gaussian initialization is crucial for high-dimensional problems.  Combining this with an archive to maintain diversity and an adaptive mutation strategy significantly improves exploration and exploitation capabilities, leading to better performance in complex search spaces.

2025-06-20 20:15:07 INFO Generating offspring via Crossover...
2025-06-20 20:15:17 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 20:15:17 ERROR Can not run the algorithm
2025-06-20 20:15:17 INFO Run function 4 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 20:15:17 INFO FeHistory: []
2025-06-20 20:15:17 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 20:15:17 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 20:15:17 ERROR Can not run the algorithm
2025-06-20 20:15:17 INFO Run function 7 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 20:15:17 INFO FeHistory: []
2025-06-20 20:15:17 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 20:15:17 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 20:15:17 ERROR Can not run the algorithm
2025-06-20 20:15:17 INFO Run function 8 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 20:15:17 INFO FeHistory: []
2025-06-20 20:15:17 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 20:15:17 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 20:15:17 ERROR Can not run the algorithm
2025-06-20 20:15:17 INFO Run function 24 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 20:15:17 INFO FeHistory: []
2025-06-20 20:15:17 INFO Expected Optimum FE: -100
2025-06-20 20:15:17 INFO Unimodal AOCC mean: 0.0000
2025-06-20 20:15:17 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 20:15:17 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-20 20:15:17 INFO AOCC mean: 0.0000
2025-06-20 20:15:26 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 20:16:26 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-20 20:21:45 INFO Run function 4 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 20:21:45 INFO FeHistory: [1113606.72131939 1268060.501824   1142940.33978028 ...    5751.40975919
    8549.10396742    5448.67594323]
2025-06-20 20:21:45 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 20:21:45 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 20:28:02 INFO Run function 7 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-20 20:28:02 INFO FeHistory: [266325.61749293 392210.97609377 301762.09282251 ...   2139.50946586
   2293.58900891   3070.23226741]
2025-06-20 20:28:02 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 20:28:02 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
