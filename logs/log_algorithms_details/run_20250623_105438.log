2025-06-23 10:54:39 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:54:39 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:54:39 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:54:39 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:54:39 ERROR Can not run the algorithm
2025-06-23 10:54:39 ERROR Can not run the algorithm
2025-06-23 10:54:39 ERROR Can not run the algorithm
2025-06-23 10:54:40 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1754
2025-06-23 10:54:40 INFO FeHistory: [-701.30939114 -701.31556213 -701.31081164 -701.32815977 -701.30908142
 -701.30616717 -701.30447334 -701.33158185 -701.32206744 -701.30724069
 -701.30829345 -701.3370546  -701.32231845 -701.3038442  -701.3256293
 -701.34588369 -701.2987581  -701.31074181 -701.32386932 -701.30703473
 -701.30786463 -701.29375789 -701.32450402 -701.34366798 -701.37193307
 -701.27452325 -701.31306289 -701.31810299 -701.28629947 -701.2962067
 -701.29131021 -701.32436476 -701.30283213 -701.28791152 -701.32253951
 -701.33431363 -701.33118999 -701.29332712 -701.33106817 -701.2836126
 -701.32688447 -701.30056492 -701.32810929 -701.3091254  -701.30472634
 -701.29879518 -701.29316074 -701.30910022 -701.3280772  -701.33479104
 -701.29273042 -701.33659836 -701.31482979 -701.33729124 -701.29544075
 -701.30945231 -701.34254016 -701.3127766  -701.32254252 -701.29506756
 -701.30395136 -701.30811617 -701.32731451 -701.31049832 -701.30690764
 -701.33349953 -701.27872504 -701.30327242 -701.27988924 -701.2770298
 -701.31489245 -701.3141879  -701.3486274  -701.28565262 -701.3076905
 -701.31984966 -701.3110827  -701.29626378 -701.26174354 -701.30328616
 -701.33484545 -701.29216745 -701.29245608 -701.29774339 -701.29747274
 -701.32355385 -701.31623178 -701.28900078 -701.30387453 -701.28228367
 -701.31386277 -701.3156693  -701.29014723 -701.29764364 -701.31710875
 -701.29917371 -701.28503579 -701.30971024 -701.32309555 -701.30549478
 -701.29648211]
2025-06-23 10:54:40 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:54:40 INFO Good algorithm:
Algorithm Name: AdaptiveDEWithFitnessBasedCrowdingAndGaussian
import numpy as np
from scipy.spatial.distance import pdist, squareform

# Name: AdaptiveDEWithFitnessBasedCrowdingAndGaussian
# Description: Differential evolution with fitness-based crowding and Gaussian mutation for multimodal optimization.
# Code:
class AdaptiveDEWithFitnessBasedCrowdingAndGaussian:
    """
    Combines adaptive differential evolution with a fitness-based crowding archive and Gaussian mutation 
    to enhance exploration and exploitation in high-dimensional multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sigma = 0.1*(self.upper_bounds - self.lower_bounds) # Initial Gaussian mutation std dev
        self.sigma_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring = self._gaussian_mutation(offspring) # Add Gaussian mutation
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population = self._crowding(population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(population)
            self.eval_count += self.population_size
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(population, fitness_values)
            self._adapt_parameters(fitness_values)
            self.sigma *= self.sigma_decay # Decay Gaussian mutation

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _gaussian_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_distinct(self, exclude_index, population_size):
        indices = np.random.choice(population_size, 3, replace=False)
        while exclude_index in indices:
            indices = np.random.choice(population_size, 3, replace=False)
        return indices

    def _crossover(self, x, v):
        mask = np.random.rand(self.dim) < self.CR
        return np.where(mask, v, x)

    def _crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined = np.column_stack((np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness))))
        sorted_indices = np.argsort(combined[:, -1])
        return combined[sorted_indices[:self.population_size], :-1]


    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        if len(self.archive) < self.archive_size:
            self.archive = np.concatenate((self.archive, combined))
        else:
            self.archive = self._fitness_based_crowding(self.archive, combined)
        return self.archive

    def _fitness_based_crowding(self, archive, new_solutions):
      combined = np.concatenate((archive, new_solutions))
      sorted_combined = combined[np.argsort(combined[:,-1])]
      return sorted_combined[:self.archive_size]


    def _adapt_parameters(self, fitness_values):
        std_dev = np.std(fitness_values)
        if std_dev < 1e-3:
            self.F *= 1.1
            self.CR *= 0.9
        elif std_dev > 10:
            self.F *= 0.9
            self.CR *= 1.1
        self.F = np.clip(self.F, 0.1, 1.0)
        self.CR = np.clip(self.CR, 0.1, 1.0)

2025-06-23 10:54:40 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1752
2025-06-23 10:54:40 INFO FeHistory: [-701.31654336 -701.32505091 -701.2910536  -701.31152658 -701.30988807
 -701.27609815 -701.2931156  -701.29830025 -701.29398316 -701.30444559
 -701.30576747 -701.32011997 -701.31674348 -701.3028026  -701.2731406
 -701.28036539 -701.36232973 -701.26114999 -701.2931239  -701.3121237
 -701.29932752 -701.32104154 -701.29350846 -701.34074359 -701.28648523
 -701.30604393 -701.30699987 -701.30651768 -701.29769599 -701.2937698
 -701.29915362 -701.33560085 -701.3142005  -701.29925366 -701.29878689
 -701.34309875 -701.34379369 -701.29648561 -701.28576218 -701.31844238
 -701.32155511 -701.32467263 -701.30473303 -701.3226001  -701.28849422
 -701.30657378 -701.32118162 -701.31424491 -701.29101394 -701.29418943
 -701.30123283 -701.27652031 -701.29306394 -701.30919442 -701.33700549
 -701.30566075 -701.31469823 -701.32772573 -701.28858956 -701.29994988
 -701.31400408 -701.29291341 -701.31936029 -701.2861958  -701.30252269
 -701.29098541 -701.28980978 -701.29297512 -701.29813546 -701.3253801
 -701.32996386 -701.30062569 -701.33950168 -701.32242322 -701.30541577
 -701.33676387 -701.3007083  -701.30537482 -701.29879807 -701.26659346
 -701.30814232 -701.30302318 -701.29318025 -701.3106662  -701.30846206
 -701.30760648 -701.33617163 -701.30468587 -701.32245542 -701.30303507
 -701.31485756 -701.3375403  -701.30685485 -701.29271198 -701.28931201
 -701.28478391 -701.29414703 -701.30427624 -701.26952611 -701.29834502
 -701.27319845]
2025-06-23 10:54:40 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:54:40 INFO Good algorithm:
Algorithm Name: AdaptiveDEWithFitnessBasedCrowdingAndGaussian
import numpy as np
from scipy.spatial.distance import pdist, squareform

# Name: AdaptiveDEWithFitnessBasedCrowdingAndGaussian
# Description: Differential evolution with adaptive parameters, fitness-based crowding, and Gaussian mutation for multimodal optimization.
# Code:
class AdaptiveDEWithFitnessBasedCrowdingAndGaussian:
    """
    Combines adaptive differential evolution, fitness-based crowding archive, and Gaussian mutation for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []
        self.adaptive_factor = 1.0
        self.gaussian_mutation_rate = 0.1 #probability of applying gaussian mutation


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population = self._crowding(population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(population)
            self.eval_count += self.population_size
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(population, fitness_values)
            self._adapt_parameters(fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            #Gaussian mutation
            if np.random.rand() < self.gaussian_mutation_rate:
                trial += np.random.normal(0, self.adaptive_factor * 0.1 * (self.upper_bounds - self.lower_bounds), self.dim)
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
            offspring[i] = trial
        return offspring

    def _select_distinct(self, exclude_index, population_size):
        indices = np.random.choice(population_size, 3, replace=False)
        while exclude_index in indices:
            indices = np.random.choice(population_size, 3, replace=False)
        return indices

    def _crossover(self, x, v):
        mask = np.random.rand(self.dim) < self.CR
        return np.where(mask, v, x)

    def _crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined = np.vstack((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness))
        
        new_population = []
        for i in range(0, len(combined), 2):
            parent1 = combined[i]
            parent2 = combined[i+1]
            fit1 = combined_fitness[i]
            fit2 = combined_fitness[i+1]
            if fit1 < fit2:
                new_population.append(parent1)
            else:
                new_population.append(parent2)

        return np.array(new_population[:self.population_size])


    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        if len(self.archive) < self.archive_size:
            self.archive = np.concatenate((self.archive, combined))
        else:
            distances = pdist(self.archive[:, :-1])
            distances = squareform(distances)
            for sol in combined:
                closest = np.argmin(distances[self.archive.shape[0]-1,:])
                if sol[-1] < self.archive[closest][-1]:
                    self.archive[closest] = sol
        return self.archive

    def _adapt_parameters(self, fitness_values):
        std_dev = np.std(fitness_values)
        if std_dev < 1e-3:
            self.F *= 1.1
            self.CR *= 0.9
        elif std_dev > 10:
            self.F *= 0.9
            self.CR *= 1.1
        self.F = np.clip(self.F, 0.1, 1.0)
        self.CR = np.clip(self.CR, 0.1, 1.0)
        self.adaptive_factor = 1.0 / (1 + std_dev) #Adaptive factor based on diversity


2025-06-23 10:54:40 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1751
2025-06-23 10:54:40 INFO FeHistory: [-701.29574459 -701.31789122 -701.33762924 -701.28587786 -701.34021842
 -701.31079577 -701.35531564 -701.33253089 -701.32077653 -701.3172586
 -701.32650764 -701.35669145 -701.3025774  -701.28007395 -701.30330716
 -701.30287124 -701.30312813 -701.29986098 -701.32025621 -701.31581801
 -701.31355794 -701.31964648 -701.27003471 -701.31820038 -701.30675718
 -701.3459343  -701.28760456 -701.32736171 -701.3032386  -701.30284103
 -701.31176642 -701.32444391 -701.32072912 -701.33060485 -701.31660159
 -701.30650372 -701.29441041 -701.29644834 -701.29103305 -701.29804906
 -701.34115458 -701.30389294 -701.32007368 -701.29461655 -701.33659574
 -701.32330631 -701.31730143 -701.33572951 -701.29195356 -701.32657825
 -701.29388905 -701.30218071 -701.28524833 -701.29583358 -701.32838083
 -701.29681301 -701.31695331 -701.28839108 -701.3461259  -701.31143938
 -701.28605354 -701.30983527 -701.29013501 -701.30940378 -701.32116594
 -701.34096659 -701.28340673 -701.28796762 -701.30840964 -701.29179109
 -701.30363358 -701.33269669 -701.3020249  -701.29952122 -701.31243309
 -701.33290396 -701.29305728 -701.28347943 -701.29980258 -701.31337254
 -701.28311153 -701.33395234 -701.30263724 -701.31943256 -701.30334041
 -701.30928059 -701.31799005 -701.27507825 -701.28640947 -701.30275539
 -701.32134603 -701.30983796 -701.28908946 -701.32460131 -701.32150158
 -701.28066575 -701.31476433 -701.28968686 -701.30756025 -701.28441407
 -701.30818351]
2025-06-23 10:54:40 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:54:40 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithFitnessBasedCrowdingAndLHS
import numpy as np
from scipy.stats import qmc
from scipy.spatial.distance import cdist

# Name: AdaptiveDEwithFitnessBasedCrowdingAndLHS
# Description: Adaptive Differential Evolution with fitness-based crowding and Latin Hypercube Sampling for multimodal optimization.
# Code:
class AdaptiveDEwithFitnessBasedCrowdingAndLHS:
    """
    Combines adaptive Differential Evolution (DE), fitness-based crowding, and Latin Hypercube Sampling (LHS)
    for efficient multimodal optimization.  Adapts parameters based on population diversity and success rate.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.archive = []
        self.diversity_threshold = 0.2 * (self.upper_bounds.mean() - self.lower_bounds.mean())
        self.exploration_rate = 0.8
        self.crowding_factor = 0.1 # Controls crowding pressure


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:
                a, b, c = self._select_different_archive(i)
            else:
                a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _select_different_archive(self, exclude):
        candidates = list(range(len(self.archive)))
        if len(candidates) < 3:
            return self._select_different(exclude)
        np.random.shuffle(candidates)
        return [self.archive[i][:-1] for i in candidates[:3]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        #Crowding based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            distances = cdist(sorted_data[i, :-1].reshape(1, -1), np.array([x[:-1] for x in new_archive]))
            closest_index = np.argmin(distances)
            if distances[closest_index] > self.crowding_factor * (np.max(self.upper_bounds) - np.min(self.lower_bounds)):
                new_archive.append(sorted_data[i])
            elif sorted_data[i,-1] < new_archive[closest_index][-1]:
                new_archive[closest_index] = sorted_data[i]
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        diversity = np.std(population, axis=0).mean()
        if diversity < self.diversity_threshold:
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))
2025-06-23 10:54:40 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:54:40 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:54:40 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:54:40 ERROR Can not run the algorithm
2025-06-23 10:54:40 ERROR Can not run the algorithm
2025-06-23 10:54:40 ERROR Can not run the algorithm
2025-06-23 10:54:40 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1025
2025-06-23 10:54:40 INFO FeHistory: [-222.55660595 -223.98438423 -220.99698712 -223.13234273 -221.63040477
 -222.08361756 -220.47062274 -220.84884463 -222.13614692 -223.5828951
 -222.54363326 -222.24168765 -222.36051345 -222.06706309 -222.70979951
 -222.52280099 -222.86536715 -222.88257508 -222.90494144 -223.60531791
 -222.05278046 -222.27743747 -222.07125607 -221.58694959 -221.41631485
 -221.0398169  -221.97706892 -222.31908997 -221.50181537 -222.42870326
 -221.91335013 -221.75488973 -222.77416595 -223.02352321 -221.83956472
 -221.18196458 -222.33001708 -221.61230366 -221.49016217 -220.69734767
 -221.98545593 -222.00647594 -222.1694402  -221.85904078 -224.84927174
 -223.20179164 -221.47485593 -222.31502559 -220.32653338 -221.10293595
 -222.24913005 -222.97062773 -220.8106403  -220.4957331  -222.22391064
 -222.61724391 -221.53023245 -221.64116618 -222.51844712 -221.78352268
 -223.17268299 -222.77702577 -222.27088002 -221.55222981 -222.21686967
 -220.21791896 -221.54437194 -222.06451257 -221.99046296 -221.13872854
 -221.27693006 -221.85403544 -222.04579981 -222.88205293 -221.75044401
 -222.19755213 -222.02250756 -221.37522707 -222.59257318 -221.42608545
 -221.4919567  -221.2204046  -221.82967914 -220.99586235 -222.97335533
 -222.66863424 -222.98422107 -222.01750097 -221.26201697 -221.64603027
 -223.65467962 -221.99875377 -221.58969502 -220.0673055  -224.00996348
 -220.32440708 -222.72467054 -221.99327109 -222.47272854 -221.95780606
 -222.09904569]
2025-06-23 10:54:40 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:54:40 INFO Good algorithm:
Algorithm Name: AdaptiveDEWithFitnessBasedCrowdingAndGaussian
import numpy as np
from scipy.spatial.distance import pdist, squareform

# Name: AdaptiveDEWithFitnessBasedCrowdingAndGaussian
# Description: Differential evolution with adaptive parameters, fitness-based crowding, and Gaussian mutation for multimodal optimization.
# Code:
class AdaptiveDEWithFitnessBasedCrowdingAndGaussian:
    """
    Combines adaptive differential evolution, fitness-based crowding archive, and Gaussian mutation for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []
        self.adaptive_factor = 1.0
        self.gaussian_mutation_rate = 0.1 #probability of applying gaussian mutation


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population = self._crowding(population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(population)
            self.eval_count += self.population_size
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(population, fitness_values)
            self._adapt_parameters(fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            #Gaussian mutation
            if np.random.rand() < self.gaussian_mutation_rate:
                trial += np.random.normal(0, self.adaptive_factor * 0.1 * (self.upper_bounds - self.lower_bounds), self.dim)
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
            offspring[i] = trial
        return offspring

    def _select_distinct(self, exclude_index, population_size):
        indices = np.random.choice(population_size, 3, replace=False)
        while exclude_index in indices:
            indices = np.random.choice(population_size, 3, replace=False)
        return indices

    def _crossover(self, x, v):
        mask = np.random.rand(self.dim) < self.CR
        return np.where(mask, v, x)

    def _crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined = np.vstack((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness))
        
        new_population = []
        for i in range(0, len(combined), 2):
            parent1 = combined[i]
            parent2 = combined[i+1]
            fit1 = combined_fitness[i]
            fit2 = combined_fitness[i+1]
            if fit1 < fit2:
                new_population.append(parent1)
            else:
                new_population.append(parent2)

        return np.array(new_population[:self.population_size])


    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        if len(self.archive) < self.archive_size:
            self.archive = np.concatenate((self.archive, combined))
        else:
            distances = pdist(self.archive[:, :-1])
            distances = squareform(distances)
            for sol in combined:
                closest = np.argmin(distances[self.archive.shape[0]-1,:])
                if sol[-1] < self.archive[closest][-1]:
                    self.archive[closest] = sol
        return self.archive

    def _adapt_parameters(self, fitness_values):
        std_dev = np.std(fitness_values)
        if std_dev < 1e-3:
            self.F *= 1.1
            self.CR *= 0.9
        elif std_dev > 10:
            self.F *= 0.9
            self.CR *= 1.1
        self.F = np.clip(self.F, 0.1, 1.0)
        self.CR = np.clip(self.CR, 0.1, 1.0)
        self.adaptive_factor = 1.0 / (1 + std_dev) #Adaptive factor based on diversity


2025-06-23 10:54:40 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.0993
2025-06-23 10:54:40 INFO FeHistory: [-222.93803648 -221.66253424 -221.65823686 -221.42965878 -221.86172628
 -222.90962704 -222.27981433 -222.3447224  -220.23251889 -221.48983038
 -222.78388836 -222.4355594  -221.21817571 -221.57915084 -222.08301842
 -222.69272809 -224.12695896 -222.36443076 -221.13771308 -221.82080892
 -222.41680077 -222.09439515 -221.62339803 -221.8312776  -222.07076356
 -220.67142156 -223.80830658 -222.85425157 -220.59708211 -221.60385439
 -221.46673779 -222.71020834 -222.41170267 -221.42292465 -221.80652702
 -221.2756127  -222.32463932 -221.56251953 -222.25672431 -221.68451806
 -220.97853057 -222.72507135 -223.29909192 -221.65202851 -220.08599898
 -222.59435181 -222.8199904  -222.71410979 -222.61490491 -221.51379928
 -221.06631793 -221.3928748  -222.46864228 -221.93545342 -221.92526324
 -222.35146101 -221.4771586  -220.71023102 -222.18212177 -221.67480639
 -223.52749178 -220.70653457 -221.50586115 -221.80873537 -220.56936113
 -221.50914379 -220.51484335 -222.6877273  -223.52706691 -221.40695846
 -221.53375277 -223.31702864 -222.75028761 -222.87820434 -222.7171371
 -222.72007107 -223.01616448 -223.21609955 -222.08230512 -222.67260342
 -221.75779568 -222.01014817 -222.70192165 -220.9639457  -220.97436735
 -222.76091048 -223.41162638 -221.2957479  -222.15043025 -223.08877032
 -222.30789302 -222.44839628 -222.7855113  -222.85694705 -223.14806592
 -222.9439715  -221.5365825  -221.92666477 -223.6895317  -220.1507612
 -222.18502348]
2025-06-23 10:54:40 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:54:40 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1071
2025-06-23 10:54:40 INFO FeHistory: [-223.07918356 -222.16052021 -222.56958898 -221.1028619  -221.82544647
 -220.66432537 -221.51020812 -222.03050949 -223.4180909  -222.31284123
 -221.49296339 -222.51008599 -221.84039564 -223.28798985 -221.32429343
 -222.93804159 -222.69856563 -221.99532113 -222.51537222 -221.14984674
 -222.80470702 -223.04828065 -222.29797395 -221.50384866 -221.17528458
 -222.86020791 -222.50891183 -222.29788563 -225.78962929 -221.81009084
 -221.66341393 -221.91331723 -221.98924632 -221.17428955 -221.16763923
 -221.69841268 -221.51477788 -220.84541025 -221.20621643 -223.04656131
 -222.51862909 -222.12075418 -222.40983344 -221.790759   -221.52131008
 -222.16167661 -223.19386505 -221.6878292  -223.22679777 -221.76403768
 -220.70916714 -222.95675967 -222.3149323  -221.38316295 -221.63665139
 -221.20088566 -220.99995367 -222.63804596 -221.0881043  -222.63558144
 -221.87733698 -221.75139179 -222.39920577 -220.35817807 -220.78703986
 -222.49563797 -223.11812947 -220.71754823 -224.08092616 -221.72987731
 -220.86802345 -222.50902531 -222.26678858 -221.57455134 -223.44273801
 -221.29914175 -223.43558137 -222.04515364 -221.19567616 -223.1648965
 -220.82247536 -222.54838088 -222.69122321 -222.68844978 -222.7410132
 -222.31000194 -222.17798209 -223.54967875 -221.39063018 -221.06523945
 -220.32681582 -221.38306939 -221.31992751 -222.30637129 -222.54283249
 -221.94815396 -220.97902272 -221.39451542 -221.61129453 -220.99971924
 -222.56884162]
2025-06-23 10:54:40 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:54:40 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithFitnessBasedCrowdingAndLHS
import numpy as np
from scipy.stats import qmc
from scipy.spatial.distance import cdist

# Name: AdaptiveDEwithFitnessBasedCrowdingAndLHS
# Description: Adaptive Differential Evolution with fitness-based crowding and Latin Hypercube Sampling for multimodal optimization.
# Code:
class AdaptiveDEwithFitnessBasedCrowdingAndLHS:
    """
    Combines adaptive Differential Evolution (DE), fitness-based crowding, and Latin Hypercube Sampling (LHS)
    for efficient multimodal optimization.  Adapts parameters based on population diversity and success rate.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.archive = []
        self.diversity_threshold = 0.2 * (self.upper_bounds.mean() - self.lower_bounds.mean())
        self.exploration_rate = 0.8
        self.crowding_factor = 0.1 # Controls crowding pressure


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:
                a, b, c = self._select_different_archive(i)
            else:
                a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _select_different_archive(self, exclude):
        candidates = list(range(len(self.archive)))
        if len(candidates) < 3:
            return self._select_different(exclude)
        np.random.shuffle(candidates)
        return [self.archive[i][:-1] for i in candidates[:3]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        #Crowding based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            distances = cdist(sorted_data[i, :-1].reshape(1, -1), np.array([x[:-1] for x in new_archive]))
            closest_index = np.argmin(distances)
            if distances[closest_index] > self.crowding_factor * (np.max(self.upper_bounds) - np.min(self.lower_bounds)):
                new_archive.append(sorted_data[i])
            elif sorted_data[i,-1] < new_archive[closest_index][-1]:
                new_archive[closest_index] = sorted_data[i]
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        diversity = np.std(population, axis=0).mean()
        if diversity < self.diversity_threshold:
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))
2025-06-23 10:54:40 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:54:40 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:54:40 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:54:40 ERROR Can not run the algorithm
2025-06-23 10:54:40 ERROR Can not run the algorithm
2025-06-23 10:54:40 ERROR Can not run the algorithm
2025-06-23 10:54:40 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 10:54:40 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 10:54:40 INFO FeHistory: [203.51924367 200.11405235 218.76443094 138.075978   176.93841815
 169.45294947 189.36803198 164.3779489  155.12055383 197.44806819
 182.55033887 179.61660398 157.7199774  191.58006857 177.90401787
 156.32503145 187.81870778 169.13139152 181.89727515 235.93774025
 211.51873899 175.36071082 184.94586329 180.35042439 161.20542938
 186.36497949 200.06874249 203.12432077 198.55823532 162.87660404
 198.52334662 199.590745   167.21899773 195.15598861 179.85678601
 199.27623064 154.13770364 201.01989961 179.52727826 224.69662485
 218.58592778 164.93219713 227.26395904 210.12314399 173.48607483
 196.00760575 179.98405016 216.05524769 201.27509664 172.40185911
 178.13510125 224.52833521 201.59771077 151.61163699 219.62809647
 173.16884075 200.29999122 189.75146565 194.06236847 169.26251885
 174.9574919  193.0421741  168.08225322 189.73530033 164.72868243
 189.49406701 216.03269805 185.2186017  169.338066   207.73933455
 182.95938582 190.17197284 155.2906335  190.18042585 222.20655794
 180.7980359  209.87846139 230.97186832 185.68433158 174.7396706
 174.43968748 159.62183503 210.17838828 178.85548384 185.58313253
 171.34680804 167.16075098 198.29207921 187.90730423 161.88625268
 235.01560201 193.87484227 177.05841053 195.51215007 179.50515943
 173.20347083 170.59571902 184.55855311 204.76181601 197.95631921
 144.04832722]
2025-06-23 10:54:40 INFO Expected Optimum FE: -100
2025-06-23 10:54:40 INFO Unimodal AOCC mean: 0.1751
2025-06-23 10:54:40 INFO Multimodal (single component) AOCC mean: 0.1071
2025-06-23 10:54:40 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:54:40 INFO FeHistory: [197.82191224 175.31136429 180.19264097 164.57815523 195.67470527
 207.06338202 192.33818095 191.71055335 202.94517423 183.50221027
 199.60254936 177.85451147 174.71970808 148.01822878 188.82786837
 186.89075517 176.37862394 204.91718518 180.92842864 156.95205122
 187.59686576 190.78052928 190.92846028 197.17167788 202.4153186
 208.33715239 213.2382895  182.19004438 181.70163285 204.01110558
 230.25784876 198.48305942 205.2261678  194.7271587  158.87026676
 162.49717841 160.36311836 201.11901292 192.0434962  179.74700834
 136.32897329 170.24682111 173.75639746 167.88959061 171.96414963
 161.67715325 199.73628413 166.41692659 183.33718898 176.02307615
 192.81385248 198.06497348 204.08121476 180.44076848 185.30790686
 171.34083296 178.02996911 218.80085166 200.35738725 170.56443582
 155.36680995 180.38558372 199.41304236 180.10527879 211.86496947
 198.21095101 180.97645977 198.19893072 206.36589776 185.8860177
 194.86190704 174.77917154 192.34423175 191.93849412 198.10495203
 150.60236585 197.00414401 203.26920283 181.64459036 183.67894691
 129.14519046 172.49425471 148.44788724 183.16253292 171.9635692
 215.81676133 188.73658324 218.21021489 194.16063455 183.80233822
 175.14501225 228.09648922 186.0066392  176.01962251 180.53415635
 174.53951363 202.84362545 232.03928967 171.9174283  180.12101192
 179.97059155]
2025-06-23 10:54:40 INFO AOCC mean: 0.0941
2025-06-23 10:54:40 INFO Expected Optimum FE: -100
2025-06-23 10:54:40 INFO Unimodal AOCC mean: 0.1754
2025-06-23 10:54:40 INFO Multimodal (single component) AOCC mean: 0.0993
2025-06-23 10:54:40 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:54:40 INFO AOCC mean: 0.0916
2025-06-23 10:54:40 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 10:54:40 INFO FeHistory: [206.56601503 205.76988391 198.97466461 197.84503484 158.83724984
 151.04133009 243.40458152 207.63762295 194.86066151 160.53441712
 210.07276466 195.02617506 241.97555693 177.64773694 185.68566958
 185.25201818 183.70459329 207.83213693 193.18209561 194.47199788
 205.41145061 190.64399732 185.91960131 189.61949884 152.6534635
 198.90834082 162.8858066  204.82172561 191.93426981 175.09785552
 154.65235301 223.25324554 204.48626492 179.83625985 229.74047242
 167.29044447 222.00699359 182.72487168 160.73427571 175.5304482
 184.81657332 187.14606649 171.11553819 163.28446416 189.54491762
 143.12115724 191.77737501 178.6898783  169.00437355 197.87814591
 194.50530644 182.07045778 194.15345631 208.47588706 178.5890158
 189.4923278  189.26916285 185.08819368 195.80110927 176.71347294
 192.49727175 165.16391207 175.06415533 214.20718863 166.17392141
 140.7661007  191.71911277 164.8382994  224.8883719  206.94215998
 199.95943553 193.5327636  216.32068201 183.12533843 202.21402023
 195.07696321 211.36260789 210.70123797 181.45630848 178.55286919
 170.03777971 175.40305298 185.50582539 181.14314898 234.1308273
 210.01372097 192.99717737 210.30451116 175.5641151  165.32840358
 185.06159057 166.9721606  207.710473   199.3176584  198.93388367
 154.31163612 193.2082538  184.29026935 196.87584341 212.74176923
 183.99822356]
2025-06-23 10:54:40 INFO Expected Optimum FE: -100
2025-06-23 10:54:40 INFO Unimodal AOCC mean: 0.1752
2025-06-23 10:54:40 INFO Multimodal (single component) AOCC mean: 0.1025
2025-06-23 10:54:40 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:54:40 INFO AOCC mean: 0.0926
2025-06-23 10:54:45 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1779
2025-06-23 10:54:45 INFO FeHistory: [-701.2879636  -701.32542565 -701.3323076  ... -701.50794643 -701.50808452
 -701.50794291]
2025-06-23 10:54:45 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:54:45 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_FitnessCrowding
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_FitnessCrowding
# Description: Combines adaptive Differential Evolution, Latin Hypercube Sampling, and a fitness-based crowding archive for robust multimodal optimization.
# Code:
class AdaptiveDE_LHS_FitnessCrowding:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8  # Initial exploration rate
        self.diversity_threshold = 0.2  # Threshold for diversity control

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        sorted_data = combined[combined[:, -1].argsort()]
        new_archive = [sorted_data[0]]
        for i in range(1,len(sorted_data)):
            if sorted_data[i,-1] < new_archive[-1][-1]:
                new_archive[-1] = sorted_data[i]
            else:
                new_archive.append(sorted_data[i])

        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Diversity based adaptation
        diversity = np.std(population, axis=0).mean()
        if diversity < self.diversity_threshold * (np.max(self.upper_bounds) - np.min(self.lower_bounds)):
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        #Success rate based adaptation
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 10:54:45 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:54:50 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1085
2025-06-23 10:54:50 INFO FeHistory: [-222.01581087 -220.87864131 -221.39507115 ... -227.14249782 -226.87190969
 -224.67990508]
2025-06-23 10:54:50 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:54:50 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_FitnessCrowding
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_FitnessCrowding
# Description: Combines adaptive Differential Evolution, Latin Hypercube Sampling, and a fitness-based crowding archive for robust multimodal optimization.
# Code:
class AdaptiveDE_LHS_FitnessCrowding:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8  # Initial exploration rate
        self.diversity_threshold = 0.2  # Threshold for diversity control

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        sorted_data = combined[combined[:, -1].argsort()]
        new_archive = [sorted_data[0]]
        for i in range(1,len(sorted_data)):
            if sorted_data[i,-1] < new_archive[-1][-1]:
                new_archive[-1] = sorted_data[i]
            else:
                new_archive.append(sorted_data[i])

        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Diversity based adaptation
        diversity = np.std(population, axis=0).mean()
        if diversity < self.diversity_threshold * (np.max(self.upper_bounds) - np.min(self.lower_bounds)):
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        #Success rate based adaptation
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 10:54:50 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:55:07 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 10:55:07 INFO FeHistory: [190.41899645 206.01382408 197.56736453 ...  55.73646718  60.9438414
  62.20297013]
2025-06-23 10:55:07 INFO Expected Optimum FE: -100
2025-06-23 10:55:07 INFO Unimodal AOCC mean: 0.1779
2025-06-23 10:55:07 INFO Multimodal (single component) AOCC mean: 0.1085
2025-06-23 10:55:07 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:55:07 INFO AOCC mean: 0.0955
2025-06-23 10:58:04 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:58:04 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:00:56 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1805
2025-06-23 11:00:56 INFO FeHistory: [-701.35849381 -701.27806425 -701.32765176 ... -701.59455731 -701.58218163
 -701.56637346]
2025-06-23 11:00:56 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:00:56 INFO Good algorithm:
Algorithm Name: AdaptiveLHS_DE_ArchiveEA
import numpy as np
from scipy.stats import qmc

class AdaptiveLHS_DE_ArchiveEA:
    """
    Combines Adaptive Latin Hypercube Sampling (LHS), Differential Evolution (DE), and a fitness-based archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9 # DE crossover rate
        self.lhs_sampler = qmc.LatinHypercube(self.dim)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sample = self.lhs_sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i, self.population_size)
            v = population[a] + self.F * (population[b] - population[c])
            offspring[i] = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, population[i]), self.lower_bounds, self.upper_bounds)
        return offspring


    def _select_different(self, i, n):
        candidates = np.arange(n)
        candidates = candidates[candidates != i]
        np.random.shuffle(candidates)
        return candidates[:3]

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])  # Sort by fitness
        return np.array(new_archive[:self.archive_size])

2025-06-23 11:00:56 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:02:16 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.2032
2025-06-23 11:02:16 INFO FeHistory: [-701.32080784 -701.32280136 -701.33847746 ... -702.64422498 -702.64744398
 -702.64999606]
2025-06-23 11:02:16 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:02:16 INFO Good algorithm:
Algorithm Name: FitnessShapingLHS_EA
import numpy as np
from scipy.stats import qmc

# Name: FitnessShapingLHS_EA
# Description: An evolutionary algorithm using fitness-shaping, Latin Hypercube Sampling, and adaptive Gaussian mutation for multimodal optimization.

class FitnessShapingLHS_EA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []
        self.fitness_scaling_factor = 1.0 # Initialize fitness scaling factor

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        sampler = qmc.LatinHypercube(self.dim)
        initial_samples = sampler.random(self.population_size)
        population = initial_samples * (self.upper_bounds - self.lower_bounds) + self.lower_bounds
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )
            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay
            self.fitness_scaling_factor = 1.0 + 0.1* (1 - self.eval_count/self.budget) # Adaptive Fitness Scaling

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        # Fitness Shaping: scale fitness values based on current progress

        scaled_fitness = combined_fit**self.fitness_scaling_factor # Apply fitness scaling
        sorted_indices = np.argsort(scaled_fitness)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 11:02:16 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:03:47 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1085
2025-06-23 11:03:47 INFO FeHistory: [-221.48640531 -222.21006057 -221.70561855 ... -223.55033359 -222.41610187
 -221.12879232]
2025-06-23 11:03:47 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:03:47 INFO Good algorithm:
Algorithm Name: AdaptiveLHS_DE_ArchiveEA
import numpy as np
from scipy.stats import qmc

class AdaptiveLHS_DE_ArchiveEA:
    """
    Combines Adaptive Latin Hypercube Sampling (LHS), Differential Evolution (DE), and a fitness-based archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9 # DE crossover rate
        self.lhs_sampler = qmc.LatinHypercube(self.dim)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sample = self.lhs_sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i, self.population_size)
            v = population[a] + self.F * (population[b] - population[c])
            offspring[i] = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, population[i]), self.lower_bounds, self.upper_bounds)
        return offspring


    def _select_different(self, i, n):
        candidates = np.arange(n)
        candidates = candidates[candidates != i]
        np.random.shuffle(candidates)
        return candidates[:3]

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])  # Sort by fitness
        return np.array(new_archive[:self.archive_size])

2025-06-23 11:03:47 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:11:52 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1286
2025-06-23 11:11:52 INFO FeHistory: [-221.19460887 -220.7734001  -221.21979272 ... -229.94023257 -229.94023238
 -229.94023323]
2025-06-23 11:11:52 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:11:52 INFO Good algorithm:
Algorithm Name: FitnessShapingLHS_EA
import numpy as np
from scipy.stats import qmc

# Name: FitnessShapingLHS_EA
# Description: An evolutionary algorithm using fitness-shaping, Latin Hypercube Sampling, and adaptive Gaussian mutation for multimodal optimization.

class FitnessShapingLHS_EA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []
        self.fitness_scaling_factor = 1.0 # Initialize fitness scaling factor

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        sampler = qmc.LatinHypercube(self.dim)
        initial_samples = sampler.random(self.population_size)
        population = initial_samples * (self.upper_bounds - self.lower_bounds) + self.lower_bounds
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )
            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay
            self.fitness_scaling_factor = 1.0 + 0.1* (1 - self.eval_count/self.budget) # Adaptive Fitness Scaling

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        # Fitness Shaping: scale fitness values based on current progress

        scaled_fitness = combined_fit**self.fitness_scaling_factor # Apply fitness scaling
        sorted_indices = np.argsort(scaled_fitness)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 11:11:52 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:12:13 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 11:12:13 INFO FeHistory: [160.78515996 194.16547848 188.85296057 ...  61.4157976   62.97364285
  87.87191458]
2025-06-23 11:12:13 INFO Expected Optimum FE: -100
2025-06-23 11:12:13 INFO Unimodal AOCC mean: 0.1805
2025-06-23 11:12:13 INFO Multimodal (single component) AOCC mean: 0.1085
2025-06-23 11:12:13 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:12:13 INFO AOCC mean: 0.0963
2025-06-23 11:15:10 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0127
2025-06-23 11:15:10 INFO FeHistory: [ 1.50708491e+02  2.04804904e+02  1.85231545e+02 ... -1.69813790e-04
  8.44469240e-04  1.48480751e-03]
2025-06-23 11:15:10 INFO Expected Optimum FE: -100
2025-06-23 11:15:10 INFO Unimodal AOCC mean: 0.2032
2025-06-23 11:15:10 INFO Multimodal (single component) AOCC mean: 0.1286
2025-06-23 11:15:10 INFO Multimodal (multiple components) AOCC mean: 0.0127
2025-06-23 11:15:10 INFO AOCC mean: 0.1148
2025-06-23 11:16:34 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:16:34 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:16:34 ERROR Can not run the algorithm
2025-06-23 11:16:34 ERROR Can not run the algorithm
2025-06-23 11:16:34 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1750
2025-06-23 11:16:34 INFO FeHistory: [-701.31440257 -701.30287598 -701.30452264 -701.31014196 -701.31955252
 -701.32330117 -701.33549282 -701.32400927 -701.30790797 -701.30384348
 -701.29044516 -701.33741521 -701.31989269 -701.29388825 -701.34530966
 -701.27455909 -701.30500788 -701.30912513 -701.29251853 -701.3000762
 -701.33692296 -701.31850547 -701.30768089 -701.30818522 -701.32661081
 -701.30449236 -701.28779145 -701.2732146  -701.30587093 -701.33657481
 -701.2949568  -701.32689465 -701.30612879 -701.34294054 -701.311423
 -701.30394222 -701.31545339 -701.29193872 -701.32796965 -701.27603168
 -701.34794782 -701.29159499 -701.29822987 -701.32984911 -701.31461755
 -701.34971196 -701.30178538 -701.31861069 -701.29731502 -701.29294368
 -701.28491333 -701.29177775 -701.29977268 -701.31168873 -701.32556138
 -701.33538281 -701.28403538 -701.26430059 -701.30092183 -701.28829428
 -701.34690416 -701.3124631  -701.34198084 -701.3415385  -701.32595386
 -701.28472784 -701.28711398 -701.29982907 -701.30554844 -701.29914891
 -701.30197004 -701.29930466 -701.30093983 -701.28226365 -701.33442387
 -701.31251517 -701.28062447 -701.31228066 -701.29300504 -701.30060632
 -701.29229817 -701.30407557 -701.3231791  -701.32131581 -701.34298375
 -701.30076911 -701.31791649 -701.32271417 -701.29678166 -701.29529789
 -701.35413182 -701.29723442 -701.28717276 -701.35026104 -701.32949341
 -701.30056934 -701.27926854 -701.31417554 -701.32007099 -701.31408617
 -701.32096085]
2025-06-23 11:16:34 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:16:34 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_NoveltyArchiveEA
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_NoveltyArchiveEA
# Description: Combines adaptive DE, LHS, and a novelty-based archive for robust multimodal optimization.
# Code:
class AdaptiveDE_LHS_NoveltyArchiveEA:
    """
    Combines adaptive Differential Evolution (DE), Latin Hypercube Sampling (LHS), and a novelty-based archive 
    to efficiently explore and exploit multimodal landscapes.  The archive prioritizes solutions with both 
    high novelty and good fitness.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8  # Initial exploration rate
        self.novelty_weight = 0.5 # Weight for novelty in archive selection

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_novelty_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_novelty_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        # Novelty calculation (simple Euclidean distance to existing archive)
        novelty_scores = np.array([self._calculate_novelty(x, self.archive[:, :-1]) for x in combined_pop])
        
        # Weighted combined score:  Fitness + Novelty
        combined_scores = combined_fit + self.novelty_weight * novelty_scores
        
        sorted_indices = np.argsort(combined_scores)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _calculate_novelty(self, solution, archive):
        if not archive.size: return 0  # Handle empty archive
        distances = np.linalg.norm(archive - solution, axis=1)
        return np.min(distances)

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        #Sort by fitness then novelty (more fit solutions are preferred even if less novel)
        novelty_scores = np.array([self._calculate_novelty(x, self.archive[:, :-1]) for x in combined[:,:-1]])
        sorted_indices = np.lexsort((novelty_scores, combined[:,-1]))
        
        return combined[sorted_indices[:min(len(combined), self.archive_size)]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Diversity based adaptation
        diversity = np.std(population, axis=0).mean()
        diversity_threshold = 0.2 * (np.max(self.upper_bounds) - np.min(self.lower_bounds))
        if diversity < diversity_threshold:
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        #Success rate based adaptation (adjusting F and CR)
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 11:16:34 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:16:34 ERROR Can not run the algorithm
2025-06-23 11:16:34 INFO Run function 2 complete. FEHistory len: 301, AOCC: 0.1753
2025-06-23 11:16:34 INFO FeHistory: [-701.28456279 -701.30979876 -701.30936422 -701.30865018 -701.27996477
 -701.29024535 -701.32778531 -701.28854223 -701.30107763 -701.33236922
 -701.31410338 -701.27719254 -701.2967896  -701.3307521  -701.30213874
 -701.30796786 -701.28293898 -701.31367002 -701.33072893 -701.33103003
 -701.35145514 -701.29964998 -701.29569942 -701.30547892 -701.31895139
 -701.30824667 -701.31035393 -701.30612607 -701.30044986 -701.30726178
 -701.31082288 -701.32183414 -701.31663058 -701.29988365 -701.29324523
 -701.28475859 -701.28910559 -701.34257064 -701.2897921  -701.32087676
 -701.30905351 -701.30322106 -701.28499582 -701.29971892 -701.30880381
 -701.32653787 -701.31723587 -701.29634752 -701.32934116 -701.26976064
 -701.32900229 -701.29934319 -701.31946121 -701.31320082 -701.30556381
 -701.29009166 -701.29795077 -701.31980905 -701.31451014 -701.31247977
 -701.31029974 -701.3396298  -701.30908139 -701.30599661 -701.3164742
 -701.35709657 -701.32305535 -701.36642651 -701.30450466 -701.31479704
 -701.28982474 -701.31334702 -701.30462956 -701.29461128 -701.31402467
 -701.30582492 -701.32930398 -701.31529683 -701.31797367 -701.27701548
 -701.32474619 -701.34520368 -701.27997573 -701.32033976 -701.2636103
 -701.29632336 -701.31442898 -701.27861832 -701.31197612 -701.32761451
 -701.31600704 -701.31043684 -701.28388202 -701.3440047  -701.31247351
 -701.33508261 -701.30057919 -701.28760994 -701.31382736 -701.31452244
 -701.31688504 -701.28243115 -701.30096055 -701.33799578 -701.27214097
 -701.33123331 -701.28619414 -701.28866114 -701.26221608 -701.28374567
 -701.30639335 -701.27923985 -701.30800582 -701.29436563 -701.32504974
 -701.3105224  -701.30235407 -701.30107412 -701.30463052 -701.29387255
 -701.30553063 -701.28456353 -701.27542848 -701.29852852 -701.28505943
 -701.31064083 -701.30201877 -701.30014704 -701.32364875 -701.28384032
 -701.2991479  -701.31775629 -701.30673182 -701.27459491 -701.30872613
 -701.30381335 -701.2925672  -701.29489945 -701.30893249 -701.31271719
 -701.28875922 -701.30401635 -701.32975061 -701.31953597 -701.30564355
 -701.28594294 -701.29464372 -701.30085242 -701.3040487  -701.32198088
 -701.28394218 -701.30313495 -701.28604276 -701.30102211 -701.31114228
 -701.30587522 -701.29537743 -701.29998455 -701.31697233 -701.28073306
 -701.33349061 -701.28728434 -701.28325359 -701.29980223 -701.29215441
 -701.31081119 -701.30783053 -701.31337585 -701.3229628  -701.2973381
 -701.30859786 -701.31217656 -701.32092924 -701.30760677 -701.28417354
 -701.3479669  -701.3123368  -701.31802301 -701.28670319 -701.29500244
 -701.29938836 -701.30641475 -701.306166   -701.34829334 -701.29981399
 -701.30479975 -701.2745104  -701.28569094 -701.27865393 -701.29208519
 -701.29627287 -701.28435561 -701.28044756 -701.28558237 -701.31165692
 -701.29477934 -701.30327482 -701.27884767 -701.28618663 -701.29560016
 -701.28924956 -701.29792058 -701.28310918 -701.31913677 -701.29623119
 -701.30048538 -701.30475883 -701.30229764 -701.28941685 -701.29450423
 -701.30674972 -701.28285755 -701.28951138 -701.29567649 -701.28511373
 -701.30066456 -701.31669023 -701.31338547 -701.3058403  -701.2964206
 -701.28781856 -701.2888371  -701.27818613 -701.31685592 -701.29572141
 -701.29371454 -701.27248833 -701.29049771 -701.30567476 -701.28680495
 -701.29672819 -701.30194777 -701.28613726 -701.28528637 -701.29695429
 -701.3326366  -701.29745999 -701.29614329 -701.30265754 -701.28044744
 -701.29341021 -701.29606292 -701.33085884 -701.28856588 -701.32382584
 -701.27237402 -701.33384797 -701.31618493 -701.28369394 -701.32237348
 -701.28761162 -701.29674537 -701.31390353 -701.30184167 -701.30138673
 -701.28843713 -701.28862039 -701.31594384 -701.32401306 -701.30306746
 -701.28011898 -701.27074281 -701.30609675 -701.2911122  -701.32275421
 -701.29188817 -701.27800037 -701.28605206 -701.33522625 -701.31619322
 -701.30488822 -701.30462331 -701.30807599 -701.29199197 -701.29689117
 -701.2919129  -701.29817923 -701.29057154 -701.26818569 -701.34176263
 -701.30803335 -701.30731316 -701.32288746 -701.3397267  -701.29127331
 -701.30782874 -701.29488132 -701.31829987 -701.29447554 -701.30967
 -701.30712    -701.33080164 -701.26564408 -701.3340248  -701.31407874
 -701.28855363 -701.29210384 -701.31136984 -701.32240454 -701.27440656
 -701.28307094]
2025-06-23 11:16:34 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:16:34 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_NoveltyArchiveEA
import numpy as np
from scipy.spatial.distance import cdist
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_NoveltyArchiveEA
# Description: Combines adaptive DE, LHS, and a novelty-based archive for multimodal optimization.
# Code:

class AdaptiveDE_LHS_NoveltyArchiveEA:
    """
    Combines adaptive Differential Evolution (DE), Latin Hypercube Sampling (LHS), and a novelty-based archive 
    to efficiently explore and exploit multimodal landscapes.  Adapts DE parameters based on diversity and success rate.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5
        self.CR = 0.9
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8
        self.novelty_weight = 0.1 #weight for novelty in archive selection


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_novelty(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_novelty(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        # Novelty Calculation
        if len(self.archive) > 0:
            novelties = np.max(cdist(combined_pop, self.archive[:, :-1]), axis=1)  #Max distance to existing archive members
        else:
            novelties = np.zeros(len(combined_pop))

        #Weighted combination of fitness and novelty
        weighted_scores = combined_fit + self.novelty_weight * novelties

        sorted_indices = np.argsort(weighted_scores)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Prioritize novelty in archive update
        if len(self.archive) > 0:
            novelties = np.max(cdist(population, self.archive[:, :-1]), axis=1)
            combined = np.column_stack((combined, novelties))
            sorted_data = combined[combined[:, -1].argsort()]
        else:
             sorted_data = combined[combined[:, -1].argsort()]

        new_archive = [sorted_data[0]]
        for i in range(1,len(sorted_data)):
            if sorted_data[i,-1] < new_archive[-1][-1]:
                new_archive[-1] = sorted_data[i]
            else:
                new_archive.append(sorted_data[i])
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        diversity = np.std(population, axis=0).mean()
        if diversity < 0.2 * (np.max(self.upper_bounds) - np.min(self.lower_bounds)):
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))
2025-06-23 11:16:34 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:16:34 ERROR Can not run the algorithm
2025-06-23 11:16:35 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1043
2025-06-23 11:16:35 INFO FeHistory: [-224.18523942 -221.91419335 -221.60747887 -223.44234607 -222.67512294
 -221.97260542 -222.01507644 -222.38763558 -221.86074322 -221.69482151
 -222.81555622 -221.4739847  -220.74871989 -221.06821631 -223.20862782
 -220.61367174 -222.57197169 -223.287284   -220.60771446 -221.81393385
 -222.40978923 -223.34765516 -220.90402667 -220.44340182 -222.13023736
 -221.21370298 -222.32014988 -222.22954261 -221.99750873 -221.91372262
 -223.46271453 -221.96774866 -222.21213203 -222.69835553 -221.1718228
 -221.68232412 -222.51678694 -221.91630385 -222.65515352 -220.85245323
 -223.05534677 -222.23725866 -220.84624344 -221.50682387 -221.42267554
 -221.16790377 -221.35773803 -222.57886263 -222.57404849 -222.75341237
 -223.26186408 -221.00997628 -222.18659645 -221.1933027  -221.83445615
 -223.02151023 -222.92368782 -222.79161349 -220.6367598  -220.71436308
 -222.22506659 -221.85640967 -222.21811311 -222.36963371 -221.35668143
 -223.25838346 -221.34161898 -220.85360353 -221.56816769 -222.11777615
 -221.91226496 -225.21546646 -222.28476642 -222.45525897 -220.04527261
 -222.23504202 -222.34493801 -222.31123683 -221.05509647 -222.00771818
 -222.28090126 -222.79752371 -224.03162028 -221.58777373 -223.29603925
 -222.76688178 -221.59492492 -221.87295536 -221.76782133 -220.17169165
 -220.95515801 -221.78441333 -222.80857635 -222.31066879 -221.0290284
 -220.54840643 -222.97802266 -221.67527683 -221.70558399 -221.85504358
 -221.61765005]
2025-06-23 11:16:35 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:16:35 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_NoveltyArchiveEA
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_NoveltyArchiveEA
# Description: Combines adaptive DE, LHS, and a novelty-based archive for robust multimodal optimization.
# Code:
class AdaptiveDE_LHS_NoveltyArchiveEA:
    """
    Combines adaptive Differential Evolution (DE), Latin Hypercube Sampling (LHS), and a novelty-based archive 
    to efficiently explore and exploit multimodal landscapes.  The archive prioritizes solutions with both 
    high novelty and good fitness.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8  # Initial exploration rate
        self.novelty_weight = 0.5 # Weight for novelty in archive selection

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_novelty_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_novelty_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        # Novelty calculation (simple Euclidean distance to existing archive)
        novelty_scores = np.array([self._calculate_novelty(x, self.archive[:, :-1]) for x in combined_pop])
        
        # Weighted combined score:  Fitness + Novelty
        combined_scores = combined_fit + self.novelty_weight * novelty_scores
        
        sorted_indices = np.argsort(combined_scores)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _calculate_novelty(self, solution, archive):
        if not archive.size: return 0  # Handle empty archive
        distances = np.linalg.norm(archive - solution, axis=1)
        return np.min(distances)

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        #Sort by fitness then novelty (more fit solutions are preferred even if less novel)
        novelty_scores = np.array([self._calculate_novelty(x, self.archive[:, :-1]) for x in combined[:,:-1]])
        sorted_indices = np.lexsort((novelty_scores, combined[:,-1]))
        
        return combined[sorted_indices[:min(len(combined), self.archive_size)]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Diversity based adaptation
        diversity = np.std(population, axis=0).mean()
        diversity_threshold = 0.2 * (np.max(self.upper_bounds) - np.min(self.lower_bounds))
        if diversity < diversity_threshold:
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        #Success rate based adaptation (adjusting F and CR)
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 11:16:35 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:16:35 INFO Run function 15 complete. FEHistory len: 301, AOCC: 0.1039
2025-06-23 11:16:35 INFO FeHistory: [-222.65314576 -221.38914164 -222.39647768 -221.52792854 -222.02187405
 -221.61898927 -221.74902077 -222.1232686  -221.07152478 -223.53892229
 -220.56050645 -222.74340749 -223.90648711 -222.50259963 -221.63130458
 -221.27832017 -222.26738279 -222.55468643 -222.3413257  -223.55802072
 -220.53098369 -221.44109767 -221.17547549 -221.76077401 -222.53572691
 -222.55327443 -221.20155971 -223.07906635 -221.65011522 -221.66193695
 -222.3199985  -221.81091446 -223.89587403 -222.40794014 -221.73517544
 -222.54254127 -222.57102839 -223.73823387 -222.39778415 -221.11380265
 -222.93117609 -221.44067411 -222.67176257 -222.0094198  -223.60380289
 -220.91953951 -223.2582771  -221.43232883 -221.62089582 -221.68273442
 -222.03518072 -221.99155565 -222.55977997 -221.93720352 -222.01772367
 -223.23232562 -221.39393577 -221.45901979 -221.88133096 -222.42703032
 -222.79056845 -223.73306414 -222.25901708 -221.34620901 -221.65823521
 -221.50448331 -221.48464117 -222.08024576 -223.65302666 -220.43423085
 -222.56455921 -222.71020591 -221.46066633 -219.71838623 -221.96963601
 -221.77346302 -222.65011251 -221.3193615  -223.01226919 -221.20885743
 -223.86104253 -221.90044943 -222.91154293 -222.00080509 -223.23110522
 -221.13822102 -220.00309177 -223.43561985 -221.6232757  -222.90936042
 -223.19165177 -220.96674658 -220.90237113 -221.92849608 -222.19567191
 -223.88362187 -222.33368502 -221.67272723 -223.87807367 -220.57864255
 -221.36016574 -222.2225838  -222.02489513 -223.17508182 -222.77930826
 -222.1283978  -220.9190281  -221.70312478 -221.54830408 -223.70782348
 -222.38463746 -222.82448354 -222.67567052 -222.01027275 -220.610618
 -222.77479882 -222.43291104 -222.08603542 -222.06879222 -222.1908663
 -223.03061583 -221.05962074 -220.87150002 -220.48482695 -222.14601769
 -221.24183458 -221.17199138 -221.57181908 -222.67213984 -221.60964594
 -223.1462148  -222.7901462  -222.79743559 -221.32211933 -221.92315221
 -223.83022284 -222.29532278 -222.17146572 -220.92307144 -223.5820582
 -221.96152163 -221.49019931 -222.78135368 -222.53001108 -224.11298457
 -222.1511401  -220.75559558 -222.5425917  -222.2508978  -222.22800097
 -222.3867029  -222.11476634 -222.02309725 -220.6368362  -220.81582875
 -219.87675428 -221.64146938 -221.8337467  -222.15887979 -222.3964199
 -221.44328104 -222.50181139 -222.62164285 -223.39885167 -221.82833466
 -222.23830781 -222.73550713 -222.8632075  -222.85585243 -222.09563806
 -223.44165824 -223.22110051 -222.13371038 -221.41387773 -220.33889386
 -221.50815592 -223.00737664 -222.58225732 -222.4382188  -221.02981073
 -221.12433995 -221.81946715 -222.44014182 -222.31062576 -219.88654628
 -223.5746317  -222.22336965 -220.65611361 -222.88740011 -220.96171515
 -222.48076919 -223.05131481 -220.79839966 -222.10103095 -221.22217923
 -222.48861199 -220.73927277 -222.63551803 -220.84443414 -221.35564411
 -222.38206987 -223.80048205 -221.92352009 -222.42122581 -221.50277245
 -222.83489737 -222.84831649 -223.073402   -222.30761149 -222.07846589
 -221.90942223 -221.83131849 -220.79463221 -220.6746185  -222.32530281
 -222.02962608 -222.22087888 -220.42226135 -222.83303853 -222.49879801
 -222.28129719 -221.01813325 -222.03521546 -222.77265143 -221.61119623
 -223.08502266 -220.65655539 -221.90397357 -222.12281789 -223.58382763
 -222.79546387 -224.17517583 -221.75367636 -221.63301305 -222.53858104
 -220.68938369 -221.86351459 -222.48825171 -221.80662299 -222.72728319
 -223.45573629 -221.34335869 -225.13301726 -222.77074424 -221.71855112
 -221.98609764 -222.5242444  -221.36600019 -222.34366191 -220.73809018
 -222.97098022 -222.57742375 -222.99436823 -221.19061426 -221.62728976
 -223.73566917 -223.1145706  -221.09382657 -220.58806453 -223.39591342
 -222.73952775 -223.44184302 -221.6040059  -223.73101877 -222.07385991
 -222.55397921 -220.89834961 -221.7174135  -221.81336283 -221.12879945
 -223.13672842 -222.12939094 -222.07941372 -221.23958176 -221.60851648
 -220.7940992  -221.96275732 -221.68444285 -221.62268545 -221.08653279
 -222.29543677 -222.32866852 -221.76495002 -220.46350316 -220.02879623
 -221.60803677 -222.94418657 -221.47369325 -220.8096077  -221.77696375
 -223.21509667 -220.76745835 -223.15098372 -221.7631964  -221.52353858
 -222.02836558 -221.39967643 -222.46895127 -221.33336107 -223.04034562
 -222.78148213]
2025-06-23 11:16:35 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:16:35 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_NoveltyArchiveEA
import numpy as np
from scipy.spatial.distance import cdist
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_NoveltyArchiveEA
# Description: Combines adaptive DE, LHS, and a novelty-based archive for multimodal optimization.
# Code:

class AdaptiveDE_LHS_NoveltyArchiveEA:
    """
    Combines adaptive Differential Evolution (DE), Latin Hypercube Sampling (LHS), and a novelty-based archive 
    to efficiently explore and exploit multimodal landscapes.  Adapts DE parameters based on diversity and success rate.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5
        self.CR = 0.9
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8
        self.novelty_weight = 0.1 #weight for novelty in archive selection


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_novelty(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_novelty(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        # Novelty Calculation
        if len(self.archive) > 0:
            novelties = np.max(cdist(combined_pop, self.archive[:, :-1]), axis=1)  #Max distance to existing archive members
        else:
            novelties = np.zeros(len(combined_pop))

        #Weighted combination of fitness and novelty
        weighted_scores = combined_fit + self.novelty_weight * novelties

        sorted_indices = np.argsort(weighted_scores)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Prioritize novelty in archive update
        if len(self.archive) > 0:
            novelties = np.max(cdist(population, self.archive[:, :-1]), axis=1)
            combined = np.column_stack((combined, novelties))
            sorted_data = combined[combined[:, -1].argsort()]
        else:
             sorted_data = combined[combined[:, -1].argsort()]

        new_archive = [sorted_data[0]]
        for i in range(1,len(sorted_data)):
            if sorted_data[i,-1] < new_archive[-1][-1]:
                new_archive[-1] = sorted_data[i]
            else:
                new_archive.append(sorted_data[i])
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        diversity = np.std(population, axis=0).mean()
        if diversity < 0.2 * (np.max(self.upper_bounds) - np.min(self.lower_bounds)):
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))
2025-06-23 11:16:35 ERROR Can not run the algorithm
2025-06-23 11:16:35 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:16:35 ERROR Can not run the algorithm
2025-06-23 11:16:35 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 11:16:35 INFO FeHistory: [222.18403603 202.75849641 160.51757897 210.66813532 194.88646657
 177.08162161 197.43185302 189.84259593 209.31947521 195.50198157
 195.15767285 189.61340207 179.38529248 197.29848513 203.95470012
 180.87482672 198.74267003 185.81558783 197.69932701 174.73522672
 174.42524244 190.02104323 176.89735649 203.56382955 205.3731648
 199.43766014 170.96388317 217.72361952 185.80673327 198.30450376
 182.91114994 185.78263256 184.65286571 189.08659832 186.71560658
 224.25953069 153.22292483 195.573777   203.03504796 175.03228349
 197.66328406 174.05689286 143.04759648 207.91503162 177.40790859
 197.50023957 195.61902238 211.60421622 188.53489837 230.93399912
 202.46854637 203.59998572 180.81631499 233.34764229 222.77218014
 175.17459626 188.05021132 198.86595809 169.69115334 217.7948628
 156.63443571 174.13139891 197.20445392 157.21186374 206.80781656
 190.33775514 201.79904406 168.719801   203.70265805 186.87395495
 203.27557745 221.73041813 197.06164009 197.30525248 129.99624335
 169.49783101 169.79897447 178.48584681 183.61886144 162.32419416
 230.91623117 197.41317327 153.13330168 174.38257537 183.75623666
 187.60219787 220.38845569 203.01882311 183.44164677 188.46307877
 183.31873502 200.28355375 222.99364467 183.6726782  188.49449563
 183.99740051 179.70189165 203.01326058 192.6067431  176.11455163
 178.48419213]
2025-06-23 11:16:35 INFO Expected Optimum FE: -100
2025-06-23 11:16:35 INFO Unimodal AOCC mean: 0.1750
2025-06-23 11:16:35 INFO Multimodal (single component) AOCC mean: 0.1043
2025-06-23 11:16:35 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:16:35 INFO AOCC mean: 0.0931
2025-06-23 11:16:35 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:16:35 ERROR Can not run the algorithm
2025-06-23 11:16:35 INFO Run function 24 complete. FEHistory len: 301, AOCC: 0.0000
2025-06-23 11:16:35 INFO FeHistory: [196.34539363 210.940693   199.03618675 165.61818464 204.37555275
 193.55454068 169.09130327 220.86407146 232.98787147 156.14906341
 117.31121447 145.45355468 183.00850759 226.64205649 222.47483355
 175.62965654 172.59814203 188.86048297 187.6653497  222.40849632
 226.91112871 172.16492022 173.23082628 217.80356902 199.39241644
 158.02864297 229.75516419 201.27323085 169.66802708 174.28696444
 156.6231925  195.86824631 182.26249109 233.79301281 183.45057507
 192.04569604 167.73902664 171.03055576 192.98737873 229.84490345
 202.57890396 178.27352813 152.01097541 170.37669537 199.75271494
 167.43503524 176.31557475 182.64044968 204.90126192 206.46208961
 231.9112383  161.56040288 175.25714451 130.02965144 162.62582743
 178.95767315 177.54593474 203.60041198 164.26315721 199.90821466
 182.19578372 192.63147302 191.36769907 166.360468   165.63393192
 177.41310424 194.6911726  196.66743012 176.55039121 180.66417914
 179.34872674 222.30393547 155.81361589 213.30338008 181.48278484
 152.92505244 181.16405662 179.48682489 186.53074039 205.10219006
 195.2568729  191.33719551 192.51576655 219.46691005 196.72880108
 171.73835163 182.1085864  166.02900378 218.76315142 180.43417842
 189.89884564 218.97224036 163.57940254 196.68237147 198.29504408
 172.4055343  189.34078531 154.4543345  152.6248582  183.38808881
 173.74797364 134.55555544 242.48649589 188.95871146 220.66677887
 209.82656517 210.33469516 217.16138658 166.49961001 184.76788772
 185.78506959 195.35030889 202.11757306 172.46260285 196.71388196
 206.07536284 177.49226826 193.36845574 199.87739569 228.43692508
 221.8573307  194.80337697 208.77747959 204.66834565 182.34360583
 205.90861573 204.39783016 197.27574277 208.90534084 196.56159408
 202.44715055 202.20641943 192.20799829 169.29338634 184.09913902
 181.00025038 165.57680346 184.22197728 221.959928   199.74223627
 183.76797195 201.52730888 203.54117978 184.61009849 241.28556336
 183.62245456 243.5113286  177.20906151 173.65636821 205.65338617
 198.4064714  193.95984765 187.40467731 210.45578132 167.83201629
 191.77577231 206.10037359 176.48725805 198.30890694 205.76076432
 212.81523243 204.00505721 206.54332045 166.11185734 167.59198958
 179.63882368 173.07096146 195.19750155 201.02453801 205.47366954
 198.76776533 192.14137156 185.77438578 208.69840326 199.62217548
 192.101632   202.12345294 190.32111    177.20257538 236.49368183
 213.0225087  196.44559578 190.06710488 198.04460738 191.8053429
 155.39990723 244.1398222  190.58393152 175.80893522 164.44359837
 165.28029128 176.21222347 173.79697154 139.66722722 200.27640018
 183.07627768 228.65336227 180.71494298 208.47576712 194.42134916
 206.17634393 175.85079489 195.1025281  212.97237963 207.24143118
 190.32988338 173.57240566 178.8025864  208.80900223 161.33160271
 152.61619562 221.71299912 200.87434971 181.96296213 211.83949278
 186.13324346 213.41791561 201.70794932 180.36922751 219.68850661
 176.03654548 168.03979428 192.10327834 173.4177916  174.01065907
 183.82170346 206.18346766 187.53447549 202.79348062 181.93915859
 147.03548978 185.47952948 177.39758912 226.51699055 200.30401279
 154.73858184 162.85131049 199.83191154 247.37968568 208.97031201
 217.05501676 213.95394718 205.4877113  196.01841568 197.54658927
 189.78815788 187.46219282 158.42710327 221.77565611 185.90829394
 201.23311839 203.54110588 187.77403769 180.15316671 169.96322195
 161.61660523 198.21497282 188.12239801 197.93036542 239.22135879
 205.06522447 187.73127264 197.89710503 220.32295058 191.92765279
 194.63219264 224.23730815 220.63657662 212.57722876 161.63205806
 217.98300294 151.42579557 173.22442491 185.27161187 177.43440519
 225.53440778 211.78067462 186.63310675 156.34491236 200.26580341
 189.23685908 192.78537256 203.20312016 180.08039765 173.18858938
 211.78358461 198.0298635  203.34833438 153.06964295 223.31925238
 191.01404801 134.60194041 193.49341106 137.26758987 161.59483189
 184.02023179 197.69057817 198.02884216 196.96888023 209.53713732
 187.13662706]
2025-06-23 11:16:35 INFO Expected Optimum FE: -100
2025-06-23 11:16:35 INFO Unimodal AOCC mean: 0.1753
2025-06-23 11:16:35 INFO Multimodal (single component) AOCC mean: 0.1039
2025-06-23 11:16:35 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:16:35 INFO AOCC mean: 0.0930
2025-06-23 11:16:35 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:16:35 ERROR Can not run the algorithm
2025-06-23 11:16:35 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1753
2025-06-23 11:16:35 INFO FeHistory: [-701.3408965  -701.3185549  -701.30926313 -701.33645948 -701.28562519
 -701.30075653 -701.33851804 -701.34402022 -701.33684458 -701.30808546
 -701.32796982 -701.29405784 -701.29229679 -701.33710393 -701.3265768
 -701.28743654 -701.30893849 -701.30866233 -701.27878334 -701.30175564
 -701.28280402 -701.31923023 -701.31963554 -701.30149496 -701.32170255
 -701.29952332 -701.2974887  -701.31127594 -701.28587757 -701.31435381
 -701.2925481  -701.29528951 -701.31331195 -701.3295084  -701.29746189
 -701.31431443 -701.30717353 -701.32420179 -701.30295126 -701.31481786
 -701.3112428  -701.29829376 -701.33428446 -701.310135   -701.33803418
 -701.30372541 -701.28049933 -701.30662301 -701.28984042 -701.30971017
 -701.29768228 -701.32208853 -701.31767117 -701.32452991 -701.32227795
 -701.3122325  -701.30248206 -701.30391171 -701.29888251 -701.30323078
 -701.30359343 -701.32401926 -701.30053226 -701.28526425 -701.31146635
 -701.3022452  -701.31450148 -701.29869558 -701.30120227 -701.36806275
 -701.29310153 -701.30147147 -701.31212705 -701.29520897 -701.30564899
 -701.32274956 -701.31267537 -701.30677919 -701.29944858 -701.30344739
 -701.2871192  -701.31731231 -701.31270852 -701.3084691  -701.32960937
 -701.29852903 -701.29708685 -701.32168991 -701.2851639  -701.30836648
 -701.35911069 -701.29858807 -701.30039429 -701.31620647 -701.29607538
 -701.29451285 -701.29461391 -701.29331581 -701.3233489  -701.32898498
 -701.30909043]
2025-06-23 11:16:35 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:16:35 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithNoveltyCrowdingAndLHS
import numpy as np
from scipy.stats import qmc
from scipy.spatial.distance import cdist

class AdaptiveDEwithNoveltyCrowdingAndLHS:
    """
    Combines adaptive Differential Evolution (DE), novelty-based crowding, and Latin Hypercube Sampling (LHS) for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5
        self.CR = 0.9
        self.archive = []
        self.novelty_threshold = 0.2  # Adjust as needed
        self.exploration_rate = 0.8


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._novelty_based_selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _novelty_based_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        next_gen = []
        next_fit = []
        
        for i in range(self.population_size):
            best_candidate = None
            best_novelty = -1

            for j in range(len(combined_pop)):
                novelty = self._calculate_novelty(combined_pop[j], self.archive)
                if best_candidate is None or novelty > best_novelty:
                    best_candidate = j
                    best_novelty = novelty
            
            next_gen.append(combined_pop[best_candidate])
            next_fit.append(combined_fit[best_candidate])

        return np.array(next_gen), np.array(next_fit)

    def _calculate_novelty(self, solution, archive):
        if not archive.size: return 1.0
        distances = cdist(solution.reshape(1, -1), archive[:, :-1])
        return 1.0 / np.min(distances) if np.min(distances) > 0 else 0.0


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        # Novelty based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            novelty = self._calculate_novelty(sorted_data[i, :-1], np.array(new_archive)[:,:-1])
            if novelty > self.novelty_threshold:
                new_archive.append(sorted_data[i])
                
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        diversity = np.std(population, axis=0).mean()
        if diversity < self.novelty_threshold:
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 11:16:35 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:16:35 ERROR Can not run the algorithm
2025-06-23 11:16:35 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1751
2025-06-23 11:16:35 INFO FeHistory: [-701.32270624 -701.32979253 -701.31589418 -701.34375706 -701.31805218
 -701.30053326 -701.26427124 -701.28844433 -701.30162488 -701.2962002
 -701.29445216 -701.32535276 -701.3133695  -701.28638907 -701.30132481
 -701.29672577 -701.32036209 -701.2960151  -701.28239693 -701.31475151
 -701.29980822 -701.31943895 -701.29293633 -701.32948948 -701.31297771
 -701.30356407 -701.31113946 -701.30177837 -701.29666687 -701.34413664
 -701.30267212 -701.30819996 -701.32075443 -701.3444067  -701.33347224
 -701.31502739 -701.33894534 -701.2967396  -701.27504498 -701.29693291
 -701.32217291 -701.28612437 -701.34308033 -701.3164369  -701.29520354
 -701.30656172 -701.32031086 -701.30970276 -701.31874181 -701.33250021
 -701.28803715 -701.30173842 -701.31634627 -701.29422651 -701.32957485
 -701.31415252 -701.34428521 -701.30356266 -701.29724486 -701.3454193
 -701.29287908 -701.28954107 -701.28875429 -701.29736308 -701.33198198
 -701.30031424 -701.31758328 -701.30534694 -701.30367563 -701.32830819
 -701.3094337  -701.31818143 -701.28151277 -701.31045009 -701.29208539
 -701.27728369 -701.30905347 -701.27435279 -701.32862459 -701.31394829
 -701.31471727 -701.32937232 -701.28987581 -701.30770706 -701.30049493
 -701.28280098 -701.30642634 -701.31511603 -701.35662135 -701.31938853
 -701.3220995  -701.32737375 -701.32541074 -701.29407539 -701.30532188
 -701.30771746 -701.30623234 -701.31645677 -701.32754589 -701.30271489
 -701.30376585]
2025-06-23 11:16:35 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:16:35 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_NoveltyCrowding
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_NoveltyCrowding
# Description: Combines adaptive DE, LHS, and a novelty-based crowding archive for multimodal optimization.
# Code:
class AdaptiveDE_LHS_NoveltyCrowding:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5
        self.CR = 0.9
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.novelty_weight = 0.5 # Weight for novelty in archive selection

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1
        
        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._novelty_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _novelty_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        #Calculate novelty using euclidean distance to existing archive members.  
        if len(self.archive) > 0:
            novelties = np.min(np.linalg.norm(combined_pop - self.archive[:,:-1], axis=1), axis=0)
        else:
            novelties = np.zeros(len(combined_pop))
            
        #Weighted combination of fitness and novelty
        scores = combined_fit + self.novelty_weight * novelties
        
        sorted_indices = np.argsort(scores)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        if len(self.archive) < self.archive_size:
            self.archive = np.concatenate((self.archive, combined))
        else:
            self.archive = self._novelty_based_update(self.archive, combined)
        return self.archive

    def _novelty_based_update(self, archive, new_solutions):
        combined = np.concatenate((archive, new_solutions))
        #Sort by a combined fitness and novelty score (adjust weights as needed)
        if len(archive) > 0:
            novelties = np.min(np.linalg.norm(combined[:,:-1] - archive[:,:-1], axis=1).reshape(-1,1), axis=0)
            scores = combined[:,-1] + self.novelty_weight * novelties.flatten()
            sorted_indices = np.argsort(scores)
            return combined[sorted_indices[:self.archive_size]]
        else:
            return combined[:self.archive_size]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptation based on the standard deviation of fitness values
        std_dev = np.std(fitness_values)
        if std_dev < 1e-3:
            self.F *= 1.1
            self.CR *= 0.9
        elif std_dev > 10:
            self.F *= 0.9
            self.CR *= 1.1
        self.F = np.clip(self.F, 0.1, 1.0)
        self.CR = np.clip(self.CR, 0.1, 1.0)

2025-06-23 11:16:35 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:16:35 ERROR Can not run the algorithm
2025-06-23 11:16:35 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1014
2025-06-23 11:16:35 INFO FeHistory: [-221.1446896  -223.18096092 -221.26613955 -221.86615424 -220.55427502
 -221.6176372  -222.18615377 -222.35066958 -222.75323202 -223.58206673
 -223.66120932 -222.01837436 -221.36420291 -222.81928808 -221.77724349
 -222.6640475  -222.23511227 -221.28605564 -220.9908523  -221.88438164
 -221.72498331 -220.63999943 -223.55187265 -222.61695793 -221.76557887
 -220.36444888 -223.26640926 -220.57174407 -221.67366457 -222.77318998
 -220.68654523 -221.51501636 -222.21840568 -221.8860982  -221.40282006
 -223.04922596 -221.34325183 -222.07596642 -223.80034611 -222.71524231
 -222.20495149 -222.45957181 -222.2370689  -222.0059309  -223.1314278
 -222.20078673 -222.25713434 -221.95259569 -220.89549261 -221.38637719
 -221.79698248 -221.87207724 -222.9326853  -221.43347059 -222.39203833
 -222.86299972 -222.5423287  -222.31830741 -221.42322237 -221.21146896
 -222.63248716 -221.83119765 -221.76061412 -221.02129537 -220.22978373
 -221.06820511 -222.20060693 -220.35228754 -222.33056447 -223.07423338
 -223.17597665 -220.73921865 -222.85052192 -223.06564954 -222.05173419
 -221.89720702 -221.46517931 -221.96423042 -222.04896828 -222.10620085
 -221.88536167 -221.22663234 -221.55333119 -222.49738278 -223.48529771
 -222.04249096 -222.42195748 -224.60862472 -221.96598526 -222.43030264
 -222.89461246 -222.30502411 -221.21814848 -220.60926469 -222.71649981
 -222.94589929 -221.56320992 -221.89808848 -222.00272865 -222.32957502
 -222.45382108]
2025-06-23 11:16:35 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:16:35 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithNoveltyCrowdingAndLHS
import numpy as np
from scipy.stats import qmc
from scipy.spatial.distance import cdist

class AdaptiveDEwithNoveltyCrowdingAndLHS:
    """
    Combines adaptive Differential Evolution (DE), novelty-based crowding, and Latin Hypercube Sampling (LHS) for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5
        self.CR = 0.9
        self.archive = []
        self.novelty_threshold = 0.2  # Adjust as needed
        self.exploration_rate = 0.8


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._novelty_based_selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _novelty_based_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        next_gen = []
        next_fit = []
        
        for i in range(self.population_size):
            best_candidate = None
            best_novelty = -1

            for j in range(len(combined_pop)):
                novelty = self._calculate_novelty(combined_pop[j], self.archive)
                if best_candidate is None or novelty > best_novelty:
                    best_candidate = j
                    best_novelty = novelty
            
            next_gen.append(combined_pop[best_candidate])
            next_fit.append(combined_fit[best_candidate])

        return np.array(next_gen), np.array(next_fit)

    def _calculate_novelty(self, solution, archive):
        if not archive.size: return 1.0
        distances = cdist(solution.reshape(1, -1), archive[:, :-1])
        return 1.0 / np.min(distances) if np.min(distances) > 0 else 0.0


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        # Novelty based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            novelty = self._calculate_novelty(sorted_data[i, :-1], np.array(new_archive)[:,:-1])
            if novelty > self.novelty_threshold:
                new_archive.append(sorted_data[i])
                
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        diversity = np.std(population, axis=0).mean()
        if diversity < self.novelty_threshold:
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 11:16:35 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:16:35 ERROR Can not run the algorithm
2025-06-23 11:16:35 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1049
2025-06-23 11:16:35 INFO FeHistory: [-221.13314105 -222.39370923 -221.03126879 -222.6278599  -223.78233494
 -222.75972164 -221.3675879  -220.52506087 -222.15104215 -221.33730965
 -221.57350827 -222.94161804 -221.35596457 -222.25138933 -221.40873551
 -221.45957096 -224.93356064 -222.76240216 -221.10418042 -222.85578674
 -223.93793468 -222.73918801 -222.05269673 -222.17004813 -223.42041458
 -223.13622268 -223.35231119 -221.32294895 -222.50112435 -222.61563689
 -222.28191061 -220.91140673 -222.15107428 -221.74257389 -221.19977317
 -222.78863254 -220.65986449 -221.01984783 -223.1424178  -221.96643265
 -221.69046475 -221.06181305 -220.92379496 -222.1777275  -220.66422369
 -222.00004368 -221.48511592 -222.19093355 -222.3864873  -223.42781682
 -222.80300472 -222.54530001 -221.82043474 -221.75850708 -222.51089295
 -222.40788444 -221.44420394 -221.50615345 -221.37310222 -221.72345247
 -221.53022044 -225.33964083 -222.02381135 -220.66495025 -221.91864973
 -221.92170193 -221.92707411 -222.4170471  -220.7002704  -221.27030846
 -221.0964058  -221.0932528  -222.4382491  -221.4762238  -221.11867738
 -221.73064853 -222.70236493 -221.37197895 -222.5459998  -222.30587885
 -222.17847238 -221.88689733 -222.92113583 -221.62871297 -222.40552096
 -221.31801131 -223.2607743  -220.51982897 -222.37931703 -221.67158046
 -223.05914768 -222.05638265 -221.10625079 -221.99601167 -223.34081018
 -223.13251332 -221.56168981 -222.29884229 -223.40164774 -223.51285107
 -221.43990677]
2025-06-23 11:16:35 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:16:35 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_NoveltyCrowding
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_NoveltyCrowding
# Description: Combines adaptive DE, LHS, and a novelty-based crowding archive for multimodal optimization.
# Code:
class AdaptiveDE_LHS_NoveltyCrowding:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5
        self.CR = 0.9
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.novelty_weight = 0.5 # Weight for novelty in archive selection

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1
        
        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._novelty_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _novelty_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        #Calculate novelty using euclidean distance to existing archive members.  
        if len(self.archive) > 0:
            novelties = np.min(np.linalg.norm(combined_pop - self.archive[:,:-1], axis=1), axis=0)
        else:
            novelties = np.zeros(len(combined_pop))
            
        #Weighted combination of fitness and novelty
        scores = combined_fit + self.novelty_weight * novelties
        
        sorted_indices = np.argsort(scores)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        if len(self.archive) < self.archive_size:
            self.archive = np.concatenate((self.archive, combined))
        else:
            self.archive = self._novelty_based_update(self.archive, combined)
        return self.archive

    def _novelty_based_update(self, archive, new_solutions):
        combined = np.concatenate((archive, new_solutions))
        #Sort by a combined fitness and novelty score (adjust weights as needed)
        if len(archive) > 0:
            novelties = np.min(np.linalg.norm(combined[:,:-1] - archive[:,:-1], axis=1).reshape(-1,1), axis=0)
            scores = combined[:,-1] + self.novelty_weight * novelties.flatten()
            sorted_indices = np.argsort(scores)
            return combined[sorted_indices[:self.archive_size]]
        else:
            return combined[:self.archive_size]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptation based on the standard deviation of fitness values
        std_dev = np.std(fitness_values)
        if std_dev < 1e-3:
            self.F *= 1.1
            self.CR *= 0.9
        elif std_dev > 10:
            self.F *= 0.9
            self.CR *= 1.1
        self.F = np.clip(self.F, 0.1, 1.0)
        self.CR = np.clip(self.CR, 0.1, 1.0)

2025-06-23 11:16:35 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:16:35 ERROR Can not run the algorithm
2025-06-23 11:16:35 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 11:16:35 INFO FeHistory: [184.27064623 193.01848176 200.47889261 208.9724546  188.28307181
 179.9138056  181.29237657 207.47453866 158.8090582  182.52707629
 209.09850533 194.41311152 199.93916456 182.38062945 198.41530509
 146.50740056 182.15183058 194.79339335 199.44125496 182.90561699
 198.75433591 156.22020247 153.17201645 205.73614306 180.55521808
 204.53194808 202.00752922 203.65470536 168.07831632 186.36457908
 237.956795   158.4101279  195.71567972 200.21091761 191.09599318
 177.73036636 207.24947248 200.50491384 171.11848904 160.66546274
 165.81706464 158.54827145 191.18758796 240.1137291  228.59661965
 202.9018493  219.65061982 200.3507595  178.95386386 171.85157722
 200.32659654 155.32456566 165.15157389 150.80202521 197.61508838
 197.68956257 165.76262069 192.29917477 142.88335389 207.80149587
 191.96374505 186.74601209 174.44177652 197.00459711 183.0513056
 194.7234972  153.94071724 179.71526927 219.35358459 204.56151344
 170.94086155 170.35282695 174.59712548 189.46251742 173.1665821
 172.22131359 169.31461368 176.75418973 174.2419591  216.43839044
 207.00960571 188.86240946 200.89850828 175.19463536 185.95208668
 199.58029249 195.08388136 168.81160266 165.7510211  188.35837069
 163.95821788 198.7902288  164.09932709 168.0411555  167.95912722
 200.02818483 186.65364854 202.02774491 199.56122056 214.01894439
 189.89041163]
2025-06-23 11:16:35 INFO Expected Optimum FE: -100
2025-06-23 11:16:35 INFO Unimodal AOCC mean: 0.1753
2025-06-23 11:16:35 INFO Multimodal (single component) AOCC mean: 0.1014
2025-06-23 11:16:35 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:16:35 INFO AOCC mean: 0.0923
2025-06-23 11:16:35 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 11:16:35 INFO FeHistory: [211.55320381 196.22014794 210.88150844 186.65003358 193.26598055
 194.28388371 195.54164748 183.62531827 181.70948503 162.86185318
 205.76605119 169.08091764 196.39571124 167.34086079 181.03184027
 163.92379355 144.01013707 184.98965225 194.17523558 190.99930244
 195.75212753 196.19252311 207.21415999 149.73177557 176.58821657
 179.63320247 211.17058397 190.63768133 172.07938645 191.60844318
 154.45620606 193.53698393 187.14044106 173.94805661 198.09097343
 176.53296342 209.95385681 196.71102288 173.31060185 153.32904951
 181.98119515 187.46427095 169.10616098 188.1482136  196.39108292
 155.5131846  182.20317163 188.54032988 169.63827874 163.83485512
 188.95634735 197.48813471 204.56181987 206.78740335 201.85966351
 182.85176009 223.02726128 214.619957   203.93398248 215.9258354
 181.49757573 172.38155429 167.55615699 208.17875244 205.53964292
 194.01553932 220.16545195 189.6028578  205.97927423 164.95778563
 190.89599956 204.36959644 201.81149934 179.23615023 177.1241722
 154.69520991 166.95095109 164.94572236 197.89997709 206.93051861
 206.11956895 167.21349266 149.26145655 186.45713167 198.05250655
 186.53672903 199.40611786 162.20698494 188.14612714 195.01855515
 175.27019455 150.75301322 185.54310129 148.75289463 199.99623101
 209.62085861 188.34023218 193.76354162 198.88526757 167.06973376
 198.76131799]
2025-06-23 11:16:35 INFO Expected Optimum FE: -100
2025-06-23 11:16:35 INFO Unimodal AOCC mean: 0.1751
2025-06-23 11:16:35 INFO Multimodal (single component) AOCC mean: 0.1049
2025-06-23 11:16:35 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:16:35 INFO AOCC mean: 0.0933
2025-06-23 11:18:21 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:18:21 ERROR Can not run the algorithm
2025-06-23 11:18:21 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:18:21 ERROR Can not run the algorithm
2025-06-23 11:18:21 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1752
2025-06-23 11:18:21 INFO FeHistory: [-701.28579141 -701.30164123 -701.28818726 -701.31528782 -701.32968421
 -701.30288532 -701.30631733 -701.31641763 -701.31515258 -701.33675622
 -701.31102859 -701.31037589 -701.31915094 -701.32795386 -701.32640745
 -701.33915846 -701.29444454 -701.2989724  -701.29560216 -701.30675456
 -701.30562358 -701.27478946 -701.30376348 -701.31289137 -701.29679267
 -701.29224666 -701.29828504 -701.3528692  -701.2902688  -701.27641348
 -701.30079037 -701.31708496 -701.30135053 -701.29938174 -701.32109812
 -701.28914224 -701.32173848 -701.31038028 -701.31001843 -701.3067676
 -701.36207844 -701.301742   -701.30216936 -701.33127233 -701.27455349
 -701.32713507 -701.26179946 -701.32750885 -701.30030874 -701.29563962
 -701.32251684 -701.32027878 -701.31261588 -701.30265178 -701.33585003
 -701.31196143 -701.2943864  -701.34387401 -701.29380771 -701.3065985
 -701.28599055 -701.30767709 -701.30787366 -701.30514141 -701.31122241
 -701.30522047 -701.32878984 -701.30383155 -701.3264404  -701.32126865
 -701.30455753 -701.31165296 -701.31444281 -701.31426784 -701.28817387
 -701.30337554 -701.3214594  -701.34658175 -701.28908528 -701.32924783
 -701.30159137 -701.30394956 -701.32277888 -701.29563777 -701.30613144
 -701.36239329 -701.28202285 -701.30253948 -701.34497513 -701.30114887
 -701.302718   -701.34099958 -701.31237022 -701.2637254  -701.30235796
 -701.30420767 -701.30049237 -701.31905624 -701.31026521 -701.30123468
 -701.29995827]
2025-06-23 11:18:21 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:18:21 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithNoveltyGuidedArchive
import numpy as np
from scipy.spatial.distance import cdist
from scipy.stats import levy

class AdaptiveDEwithNoveltyGuidedArchive:
    """
    Combines adaptive Differential Evolution (DE) with a novelty-guided archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 100, archive_size: int = 200, novelty_threshold: float = 0.2,
                 F_init: float = 0.5, CR_init: float = 0.9, levy_alpha: float = 1.5):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.novelty_threshold = novelty_threshold
        self.F = F_init
        self.CR = CR_init
        self.archive = []
        self.levy_alpha = levy_alpha


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()  # Using LHS for initialization
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._novelty_based_selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Latin Hypercube Sampling (LHS) for diverse initialization
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample


    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _novelty_based_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        next_gen = []
        next_fit = []
        
        for i in range(self.population_size):
            best_candidate = None
            best_novelty = -1

            for j in range(len(combined_pop)):
                novelty = self._calculate_novelty(combined_pop[j], self.archive)
                if best_candidate is None or novelty > best_novelty:
                    best_candidate = j
                    best_novelty = novelty
            
            next_gen.append(combined_pop[best_candidate])
            next_fit.append(combined_fit[best_candidate])

        return np.array(next_gen), np.array(next_fit)

    def _calculate_novelty(self, solution, archive):
        if not archive.size: return 1.0
        distances = cdist(solution.reshape(1, -1), archive[:, :-1])
        return 1.0 / np.min(distances) if np.min(distances) > 0 else 0.0


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        # Novelty based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            novelty = self._calculate_novelty(sorted_data[i, :-1], np.array(new_archive)[:,:-1])
            if novelty > self.novelty_threshold:
                new_archive.append(sorted_data[i])
                
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        # Adaptive parameter control based on population diversity and success rate
        diversity = np.std(population, axis=0).mean()
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])

        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))
        self.novelty_threshold = max(0.05, min(0.5, self.novelty_threshold + 0.02*(0.5 - success_rate))) #Adjust novelty threshold

from scipy.stats import qmc #import here to avoid error

2025-06-23 11:18:21 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:18:21 ERROR Can not run the algorithm
2025-06-23 11:18:21 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1753
2025-06-23 11:18:21 INFO FeHistory: [-701.29981955 -701.28978044 -701.31118593 -701.30505444 -701.28030592
 -701.30654919 -701.29125509 -701.30224873 -701.32099703 -701.32023613
 -701.3217431  -701.30276192 -701.35075247 -701.29714837 -701.28599659
 -701.31257931 -701.27278698 -701.31529967 -701.29981783 -701.32528324
 -701.30417194 -701.34054532 -701.29327336 -701.30017035 -701.32621742
 -701.30238728 -701.31554699 -701.301129   -701.30954512 -701.29062345
 -701.31172608 -701.29184028 -701.31218003 -701.32413049 -701.2883671
 -701.27155719 -701.33727895 -701.33755508 -701.27884212 -701.3085935
 -701.32456593 -701.30723005 -701.28789663 -701.29769721 -701.30972927
 -701.32434126 -701.32889375 -701.30877726 -701.29405681 -701.30358046
 -701.31320216 -701.30874078 -701.32843193 -701.32232345 -701.31840032
 -701.30729226 -701.31734412 -701.28507162 -701.31560285 -701.2715856
 -701.30654876 -701.28761919 -701.28786832 -701.31029603 -701.28958477
 -701.29130239 -701.31130111 -701.3113753  -701.29746997 -701.29282921
 -701.31372086 -701.29648876 -701.321755   -701.31183679 -701.29576808
 -701.29939631 -701.32147508 -701.25488388 -701.30104508 -701.32275028
 -701.33822864 -701.30425843 -701.36778046 -701.29439639 -701.28939987
 -701.30410377 -701.2804582  -701.30203205 -701.29540223 -701.3019625
 -701.31248121 -701.32444393 -701.32739598 -701.30436264 -701.30239405
 -701.28826647 -701.31351354 -701.32608105 -701.28599763 -701.34176032
 -701.28721433]
2025-06-23 11:18:21 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:18:21 INFO Good algorithm:
Algorithm Name: AdaptiveDELevyNoveltyEA
import numpy as np
from scipy.spatial.distance import pdist, squareform

# Name: AdaptiveDELevyNoveltyEA
# Description: Combines adaptive DE, Lvy flights, and novelty-based archive for efficient multimodal optimization.
class AdaptiveDELevyNoveltyEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.alpha = 1.5 # Levy flight parameter
        self.beta = 0.2 # Novelty weighting parameter
        self.k=0.1


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._novelty_selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            #Differential Evolution
            a, b, c = self._select_three(population, i)
            offspring[i] = population[i] + self.F * (b - c)
            offspring[i] = self._crossover(population[i], offspring[i])

            #Levy Flight perturbation
            step = np.random.levy(self.alpha, size=self.dim)
            step = (self.upper_bounds - self.lower_bounds) * step / np.max(np.abs(step))
            offspring[i] += self.k * step
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)
        return offspring

    def _select_three(self, population, i):
      indices = np.random.choice(np.arange(self.population_size), 3, replace=False)
      while i in indices:
        indices = np.random.choice(np.arange(self.population_size), 3, replace=False)
      return population[indices[0]], population[indices[1]], population[indices[2]]


    def _crossover(self, x, v):
        jrand = np.random.randint(0, self.dim)
        y = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                y[j] = v[j]
        return y


    def _novelty_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        novelty_scores = self._calculate_novelty(combined_pop)
        weighted_scores = combined_fit * (1 - self.beta) + novelty_scores * self.beta
        sorted_indices = np.argsort(weighted_scores)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _calculate_novelty(self, solutions):
        if not self.archive.size:
            return np.zeros(len(solutions))
        archive_solutions = self.archive[:, :-1]
        distances = squareform(pdist(solutions, 'euclidean'))
        min_distances = np.min(distances[:, :len(archive_solutions)], axis=1)
        novelty = 1 / (1 + min_distances)
        return novelty

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptation: adjust F and CR based on success rate
        success_rate = np.mean(fitness_values < np.median(fitness_values))
        self.F = max(0.1, min(1, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0, min(1, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 11:18:21 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:18:22 ERROR Can not run the algorithm
2025-06-23 11:18:22 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1041
2025-06-23 11:18:22 INFO FeHistory: [-221.40569444 -221.42665557 -222.01355414 -223.22830326 -220.87466173
 -223.2079966  -223.18249733 -222.33065597 -220.96858437 -222.3939646
 -223.38967405 -221.68834185 -221.72865462 -220.76452291 -222.00462785
 -221.34192045 -220.84648306 -223.03009567 -221.98431779 -221.02478825
 -222.93509559 -220.46299015 -221.36331647 -221.7867675  -222.0493744
 -221.74220526 -224.25076221 -223.08644703 -222.63545019 -223.97693627
 -221.54807531 -221.91773445 -221.50023214 -221.00301246 -221.5819999
 -221.5852431  -223.09992633 -220.8197304  -222.17488842 -222.50872632
 -222.4544966  -222.85488566 -220.92672427 -220.78186855 -221.97242307
 -219.5599788  -220.66352349 -221.82167516 -222.38209172 -221.19861546
 -223.15247456 -221.85306134 -221.14562661 -222.43975334 -222.42649539
 -222.44906195 -222.91187429 -221.01027105 -221.36674365 -222.2021057
 -220.79573108 -221.77545933 -222.63439366 -222.89594146 -220.38798618
 -221.54693545 -221.96389049 -221.16349273 -221.20350259 -220.79221329
 -221.62728409 -221.00293329 -221.73693731 -221.76245299 -220.98278256
 -221.70937065 -221.83566303 -222.66411767 -222.50102372 -221.5422798
 -220.63455684 -220.89945878 -225.1921187  -223.00340229 -221.27732906
 -221.8388581  -221.26532099 -222.84862844 -221.10886748 -221.76720896
 -223.11107558 -222.15247561 -221.17427663 -222.27008474 -222.13437368
 -221.95427694 -220.95739681 -221.88014709 -222.54688618 -223.2094965
 -221.19845485]
2025-06-23 11:18:22 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:18:22 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithNoveltyGuidedArchive
import numpy as np
from scipy.spatial.distance import cdist
from scipy.stats import levy

class AdaptiveDEwithNoveltyGuidedArchive:
    """
    Combines adaptive Differential Evolution (DE) with a novelty-guided archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 100, archive_size: int = 200, novelty_threshold: float = 0.2,
                 F_init: float = 0.5, CR_init: float = 0.9, levy_alpha: float = 1.5):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.novelty_threshold = novelty_threshold
        self.F = F_init
        self.CR = CR_init
        self.archive = []
        self.levy_alpha = levy_alpha


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()  # Using LHS for initialization
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._novelty_based_selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        # Latin Hypercube Sampling (LHS) for diverse initialization
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample


    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _novelty_based_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        next_gen = []
        next_fit = []
        
        for i in range(self.population_size):
            best_candidate = None
            best_novelty = -1

            for j in range(len(combined_pop)):
                novelty = self._calculate_novelty(combined_pop[j], self.archive)
                if best_candidate is None or novelty > best_novelty:
                    best_candidate = j
                    best_novelty = novelty
            
            next_gen.append(combined_pop[best_candidate])
            next_fit.append(combined_fit[best_candidate])

        return np.array(next_gen), np.array(next_fit)

    def _calculate_novelty(self, solution, archive):
        if not archive.size: return 1.0
        distances = cdist(solution.reshape(1, -1), archive[:, :-1])
        return 1.0 / np.min(distances) if np.min(distances) > 0 else 0.0


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        # Novelty based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            novelty = self._calculate_novelty(sorted_data[i, :-1], np.array(new_archive)[:,:-1])
            if novelty > self.novelty_threshold:
                new_archive.append(sorted_data[i])
                
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        # Adaptive parameter control based on population diversity and success rate
        diversity = np.std(population, axis=0).mean()
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])

        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))
        self.novelty_threshold = max(0.05, min(0.5, self.novelty_threshold + 0.02*(0.5 - success_rate))) #Adjust novelty threshold

from scipy.stats import qmc #import here to avoid error

2025-06-23 11:18:22 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:18:22 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.0992
2025-06-23 11:18:22 INFO FeHistory: [-221.71568043 -222.08520138 -222.63641121 -223.1072746  -220.89435955
 -222.06324493 -221.75458647 -221.78531001 -222.3775185  -223.42056645
 -222.50979623 -222.52942264 -223.26332001 -222.94443828 -222.74449821
 -223.11252563 -220.60224515 -223.68575838 -221.61593294 -221.622461
 -222.44402688 -222.3527905  -221.65491406 -221.06024313 -221.50157854
 -222.31896287 -222.62234655 -222.30962532 -222.4809642  -222.20955497
 -220.37342793 -221.62789447 -221.79352163 -223.27821573 -223.31195686
 -222.45561945 -222.04535385 -224.09945973 -222.70165907 -221.88752654
 -221.45949476 -223.83704698 -223.62962183 -220.43659055 -223.08281366
 -222.47559539 -221.51262489 -223.07948055 -221.10781761 -221.64833133
 -222.84075505 -221.57296617 -220.76587568 -222.53731545 -221.37552692
 -221.05777415 -222.58628343 -221.89582357 -222.50032098 -223.24502906
 -221.42987834 -222.8415536  -222.89967269 -222.33830705 -223.08563797
 -222.57349618 -221.34177425 -220.99440719 -221.9152126  -220.80257654
 -221.82667278 -221.55642435 -221.95821603 -222.5345496  -221.91477392
 -221.7582528  -221.23514709 -222.29576701 -222.53333605 -221.88994241
 -222.2263294  -222.89764705 -220.45645843 -221.53738594 -220.30160361
 -222.46098812 -222.00709592 -222.48466724 -220.54742629 -222.17103275
 -220.85817812 -222.1590823  -220.99273456 -223.00654151 -221.74278976
 -220.72115639 -220.68339166 -221.81930905 -222.92946443 -221.81434505
 -221.65815718]
2025-06-23 11:18:22 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:18:22 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:18:22 ERROR Can not run the algorithm
2025-06-23 11:18:22 ERROR Can not run the algorithm
2025-06-23 11:18:22 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 11:18:22 INFO FeHistory: [175.27769118 191.54807019 209.4945802  202.43898895 181.1065069
 170.57945338 201.46742043 181.3690267  186.95541523 179.32129241
 170.91211083 157.79347795 124.46658674 176.12093781 195.49570598
 168.7549155  185.02011162 207.60451582 167.27065055 222.24745101
 177.1956763  193.80651575 178.87704727 222.28091169 215.87686212
 233.45016985 169.60881535 219.49050932 180.86906761 195.37051492
 206.33917505 184.18367389 179.68717466 192.79444238 215.24393766
 171.18637083 195.07791554 151.75704118 175.47721365 214.0555943
 216.02009158 182.99421998 187.62966216 203.70933381 206.26280291
 150.22337933 187.14965432 157.52990231 208.40565772 190.87390764
 194.91787196 208.69106042 190.9595717  202.36770566 190.75294283
 177.35445425 214.59088738 195.81346946 156.32876527 154.08580512
 219.60862211 240.42817767 163.05224748 182.3934786  173.66421588
 204.05691906 168.71264323 209.33898087 187.79514317 181.74164996
 217.19894059 188.73941265 179.90209125 176.82385635 188.74582999
 170.7300013  139.5745032  224.03899793 145.79745942 140.308557
 208.84050885 194.93407692 166.54536408 175.3935038  206.0565574
 177.9787469  184.73978406 217.3317741  164.37761928 169.26521282
 198.89486934 182.76516903 205.6340436  204.90576637 190.44357464
 191.1054563  203.04826508 219.84511217 176.06427766 134.0884029
 179.79172364]
2025-06-23 11:18:22 INFO Expected Optimum FE: -100
2025-06-23 11:18:22 INFO Unimodal AOCC mean: 0.1752
2025-06-23 11:18:22 INFO Multimodal (single component) AOCC mean: 0.1041
2025-06-23 11:18:22 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:18:22 INFO AOCC mean: 0.0931
2025-06-23 11:18:22 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 11:18:22 INFO FeHistory: [223.90972672 183.14298317 214.39200092 210.9701835  150.4432293
 183.3218609  211.10204612 184.10182831 172.11853694 212.91350091
 178.79272653 202.71050482 209.74750993 149.05798947 211.50937023
 175.72077795 193.67065817 195.35587232 188.37815541 185.93959314
 175.73715256 167.26737448 179.19775478 150.33216191 218.20667115
 202.65075342 211.85227555 208.17541518 204.49131463 200.04362499
 187.68183728 187.07054832 198.7483697  217.24067441 163.49214492
 201.99906681 187.90719558 183.90052765 212.89731598 206.37528155
 174.86356302 166.42103566 187.43418703 185.27887048 219.34542015
 202.97448562 199.36188389 207.36569602 201.32880698 184.63074664
 208.00934676 198.36649467 172.68549214 206.60143016 208.082555
 187.51506586 178.61402495 208.06809116 190.30636766 219.70502766
 179.34868983 204.72525873 232.73283177 161.9500599  194.25943291
 197.70798483 177.61511846 140.89171936 162.16235753 183.22144571
 198.35621441 171.32147166 203.4021262  199.11302906 211.7811929
 176.68759887 184.05479921 190.79138264 188.69214419 199.51394088
 172.11260334 167.80788914 182.61455085 208.02599407 202.57793623
 192.10284343 160.88069679 189.05114624 168.58317089 181.94263231
 177.84472882 209.05779593 154.89021679 162.10590034 210.07854263
 214.32557826 157.59971652 184.89937731 171.06803737 164.32313179
 181.71856643]
2025-06-23 11:18:22 INFO Expected Optimum FE: -100
2025-06-23 11:18:22 INFO Unimodal AOCC mean: 0.1753
2025-06-23 11:18:22 INFO Multimodal (single component) AOCC mean: 0.0992
2025-06-23 11:18:22 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:18:22 INFO AOCC mean: 0.0915
2025-06-23 11:21:32 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:21:32 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:21:32 ERROR Can not run the algorithm
2025-06-23 11:21:32 ERROR Can not run the algorithm
2025-06-23 11:21:32 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1750
2025-06-23 11:21:32 INFO FeHistory: [-701.31174034 -701.33216889 -701.29446231 -701.31962999 -701.30745104
 -701.32808382 -701.34890255 -701.31577665 -701.33942299 -701.32681584
 -701.29359442 -701.29188372 -701.32001246 -701.31031022 -701.31177851
 -701.29699713 -701.32057601 -701.3067831  -701.30271977 -701.3264482
 -701.33191441 -701.28068955 -701.32016773 -701.34830837 -701.31625683
 -701.29948811 -701.28975867 -701.35126692 -701.30250083 -701.30700056
 -701.3160095  -701.35130387 -701.30811279 -701.35546624 -701.3008092
 -701.29876212 -701.32134819 -701.2897906  -701.30254532 -701.32596804
 -701.32066535 -701.31549719 -701.29156994 -701.30279194 -701.29726036
 -701.32655796 -701.32169449 -701.31567252 -701.35339636 -701.32023045
 -701.29755516 -701.32404463 -701.31006884 -701.30507485 -701.32157233
 -701.27308619 -701.29814123 -701.34099495 -701.29199731 -701.31599192
 -701.28288541 -701.31379234 -701.32854341 -701.29399958 -701.27894361
 -701.31250469 -701.29487118 -701.35062773 -701.31378215 -701.28384262
 -701.28189331 -701.28395645 -701.29338922 -701.29068897 -701.32103474
 -701.31896207 -701.34184848 -701.32601914 -701.33018398 -701.31890278
 -701.27035649 -701.28085297 -701.3238259  -701.32343242 -701.29465115
 -701.32851125 -701.3045289  -701.32864035 -701.31390794 -701.32553558
 -701.30138134 -701.29626859 -701.29160415 -701.30723335 -701.32351426
 -701.30461535 -701.27656054 -701.30020312 -701.31733558 -701.30835176
 -701.29179526]
2025-06-23 11:21:32 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:21:32 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithLevyFlightAndNoveltyArchive
import numpy as np
from scipy.spatial.distance import cdist
from scipy.stats import levy

# Name: AdaptiveDEwithLevyFlightAndNoveltyArchive
# Description: Combines adaptive Differential Evolution, Lvy flights, and a novelty-guided archive for robust multimodal optimization.
# Code:
class AdaptiveDEwithLevyFlightAndNoveltyArchive:
    """
    Combines adaptive Differential Evolution (DE), Lvy flights, and a novelty-guided archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 100, archive_size: int = 200, novelty_threshold: float = 0.2,
                 F_init: float = 0.5, CR_init: float = 0.9, levy_alpha: float = 1.5):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.novelty_threshold = novelty_threshold
        self.F = F_init
        self.CR = CR_init
        self.archive = []
        self.levy_alpha = levy_alpha


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._novelty_based_selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            #Differential Evolution
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            #Levy Flight perturbation
            levy_step = levy.rvs(self.levy_alpha, size=self.dim)
            mutant = mutant + 0.1 * levy_step * (self.upper_bounds - self.lower_bounds) #scale levy step
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _novelty_based_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _calculate_novelty(self, solution, archive):
        if not archive.size: return 1.0
        distances = cdist(solution.reshape(1, -1), archive[:, :-1])
        return 1.0 / np.min(distances) if np.min(distances) > 0 else 0.0


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        # Novelty based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            novelty = self._calculate_novelty(sorted_data[i, :-1], np.array(new_archive)[:,:-1])
            if novelty > self.novelty_threshold:
                new_archive.append(sorted_data[i])
                
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptation for demonstration; replace with more sophisticated methods
        self.F = max(0.1, min(1.0, self.F + 0.1 * np.random.randn()))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * np.random.randn()))
        self.novelty_threshold = max(0.05, min(0.5, self.novelty_threshold + 0.01 * np.random.randn()))

2025-06-23 11:21:32 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:21:32 ERROR Can not run the algorithm
2025-06-23 11:21:32 INFO Run function 2 complete. FEHistory len: 301, AOCC: 0.1754
2025-06-23 11:21:32 INFO FeHistory: [-701.30631826 -701.28626092 -701.33670835 -701.34653318 -701.33387069
 -701.29112287 -701.30001736 -701.28694091 -701.28913919 -701.31303917
 -701.34231613 -701.29690977 -701.29025169 -701.32447722 -701.27967235
 -701.29610805 -701.3303941  -701.30364162 -701.37225579 -701.31382341
 -701.30918278 -701.33143159 -701.27961711 -701.31539893 -701.29933827
 -701.3536441  -701.29804637 -701.30911353 -701.29062535 -701.29686037
 -701.32523576 -701.33475188 -701.30020755 -701.2775844  -701.29903555
 -701.30836873 -701.29985998 -701.31490805 -701.30041662 -701.29449667
 -701.31367915 -701.31887629 -701.29606949 -701.34475431 -701.3143855
 -701.33606731 -701.29902004 -701.29429884 -701.34402554 -701.30274998
 -701.31558676 -701.3069099  -701.30918685 -701.30047989 -701.33062702
 -701.28625655 -701.29984986 -701.32509882 -701.30026738 -701.30344342
 -701.28090945 -701.30621195 -701.30057556 -701.31378999 -701.29316377
 -701.34189083 -701.29564488 -701.2822001  -701.27785797 -701.28981187
 -701.31963166 -701.32764072 -701.30932898 -701.33499276 -701.33005823
 -701.27142535 -701.28564487 -701.31312522 -701.28698403 -701.34120165
 -701.31623191 -701.30111043 -701.29328752 -701.2968965  -701.3235715
 -701.33084926 -701.30418164 -701.31845879 -701.28766774 -701.31485137
 -701.33316084 -701.30082441 -701.29158546 -701.32497779 -701.30409715
 -701.30121153 -701.3035878  -701.29810157 -701.31637891 -701.28084096
 -701.30911588 -701.28644113 -701.33715955 -701.34530555 -701.2927048
 -701.28788781 -701.29914592 -701.2885655  -701.28890222 -701.31304148
 -701.34228205 -701.29724808 -701.29009611 -701.32457488 -701.27949961
 -701.29649634 -701.32987088 -701.30576195 -701.37259081 -701.31358225
 -701.30904284 -701.33167666 -701.28011102 -701.31533269 -701.29937027
 -701.35430745 -701.29600419 -701.29561279 -701.28484951 -701.31321423
 -701.32569783 -701.33536937 -701.28462322 -701.28846222 -701.2991039
 -701.27320953 -701.29960061 -701.31459365 -701.3297731  -701.2944032
 -701.35851131 -701.31880137 -701.28188408 -701.34382062 -701.27825203
 -701.33636403 -701.29854822 -701.29440306 -701.30050862 -701.3028696
 -701.31536046 -701.32378769 -701.30928709 -701.32360511 -701.30960348
 -701.28613163 -701.32267048 -701.32505252 -701.29999814 -701.303189
 -701.28229566 -701.30618221 -701.30035372 -701.31402279 -701.29302205
 -701.34217943 -701.29573435 -701.28234483 -701.27314176 -701.28998294
 -701.32010573 -701.32782231 -701.30169502 -701.33566563 -701.28267782
 -701.27195183 -701.28565375 -701.31288351 -701.28783419 -701.3413263
 -701.31634451 -701.30083467 -701.29554545 -701.29687014 -701.32445736
 -701.33098062 -701.30421241 -701.31844276 -701.28741212 -701.3146762
 -701.33290444 -701.2780971  -701.27818662 -701.32489128 -701.30294357
 -701.30091018 -701.30392722 -701.30015989 -701.31589571 -701.28074206
 -701.30932792 -701.37007745 -701.3725815  -701.35853334 -701.35323749
 -701.35397732 -701.34607535 -701.34529587 -701.34503041 -701.29644437
 -701.34351138 -701.34236888 -701.34238678 -701.34096952 -701.34174847
 -701.34152397 -701.34102816 -701.33725118 -701.33663509 -701.3365138
 -701.33584353 -701.33543563 -701.30482622 -701.33491091 -701.33493032
 -701.30282089 -701.33288058 -701.33242643 -701.33370546 -701.30353547
 -701.33082775 -701.33096007 -701.29794889 -701.33030593 -701.33019607
 -701.32923745 -701.32989625 -701.30358098 -701.32785771 -701.32536678
 -701.32495754 -701.32565171 -701.34906177 -701.32444002 -701.32523484
 -701.32468812 -701.32457194 -701.32490921 -701.30837722 -701.32355185
 -701.32336232 -701.32251692 -701.32024445 -701.31195422 -701.31879963
 -701.31867651 -701.31797747 -701.31858991 -701.31638977 -701.31642222
 -701.31629616 -701.31770561 -701.31597045 -701.31524496 -701.31533066
 -701.31533065 -701.31502821 -701.31513669 -701.3048724  -701.29443659
 -701.31482449 -701.31379619 -701.31373664 -701.31350765 -701.32244359
 -701.31320576 -701.31334266 -701.31346353 -701.31306501 -701.31289157
 -701.31283973 -701.30897722 -701.31391995 -701.30878452 -701.30855382
 -701.30920768 -701.28987153 -701.30919468 -701.28742768 -701.30882988
 -701.30872646 -701.30683227 -701.30618268 -701.30651074 -701.30566333
 -701.30409857 -701.30360954 -701.30453843 -701.30370619 -701.30375123
 -701.30351556]
2025-06-23 11:21:32 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:21:32 INFO Good algorithm:
Algorithm Name: AdaptiveDELevyFlightArchiveEA
import numpy as np
from scipy.stats import levy
from scipy.spatial.distance import pdist, squareform

# Name: AdaptiveDELevyFlightArchiveEA
# Description: Combines adaptive DE, Lvy flights, and a novelty archive for multimodal optimization.
# Code:
class AdaptiveDELevyFlightArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.alpha = 1.5  # Lvy flight exponent
        self.beta = 0.2  # Lvy flight scaling factor (adaptive)
        self.novelty_threshold = 0.1  # Archive novelty threshold
        self.exploration_rate = 0.8 # Probability of using Levy flight

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:
                offspring[i] = self._levy_flight(population[i])
            else:
                a, b, c = self._select_different(i)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                offspring[i] = self._crossover(population[i], mutant)
        return offspring

    def _levy_flight(self, x):
        step = self._levy_flight_step(self.alpha, self.dim) * self.beta
        return np.clip(x + step, self.lower_bounds, self.upper_bounds)

    def _levy_flight_step(self, alpha, dim):
        u = np.random.normal(0, 1, size=dim)
        v = np.random.normal(0, 1, size=dim)
        step = u / (np.abs(v)**(1/alpha))
        return step

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        if len(self.archive) == 0:
            self.archive = combined[:min(len(combined), self.archive_size)]
            return self.archive

        distances = pdist(combined[:, :-1], 'euclidean')
        distances = squareform(distances)
        
        new_archive = []
        for i, sol in enumerate(combined):
            novel = True
            for j, arch in enumerate(self.archive):
                if distances[i, len(self.archive) + j] < self.novelty_threshold:
                    novel = False
                    break
            if novel and len(new_archive) < self.archive_size:
                new_archive.append(sol)

        self.archive = np.vstack((self.archive, np.array(new_archive)))
        self.archive = self.archive[np.argsort(self.archive[:, -1])][:min(len(self.archive), self.archive_size)]
        return self.archive

    def _adapt_parameters(self, population, fitness_values):
        #Adapt Beta only.
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.beta = max(0.01, self.beta * (0.95 + 0.1 * (success_rate - 0.5) ))

2025-06-23 11:21:32 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:21:32 ERROR Can not run the algorithm
2025-06-23 11:21:32 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.0983
2025-06-23 11:21:32 INFO FeHistory: [-222.12430988 -221.8678276  -222.31114284 -222.06141349 -223.65170587
 -221.64346815 -221.50081803 -222.29923892 -221.74062162 -221.48034446
 -221.93081197 -222.07037303 -221.51277225 -222.86421143 -221.68841076
 -221.032019   -220.70191948 -222.02951336 -222.04061528 -222.85508229
 -220.98282958 -222.70476106 -220.73611309 -222.09680154 -221.71820895
 -220.99054771 -222.08350858 -222.76875027 -222.64676281 -223.07108858
 -222.29586589 -222.96464074 -221.4827327  -222.36365773 -220.4811818
 -221.94779642 -221.5722508  -222.7498047  -222.03476722 -220.44891394
 -220.89411656 -222.39492182 -222.99618823 -221.72743233 -221.5513157
 -221.5064833  -220.89810715 -221.8571694  -223.79054804 -221.2635556
 -220.74698059 -220.82970846 -221.05066069 -221.13153199 -220.29165049
 -222.09062926 -220.86210073 -221.1151496  -221.78057919 -221.81008887
 -222.32483205 -220.37197342 -221.03343525 -220.82225357 -222.2815213
 -221.89582631 -220.77730505 -221.78873174 -222.82165076 -222.26098835
 -221.64938542 -223.01576896 -223.76734779 -221.37410685 -222.46263799
 -222.73932085 -221.89922116 -222.80745255 -221.85465372 -221.82510668
 -221.062499   -221.48087424 -221.72246482 -222.09803384 -221.13358964
 -221.62178863 -222.32769033 -223.12130665 -222.21796784 -221.06464624
 -223.30538403 -223.74775004 -221.7040572  -223.792536   -222.65892416
 -221.24902312 -221.59134026 -222.18988248 -221.21390587 -223.87387702
 -221.57922151]
2025-06-23 11:21:32 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:21:32 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:21:32 INFO Run function 15 complete. FEHistory len: 301, AOCC: 0.1014
2025-06-23 11:21:32 INFO FeHistory: [-221.54845806 -221.74853341 -222.49089445 -222.24861238 -220.96051386
 -221.30596261 -221.7043217  -221.04323157 -221.96935445 -222.8981666
 -222.1553147  -222.13872868 -224.48820752 -222.24320789 -220.51546774
 -220.27200405 -221.66279455 -222.10028025 -221.43171803 -221.92013478
 -222.27274977 -222.418752   -222.76304805 -222.50524968 -222.97893008
 -223.05051496 -221.40594162 -222.08602105 -222.02702761 -222.79017218
 -222.66064041 -222.72772997 -221.68317753 -222.30235242 -221.96159508
 -220.93015816 -221.76779405 -220.85714587 -221.61945134 -221.80092865
 -223.46244837 -221.19294871 -223.57831117 -221.99771692 -221.68599036
 -222.08484632 -222.3911138  -222.67790362 -221.3088973  -221.97576273
 -222.68211192 -222.15965242 -223.57655607 -222.04079504 -220.91810426
 -222.16852128 -221.1208113  -222.04978501 -222.50286641 -221.79627723
 -221.93529549 -222.5559557  -221.83053263 -223.21861668 -221.00247996
 -222.0227046  -221.95472846 -222.50007681 -221.14290282 -221.7749612
 -221.94166351 -222.24992525 -222.50209311 -222.0159047  -221.25457716
 -222.59623039 -220.33953253 -222.48132906 -221.40746611 -221.34989546
 -221.70209972 -222.20905691 -222.96003911 -221.70920517 -221.44754907
 -222.09925864 -223.60481531 -221.65268462 -222.81051437 -222.56133725
 -223.35475542 -222.84407306 -223.06793029 -221.95255197 -223.97332467
 -222.00960933 -221.66314757 -222.4906186  -222.53363855 -223.17095063
 -220.90972838 -221.6433645  -222.48743393 -222.25165293 -220.95043703
 -221.29397053 -220.51788271 -221.71212469 -221.92358443 -222.90337134
 -222.16484648 -222.22411281 -224.60530279 -222.24187549 -220.48228248
 -220.29469226 -221.70334993 -222.41992727 -221.91730066 -222.39079467
 -220.71402542 -222.85953459 -222.70754799 -224.44937437 -222.90121166
 -223.02416013 -221.34538184 -222.08645006 -221.06560247 -221.43899539
 -222.60694264 -222.82252264 -221.68600193 -222.30822717 -222.01286973
 -222.4030095  -221.83567863 -220.87286165 -222.36407405 -220.83106185
 -223.32773003 -221.22498603 -223.63520042 -221.84451864 -221.74867886
 -221.78148211 -222.38426901 -222.70994402 -221.32582854 -222.03123136
 -222.6884695  -222.74146272 -223.48386207 -222.04927423 -220.93133852
 -222.22056287 -222.39860581 -222.00528848 -221.02985941 -222.15855708
 -221.93757479 -223.08457167 -220.72976461 -223.39510649 -221.00061074
 -221.97726582 -222.23717815 -222.52028812 -220.96775894 -221.82006292
 -221.65400945 -222.28501176 -222.5761344  -222.07196378 -221.1758104
 -222.68846414 -220.37856552 -222.47295198 -221.34000942 -222.62493552
 -221.67195035 -222.33090782 -222.8594236  -221.7411952  -221.43734485
 -222.17722803 -221.9639581  -221.39824413 -222.83162086 -222.57913331
 -223.73211153 -222.87131247 -222.60154157 -221.93014136 -223.86393961
 -222.02899042 -221.35684648 -222.5312097  -220.01407538 -223.20594816
 -221.90132922 -224.54138174 -221.18184098 -224.12809515 -220.65048182
 -221.37467965 -223.81402876 -223.48094249 -223.31007315 -222.99865931
 -223.56570861 -223.51565384 -221.51560469 -223.29111373 -223.48429056
 -221.05666608 -223.2032133  -223.20408846 -223.33383971 -223.0436207
 -223.07898267 -223.15409914 -223.01225339 -223.05528921 -223.03857512
 -222.96862506 -222.54685405 -222.83437578 -222.87774837 -222.70857666
 -222.89519376 -222.84670794 -222.84026893 -222.82191398 -222.84471845
 -222.22849062 -222.50960035 -222.93121451 -222.73450909 -223.13701425
 -222.70349489 -222.92312771 -222.59128835 -222.79363803 -222.66956656
 -222.47808074 -221.12393954 -222.57955488 -222.5398168  -222.53630338
 -222.60798987 -222.8186528  -222.52275492 -222.48339523 -222.53542946
 -222.39459253 -222.08367924 -222.49908473 -222.53364133 -222.44162573
 -222.15803946 -222.60881901 -222.44006848 -222.43395967 -222.62977259
 -222.44682307 -222.17974987 -222.43676399 -222.38204638 -222.34862381
 -222.44030996 -222.28538117 -222.3295434  -222.33050947 -222.18644212
 -224.12459285 -222.2984374  -222.27747785 -222.26054755 -222.31313894
 -222.25287914 -222.36634775 -222.87111192 -221.80962656 -222.33486687
 -221.46933414 -222.77289694 -222.31614777 -222.13986424 -222.17295513
 -223.46885904 -222.12342846 -222.21845202 -222.27179409 -222.27190669
 -222.15393066 -222.13227814 -222.50714184 -222.06975194 -222.0400708
 -222.09905123]
2025-06-23 11:21:32 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:21:32 INFO Good algorithm:
Algorithm Name: AdaptiveDELevyFlightArchiveEA
import numpy as np
from scipy.stats import levy
from scipy.spatial.distance import pdist, squareform

# Name: AdaptiveDELevyFlightArchiveEA
# Description: Combines adaptive DE, Lvy flights, and a novelty archive for multimodal optimization.
# Code:
class AdaptiveDELevyFlightArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.alpha = 1.5  # Lvy flight exponent
        self.beta = 0.2  # Lvy flight scaling factor (adaptive)
        self.novelty_threshold = 0.1  # Archive novelty threshold
        self.exploration_rate = 0.8 # Probability of using Levy flight

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:
                offspring[i] = self._levy_flight(population[i])
            else:
                a, b, c = self._select_different(i)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                offspring[i] = self._crossover(population[i], mutant)
        return offspring

    def _levy_flight(self, x):
        step = self._levy_flight_step(self.alpha, self.dim) * self.beta
        return np.clip(x + step, self.lower_bounds, self.upper_bounds)

    def _levy_flight_step(self, alpha, dim):
        u = np.random.normal(0, 1, size=dim)
        v = np.random.normal(0, 1, size=dim)
        step = u / (np.abs(v)**(1/alpha))
        return step

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        if len(self.archive) == 0:
            self.archive = combined[:min(len(combined), self.archive_size)]
            return self.archive

        distances = pdist(combined[:, :-1], 'euclidean')
        distances = squareform(distances)
        
        new_archive = []
        for i, sol in enumerate(combined):
            novel = True
            for j, arch in enumerate(self.archive):
                if distances[i, len(self.archive) + j] < self.novelty_threshold:
                    novel = False
                    break
            if novel and len(new_archive) < self.archive_size:
                new_archive.append(sol)

        self.archive = np.vstack((self.archive, np.array(new_archive)))
        self.archive = self.archive[np.argsort(self.archive[:, -1])][:min(len(self.archive), self.archive_size)]
        return self.archive

    def _adapt_parameters(self, population, fitness_values):
        #Adapt Beta only.
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.beta = max(0.01, self.beta * (0.95 + 0.1 * (success_rate - 0.5) ))

2025-06-23 11:21:32 ERROR Can not run the algorithm
2025-06-23 11:21:32 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:21:32 ERROR Can not run the algorithm
2025-06-23 11:21:32 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 11:21:32 INFO FeHistory: [194.80899718 184.74662968 208.60750783 148.36950253 184.72197726
 210.18709618 172.5518517  198.20930097 185.1443409  172.89484439
 204.49588488 180.84176577 202.40585007 170.46070768 212.93970573
 201.51423405 184.51162313 172.55178047 183.43145238 183.08941054
 158.32967756 228.67678735 152.55050179 214.70617163 211.35579879
 172.1208796  190.26042821 192.6706615  205.45336879 210.88993207
 201.71798841 193.19296827 185.48948194 209.27208048 190.76900246
 176.73689151 193.08690717 158.76872475 178.6335554  186.96823475
 175.15058799 199.78580647 166.93185566 174.01882864 169.7599806
 183.1233963  180.46966565 211.16286205 208.2066867  166.80982311
 174.59178687 194.55209104 187.95241442 160.81347467 181.75172175
 172.90399352 228.34715858 207.36638808 200.91932327 183.12730541
 204.09114098 204.72372597 170.88502403 189.57706747 206.49131196
 182.7851681  184.925298   165.36254577 179.85917921 169.91893052
 188.50747236 188.65939147 191.42707629 158.34349317 208.94070099
 149.38008489 182.28111014 183.36743303 190.8449885  176.04854157
 161.70490795 191.63539651 162.04423793 167.16272837 196.15478066
 214.7614671  202.15354813 185.70432252 177.02321218 193.8366328
 201.92829755 215.61559628 164.76475587 167.56390803 183.42144643
 181.55781589 187.56951496 182.59768044 185.19834658 173.15198064
 161.49603656]
2025-06-23 11:21:32 INFO Expected Optimum FE: -100
2025-06-23 11:21:32 INFO Unimodal AOCC mean: 0.1750
2025-06-23 11:21:32 INFO Multimodal (single component) AOCC mean: 0.0983
2025-06-23 11:21:32 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:21:32 INFO AOCC mean: 0.0911
2025-06-23 11:21:32 INFO Run function 24 complete. FEHistory len: 301, AOCC: 0.0000
2025-06-23 11:21:32 INFO FeHistory: [191.24699119 194.78436374 200.42178059 182.27432491 226.85538714
 166.15488553 186.85036807 159.38168471 198.2324149  192.28492268
 192.59353863 248.29779043 210.29639699 155.87795303 176.40951797
 152.97561422 201.71922398 187.89755528 184.98574778 181.49421654
 183.63810294 174.15019692 216.97072755 223.20083342 150.67963703
 141.70093034 208.36965746 176.54683477 193.69501624 197.84658121
 178.03121471 198.17851234 216.28895701 153.69515215 166.16212217
 198.09642472 193.86356503 208.91672544 191.89361939 163.97937153
 224.87372016 189.31765937 180.97461244 221.98662693 198.4277124
 196.46908524 205.04687061 198.86310178 167.71107407 176.30718914
 147.21109169 155.39237581 178.79528462 150.36072319 204.69427063
 203.22773351 162.22005215 214.52827604 213.49380003 172.00760241
 178.24639343 170.32665396 187.88546511 176.97832164 170.01592513
 205.73352776 182.00213305 168.74311959 196.73743753 208.9629413
 176.18587853 195.6319323  175.48980801 160.77211749 191.54226389
 164.73656817 199.85136967 170.3177204  222.75650543 209.42802917
 198.6627971  184.80444894 168.2357786  181.29930804 183.69114303
 166.78899865 206.7518448  196.3762179  172.91028818 192.90270955
 186.10759849 182.57998459 182.02580224 175.46228901 193.06874513
 205.88675026 201.28804679 131.95756154 172.16422587 202.04326679
 180.51885961 198.34769817 196.307116   169.51100109 205.81106194
 164.51841561 196.29063985 158.56979281 197.65647618 192.62413302
 191.6749897  241.28458418 200.52230048 157.99201966 168.55691928
 149.59240285 202.23079767 185.49135168 185.05467074 180.76166211
 183.92520271 180.17958531 217.73452422 217.30038908 156.42741373
 141.61562514 209.64617522 171.33354169 190.75077488 196.55793062
 191.42631057 199.26427719 205.28544742 155.54317275 166.89913488
 195.05670354 193.23196013 175.03094636 189.19431576 158.80497558
 219.23547073 191.84377621 178.04406028 221.74430329 195.35933429
 194.98396503 201.86011137 214.99333183 200.59669603 176.94166309
 174.73654315 156.05627254 201.5041351  179.24672991 206.24769429
 203.58100687 162.98175555 215.22703477 214.1681162  171.20080125
 195.40300333 166.0538514  168.24536929 224.29728001 172.94602636
 205.16525026 182.58862904 184.66088256 200.32403998 211.06806576
 179.30858241 195.41717687 168.14657439 162.10105131 197.69094328
 166.58254361 185.79209417 167.23375121 213.28139182 183.98426153
 209.85335736 183.18596649 166.42778897 177.28073891 206.91224982
 163.61542179 207.90215192 192.43571674 172.95452317 191.52540994
 189.04713656 186.2564005  186.11855855 170.16330505 192.90264483
 209.03694168 193.53643351 131.78711463 159.89312946 201.30702777
 181.63537073 133.18442156 131.18265611 140.54975832 145.14431542
 149.03247647 162.17779621 152.23487791 154.75354672 155.45049537
 162.49137679 157.54093004 157.88742683 156.75406937 157.83677341
 152.02377484 161.19237967 173.81796551 175.50443441 160.87629417
 153.25681159 156.64441866 160.62882213 163.30506052 163.5105335
 170.85175408 217.76064942 216.67761229 166.00385262 166.6440496
 204.16777604 174.17864044 166.38391223 161.59666692 166.71980616
 208.31264021 170.14244958 171.48586243 153.23255957 190.79548807
 151.68139932 167.77770939 173.96480131 175.84774389 178.53427433
 170.81155057 203.56112288 168.78872701 169.41346667 170.94813307
 151.80017361 175.66707221 175.27993464 176.16265444 156.15714456
 177.01661212 175.35583845 175.0865002  205.87557358 176.92581049
 240.13189276 175.7732917  178.66303159 176.81280774 178.73586528
 177.74140936 180.36571365 176.14752638 211.86257016 180.25285722
 182.85233532 180.57453788 193.54909597 170.68647302 179.00830286
 181.44350131 184.11434426 176.96800525 221.34403061 183.96876763
 179.99418984 183.15699275 186.5397518  213.30975357 181.2737303
 180.78207997 182.25278089 184.62701952 185.72924031 165.0706679
 182.70883197 182.29248269 183.77545914 170.3619738  186.15857105
 185.18528553 188.61021925 184.87707189 182.98650291 183.59972206
 185.97168927]
2025-06-23 11:21:32 INFO Expected Optimum FE: -100
2025-06-23 11:21:32 INFO Unimodal AOCC mean: 0.1754
2025-06-23 11:21:32 INFO Multimodal (single component) AOCC mean: 0.1014
2025-06-23 11:21:32 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:21:32 INFO AOCC mean: 0.0923
2025-06-23 11:22:27 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:25:51 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1779
2025-06-23 11:25:51 INFO FeHistory: [-701.3104442  -701.31976096 -701.29586943 ... -701.47275998 -701.47276226
 -701.47276117]
2025-06-23 11:25:51 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:25:51 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description:  A differential evolution algorithm enhanced with adaptive mutation and clustering for efficient multimodal optimization.

class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.cluster_threshold = 0.2 # Adjust based on problem difficulty.
        self.archive = []
        self.max_archive_size = 200

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adaptive_parameter_control(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sampler = qmc.LatinHypercube(self.dim, seed=42) # Latin Hypercube for better initial diversity
        sample = sampler.random(self.population_size)
        population = sample * (self.upper_bounds - self.lower_bounds) + self.lower_bounds
        return population

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_difference_vectors(i, population)
            v = population[a] + self.F * (population[b] - population[c])
            offspring[i] = self._crossover(population[i], v)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_difference_vectors(self, i, population):
        indices = list(range(self.population_size))
        indices.remove(i)
        np.random.shuffle(indices)
        return indices[0], indices[1], indices[2]

    def _crossover(self, x, v):
        u = np.zeros_like(x)
        jrand = np.random.randint(0, self.dim)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                u[j] = v[j]
            else:
                u[j] = x[j]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.max_archive_size])

    def _adaptive_parameter_control(self, population, fitness_values):
        # Simple adaptive control: increase F if solutions are clustered, decrease otherwise
        distances = np.linalg.norm(population[:, np.newaxis, :] - population[np.newaxis, :, :], axis=2)
        avg_distance = np.mean(distances[distances > 0]) # exclude diagonal

        if avg_distance < self.cluster_threshold:  # Solutions are clustered, increase exploration
            self.F = min(1.0, self.F + 0.1)
            self.CR = max(0.1, self.CR -0.1) # Reduce exploitation
        else:  # Solutions are spread out, increase exploitation
            self.F = max(0.1, self.F - 0.05)
            self.CR = min(0.99, self.CR + 0.05)


2025-06-23 11:25:51 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:31:30 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1179
2025-06-23 11:31:30 INFO FeHistory: [-221.07962686 -221.54650171 -222.67374958 ... -227.84886242 -227.84889214
 -227.84895507]
2025-06-23 11:31:30 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:31:30 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description:  A differential evolution algorithm enhanced with adaptive mutation and clustering for efficient multimodal optimization.

class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.cluster_threshold = 0.2 # Adjust based on problem difficulty.
        self.archive = []
        self.max_archive_size = 200

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adaptive_parameter_control(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sampler = qmc.LatinHypercube(self.dim, seed=42) # Latin Hypercube for better initial diversity
        sample = sampler.random(self.population_size)
        population = sample * (self.upper_bounds - self.lower_bounds) + self.lower_bounds
        return population

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_difference_vectors(i, population)
            v = population[a] + self.F * (population[b] - population[c])
            offspring[i] = self._crossover(population[i], v)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_difference_vectors(self, i, population):
        indices = list(range(self.population_size))
        indices.remove(i)
        np.random.shuffle(indices)
        return indices[0], indices[1], indices[2]

    def _crossover(self, x, v):
        u = np.zeros_like(x)
        jrand = np.random.randint(0, self.dim)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                u[j] = v[j]
            else:
                u[j] = x[j]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
    
    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.max_archive_size])

    def _adaptive_parameter_control(self, population, fitness_values):
        # Simple adaptive control: increase F if solutions are clustered, decrease otherwise
        distances = np.linalg.norm(population[:, np.newaxis, :] - population[np.newaxis, :, :], axis=2)
        avg_distance = np.mean(distances[distances > 0]) # exclude diagonal

        if avg_distance < self.cluster_threshold:  # Solutions are clustered, increase exploration
            self.F = min(1.0, self.F + 0.1)
            self.CR = max(0.1, self.CR -0.1) # Reduce exploitation
        else:  # Solutions are spread out, increase exploitation
            self.F = max(0.1, self.F - 0.05)
            self.CR = min(0.99, self.CR + 0.05)


2025-06-23 11:31:30 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:44:41 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 11:44:41 INFO FeHistory: [186.06792285 224.17829656 185.89793067 ...  51.4317693   51.43153204
  51.43151542]
2025-06-23 11:44:41 INFO Expected Optimum FE: -100
2025-06-23 11:44:41 INFO Unimodal AOCC mean: 0.1779
2025-06-23 11:44:41 INFO Multimodal (single component) AOCC mean: 0.1179
2025-06-23 11:44:41 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:44:41 INFO AOCC mean: 0.0986
2025-06-23 11:46:02 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:46:02 ERROR Can not run the algorithm
2025-06-23 11:46:02 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1753
2025-06-23 11:46:02 INFO FeHistory: [-701.30801431 -701.31603201 -701.30399876 -701.30179326 -701.31917885
 -701.30620299 -701.3129646  -701.29169371 -701.28791619 -701.30399127
 -701.33202163 -701.29511049 -701.36330754 -701.31591216 -701.29939607
 -701.33332938 -701.31543771 -701.28166616 -701.27466974 -701.33523951
 -701.30640927 -701.33319532 -701.31142369 -701.29663158 -701.33310127
 -701.30525911 -701.30558506 -701.32824217 -701.34841294 -701.32350027
 -701.28153402 -701.32799465 -701.32405338 -701.28499579 -701.33449074
 -701.30785961 -701.28650686 -701.30773297 -701.28772887 -701.31076452
 -701.28709522 -701.30073634 -701.30259904 -701.32828594 -701.30944543
 -701.31805553 -701.32635774 -701.36545749 -701.29494036 -701.30948539
 -701.31653233 -701.34233772 -701.32740925 -701.2910411  -701.3142561
 -701.29810215 -701.32162446 -701.31804879 -701.3644625  -701.3058258
 -701.32894583 -701.33194888 -701.30426649 -701.3078173  -701.27711677
 -701.29823221 -701.29735784 -701.29510516 -701.28427409 -701.32777754
 -701.30718322 -701.30916172 -701.28627526 -701.33505959 -701.32416681
 -701.34461957 -701.29796031 -701.29318015 -701.3073077  -701.30598201
 -701.31824988 -701.31274511 -701.31038823 -701.27647347 -701.28590968
 -701.29058807 -701.31707383 -701.25863599 -701.29110437 -701.29929604
 -701.29937702 -701.29838971 -701.32467327 -701.30792229 -701.29118848
 -701.30780958 -701.32307608 -701.30454031 -701.3248319  -701.28839416
 -701.32611571]
2025-06-23 11:46:02 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:46:02 INFO Good algorithm:
Algorithm Name: AdaptiveLevyDEwithNoveltyArchive
import numpy as np
from scipy.stats import levy

# Name: AdaptiveLevyDEwithNoveltyArchive
# Description: Combines adaptive Differential Evolution, Lvy flights, and a novelty archive for robust multimodal optimization.
# Code:
class AdaptiveLevyDEwithNoveltyArchive:
    """
    Combines adaptive Differential Evolution, Lvy flights, and a novelty archive for multimodal optimization.  Uses adaptive parameters and a novelty-based archive update to balance exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 100, archive_size: int = 200, novelty_threshold: float = 0.2,
                 F_init: float = 0.5, CR_init: float = 0.9, levy_alpha: float = 1.5):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.novelty_threshold = novelty_threshold
        self.F = F_init
        self.CR = CR_init
        self.archive = []
        self.levy_alpha = levy_alpha


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._novelty_based_selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            #Levy Flight perturbation
            levy_step = levy.rvs(self.levy_alpha, size=self.dim)
            mutant = mutant + 0.1 * levy_step * (self.upper_bounds - self.lower_bounds)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _novelty_based_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _calculate_novelty(self, solution, archive):
        if not archive.size: return 1.0
        distances = np.linalg.norm(archive[:, :-1] - solution, axis=1)
        return 1.0 / np.min(distances) if np.min(distances) > 0 else 0.0


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        # Novelty based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            novelty = self._calculate_novelty(sorted_data[i, :-1], np.array(new_archive)[:,:-1])
            if novelty > self.novelty_threshold:
                new_archive.append(sorted_data[i])
                
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptation for demonstration; replace with more sophisticated methods
        self.F = max(0.1, min(1.0, self.F + 0.1 * np.random.randn()))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * np.random.randn()))
        self.novelty_threshold = max(0.05, min(0.5, self.novelty_threshold + 0.01 * np.random.randn()))

2025-06-23 11:46:02 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:46:02 ERROR Can not run the algorithm
2025-06-23 11:46:02 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1000
2025-06-23 11:46:02 INFO FeHistory: [-221.54936675 -222.29096618 -221.54892195 -223.1042719  -222.45599268
 -221.10244308 -223.57629406 -222.47730621 -222.82163229 -221.91007771
 -221.1295295  -220.78929861 -221.41532461 -220.96343238 -222.76790419
 -222.39090567 -222.52977192 -222.17024876 -221.58133143 -223.04038655
 -222.32258877 -221.60092566 -222.23289482 -223.49762293 -221.57279893
 -220.2287894  -220.54341553 -221.32550433 -222.84733601 -221.79861279
 -221.92078351 -221.25788802 -221.2258552  -221.82984907 -221.76319798
 -222.06407521 -222.13288425 -221.23541211 -222.68572135 -222.35542496
 -221.55322902 -221.82675819 -221.40372269 -223.3559719  -222.92382328
 -223.18120683 -222.66868522 -221.80948577 -222.02374759 -221.54646391
 -220.68153366 -224.27125352 -222.68218733 -221.38367643 -222.25216068
 -222.01581799 -222.21580773 -222.67038066 -222.98333892 -221.85345872
 -221.94998274 -222.71523961 -223.11109608 -222.70994204 -222.04029341
 -221.56739911 -222.90063339 -221.17844012 -223.23679086 -222.7954387
 -221.74493729 -221.88404574 -221.16670298 -222.04199008 -222.2012365
 -222.59088534 -222.87633512 -220.79440829 -222.64087431 -223.53336129
 -223.31943931 -222.28025354 -221.43531603 -222.48782782 -220.73841068
 -222.22878959 -222.64195473 -222.8493196  -222.86259953 -223.58107709
 -221.23659613 -223.49874773 -220.69822058 -222.09605535 -222.42176981
 -222.07186343 -222.76883984 -221.82186622 -222.12978766 -221.2819148
 -222.32891271]
2025-06-23 11:46:02 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:46:02 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:46:02 ERROR Can not run the algorithm
2025-06-23 11:46:03 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 11:46:03 INFO FeHistory: [159.94021893 166.51822676 152.03956191 199.5614554  207.53100083
 217.04881346 179.81916188 167.4627015  211.07011548 187.54771462
 217.5227745  194.7034621  187.06281235 154.03873892 193.72140177
 198.22972649 200.26976451 181.16090583 175.51076003 204.9646887
 192.22421961 197.27762104 187.87934604 190.3447478  192.98124577
 200.20075471 210.51208977 205.26285013 173.60231333 167.56914949
 165.31572573 217.27495136 223.56014506 193.46734789 174.54108368
 152.24404318 157.74806825 199.40818454 159.16600018 196.61187805
 144.21936618 190.07482349 181.20245089 154.54059088 192.84854864
 198.93324353 191.42209333 179.97797859 202.58409066 210.6440494
 196.64171649 181.92769763 159.88821711 178.59519559 194.68714762
 215.36866308 173.97148652 175.97265086 191.29890046 175.58859335
 195.07863485 172.00394924 174.00590777 182.57986166 188.2191783
 203.42957939 184.54430442 133.64750606 205.15645726 240.15938006
 214.07941503 182.61630467 201.98611742 182.17511817 206.04494536
 196.06687055 184.08376885 195.18422425 203.50372792 184.74840108
 165.78544729 187.56385649 200.57277403 167.14445938 221.30291219
 195.66936812 174.95683407 210.54938591 188.07935459 178.19645748
 169.33827967 230.58643226 198.31989599 156.1298466  203.131814
 201.6325039  193.1127122  206.68244954 190.96664953 200.53314639
 165.91910168]
2025-06-23 11:46:03 INFO Expected Optimum FE: -100
2025-06-23 11:46:03 INFO Unimodal AOCC mean: 0.1753
2025-06-23 11:46:03 INFO Multimodal (single component) AOCC mean: 0.1000
2025-06-23 11:46:03 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:46:03 INFO AOCC mean: 0.0917
2025-06-23 11:46:03 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 11:46:03 ERROR Can not run the algorithm
2025-06-23 11:46:03 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1750
2025-06-23 11:46:03 INFO FeHistory: [-701.32481457 -701.27809657 -701.31754786 -701.32842219 -701.29386427
 -701.29096054 -701.33625135 -701.32035661 -701.2961023  -701.30917043
 -701.30682254 -701.3190471  -701.28204693 -701.30190373 -701.29510488
 -701.30033703 -701.29628621 -701.31928233 -701.31632008 -701.32782697
 -701.30608414 -701.33073579 -701.356316   -701.27586341 -701.29670395
 -701.29208193 -701.30087702 -701.31763233 -701.27550177 -701.30400824
 -701.31732412 -701.29592743 -701.33965981 -701.31151367 -701.29146951
 -701.31088323 -701.29859735 -701.32316742 -701.27384723 -701.29434199
 -701.30946127 -701.32723885 -701.2899405  -701.2751609  -701.29088443
 -701.28879334 -701.30956796 -701.31103286 -701.30161271 -701.28129719
 -701.31484765 -701.32790542 -701.29718397 -701.32027922 -701.31628995
 -701.35638835 -701.28966769 -701.33427706 -701.32150225 -701.32420299
 -701.33627688 -701.32356872 -701.29236223 -701.35520101 -701.29948358
 -701.27665178 -701.27997893 -701.30001961 -701.26926713 -701.30211327
 -701.27805098 -701.28796911 -701.30261973 -701.32530387 -701.29542713
 -701.30797647 -701.33525122 -701.29723292 -701.33835111 -701.31697099
 -701.32461205 -701.32243865 -701.2947506  -701.32281457 -701.30920334
 -701.30807216 -701.30765903 -701.28392549 -701.30768149 -701.32141926
 -701.30523501 -701.28571935 -701.33650806 -701.31961992 -701.33163603
 -701.31952609 -701.297351   -701.31741088 -701.29682826 -701.32117336
 -701.33458587]
2025-06-23 11:46:03 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 11:46:03 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithLevyAndArchive
import numpy as np
from scipy.stats import levy

# Name: AdaptiveDEwithLevyAndArchive
# Description: Combines adaptive DE, Lvy flights, and an archive for efficient multimodal optimization.
# Code:
class AdaptiveDEwithLevyAndArchive:
    """
    Combines adaptive differential evolution (DE) with Lvy flights and a novelty archive 
    to enhance exploration and exploitation in multimodal optimization problems.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.levy_alpha = 1.5 # Levy flight parameter
        self.exploration_rate = 0.7 # Probability of using Levy flight


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1
        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:
                offspring[i] = self._levy_flight(population[i])
            else:
                offspring[i] = self._differential_evolution(population, i)
        return offspring

    def _levy_flight(self, x):
        step = levy.rvs(self.levy_alpha, size=self.dim)
        step = (self.upper_bounds - self.lower_bounds) * step / np.max(np.abs(step))
        return np.clip(x + step, self.lower_bounds, self.upper_bounds)

    def _differential_evolution(self, population, i):
        a, b, c = self._select_different(i, population.shape[0])
        mutant = population[a] + self.F * (population[b] - population[c])
        mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
        return self._crossover(population[i], mutant)

    def _select_different(self, exclude, pop_size):
        candidates = list(range(pop_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        if len(self.archive) < self.archive_size:
            self.archive = np.vstack((self.archive, combined))
        else:
            self.archive = np.vstack((self.archive, combined))
            sorted_indices = np.argsort(self.archive[:, -1])
            self.archive = self.archive[sorted_indices][:self.archive_size]

        return self.archive

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 11:46:03 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 11:46:03 ERROR Can not run the algorithm
2025-06-23 11:46:03 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.0994
2025-06-23 11:46:03 INFO FeHistory: [-222.32000783 -222.28619248 -221.15364166 -222.00548306 -221.94458852
 -221.24873837 -222.39678539 -222.63840727 -223.50238639 -221.63191369
 -222.05287501 -222.21880654 -222.04017665 -220.72142728 -220.49066061
 -221.89137697 -222.41471828 -222.5325843  -222.60959761 -223.16114395
 -222.17866163 -221.97611283 -221.85071305 -222.10623819 -222.09389079
 -222.27754277 -222.59234622 -220.58074482 -222.91117021 -220.48723653
 -220.32941008 -220.17339995 -223.08201102 -224.1420629  -223.22376578
 -220.86169983 -222.03494868 -221.89053995 -222.55526447 -223.69027313
 -222.92159964 -222.050664   -222.83678235 -221.67739541 -220.89001479
 -222.32971839 -221.23370629 -220.91496427 -220.36967632 -221.78178865
 -222.71914415 -221.9534689  -222.36767022 -223.33325381 -224.01223117
 -220.70742152 -223.12329223 -221.39523377 -223.36287034 -223.53855001
 -220.65650603 -221.95858969 -223.10311961 -220.62060867 -222.82601323
 -221.68536651 -221.64709804 -221.78528916 -222.36013706 -222.43783472
 -222.29432046 -221.30922951 -221.70628898 -221.84197226 -221.22674631
 -221.47729468 -222.02040059 -222.68392428 -221.29649525 -221.61041547
 -221.65362761 -220.62812845 -222.28434734 -222.17333504 -221.75727048
 -220.72251056 -222.42906119 -222.29076067 -221.61839598 -223.20567187
 -223.0838279  -223.36039059 -222.84380595 -222.77113484 -222.34306958
 -221.27350387 -220.5181641  -221.14932507 -222.95219955 -221.57252608
 -221.19530987]
2025-06-23 11:46:03 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 11:46:03 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 11:46:03 ERROR Can not run the algorithm
2025-06-23 11:46:03 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 11:46:03 INFO FeHistory: [190.41890179 182.27892921 204.34374197 203.14340107 132.80562762
 186.26104018 222.60137871 171.86444021 178.45376079 212.53809884
 202.18828336 213.33022134 180.93812724 173.60466936 204.27261408
 190.68760159 192.65748403 191.96731956 192.26835879 178.10501457
 177.0293745  177.88008314 196.13150273 220.41786387 169.3602
 152.4054154  184.49415051 169.38938696 145.01880773 201.38601229
 223.0376775  167.38002081 194.17088611 202.81391356 158.72614076
 182.51160058 168.05883814 172.37165627 169.12005259 153.55474813
 161.92152455 179.64361509 205.50684304 192.58604506 173.8088926
 218.83004554 211.42775853 158.58403083 213.41719265 176.16788297
 176.38434732 173.57864468 165.53857941 209.30951273 211.67350779
 187.98124685 180.04146748 199.31360086 156.91774265 184.93755603
 191.46469997 180.04560982 188.88538114 220.78724422 138.12354122
 187.72112481 188.53896998 193.50981042 195.27644584 180.19067783
 158.33537573 180.46383837 206.10798131 210.92510355 179.23746692
 187.1147758  201.99652971 234.09763377 167.65554818 207.12813346
 194.17231627 205.86059013 175.63499583 162.20339295 171.55010577
 171.81919447 158.96041221 201.45020644 198.53932508 134.25807965
 162.02378558 189.98650652 225.56270402 175.89743439 187.99584295
 180.87322626 157.67580766 177.50270935 193.23222892 185.63092952
 207.3046052 ]
2025-06-23 11:46:03 INFO Expected Optimum FE: -100
2025-06-23 11:46:03 INFO Unimodal AOCC mean: 0.1750
2025-06-23 11:46:03 INFO Multimodal (single component) AOCC mean: 0.0994
2025-06-23 11:46:03 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 11:46:03 INFO AOCC mean: 0.0915
