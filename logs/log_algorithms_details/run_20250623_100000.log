2025-06-23 10:00:01 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:00:01 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:00:01 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:00:01 ERROR Can not run the algorithm
2025-06-23 10:00:01 ERROR Can not run the algorithm
2025-06-23 10:00:01 INFO Run function 2 complete. FEHistory len: 201, AOCC: 0.1753
2025-06-23 10:00:01 INFO FeHistory: [-701.31252996 -701.33847914 -701.29166151 -701.28184863 -701.27367545
 -701.33132547 -701.32760061 -701.31650379 -701.2970987  -701.32391536
 -701.27713434 -701.31389849 -701.33146521 -701.31917495 -701.29961444
 -701.30471566 -701.3435335  -701.27787583 -701.30779546 -701.33744889
 -701.30853647 -701.31748802 -701.30425905 -701.36623009 -701.32333564
 -701.3121845  -701.29242931 -701.28984173 -701.30695975 -701.27025216
 -701.28049279 -701.31894892 -701.35910961 -701.30099376 -701.31749061
 -701.31671205 -701.29869948 -701.30699336 -701.29214329 -701.27843935
 -701.30889223 -701.30284728 -701.28282023 -701.29305446 -701.32235712
 -701.30667914 -701.31656925 -701.31028951 -701.32249358 -701.30137581
 -701.33004724 -701.36486247 -701.303138   -701.31448694 -701.30911998
 -701.31797038 -701.3059716  -701.29559769 -701.33688182 -701.31335451
 -701.31597796 -701.30779985 -701.30478871 -701.29493969 -701.30375546
 -701.28326066 -701.27844268 -701.2997101  -701.29649428 -701.31698144
 -701.32021533 -701.33438889 -701.28422491 -701.32720875 -701.3181784
 -701.30606437 -701.33957187 -701.32139022 -701.32736954 -701.29890781
 -701.32455826 -701.31486985 -701.34258973 -701.34201339 -701.30049315
 -701.30763655 -701.30652696 -701.35664276 -701.29613871 -701.31302382
 -701.32567274 -701.28567255 -701.27432221 -701.28932789 -701.30189552
 -701.29179315 -701.30548727 -701.33613249 -701.26581656 -701.29459498
 -701.3357579  -701.32946246 -701.27949683 -701.29786245 -701.30826353
 -701.28977288 -701.34834989 -701.30871904 -701.29454483 -701.30125482
 -701.29626254 -701.26624114 -701.29688257 -701.27303595 -701.29535406
 -701.2911277  -701.30368252 -701.31969023 -701.33180893 -701.29404533
 -701.31835401 -701.28415331 -701.30312143 -701.31369508 -701.30013616
 -701.31081096 -701.33564751 -701.3198138  -701.29678535 -701.29864408
 -701.30764257 -701.29070455 -701.30956054 -701.30371068 -701.30282781
 -701.3263924  -701.28354829 -701.27319911 -701.29733029 -701.32499402
 -701.27174357 -701.32238015 -701.29352024 -701.2928284  -701.29974619
 -701.29150291 -701.30290476 -701.30907613 -701.2629149  -701.31625033
 -701.30963223 -701.27027579 -701.29276116 -701.329515   -701.32676937
 -701.3337262  -701.29073054 -701.30270887 -701.31873017 -701.30664156
 -701.28183238 -701.35583398 -701.33732282 -701.30942313 -701.2864324
 -701.27736618 -701.28833444 -701.31929425 -701.30106281 -701.31685417
 -701.29077849 -701.32228844 -701.31276605 -701.29757937 -701.28350853
 -701.33604075 -701.29949327 -701.30019026 -701.28602767 -701.29454155
 -701.30711706 -701.29630743 -701.35726643 -701.32222821 -701.30494681
 -701.32812043 -701.30458348 -701.29567492 -701.29626133 -701.29622171
 -701.32211523 -701.30081003 -701.28870738 -701.34124093 -701.30741723
 -701.29152332 -701.34187128 -701.27463654 -701.28109612 -701.27926696
 -701.3260934 ]
2025-06-23 10:00:01 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:00:01 INFO Good algorithm:
Algorithm Name: AdaptiveLatinHypercubeDEwithEnhancedArchive
# Name: AdaptiveLatinHypercubeDEwithEnhancedArchive
# Description: Combines Latin Hypercube sampling, adaptive differential evolution, and an enhanced archive for robust multimodal optimization.
# Code:
import numpy as np
from scipy.stats import qmc

class AdaptiveLatinHypercubeDEwithEnhancedArchive:
    """
    Combines Latin Hypercube sampling, adaptive differential evolution, and an enhanced archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size:int=100, archive_size:int=200, initial_F=0.5, initial_CR=0.9):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.F = initial_F  # Initial DE scaling factor
        self.CR = initial_CR  # Initial DE crossover rate
        self.archive = np.empty((0, dim + 1)) # Initialize empty archive
        self.archive_diversity_threshold = 0.1 #Threshold for archive diversity check


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        # Initialize population using Latin Hypercube Sampling
        sampler = qmc.LatinHypercube(self.dim)
        population = sampler.random(self.population_size)
        population = population * (self.upper_bounds - self.lower_bounds) + self.lower_bounds
        fitness = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(self.archive)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            self.archive = self._update_archive(np.vstack((self.archive[:, :-1], offspring)), np.concatenate((self.archive[:, -1], offspring_fitness)))
            self._adapt_parameters()
            self._update_best(self.archive)
            if self._check_archive_diversity() < self.archive_diversity_threshold:
                self._inject_diversity()


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self, archive):
        offspring = []
        for _ in range(self.population_size):
            a, b, c = self._select_three_different(archive)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(archive[0, :-1], mutant) #Using best solution from archive as target
            offspring.append(trial)
        return np.array(offspring)

    def _select_three_different(self, archive):
        indices = np.random.choice(len(archive), 3, replace=False)
        return archive[indices, :-1]

    def _crossover(self, target, mutant):
        crosspoints = np.random.rand(self.dim) < self.CR
        return np.where(crosspoints, mutant, target)

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        sorted_data = combined[combined[:, -1].argsort()]
        return sorted_data[:min(len(sorted_data), self.archive_size)]

    def _adapt_parameters(self):
        if len(self.archive) > 10:
            success_rate = np.mean(self.archive[10:, -1] < self.archive[:10, -1])
            self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.2)))
            self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

    def _update_best(self, archive):
        best_index = np.argmin(archive[:, -1])
        if archive[best_index, -1] < self.best_fitness_overall:
            self.best_fitness_overall = archive[best_index, -1]
            self.best_solution_overall = archive[best_index, :-1]

    def _check_archive_diversity(self):
        if len(self.archive) < 2 : return 1.0
        distances = np.linalg.norm(self.archive[:-1, :-1] - self.archive[1:, :-1], axis=1)
        return np.mean(distances) / np.linalg.norm(self.upper_bounds - self.lower_bounds)

    def _inject_diversity(self):
        num_to_inject = int(0.2 * self.archive_size) # Inject 20% diverse solutions
        new_solutions = np.random.uniform(self.lower_bounds, self.upper_bounds, (num_to_inject, self.dim))
        new_fitness = objective_function(new_solutions)
        self.archive = self._update_archive(np.vstack((self.archive[:, :-1], new_solutions)), np.concatenate((self.archive[:, -1], new_fitness)))


2025-06-23 10:00:01 INFO Run function 2 complete. FEHistory len: 201, AOCC: 0.1758
2025-06-23 10:00:01 INFO FeHistory: [-701.35973935 -701.30135757 -701.33164644 -701.30704902 -701.30097885
 -701.32414355 -701.36364351 -701.30544329 -701.33814426 -701.29160321
 -701.34114685 -701.26666984 -701.26965107 -701.30886054 -701.28740053
 -701.38812522 -701.28486937 -701.28646492 -701.29099515 -701.34088036
 -701.30044228 -701.34130528 -701.28354214 -701.29676945 -701.32271199
 -701.28711057 -701.35596799 -701.30083792 -701.30632792 -701.28460689
 -701.30702147 -701.3020383  -701.32120372 -701.30415902 -701.29956077
 -701.2879257  -701.37534051 -701.33505023 -701.3141816  -701.29312565
 -701.30417551 -701.30308153 -701.32128192 -701.29694195 -701.29269214
 -701.31615739 -701.28260505 -701.28514118 -701.31573723 -701.30381998
 -701.2876784  -701.31007451 -701.30762772 -701.3006102  -701.31465914
 -701.32964106 -701.29643957 -701.30340184 -701.30596675 -701.27214839
 -701.28906378 -701.30718468 -701.29872062 -701.34149368 -701.29907281
 -701.31235832 -701.33618175 -701.29242602 -701.33564427 -701.35029422
 -701.30443984 -701.30465534 -701.28781936 -701.32797328 -701.29224611
 -701.28841951 -701.27122923 -701.36690024 -701.31529979 -701.33573555
 -701.3806651  -701.34247386 -701.31021016 -701.326569   -701.30355286
 -701.29691453 -701.28773113 -701.33577529 -701.32278677 -701.35753953
 -701.32272013 -701.32988185 -701.33724586 -701.29936061 -701.25324977
 -701.33804319 -701.32336394 -701.29184249 -701.27859657 -701.31002422
 -701.30123654 -701.34560357 -701.32224422 -701.27624032 -701.28171052
 -701.34361246 -701.28329794 -701.32442858 -701.29750314 -701.30273106
 -701.28809448 -701.29007798 -701.3154941  -701.3183685  -701.3235934
 -701.31418276 -701.28537364 -701.30872721 -701.30135616 -701.29554293
 -701.33443793 -701.29596921 -701.35841897 -701.27754541 -701.31579517
 -701.32832024 -701.33220408 -701.30688933 -701.31323893 -701.35469121
 -701.30043957 -701.28672002 -701.25877174 -701.27504538 -701.30479379
 -701.27339542 -701.29792724 -701.30326715 -701.32191398 -701.29167505
 -701.3288621  -701.30696574 -701.3240548  -701.28700329 -701.29577541
 -701.29333956 -701.28440318 -701.3161218  -701.32180106 -701.2895903
 -701.32137628 -701.30053353 -701.31839228 -701.31377663 -701.30218311
 -701.34746866 -701.31218821 -701.29513643 -701.28592669 -701.326669
 -701.29482584 -701.32765075 -701.27962872 -701.32426946 -701.33315887
 -701.30425196 -701.29708246 -701.33282449 -701.28605349 -701.32228624
 -701.28877801 -701.2816072  -701.31570824 -701.29134171 -701.28983099
 -701.32680187 -701.27430549 -701.37815922 -701.32852267 -701.30979054
 -701.30952883 -701.30375048 -701.34670982 -701.30187536 -701.30968843
 -701.31873411 -701.34816402 -701.32079818 -701.34072437 -701.28589871
 -701.28649147 -701.32370416 -701.28741371 -701.29993554 -701.29455188
 -701.2868301  -701.27615745 -701.33487999 -701.30146316 -701.29563722
 -701.33448947]
2025-06-23 10:00:01 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:00:01 INFO Good algorithm:
Algorithm Name: AdaptiveLatinHypercubeDEArchiveEA
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveLatinHypercubeDEArchiveEA
# Description: Combines Latin Hypercube Sampling, adaptive Differential Evolution, and a fitness-based archive for robust multimodal optimization.
# Code:
class AdaptiveLatinHypercubeDEArchiveEA:
    """
    Combines Latin Hypercube Sampling, adaptive Differential Evolution, and a fitness-based archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 100, archive_size: int = 200, archive_diversity_threshold: float = 0.1):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.archive_diversity_threshold = archive_diversity_threshold
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.F = 0.5  # Initial DE scaling factor
        self.CR = 0.9  # Initial DE crossover rate
        self.archive = np.empty((0, dim + 1))  # Initialize empty archive
        self.sampler = qmc.LatinHypercube(d=self.dim)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        # Initialize population using Latin Hypercube Sampling
        population = self._initialize_population()
        fitness = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(self.archive)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            self.archive = self._update_archive(np.vstack((self.archive[:, :-1], offspring)), np.concatenate((self.archive[:, -1], offspring_fitness)))
            self._adapt_parameters()
            self._update_best(self.archive)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sample = self.sampler.random(n=self.population_size)
        return qmc.scale(sample, self.lower_bounds, self.upper_bounds)

    def _differential_evolution(self, archive):
        offspring = []
        for _ in range(self.population_size):
            a, b, c = self._select_three_different(archive)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(archive[0, :-1], mutant)  # Using best solution from archive as target
            offspring.append(trial)
        return np.array(offspring)

    def _select_three_different(self, archive):
        indices = np.random.choice(len(archive), 3, replace=False)
        return archive[indices, :-1]

    def _crossover(self, target, mutant):
        crosspoints = np.random.rand(self.dim) < self.CR
        return np.where(crosspoints, mutant, target)

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        sorted_data = combined[combined[:, -1].argsort()]
        sorted_data = self._maintain_archive_diversity(sorted_data)
        return sorted_data[:min(len(sorted_data), self.archive_size)]

    def _maintain_archive_diversity(self, sorted_data):
        if len(sorted_data) > self.archive_size:
            archive = sorted_data[:self.archive_size, :-1]
            distances = np.linalg.norm(sorted_data[self.archive_size:, :-1][:, np.newaxis, :] - archive[np.newaxis, :, :], axis=2)
            min_distances = np.min(distances, axis=1)
            indices_to_keep = np.argsort(min_distances)[::-1][:self.archive_size - len(archive)]
            to_add = sorted_data[self.archive_size:][indices_to_keep]
            sorted_data = np.vstack((sorted_data[:self.archive_size],to_add))
        return sorted_data

    def _adapt_parameters(self):
        if len(self.archive) > 10:
            success_rate = np.mean(self.archive[10:, -1] < self.archive[:10, -1])
            archive_diversity = np.mean(np.linalg.norm(self.archive[1:, :-1] - self.archive[:-1, :-1], axis=1)) if len(self.archive) > 1 else 0
            self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.2) + 0.05 * (self.archive_diversity_threshold - archive_diversity)))
            self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

    def _update_best(self, archive):
        best_index = np.argmin(archive[:, -1])
        if archive[best_index, -1] < self.best_fitness_overall:
            self.best_fitness_overall = archive[best_index, -1]
            self.best_solution_overall = archive[best_index, :-1]

def objective_function(X):
    #Example objective function (replace with your actual benchmark functions)
    return np.sum(X**2, axis=1)
2025-06-23 10:00:01 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:00:01 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:00:01 ERROR Can not run the algorithm
2025-06-23 10:00:01 ERROR Can not run the algorithm
2025-06-23 10:00:01 INFO Run function 15 complete. FEHistory len: 201, AOCC: 0.1047
2025-06-23 10:00:01 INFO FeHistory: [-221.49283149 -223.57130551 -221.38445666 -221.16152743 -221.51668091
 -221.89501796 -222.28644764 -222.50713533 -223.03215842 -221.4067908
 -220.95186515 -223.88987873 -221.95867671 -222.32160652 -222.53858293
 -221.33790643 -221.94823263 -221.82208101 -222.12514239 -222.13673116
 -221.48312665 -223.13789287 -223.71706255 -222.79597036 -223.6643559
 -220.98902517 -221.99539986 -222.60692653 -221.92967863 -222.13340381
 -222.40342034 -221.51791901 -223.05524327 -220.25310049 -222.16620327
 -222.59195916 -222.16285471 -221.0708965  -224.08652027 -221.64091349
 -221.97078861 -221.48259591 -222.44567982 -222.94911869 -222.94257703
 -222.51122868 -222.54376638 -222.00502318 -220.7288388  -223.12194065
 -223.03007397 -223.36143618 -223.01699605 -222.20811754 -223.66268748
 -222.64475738 -222.43542881 -222.37475996 -220.81619643 -221.08411979
 -220.71816599 -221.53507077 -221.90587887 -221.57682777 -224.17948294
 -223.00307938 -222.38018178 -222.34291691 -221.2539546  -222.49210835
 -221.53650398 -223.06734448 -220.95704901 -222.0859839  -221.75593957
 -221.56003913 -221.83740568 -221.55110156 -222.66588589 -221.63414795
 -222.477106   -222.72934029 -221.76779639 -221.72658999 -222.41119176
 -223.07292025 -222.16547344 -221.13357123 -223.23356581 -220.56409124
 -222.95469826 -221.12205423 -220.99127122 -222.61632813 -225.30402622
 -222.88381253 -223.08506851 -222.57209159 -222.65750775 -223.14839737
 -223.96491638 -223.4895135  -222.94686186 -222.80472808 -220.67517548
 -222.58098794 -222.52032328 -220.3583184  -221.35244535 -222.73350198
 -221.04557853 -221.15423146 -220.87977766 -221.92729838 -224.12123817
 -221.24273773 -222.89597051 -220.7058791  -223.20528367 -223.45188685
 -221.60574551 -222.50446933 -221.59487789 -220.96941438 -221.23182499
 -222.99201663 -221.4267975  -222.13258002 -221.54489426 -220.5057868
 -220.50868698 -222.74482764 -223.77166849 -220.63686731 -223.89170395
 -221.51713303 -220.94421616 -222.82705808 -221.85326686 -223.19741607
 -221.95943916 -220.92146479 -223.5110921  -222.47584192 -221.37155715
 -221.5558464  -221.79156269 -220.69096497 -224.74485161 -221.64125801
 -222.50539896 -220.81012519 -222.13370257 -222.07750913 -221.22077094
 -221.20427984 -221.87008271 -221.07050436 -220.32798301 -221.782251
 -221.70716469 -221.41694918 -223.11770916 -221.78661264 -222.82809578
 -221.25316059 -221.66333666 -220.79411624 -220.69466368 -221.73897067
 -222.50748866 -220.54525721 -221.56372011 -224.16533026 -222.51079372
 -222.79987591 -222.14143905 -221.47613981 -220.11894044 -223.69124597
 -222.65296235 -221.85198378 -222.55465632 -220.88124404 -222.09807169
 -223.5500643  -223.19745477 -221.9824713  -221.9234962  -221.51742423
 -222.41050221 -222.51418399 -222.52793987 -220.56140535 -221.17043972
 -223.34232479 -222.69699061 -221.80754258 -220.87330693 -222.28283077
 -221.64166656]
2025-06-23 10:00:01 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:00:01 INFO Good algorithm:
Algorithm Name: AdaptiveLatinHypercubeDEwithEnhancedArchive
# Name: AdaptiveLatinHypercubeDEwithEnhancedArchive
# Description: Combines Latin Hypercube sampling, adaptive differential evolution, and an enhanced archive for robust multimodal optimization.
# Code:
import numpy as np
from scipy.stats import qmc

class AdaptiveLatinHypercubeDEwithEnhancedArchive:
    """
    Combines Latin Hypercube sampling, adaptive differential evolution, and an enhanced archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size:int=100, archive_size:int=200, initial_F=0.5, initial_CR=0.9):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.F = initial_F  # Initial DE scaling factor
        self.CR = initial_CR  # Initial DE crossover rate
        self.archive = np.empty((0, dim + 1)) # Initialize empty archive
        self.archive_diversity_threshold = 0.1 #Threshold for archive diversity check


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        # Initialize population using Latin Hypercube Sampling
        sampler = qmc.LatinHypercube(self.dim)
        population = sampler.random(self.population_size)
        population = population * (self.upper_bounds - self.lower_bounds) + self.lower_bounds
        fitness = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(self.archive)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            self.archive = self._update_archive(np.vstack((self.archive[:, :-1], offspring)), np.concatenate((self.archive[:, -1], offspring_fitness)))
            self._adapt_parameters()
            self._update_best(self.archive)
            if self._check_archive_diversity() < self.archive_diversity_threshold:
                self._inject_diversity()


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self, archive):
        offspring = []
        for _ in range(self.population_size):
            a, b, c = self._select_three_different(archive)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(archive[0, :-1], mutant) #Using best solution from archive as target
            offspring.append(trial)
        return np.array(offspring)

    def _select_three_different(self, archive):
        indices = np.random.choice(len(archive), 3, replace=False)
        return archive[indices, :-1]

    def _crossover(self, target, mutant):
        crosspoints = np.random.rand(self.dim) < self.CR
        return np.where(crosspoints, mutant, target)

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        sorted_data = combined[combined[:, -1].argsort()]
        return sorted_data[:min(len(sorted_data), self.archive_size)]

    def _adapt_parameters(self):
        if len(self.archive) > 10:
            success_rate = np.mean(self.archive[10:, -1] < self.archive[:10, -1])
            self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.2)))
            self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

    def _update_best(self, archive):
        best_index = np.argmin(archive[:, -1])
        if archive[best_index, -1] < self.best_fitness_overall:
            self.best_fitness_overall = archive[best_index, -1]
            self.best_solution_overall = archive[best_index, :-1]

    def _check_archive_diversity(self):
        if len(self.archive) < 2 : return 1.0
        distances = np.linalg.norm(self.archive[:-1, :-1] - self.archive[1:, :-1], axis=1)
        return np.mean(distances) / np.linalg.norm(self.upper_bounds - self.lower_bounds)

    def _inject_diversity(self):
        num_to_inject = int(0.2 * self.archive_size) # Inject 20% diverse solutions
        new_solutions = np.random.uniform(self.lower_bounds, self.upper_bounds, (num_to_inject, self.dim))
        new_fitness = objective_function(new_solutions)
        self.archive = self._update_archive(np.vstack((self.archive[:, :-1], new_solutions)), np.concatenate((self.archive[:, -1], new_fitness)))


2025-06-23 10:00:01 INFO Run function 15 complete. FEHistory len: 201, AOCC: 0.1032
2025-06-23 10:00:01 INFO FeHistory: [-223.19868401 -222.57335537 -222.77077255 -222.55996604 -221.94115107
 -222.05499768 -220.34864264 -221.68122192 -222.22387169 -222.10347514
 -222.59329477 -222.41861235 -223.12890823 -221.95796348 -221.04958577
 -223.77172787 -224.0396316  -222.51594633 -222.01115837 -221.10061185
 -220.96887094 -221.78823126 -221.93290253 -222.23757503 -221.66712165
 -223.11046735 -222.02997846 -221.13775272 -222.32586451 -222.40374797
 -221.38263719 -222.51158003 -221.09932521 -221.13108831 -221.25858933
 -221.73674879 -222.53579707 -223.10042562 -221.24752006 -221.09160697
 -223.29146409 -222.70765531 -222.65333741 -221.66581776 -221.92340443
 -223.52647353 -221.51783134 -221.44814999 -221.47665284 -222.82521517
 -222.32726642 -222.18651244 -220.79995847 -221.75035373 -220.07390539
 -222.12402737 -223.18772537 -221.7878598  -221.63812466 -221.23967301
 -222.44656096 -220.86823068 -222.33037622 -222.37764196 -222.01260707
 -223.57749879 -220.84799967 -222.01834882 -222.0564783  -222.08996436
 -221.81443801 -222.16209539 -223.58755407 -221.54974481 -221.97208103
 -221.23169077 -220.80353968 -220.6274291  -221.53463022 -222.30195458
 -221.45521938 -223.32403487 -221.5670413  -222.26797741 -222.0660751
 -222.28052593 -221.49524794 -224.13725578 -222.16079199 -222.37462157
 -220.99642371 -221.17944064 -224.58210258 -221.30211922 -222.6450002
 -221.39511972 -221.5616191  -220.75188502 -221.13382824 -224.90827401
 -223.63416637 -222.00314148 -223.04978243 -220.88301796 -222.18012021
 -221.98014294 -220.9361168  -221.91271531 -222.93366702 -221.9552361
 -225.00223153 -222.65241898 -221.65537168 -221.29222206 -222.94132677
 -221.88171787 -220.89959138 -221.69221955 -223.04106348 -221.96139276
 -221.29116865 -221.90908925 -221.77535174 -222.50747326 -220.86315152
 -222.51175226 -222.09302345 -221.9899799  -223.0035997  -220.78001289
 -221.93371648 -221.80829999 -221.56016593 -221.7714477  -220.73399483
 -222.53030219 -222.36687342 -222.58068257 -221.97272142 -221.34969507
 -222.59618124 -222.04365217 -222.4231087  -222.82193977 -221.85725497
 -223.48017449 -220.94800794 -222.79460353 -221.78562235 -222.06169505
 -222.53378884 -222.31541259 -222.39413479 -222.44093585 -220.84385288
 -222.07182602 -222.44643136 -221.79943619 -221.0771454  -223.36033123
 -222.98104305 -220.59782532 -221.51609788 -221.97661883 -223.39315538
 -222.21214345 -220.61708804 -222.7539094  -222.82162755 -221.67285303
 -224.43902409 -222.93110082 -220.84654325 -221.76418765 -222.13924119
 -220.86057525 -221.73872586 -222.0992766  -222.30834554 -222.94003549
 -221.0868473  -222.37042323 -222.95273061 -221.30845357 -223.06344298
 -221.32022923 -221.06524525 -222.88012071 -221.00704098 -220.30192775
 -221.94371305 -222.61138965 -222.40383651 -222.7248815  -223.88703587
 -221.24149675 -221.55779179 -222.34356205 -223.4576622  -221.37263397
 -222.54985322]
2025-06-23 10:00:01 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:00:01 INFO Good algorithm:
Algorithm Name: AdaptiveLatinHypercubeDEArchiveEA
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveLatinHypercubeDEArchiveEA
# Description: Combines Latin Hypercube Sampling, adaptive Differential Evolution, and a fitness-based archive for robust multimodal optimization.
# Code:
class AdaptiveLatinHypercubeDEArchiveEA:
    """
    Combines Latin Hypercube Sampling, adaptive Differential Evolution, and a fitness-based archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 100, archive_size: int = 200, archive_diversity_threshold: float = 0.1):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.archive_diversity_threshold = archive_diversity_threshold
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.F = 0.5  # Initial DE scaling factor
        self.CR = 0.9  # Initial DE crossover rate
        self.archive = np.empty((0, dim + 1))  # Initialize empty archive
        self.sampler = qmc.LatinHypercube(d=self.dim)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        # Initialize population using Latin Hypercube Sampling
        population = self._initialize_population()
        fitness = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(self.archive)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            self.archive = self._update_archive(np.vstack((self.archive[:, :-1], offspring)), np.concatenate((self.archive[:, -1], offspring_fitness)))
            self._adapt_parameters()
            self._update_best(self.archive)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sample = self.sampler.random(n=self.population_size)
        return qmc.scale(sample, self.lower_bounds, self.upper_bounds)

    def _differential_evolution(self, archive):
        offspring = []
        for _ in range(self.population_size):
            a, b, c = self._select_three_different(archive)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(archive[0, :-1], mutant)  # Using best solution from archive as target
            offspring.append(trial)
        return np.array(offspring)

    def _select_three_different(self, archive):
        indices = np.random.choice(len(archive), 3, replace=False)
        return archive[indices, :-1]

    def _crossover(self, target, mutant):
        crosspoints = np.random.rand(self.dim) < self.CR
        return np.where(crosspoints, mutant, target)

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        sorted_data = combined[combined[:, -1].argsort()]
        sorted_data = self._maintain_archive_diversity(sorted_data)
        return sorted_data[:min(len(sorted_data), self.archive_size)]

    def _maintain_archive_diversity(self, sorted_data):
        if len(sorted_data) > self.archive_size:
            archive = sorted_data[:self.archive_size, :-1]
            distances = np.linalg.norm(sorted_data[self.archive_size:, :-1][:, np.newaxis, :] - archive[np.newaxis, :, :], axis=2)
            min_distances = np.min(distances, axis=1)
            indices_to_keep = np.argsort(min_distances)[::-1][:self.archive_size - len(archive)]
            to_add = sorted_data[self.archive_size:][indices_to_keep]
            sorted_data = np.vstack((sorted_data[:self.archive_size],to_add))
        return sorted_data

    def _adapt_parameters(self):
        if len(self.archive) > 10:
            success_rate = np.mean(self.archive[10:, -1] < self.archive[:10, -1])
            archive_diversity = np.mean(np.linalg.norm(self.archive[1:, :-1] - self.archive[:-1, :-1], axis=1)) if len(self.archive) > 1 else 0
            self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.2) + 0.05 * (self.archive_diversity_threshold - archive_diversity)))
            self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

    def _update_best(self, archive):
        best_index = np.argmin(archive[:, -1])
        if archive[best_index, -1] < self.best_fitness_overall:
            self.best_fitness_overall = archive[best_index, -1]
            self.best_solution_overall = archive[best_index, :-1]

def objective_function(X):
    #Example objective function (replace with your actual benchmark functions)
    return np.sum(X**2, axis=1)
2025-06-23 10:00:02 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:00:02 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:00:02 ERROR Can not run the algorithm
2025-06-23 10:00:02 ERROR Can not run the algorithm
2025-06-23 10:00:02 INFO Run function 24 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-23 10:00:02 INFO FeHistory: [184.73088875 222.6894714  191.48935275 190.15718586 188.24342402
 171.49799907 182.01303612 186.69396376 196.05782313 175.55158814
 200.28435755 163.79991483 157.2311597  181.39074323 214.75552412
 185.53176234 197.11737796 170.47904416 181.04978348 201.71800652
 185.42064324 167.48747836 179.39973548 135.03472582 211.88571261
 170.31397684 214.63140466 215.89161698 156.46781419 153.18325989
 215.21163516 221.32796651 202.90418677 215.7430605  174.93745133
 150.48560078 176.07966744 179.05437594 186.10364672 161.13791505
 186.25375582 186.44351058 183.86565702 200.65176579 188.0657167
 159.92239731 168.55113598 187.51621738 209.0818591  178.83213727
 196.32379566 181.62852126 227.00926779 208.17812769 197.73455733
 182.56450373 204.51409438 206.89941189 175.35062714 178.81619387
 199.65218431 219.94386871 211.21099664 206.41926591 198.18385365
 204.15825769 181.97342011 182.44544518 197.46821567 163.73209789
 206.31371057 162.42618211 174.22540292 189.81018957 210.76246669
 159.34946779 187.65991275 164.7094192  222.04015534 170.27172276
 197.5016167  169.80685    216.58213459 216.83858076 213.77073651
 164.61126177 187.49845526 215.32630188 180.30569788 189.65189505
 178.63982506 216.19532299 202.38888837 179.91563439 201.88928009
 189.00837764 177.99757729 214.15306839 220.30306024 206.55205475
 220.46136289 199.22574738 140.1030071  190.01935142 214.19040284
 161.65328257 166.18858594 152.08900098 176.61194808 189.38116435
 216.42733168 199.05549662 206.99537726 183.10864904 162.34666325
 223.17661016 198.74563739 188.6948742  206.08299382 151.95823581
 192.81612508 209.45154524 210.31081741 193.73715984 187.43043136
 185.92243685 177.38071496 203.58192759 135.04142158 194.63076362
 141.82498561 162.12723754 197.36526218 208.02645705 208.12322307
 211.30047505 170.46726234 205.08189983 214.11485215 173.19446235
 188.312269   193.37043736 191.88331758 184.01870614 195.8143863
 188.22996764 187.96867207 184.47882694 200.50735408 190.28746059
 190.50214579 157.0957286  240.59078019 168.36939337 186.20713487
 185.26518105 193.59612285 213.99518258 188.77203756 196.33563994
 189.20644009 214.572093   171.68183654 222.05077664 211.65745303
 185.96343759 176.38141803 195.49244799 198.94690278 189.0934863
 174.85469991 192.38251042 196.67522758 217.92302633 185.8085868
 191.25324177 212.64650784 201.85343402 196.31567749 199.2265686
 169.89300553 184.55768125 188.71959167 154.42805655 221.1899622
 196.14739235 212.14813504 207.09825541 161.65516896 198.08133364
 206.09506938 181.16808205 208.53134747 186.12224264 180.89882204
 190.85184186 207.31058576 192.51674242 177.50498647 225.57564668
 205.38702678]
2025-06-23 10:00:02 INFO Expected Optimum FE: -100
2025-06-23 10:00:02 INFO Unimodal AOCC mean: 0.1753
2025-06-23 10:00:02 INFO Multimodal (single component) AOCC mean: 0.1047
2025-06-23 10:00:02 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:00:02 INFO AOCC mean: 0.0933
2025-06-23 10:00:02 INFO Run function 24 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-23 10:00:02 INFO FeHistory: [164.69332736 224.97609628 178.75259248 156.19303961 215.06047476
 161.09274439 176.61875596 166.72471695 159.73602623 187.95972408
 164.72030343 171.04234072 185.61479126 171.95679556 204.17572467
 177.05915522 174.69489408 179.32107915 192.08582425 213.62037253
 200.53574924 196.38578854 171.97755393 206.8707501  184.81115782
 181.83687932 189.73793531 190.41417838 179.98928533 180.69064443
 190.55417851 218.45720699 172.66917726 194.67334819 151.85694592
 184.07035682 147.79811385 194.99100092 220.27904666 184.66691974
 208.06657715 180.0430373  187.75210944 194.25228538 208.55803013
 180.41037916 140.1048817  210.88482027 181.49808269 175.31078229
 208.10000858 165.36768434 186.41249282 177.95289395 153.61622896
 182.1138898  167.13688444 185.45207833 170.75654244 177.74852034
 192.96431244 172.75112039 180.51379901 168.85237862 147.99760708
 219.57108262 190.70564033 134.26300813 201.06250712 178.88378723
 193.1492256  188.70642742 174.9708545  189.37985121 163.4064647
 145.23282601 227.60507806 153.89140193 197.47731017 172.84628854
 182.11323705 216.48784655 183.55367943 190.42999271 205.52669053
 197.84553488 173.5664875  211.12882863 178.02857853 191.45554407
 198.2134733  169.15175455 193.30570099 155.65100125 182.9490945
 194.19384585 205.42852574 196.43416525 192.28755413 220.82135037
 179.75266489 222.5894244  221.07776901 188.08202493 137.17784595
 150.80563558 176.60625297 230.284946   202.08268819 230.51185215
 198.84144679 159.6617555  186.06429176 242.44879077 168.50700071
 212.7396894  216.79396041 216.86833934 232.67683562 213.33942444
 153.8370271  181.2737751  198.02735105 203.95655545 169.41723477
 188.4792601  216.69850783 202.67677373 172.65127101 208.06570058
 177.66056069 150.0967619  197.25335526 166.52394531 213.33769431
 171.9239443  210.29774394 164.07107864 192.85714949 192.51766325
 220.59975775 204.64382406 199.29311607 164.53979487 164.87432719
 150.2554179  188.56452967 199.5662602  177.22077915 196.96784243
 252.66804972 201.9474997  230.63914521 202.08481997 196.70447702
 202.09072353 232.9265892  182.89718284 184.85075785 225.99339748
 203.32338988 204.16187051 193.4553445  202.21007031 223.06663563
 179.64626674 204.31824475 227.88119571 215.19120972 198.42342494
 161.51238089 231.70500012 173.42331259 195.72291517 218.66415285
 208.45109685 215.47749028 217.92490759 191.80607614 192.21349423
 161.08881096 189.97276865 180.30656377 207.94158525 201.5695162
 174.21757606 186.68391721 164.98652755 205.43551403 163.10140576
 189.8995403  178.9403946  191.62996105 212.63406243 182.33842543
 218.56422531 176.67429013 182.5392649  207.60612607 222.61357059
 203.90779527]
2025-06-23 10:00:02 INFO Expected Optimum FE: -100
2025-06-23 10:00:02 INFO Unimodal AOCC mean: 0.1758
2025-06-23 10:00:02 INFO Multimodal (single component) AOCC mean: 0.1032
2025-06-23 10:00:02 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:00:02 INFO AOCC mean: 0.0930
2025-06-23 10:00:04 ERROR Can not run the algorithm
2025-06-23 10:00:04 INFO Run function 2 complete. FEHistory len: 1601, AOCC: 0.1753
2025-06-23 10:00:04 INFO FeHistory: [-701.30125462 -701.27862831 -701.31916435 ... -701.27639621 -701.313432
 -701.25361246]
2025-06-23 10:00:04 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:00:04 INFO Good algorithm:
Algorithm Name: AdaptiveLatinHypercubeDEArchiveEA
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveLatinHypercubeDEArchiveEA
# Description: Combines Latin Hypercube sampling, adaptive Differential Evolution, and a fitness-based archive for robust multimodal optimization.
# Code:
class AdaptiveLatinHypercubeDEArchiveEA:
    """
    Combines Latin Hypercube sampling, adaptive Differential Evolution, and a fitness-based archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], 
                 population_size: int = 100, archive_size: int = 200):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.F = 0.5  # Initial DE scaling factor
        self.CR = 0.9  # Initial DE crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling(self.population_size)
        fitness = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness, offspring_fitness)))

            #Selection from archive with diversity bias
            selected_indices = self._select_diverse_solutions(self.archive, self.population_size)
            population = self.archive[selected_indices, :-1]
            fitness = self.archive[selected_indices, -1]

            self._update_best(population, fitness)
            self._adapt_parameters(population, fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self, num_samples):
        sample = self.sampler.random(n=num_samples)
        return sample * (self.upper_bounds - self.lower_bounds) + self.lower_bounds

    def _differential_evolution(self, population, fitness):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
            offspring[i] = trial
        return offspring

    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])  # Sort by fitness
        self.archive = np.array(new_archive[:self.archive_size])
        return self.archive

    def _select_diverse_solutions(self, archive, num_solutions):
        #Prioritize solutions near the best but also add diversity
        distances_to_best = np.linalg.norm(archive[:, :-1] - self.best_solution_overall, axis=1)
        selection_probabilities = 1.0/(1+distances_to_best) #inverse distance
        selection_probabilities /= np.sum(selection_probabilities)
        selected_indices = np.random.choice(len(archive), size=num_solutions, replace=False, p=selection_probabilities)
        return selected_indices

    def _adapt_parameters(self, population, fitness):
        #Adapt F and CR based on success rate and archive diversity
        success_rate = np.mean(fitness < objective_function(population))
        archive_diversity = np.std(self.archive[:,-1]) #simple diversity metric

        if success_rate < 0.2 or archive_diversity < 0.1: #low success or low diversity
            self.F *= 0.9
            self.CR *= 0.9
        elif success_rate > 0.8 and archive_diversity > 0.5: #high success and high diversity
            self.F *= 1.1
            self.CR *= 1.1
        
        self.F = np.clip(self.F, 0.1, 1.0)
        self.CR = np.clip(self.CR, 0.1, 1.0)


def objective_function(x):  #Example objective function, replace with your GNBG functions
    return np.sum(x**2, axis=1)

2025-06-23 10:00:04 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:01:54 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1059
2025-06-23 10:01:54 INFO FeHistory: [-223.5431223  -221.82779212 -220.74246115 ... -222.82641353 -219.94777779
 -222.80590903]
2025-06-23 10:01:54 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:01:54 INFO Good algorithm:
Algorithm Name: AdaptiveLatinHypercubeDEArchiveEA
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveLatinHypercubeDEArchiveEA
# Description: Combines Latin Hypercube sampling, adaptive Differential Evolution, and a fitness-based archive for robust multimodal optimization.
# Code:
class AdaptiveLatinHypercubeDEArchiveEA:
    """
    Combines Latin Hypercube sampling, adaptive Differential Evolution, and a fitness-based archive for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], 
                 population_size: int = 100, archive_size: int = 200):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.population_size = population_size
        self.archive_size = archive_size
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.F = 0.5  # Initial DE scaling factor
        self.CR = 0.9  # Initial DE crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling(self.population_size)
        fitness = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness, offspring_fitness)))

            #Selection from archive with diversity bias
            selected_indices = self._select_diverse_solutions(self.archive, self.population_size)
            population = self.archive[selected_indices, :-1]
            fitness = self.archive[selected_indices, -1]

            self._update_best(population, fitness)
            self._adapt_parameters(population, fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self, num_samples):
        sample = self.sampler.random(n=num_samples)
        return sample * (self.upper_bounds - self.lower_bounds) + self.lower_bounds

    def _differential_evolution(self, population, fitness):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
            offspring[i] = trial
        return offspring

    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])  # Sort by fitness
        self.archive = np.array(new_archive[:self.archive_size])
        return self.archive

    def _select_diverse_solutions(self, archive, num_solutions):
        #Prioritize solutions near the best but also add diversity
        distances_to_best = np.linalg.norm(archive[:, :-1] - self.best_solution_overall, axis=1)
        selection_probabilities = 1.0/(1+distances_to_best) #inverse distance
        selection_probabilities /= np.sum(selection_probabilities)
        selected_indices = np.random.choice(len(archive), size=num_solutions, replace=False, p=selection_probabilities)
        return selected_indices

    def _adapt_parameters(self, population, fitness):
        #Adapt F and CR based on success rate and archive diversity
        success_rate = np.mean(fitness < objective_function(population))
        archive_diversity = np.std(self.archive[:,-1]) #simple diversity metric

        if success_rate < 0.2 or archive_diversity < 0.1: #low success or low diversity
            self.F *= 0.9
            self.CR *= 0.9
        elif success_rate > 0.8 and archive_diversity > 0.5: #high success and high diversity
            self.F *= 1.1
            self.CR *= 1.1
        
        self.F = np.clip(self.F, 0.1, 1.0)
        self.CR = np.clip(self.CR, 0.1, 1.0)


def objective_function(x):  #Example objective function, replace with your GNBG functions
    return np.sum(x**2, axis=1)

2025-06-23 10:01:54 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:03:54 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 10:03:54 INFO FeHistory: [184.99106955 219.68684196 186.24001991 ... 238.34904021 225.8169365
 237.33080372]
2025-06-23 10:03:54 INFO Expected Optimum FE: -100
2025-06-23 10:03:54 INFO Unimodal AOCC mean: 0.1753
2025-06-23 10:03:54 INFO Multimodal (single component) AOCC mean: 0.1059
2025-06-23 10:03:54 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:03:54 INFO AOCC mean: 0.0938
2025-06-23 10:04:17 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:04:17 ERROR Can not run the algorithm
2025-06-23 10:04:17 INFO Run function 2 complete. FEHistory len: 201, AOCC: 0.1750
2025-06-23 10:04:17 INFO FeHistory: [-701.32026718 -701.31976096 -701.29586943 -701.32479483 -701.2794527
 -701.30709425 -701.32581668 -701.29244773 -701.29708717 -701.32202506
 -701.31421591 -701.28623421 -701.2804804  -701.31114652 -701.29035047
 -701.29378404 -701.33034048 -701.32700345 -701.30651689 -701.34424463
 -701.30667093 -701.32269137 -701.33411249 -701.31790644 -701.31016268
 -701.32601405 -701.30057564 -701.30045576 -701.29089772 -701.30053694
 -701.30491948 -701.31545517 -701.3102254  -701.30250039 -701.30719612
 -701.31574471 -701.2965692  -701.30218957 -701.31087124 -701.27154117
 -701.33392688 -701.29285631 -701.32519575 -701.33066279 -701.34159188
 -701.3120598  -701.30558391 -701.30181336 -701.32501832 -701.30689681
 -701.28484625 -701.32534264 -701.32056316 -701.31355939 -701.28153157
 -701.29930925 -701.32346395 -701.28270719 -701.27921819 -701.35634331
 -701.32133613 -701.31670646 -701.34655104 -701.32026385 -701.3123235
 -701.34689795 -701.30457805 -701.29663448 -701.3199952  -701.31539561
 -701.34308621 -701.28060631 -701.32185363 -701.3157257  -701.29186324
 -701.30608094 -701.30693668 -701.28686279 -701.29356532 -701.29802644
 -701.33468043 -701.29827366 -701.30714179 -701.33190057 -701.34045288
 -701.28846442 -701.32565552 -701.31442034 -701.29496727 -701.33017489
 -701.28293862 -701.32223233 -701.2821814  -701.28835717 -701.27578916
 -701.32556873 -701.3097265  -701.29551622 -701.30215042 -701.31016816
 -701.31306191 -701.29820227 -701.31970807 -701.28825993 -701.31391978
 -701.32249976 -701.30529637 -701.27315384 -701.27113947 -701.28631186
 -701.28223068 -701.28024741 -701.28200117 -701.30239096 -701.29859322
 -701.3097634  -701.29223252 -701.32929485 -701.3366304  -701.27466919
 -701.28404665 -701.2619178  -701.31986796 -701.31051781 -701.26328107
 -701.30682992 -701.27915185 -701.2943472  -701.31604761 -701.31317959
 -701.28134477 -701.28432335 -701.28592759 -701.28717807 -701.29808946
 -701.29331208 -701.2889803  -701.28900169 -701.29519413 -701.27888535
 -701.2795624  -701.28968147 -701.25603714 -701.29517071 -701.31010726
 -701.3116049  -701.27332647 -701.26214387 -701.27193511 -701.29511834
 -701.25910609 -701.31380561 -701.27809974 -701.29400908 -701.30607045
 -701.30307474 -701.28100922 -701.30846967 -701.29820811 -701.29086956
 -701.28732459 -701.2866737  -701.27888904 -701.29016886 -701.30415011
 -701.27259681 -701.29584729 -701.29557456 -701.28042156 -701.27722682
 -701.29750509 -701.31818619 -701.32328529 -701.26842529 -701.27480282
 -701.27404704 -701.29945507 -701.28339313 -701.26841985 -701.26866321
 -701.27465838 -701.31621569 -701.28828288 -701.32647388 -701.29489306
 -701.29271581 -701.29441807 -701.29542607 -701.28695319 -701.31950673
 -701.28941815 -701.27093549 -701.30207881 -701.30099781 -701.28725338
 -701.30615426 -701.29136243 -701.27051935 -701.26856425 -701.31884355
 -701.33233634]
2025-06-23 10:04:17 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:04:17 INFO Good algorithm:
Algorithm Name: AdaptiveLatinHypercubeDE
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveLatinHypercubeDE
# Description: Combines Latin Hypercube Sampling, Adaptive Differential Evolution, and an archive for robust multimodal optimization.

class AdaptiveLatinHypercubeDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim, seed=42) # Fixed seed for reproducibility


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values) #Adaptive Parameter Control

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            #Select 3 distinct random individuals (excluding current one)
            indices = np.random.choice(self.population_size, 3, replace=False)
            while i in indices:
                indices = np.random.choice(self.population_size, 3, replace=False)
            a, b, c = population[indices]
            mutant = a + self.F * (b - c)

            #Crossover
            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i] = np.where(cross_points, mutant, population[i])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

        return offspring

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptive strategy: Adjust F and CR based on success rate
        success_rate = np.mean(offspring_fitness < fitness_values) #consider previous generation
        if success_rate < 0.1: # Reduce F and increase CR for better exploitation
            self.F *= 0.9
            self.CR *= 1.1
        elif success_rate > 0.9: # Increase F and reduce CR for better exploration
            self.F *= 1.1
            self.CR *= 0.9
        self.F = np.clip(self.F, 0.1, 1.0) # Constrain F
        self.CR = np.clip(self.CR, 0.1, 1.0) # Constrain CR

2025-06-23 10:04:17 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:04:18 ERROR Can not run the algorithm
2025-06-23 10:04:18 INFO Run function 15 complete. FEHistory len: 201, AOCC: 0.1006
2025-06-23 10:04:18 INFO FeHistory: [-222.76479336 -221.54650171 -222.67374958 -222.51838041 -221.9026323
 -221.8450619  -221.96305688 -221.24106171 -222.39293268 -221.27302656
 -221.05882315 -222.36222661 -223.7042458  -222.46891499 -221.56359284
 -222.7799224  -221.92355778 -222.42096025 -221.41676875 -221.37848423
 -222.04114865 -224.41311667 -222.49417661 -221.80178591 -222.65895081
 -223.86940553 -221.8901377  -221.09024538 -221.47803094 -221.61889833
 -221.77127146 -222.24058266 -222.17892442 -223.82786269 -222.06638781
 -221.66897576 -220.7935335  -221.08349126 -223.25997536 -221.61608335
 -222.3772721  -221.95575059 -221.38914667 -220.90548285 -222.15084759
 -221.63115157 -221.09932075 -221.22333916 -222.70142668 -222.40318327
 -223.27696222 -222.43346252 -221.08852527 -222.77556559 -221.18110305
 -221.35708582 -222.10297263 -220.75990204 -220.92324182 -222.93591326
 -221.55663899 -222.30427481 -223.15230923 -222.06976697 -222.02952367
 -223.05271314 -221.87080717 -221.39732282 -221.72739118 -221.15269596
 -221.83923719 -221.37519506 -223.06835189 -221.15958276 -223.35743347
 -221.86115021 -221.5342891  -221.55850786 -221.07214084 -221.48571435
 -220.53702404 -221.32931341 -222.04294546 -221.21452702 -223.84442564
 -222.66052586 -223.03251279 -222.21304492 -223.45397113 -222.31894523
 -221.07030058 -221.54675122 -223.13380448 -221.43374057 -222.18837644
 -222.27540468 -221.84438796 -221.55545155 -222.92975742 -221.89309335
 -222.47701403 -221.43958392 -221.14966618 -222.15226088 -222.34793632
 -222.15186927 -220.59735141 -222.07133686 -220.9319211  -221.70092699
 -222.63710711 -222.18188347 -221.60315841 -222.34899939 -220.77206178
 -222.37885822 -221.42332172 -220.83091034 -221.20332157 -222.13895207
 -220.74595501 -222.53603079 -221.65342859 -220.90370615 -222.60089236
 -221.82530146 -221.82172536 -221.6112979  -221.16962028 -221.71231234
 -222.8045947  -222.30157743 -222.7436623  -221.16074137 -221.28408177
 -220.40171641 -222.96396423 -222.29632419 -222.05006381 -220.02280846
 -223.06300299 -222.40000196 -221.98280431 -221.44669814 -220.46376729
 -220.39738629 -221.84075099 -223.60583278 -221.80892751 -220.67008842
 -221.90922149 -221.57865822 -222.82024265 -221.24018232 -221.48187759
 -221.9664816  -222.3633186  -221.70609998 -221.23195877 -222.50587661
 -221.63720756 -221.42493306 -221.51769915 -223.94463567 -220.66119879
 -220.87384082 -221.81610122 -222.81071447 -223.95864649 -220.42567364
 -220.30695709 -222.0016455  -221.69260152 -221.32773298 -223.22399635
 -221.93733235 -222.07584119 -220.5854187  -222.53579189 -220.56340534
 -223.22975888 -221.34132277 -221.49090195 -222.09672414 -221.87220544
 -222.57470013 -220.93483229 -222.90706315 -221.00637182 -221.75432803
 -222.61963495 -220.3891303  -220.78491215 -222.1831713  -220.92072697
 -221.57831012 -220.69128302 -220.88395361 -220.88490319 -220.41845998
 -220.68064478]
2025-06-23 10:04:18 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:04:18 INFO Good algorithm:
Algorithm Name: AdaptiveLatinHypercubeDE
import numpy as np
from scipy.stats import qmc

# Name: AdaptiveLatinHypercubeDE
# Description: Combines Latin Hypercube Sampling, Adaptive Differential Evolution, and an archive for robust multimodal optimization.

class AdaptiveLatinHypercubeDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim, seed=42) # Fixed seed for reproducibility


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values) #Adaptive Parameter Control

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            #Select 3 distinct random individuals (excluding current one)
            indices = np.random.choice(self.population_size, 3, replace=False)
            while i in indices:
                indices = np.random.choice(self.population_size, 3, replace=False)
            a, b, c = population[indices]
            mutant = a + self.F * (b - c)

            #Crossover
            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i] = np.where(cross_points, mutant, population[i])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

        return offspring

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptive strategy: Adjust F and CR based on success rate
        success_rate = np.mean(offspring_fitness < fitness_values) #consider previous generation
        if success_rate < 0.1: # Reduce F and increase CR for better exploitation
            self.F *= 0.9
            self.CR *= 1.1
        elif success_rate > 0.9: # Increase F and reduce CR for better exploration
            self.F *= 1.1
            self.CR *= 0.9
        self.F = np.clip(self.F, 0.1, 1.0) # Constrain F
        self.CR = np.clip(self.CR, 0.1, 1.0) # Constrain CR

2025-06-23 10:04:18 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:04:18 ERROR Can not run the algorithm
2025-06-23 10:04:18 INFO Run function 24 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-23 10:04:18 INFO FeHistory: [167.94659168 224.17829656 185.89793067 181.14758383 206.56772466
 186.85547462 195.13444285 184.92882346 176.56909529 199.915588
 229.17229634 182.50281171 229.15247441 168.36190868 159.78796905
 199.87836821 200.95286803 200.24481641 130.82011885 195.9449322
 172.43401569 157.95627041 196.58006424 203.81943167 207.43418257
 211.89963144 207.09316514 198.24416646 190.434068   162.97080459
 217.33885709 180.02112297 198.04562891 192.7303882  180.18371028
 157.70711892 167.2873311  169.19065516 183.99155072 218.44041351
 200.47201632 197.19552804 179.17702064 172.91615451 181.1415282
 185.58814622 205.97406405 187.66922573 184.98585811 155.13492755
 194.9471929  185.07437188 205.98682373 204.31639817 167.23422316
 169.4916999  185.51214878 165.50935874 167.69094289 189.35649872
 220.19487157 193.86171061 217.78686367 172.72588535 201.20060966
 203.95382512 146.06173151 209.99697754 181.10401594 192.44782383
 187.41852781 190.75253623 206.08700841 220.49879052 206.74070528
 212.87155611 189.29830903 186.7173103  184.68163021 201.30088771
 191.60269742 166.5543711  202.16368335 221.97559808 142.30853483
 185.57168557 149.78176502 162.70957906 145.30220293 217.00687961
 196.18888967 192.48128353 198.30438043 198.56017531 176.39560365
 152.73000934 219.20321657 167.68459691 202.68091919 191.65463195
 193.53662945 192.00722865 185.81261757 237.22125745 190.45522925
 244.55105156 197.68152597 214.89789713 202.31946504 229.86715872
 204.29557457 181.1521099  180.48912824 174.71031829 168.48667293
 151.99833614 203.18347238 207.91582195 250.38016015 213.05287846
 194.0920755  226.12243858 217.1597557  195.1981105  177.88053915
 188.20186396 234.24277508 220.80546034 203.8504018  218.09775027
 174.15245006 208.39598217 194.89470039 197.98013705 191.06212576
 212.78333701 203.09103161 201.73567581 151.48946436 244.36401424
 205.71295282 205.21288473 211.95264418 194.52527313 201.83996482
 216.49430711 174.92334024 202.54402716 157.56108587 175.45699689
 216.52438137 202.91765962 216.51710563 213.43134263 180.80567283
 187.96319683 192.53503812 201.29109094 198.95875795 217.93784203
 192.89612523 200.64176872 195.8136376  202.92110381 219.68122378
 177.69491653 188.35338477 170.60133306 195.51488301 205.88087017
 233.47485067 204.21183446 179.3861826  218.76961581 234.32986208
 229.64467068 233.15190048 213.531451   231.18208237 219.18958954
 199.24295142 247.33881355 139.27501711 191.96631016 171.22761187
 241.48720551 207.47395874 200.76931623 174.57066757 229.82301557
 180.50599191 227.42709331 189.97101676 217.28857887 211.76245493
 209.32773943 217.52907036 227.13137343 225.85235913 199.66325929
 210.19213298]
2025-06-23 10:04:18 INFO Expected Optimum FE: -100
2025-06-23 10:04:18 INFO Unimodal AOCC mean: 0.1750
2025-06-23 10:04:18 INFO Multimodal (single component) AOCC mean: 0.1006
2025-06-23 10:04:18 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:04:18 INFO AOCC mean: 0.0919
2025-06-23 10:05:47 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:05:47 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:05:47 ERROR Can not run the algorithm
2025-06-23 10:05:47 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1751
2025-06-23 10:05:47 INFO FeHistory: [-701.29621752 -701.30953916 -701.34248608 -701.32705953 -701.33157996
 -701.32071702 -701.29263844 -701.29985976 -701.29492666 -701.31399906
 -701.28190903 -701.2816002  -701.30344792 -701.30870447 -701.34634295
 -701.31827036 -701.32097767 -701.286151   -701.32949849 -701.31654104
 -701.31071949 -701.31246195 -701.32079379 -701.30390637 -701.30956386
 -701.30582706 -701.30224011 -701.33143396 -701.34381282 -701.30948975
 -701.31714896 -701.30798969 -701.32445015 -701.31376977 -701.29845752
 -701.30674557 -701.32130613 -701.31263154 -701.29595639 -701.30480552
 -701.28796687 -701.30224616 -701.31614772 -701.29741997 -701.29637272
 -701.31178291 -701.29125658 -701.29460368 -701.34032793 -701.2956641
 -701.32702848 -701.31347715 -701.30451475 -701.30675922 -701.31737206
 -701.29559557 -701.30845126 -701.32172387 -701.30221977 -701.29233445
 -701.30779406 -701.29791378 -701.29026329 -701.30043043 -701.32468171
 -701.29254441 -701.29552769 -701.28850333 -701.28572023 -701.29740904
 -701.30386686 -701.31121353 -701.30051551 -701.30504495 -701.29985964
 -701.30918816 -701.31347057 -701.29242095 -701.33529506 -701.35892483
 -701.34892339 -701.32958345 -701.30645037 -701.31644886 -701.31495304
 -701.30189963 -701.31401428 -701.31042094 -701.28455365 -701.27631186
 -701.32304132 -701.29760701 -701.31912774 -701.3377667  -701.30536501
 -701.31292009 -701.26847663 -701.28921905 -701.29640671 -701.31350816
 -701.35326583]
2025-06-23 10:05:47 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:05:47 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_Archive
import numpy as np
from scipy.spatial.distance import pdist, squareform
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_Archive
# Description: Combines adaptive Differential Evolution, Latin Hypercube Sampling, and a diversity-preserving archive for robust multimodal optimization.
# Code:
class AdaptiveDE_LHS_Archive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8  # Initial exploration rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            
            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:  # Exploration vs. Exploitation
                a, b, c = self._select_different_archive(i)  # Explore archive
            else:
                a, b, c = self._select_different(i)  # Exploit current population
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]
    
    def _select_different_archive(self, exclude):
        candidates = list(range(len(self.archive)))
        if len(candidates) < 3:
            return self._select_different(exclude) # Fallback to population if archive too small
        np.random.shuffle(candidates)
        return [self.archive[i][:-1] for i in candidates[:3]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        #Diversity preservation (cull similar solutions)
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            is_unique = True
            for j in range(len(new_archive)):
                if np.linalg.norm(sorted_data[i, :-1] - new_archive[j][:-1]) < 0.1:
                    is_unique = False
                    break
            if is_unique:
                new_archive.append(sorted_data[i])
        
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Adaptive parameter control based on exploration/exploitation balance
        #Monitor the diversity of the population. Lower diversity indicates a need for more exploration.
        diversity = np.std(population, axis=0).mean()
        diversity_threshold = 0.2 * (self.upper_bounds.mean() - self.lower_bounds.mean())
        if diversity < diversity_threshold:  # Threshold for low diversity
            self.exploration_rate = min(1, self.exploration_rate + 0.05)  # Increase exploration
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)  # Decrease exploration

        # Adjust F and CR based on success rate (exploitation)
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 10:05:47 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:05:47 ERROR Can not run the algorithm
2025-06-23 10:05:48 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1011
2025-06-23 10:05:48 INFO FeHistory: [-222.61874342 -221.24731139 -221.4508131  -222.02112785 -221.47933787
 -223.01260753 -222.3973275  -221.20540438 -221.24160522 -222.04110778
 -221.07463999 -221.52708055 -222.08957088 -222.48141333 -222.01957778
 -223.24375518 -222.80926895 -221.98289814 -223.09055714 -222.11527643
 -221.45177754 -222.11595094 -221.75779152 -221.89002941 -221.06430311
 -220.58364465 -221.75666312 -224.00496348 -223.10416884 -223.02693813
 -221.96668553 -222.26191885 -222.16582035 -221.58664739 -221.93451936
 -222.15146264 -222.84973256 -220.53710608 -220.6714678  -221.5678043
 -221.86070505 -222.05489925 -222.81725501 -221.55403188 -219.89891159
 -221.53369869 -221.50227893 -222.15900088 -221.87135463 -221.94528054
 -221.85030124 -221.31287581 -222.58142551 -222.25457566 -222.19851979
 -221.55228837 -222.56380096 -221.99488609 -222.91823486 -222.5303495
 -221.61460831 -222.16437669 -221.7075324  -220.23738267 -220.81707995
 -220.9722088  -221.62576048 -222.36183394 -221.66843286 -221.33001481
 -222.0895321  -220.11925004 -222.49428605 -221.60820933 -222.87957958
 -222.29241031 -222.07544682 -220.60495526 -221.15530688 -220.9754922
 -224.53568575 -220.79055854 -223.57931559 -222.13279362 -221.91666488
 -223.46463719 -221.42193814 -220.94946513 -222.88941453 -222.61216602
 -222.5627089  -222.75073684 -220.29512329 -221.3297268  -221.79659656
 -223.48108903 -221.80615674 -220.91591895 -223.51082328 -221.79099102
 -223.16452679]
2025-06-23 10:05:48 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:05:48 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_Archive
import numpy as np
from scipy.spatial.distance import pdist, squareform
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_Archive
# Description: Combines adaptive Differential Evolution, Latin Hypercube Sampling, and a diversity-preserving archive for robust multimodal optimization.
# Code:
class AdaptiveDE_LHS_Archive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8  # Initial exploration rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            
            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:  # Exploration vs. Exploitation
                a, b, c = self._select_different_archive(i)  # Explore archive
            else:
                a, b, c = self._select_different(i)  # Exploit current population
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]
    
    def _select_different_archive(self, exclude):
        candidates = list(range(len(self.archive)))
        if len(candidates) < 3:
            return self._select_different(exclude) # Fallback to population if archive too small
        np.random.shuffle(candidates)
        return [self.archive[i][:-1] for i in candidates[:3]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        #Diversity preservation (cull similar solutions)
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            is_unique = True
            for j in range(len(new_archive)):
                if np.linalg.norm(sorted_data[i, :-1] - new_archive[j][:-1]) < 0.1:
                    is_unique = False
                    break
            if is_unique:
                new_archive.append(sorted_data[i])
        
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Adaptive parameter control based on exploration/exploitation balance
        #Monitor the diversity of the population. Lower diversity indicates a need for more exploration.
        diversity = np.std(population, axis=0).mean()
        diversity_threshold = 0.2 * (self.upper_bounds.mean() - self.lower_bounds.mean())
        if diversity < diversity_threshold:  # Threshold for low diversity
            self.exploration_rate = min(1, self.exploration_rate + 0.05)  # Increase exploration
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)  # Decrease exploration

        # Adjust F and CR based on success rate (exploitation)
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 10:05:48 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:05:48 ERROR Can not run the algorithm
2025-06-23 10:05:48 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 10:05:48 INFO FeHistory: [159.98116226 179.3308441  197.33565096 180.41949679 152.84841549
 172.29313155 172.72027235 197.999697   180.72002295 185.54060365
 159.4267034  213.22795961 185.99864171 189.87316635 190.17843021
 229.74755991 164.30283977 187.86455284 194.70871408 139.30557824
 202.91755487 204.48904786 210.68259369 201.13224845 186.68068887
 163.07075181 202.70615508 206.41545156 155.65353889 189.4347331
 200.55055825 171.87750261 205.0879818  179.34830108 206.67736251
 168.5097222  213.7288819  181.7317624  163.94212522 179.05357614
 194.06388366 118.74050412 170.10474202 200.86212056 161.34156971
 195.68942448 194.23767381 204.36310008 164.56577963 199.80098821
 188.42714405 174.48013631 181.0759939  183.56027129 194.8788403
 180.3088148  163.22501158 168.63763437 185.98153353 136.22771557
 194.58903225 200.21170077 189.32362994 222.24347938 172.02703176
 158.82120878 207.52465342 181.19387045 176.33014231 169.98396753
 157.70431479 189.5578725  178.96670415 158.95223778 235.55879036
 191.44910964 181.15914123 161.39051619 151.45949275 201.73153052
 194.19316264 184.2081238  192.72768093 209.31617496 206.36022003
 164.08665506 156.43031046 199.35089098 167.83800729 172.96642456
 187.82440974 187.41226236 201.61127304 193.85481961 201.26549096
 162.69802323 236.1389281  161.18912244 211.08267801 174.87626705
 206.65149254]
2025-06-23 10:05:48 INFO Expected Optimum FE: -100
2025-06-23 10:05:48 INFO Unimodal AOCC mean: 0.1751
2025-06-23 10:05:48 INFO Multimodal (single component) AOCC mean: 0.1011
2025-06-23 10:05:48 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:05:48 INFO AOCC mean: 0.0921
2025-06-23 10:09:08 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1795
2025-06-23 10:09:08 INFO FeHistory: [-701.30858219 -701.32815047 -701.32820899 ... -701.85069654 -701.85689134
 -701.8547563 ]
2025-06-23 10:09:08 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:09:08 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithGaussianArchiveDiversity
import numpy as np
from scipy.stats import qmc

class AdaptiveDEwithGaussianArchiveDiversity:
    """
    Combines adaptive Differential Evolution with Gaussian mutation and a diversity-preserving archive for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.sigma = 0.5 #Gaussian Mutation Parameter
        self.scale_factor = 1.0  # Adaptive scaling factor
        self.scale_decay = 0.95
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.acceptance_threshold = 1e-6


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            initial_sample = self.sampler.random(n=1)
            self.best_solution_overall = self._scale_sample(initial_sample)[0]
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling(self.population_size)
        fitness = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness)


        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness)
            #Gaussian Mutation for enhanced exploration
            offspring = self._gaussian_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            combined_pop = np.vstack((offspring, population))
            combined_fit = np.concatenate((offspring_fitness, fitness))
            sorted_indices = np.argsort(combined_fit)
            population = combined_pop[sorted_indices[:self.population_size]]
            fitness = combined_fit[sorted_indices[:self.population_size]]

            self._update_best(population, fitness)
            self.scale_factor *= self.scale_decay
            self.F *= self.scale_factor  # Adaptive F scaling
            self.CR *= self.scale_factor  # Adaptive CR Scaling
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness, offspring_fitness)))

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _latin_hypercube_sampling(self, num_samples):
        sample = self.sampler.random(n=num_samples)
        return self._scale_sample(sample)

    def _scale_sample(self, sample):
        return sample * (self.upper_bounds - self.lower_bounds) + self.lower_bounds

    def _differential_evolution(self, population, fitness):
        new_population = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
            trial_fitness = objective_function(trial.reshape(1, -1))
            if trial_fitness[0] < fitness[i]:
                new_population[i] = trial
            else:
                new_population[i] = population[i]
        return new_population
    
    def _gaussian_mutation(self, offspring):
        mutated_offspring = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated_offspring, self.lower_bounds, self.upper_bounds)

    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Prioritize adding solutions near the best found solution and maintain diversity
        distances_to_best = np.linalg.norm(population - self.best_solution_overall, axis=1)
        combined = combined[np.argsort(distances_to_best)]
        
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.acceptance_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        
        #Maintain diversity by adding solutions based on distance in solution space
        if len(new_archive)> self.archive_size:
            new_archive = self._maintain_diversity(new_archive, self.archive_size)
        
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _maintain_diversity(self, archive, max_size):
        archive = np.array(archive)
        if len(archive) <= max_size:
            return archive
        
        selected_archive = [archive[0]] #add the best solution
        remaining_archive = archive[1:]

        while len(selected_archive) < max_size and len(remaining_archive)>0:
            best_index = 0
            max_distance = 0
            
            for i in range(len(remaining_archive)):
                min_distance = np.min(np.linalg.norm(remaining_archive[i][:-1] - selected_archive[:,:-1], axis = 1))
                if min_distance > max_distance:
                    max_distance = min_distance
                    best_index = i

            selected_archive.append(remaining_archive[best_index])
            remaining_archive = np.delete(remaining_archive, best_index, axis=0)
        
        return np.array(selected_archive)

def objective_function(x):  #Example objective function, replace with your GNBG functions
    return np.sum(x**2, axis=1)
2025-06-23 10:09:08 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:12:14 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1178
2025-06-23 10:12:14 INFO FeHistory: [-221.97585526 -223.18948008 -220.16726717 ... -227.80031946 -227.7960849
 -227.80070219]
2025-06-23 10:12:14 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:12:14 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithGaussianArchiveDiversity
import numpy as np
from scipy.stats import qmc

class AdaptiveDEwithGaussianArchiveDiversity:
    """
    Combines adaptive Differential Evolution with Gaussian mutation and a diversity-preserving archive for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.sigma = 0.5 #Gaussian Mutation Parameter
        self.scale_factor = 1.0  # Adaptive scaling factor
        self.scale_decay = 0.95
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.acceptance_threshold = 1e-6


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            initial_sample = self.sampler.random(n=1)
            self.best_solution_overall = self._scale_sample(initial_sample)[0]
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling(self.population_size)
        fitness = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness)


        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness)
            #Gaussian Mutation for enhanced exploration
            offspring = self._gaussian_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            combined_pop = np.vstack((offspring, population))
            combined_fit = np.concatenate((offspring_fitness, fitness))
            sorted_indices = np.argsort(combined_fit)
            population = combined_pop[sorted_indices[:self.population_size]]
            fitness = combined_fit[sorted_indices[:self.population_size]]

            self._update_best(population, fitness)
            self.scale_factor *= self.scale_decay
            self.F *= self.scale_factor  # Adaptive F scaling
            self.CR *= self.scale_factor  # Adaptive CR Scaling
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness, offspring_fitness)))

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _latin_hypercube_sampling(self, num_samples):
        sample = self.sampler.random(n=num_samples)
        return self._scale_sample(sample)

    def _scale_sample(self, sample):
        return sample * (self.upper_bounds - self.lower_bounds) + self.lower_bounds

    def _differential_evolution(self, population, fitness):
        new_population = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
            trial_fitness = objective_function(trial.reshape(1, -1))
            if trial_fitness[0] < fitness[i]:
                new_population[i] = trial
            else:
                new_population[i] = population[i]
        return new_population
    
    def _gaussian_mutation(self, offspring):
        mutated_offspring = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated_offspring, self.lower_bounds, self.upper_bounds)

    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Prioritize adding solutions near the best found solution and maintain diversity
        distances_to_best = np.linalg.norm(population - self.best_solution_overall, axis=1)
        combined = combined[np.argsort(distances_to_best)]
        
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.acceptance_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        
        #Maintain diversity by adding solutions based on distance in solution space
        if len(new_archive)> self.archive_size:
            new_archive = self._maintain_diversity(new_archive, self.archive_size)
        
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _maintain_diversity(self, archive, max_size):
        archive = np.array(archive)
        if len(archive) <= max_size:
            return archive
        
        selected_archive = [archive[0]] #add the best solution
        remaining_archive = archive[1:]

        while len(selected_archive) < max_size and len(remaining_archive)>0:
            best_index = 0
            max_distance = 0
            
            for i in range(len(remaining_archive)):
                min_distance = np.min(np.linalg.norm(remaining_archive[i][:-1] - selected_archive[:,:-1], axis = 1))
                if min_distance > max_distance:
                    max_distance = min_distance
                    best_index = i

            selected_archive.append(remaining_archive[best_index])
            remaining_archive = np.delete(remaining_archive, best_index, axis=0)
        
        return np.array(selected_archive)

def objective_function(x):  #Example objective function, replace with your GNBG functions
    return np.sum(x**2, axis=1)
2025-06-23 10:12:14 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:15:18 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 10:15:18 INFO FeHistory: [194.96986446 185.29302538 195.46731151 ...  92.21095131  88.04078517
  92.32384281]
2025-06-23 10:15:18 INFO Expected Optimum FE: -100
2025-06-23 10:15:18 INFO Unimodal AOCC mean: 0.1795
2025-06-23 10:15:18 INFO Multimodal (single component) AOCC mean: 0.1178
2025-06-23 10:15:18 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:15:18 INFO AOCC mean: 0.0991
2025-06-23 10:15:40 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:19:00 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1812
2025-06-23 10:19:00 INFO FeHistory: [-701.32668831 -701.32379577 -701.3187301  ... -701.6096327  -701.60963283
 -701.60963336]
2025-06-23 10:19:00 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:19:00 INFO Good algorithm:
Algorithm Name: AdaptiveCauchyDiversityEA
import numpy as np
from scipy.stats import cauchy

# Name: AdaptiveCauchyDiversityEA
# Description: An evolutionary algorithm using adaptive Cauchy mutation and diversity-preserving mechanisms for multimodal optimization.
# Code:
class AdaptiveCauchyDiversityEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.gamma = 1.0  # Initial Cauchy scale parameter
        self.gamma_decay = 0.95
        self.archive = []
        self.diversity_threshold = 0.1  # Adjust as needed


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._cauchy_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.gamma *= self.gamma_decay
            self._adapt_gamma(population, fitness_values)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _cauchy_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + cauchy.rvs(loc=0, scale=self.gamma, size=self.dim)
            child2 = (parent1 + parent2) / 2 + cauchy.rvs(loc=0, scale=self.gamma, size=self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        mutated_offspring = offspring + cauchy.rvs(loc=0, scale=self.gamma, size=offspring.shape)
        return np.clip(mutated_offspring, self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_gamma(self, population, fitness_values):
        diversity = self._calculate_diversity(population)
        if diversity < self.diversity_threshold:
            self.gamma *= 1.1  # Increase gamma to enhance exploration
        else:
            self.gamma *= 0.9 # slightly reduce gamma

    def _calculate_diversity(self, population):
        # Simple diversity metric: average pairwise distance
        distances = np.linalg.norm(population[:, np.newaxis, :] - population[np.newaxis, :, :], axis=2)
        return np.mean(distances)
2025-06-23 10:19:00 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:22:45 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1186
2025-06-23 10:22:45 INFO FeHistory: [-221.33088881 -221.06914055 -221.37732332 ... -228.09313014 -228.09313568
 -228.09307124]
2025-06-23 10:22:45 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:22:45 INFO Good algorithm:
Algorithm Name: AdaptiveCauchyDiversityEA
import numpy as np
from scipy.stats import cauchy

# Name: AdaptiveCauchyDiversityEA
# Description: An evolutionary algorithm using adaptive Cauchy mutation and diversity-preserving mechanisms for multimodal optimization.
# Code:
class AdaptiveCauchyDiversityEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.gamma = 1.0  # Initial Cauchy scale parameter
        self.gamma_decay = 0.95
        self.archive = []
        self.diversity_threshold = 0.1  # Adjust as needed


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._cauchy_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self.gamma *= self.gamma_decay
            self._adapt_gamma(population, fitness_values)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _cauchy_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + cauchy.rvs(loc=0, scale=self.gamma, size=self.dim)
            child2 = (parent1 + parent2) / 2 + cauchy.rvs(loc=0, scale=self.gamma, size=self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        mutated_offspring = offspring + cauchy.rvs(loc=0, scale=self.gamma, size=offspring.shape)
        return np.clip(mutated_offspring, self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_gamma(self, population, fitness_values):
        diversity = self._calculate_diversity(population)
        if diversity < self.diversity_threshold:
            self.gamma *= 1.1  # Increase gamma to enhance exploration
        else:
            self.gamma *= 0.9 # slightly reduce gamma

    def _calculate_diversity(self, population):
        # Simple diversity metric: average pairwise distance
        distances = np.linalg.norm(population[:, np.newaxis, :] - population[np.newaxis, :, :], axis=2)
        return np.mean(distances)
2025-06-23 10:22:45 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:25:58 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 10:25:58 INFO FeHistory: [192.00096613 191.05686913 228.50717171 ... 242.17764079 225.66151839
 245.79770358]
2025-06-23 10:25:58 INFO Expected Optimum FE: -100
2025-06-23 10:25:58 INFO Unimodal AOCC mean: 0.1812
2025-06-23 10:25:58 INFO Multimodal (single component) AOCC mean: 0.1186
2025-06-23 10:25:58 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:25:58 INFO AOCC mean: 0.0999
2025-06-23 10:42:52 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:42:52 ERROR Can not run the algorithm
2025-06-23 10:42:52 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1752
2025-06-23 10:42:52 INFO FeHistory: [-701.27843903 -701.30065592 -701.34780153 -701.31455065 -701.30799146
 -701.30772663 -701.29824481 -701.28506808 -701.28513434 -701.29523286
 -701.35830246 -701.32413339 -701.30842229 -701.32276951 -701.30987563
 -701.3241784  -701.27975965 -701.31454304 -701.28731386 -701.29886393
 -701.3183845  -701.31497274 -701.31466565 -701.30144576 -701.32974495
 -701.31651269 -701.28563961 -701.30979079 -701.30898122 -701.33130052
 -701.27312793 -701.31778643 -701.3194162  -701.33535707 -701.29856333
 -701.32717714 -701.32497062 -701.31151631 -701.31021428 -701.31914761
 -701.31231687 -701.27725526 -701.30185544 -701.31926251 -701.3090366
 -701.31511914 -701.28558462 -701.31017445 -701.31752308 -701.32258983
 -701.27866742 -701.29181554 -701.29257933 -701.32729586 -701.2929665
 -701.31991502 -701.32458772 -701.33136659 -701.29229684 -701.28389643
 -701.2937953  -701.29596516 -701.29638205 -701.3035447  -701.30199305
 -701.36304565 -701.28752317 -701.34196098 -701.29669542 -701.28671673
 -701.30680589 -701.31156156 -701.29669985 -701.28323887 -701.32626501
 -701.32912552 -701.34817532 -701.26909862 -701.32759925 -701.32978771
 -701.30749866 -701.31016155 -701.31866867 -701.30751558 -701.3166873
 -701.32898944 -701.32442868 -701.31291058 -701.31743258 -701.31432197
 -701.30379311 -701.30247226 -701.29353591 -701.30254586 -701.2950593
 -701.31683182 -701.29032064 -701.30876216 -701.31005346 -701.33405078
 -701.29500252]
2025-06-23 10:42:52 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:42:52 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_with_Crowding
import numpy as np
from scipy.spatial.distance import pdist, squareform
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_with_Crowding
# Description: Combines adaptive Differential Evolution, Latin Hypercube Sampling, and a crowding archive for robust multimodal optimization.
# Code:
class AdaptiveDE_LHS_with_Crowding:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8  # Initial exploration rate
        self.crowding_factor = 0.1 # Parameter to control crowding pressure


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:  # Exploration vs. Exploitation
                a, b, c = self._select_different_archive(i)  # Explore archive
            else:
                a, b, c = self._select_different(i)  # Exploit current population
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _select_different_archive(self, exclude):
        candidates = list(range(len(self.archive)))
        if len(candidates) < 3:
            return self._select_different(exclude) # Fallback to population if archive too small
        np.random.shuffle(candidates)
        return [self.archive[i][:-1] for i in candidates[:3]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        next_gen = []
        next_fit = []
        
        for i in range(0, len(combined_pop), 2):
            candidate1 = combined_pop[i]
            candidate2 = combined_pop[i+1]
            fit1 = combined_fit[i]
            fit2 = combined_fit[i+1]
            
            if fit1 < fit2:
                next_gen.append(candidate1)
                next_fit.append(fit1)
            else:
                next_gen.append(candidate2)
                next_fit.append(fit2)

        
        return np.array(next_gen[:self.population_size]), np.array(next_fit[:self.population_size])


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        #Crowding based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            distances = np.linalg.norm(sorted_data[i, :-1] - np.array([x[:-1] for x in new_archive]), axis=1)
            closest_index = np.argmin(distances)
            if distances[closest_index] > self.crowding_factor * (np.max(self.upper_bounds) - np.min(self.lower_bounds)):
                new_archive.append(sorted_data[i])
            elif sorted_data[i,-1] < new_archive[closest_index][-1]:
                new_archive[closest_index] = sorted_data[i]
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Adaptive parameter control based on exploration/exploitation balance
        #Monitor the diversity of the population. Lower diversity indicates a need for more exploration.
        diversity = np.std(population, axis=0).mean()
        diversity_threshold = 0.2 * (self.upper_bounds.mean() - self.lower_bounds.mean())
        if diversity < diversity_threshold:  # Threshold for low diversity
            self.exploration_rate = min(1, self.exploration_rate + 0.05)  # Increase exploration
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)  # Decrease exploration

        # Adjust F and CR based on success rate (exploitation)
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))
2025-06-23 10:42:52 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:42:52 ERROR Can not run the algorithm
2025-06-23 10:42:52 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1004
2025-06-23 10:42:52 INFO FeHistory: [-220.93916792 -220.61479375 -221.98550198 -222.39928615 -223.44588915
 -222.29530178 -221.65400744 -221.51308012 -222.01437508 -221.33207769
 -221.74439777 -221.10103604 -221.01193634 -223.56860221 -222.52548368
 -221.55592358 -222.12207241 -222.82073536 -221.80715927 -222.55288055
 -222.84274508 -220.19891235 -222.78435098 -221.11180644 -222.74102107
 -222.64923978 -220.72393306 -222.98317812 -221.47336948 -222.12406819
 -221.07346969 -222.59142964 -222.07838687 -222.97597034 -222.69951609
 -223.27979466 -221.88336787 -224.37065639 -223.79568123 -222.00258534
 -222.77009559 -221.53973987 -222.19211264 -222.60373919 -222.83801481
 -221.38982515 -222.53130295 -220.76672831 -220.28088542 -222.49237495
 -224.33088268 -223.42164009 -221.94778785 -222.09544452 -220.04996278
 -221.9768653  -222.82577836 -221.47417581 -221.21146041 -221.35288353
 -220.88877107 -221.88883391 -221.91762348 -221.87776568 -221.86573558
 -221.99143227 -222.72231764 -222.42199088 -223.24847257 -221.40392651
 -222.4369523  -220.60000379 -221.50735753 -223.02953925 -222.34415478
 -223.80628461 -220.9909767  -221.86360817 -222.82961236 -222.56088346
 -223.27203233 -220.36097496 -220.65637501 -221.20670144 -221.19147826
 -223.42619106 -223.43227421 -221.02576029 -222.09246984 -220.87887071
 -220.32239879 -221.57796391 -221.29954086 -222.93189321 -221.6532968
 -220.66411479 -221.75269835 -221.06396162 -222.09308946 -223.11512356
 -222.49514238]
2025-06-23 10:42:52 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:42:52 INFO Good algorithm:
Algorithm Name: AdaptiveDE_LHS_with_Crowding
import numpy as np
from scipy.spatial.distance import pdist, squareform
from scipy.stats import qmc

# Name: AdaptiveDE_LHS_with_Crowding
# Description: Combines adaptive Differential Evolution, Latin Hypercube Sampling, and a crowding archive for robust multimodal optimization.
# Code:
class AdaptiveDE_LHS_with_Crowding:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.sampler = qmc.LatinHypercube(d=self.dim)
        self.exploration_rate = 0.8  # Initial exploration rate
        self.crowding_factor = 0.1 # Parameter to control crowding pressure


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._latin_hypercube_sampling()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection_with_crowding(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _latin_hypercube_sampling(self):
        sample = self.sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:  # Exploration vs. Exploitation
                a, b, c = self._select_different_archive(i)  # Explore archive
            else:
                a, b, c = self._select_different(i)  # Exploit current population
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _select_different_archive(self, exclude):
        candidates = list(range(len(self.archive)))
        if len(candidates) < 3:
            return self._select_different(exclude) # Fallback to population if archive too small
        np.random.shuffle(candidates)
        return [self.archive[i][:-1] for i in candidates[:3]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection_with_crowding(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        next_gen = []
        next_fit = []
        
        for i in range(0, len(combined_pop), 2):
            candidate1 = combined_pop[i]
            candidate2 = combined_pop[i+1]
            fit1 = combined_fit[i]
            fit2 = combined_fit[i+1]
            
            if fit1 < fit2:
                next_gen.append(candidate1)
                next_fit.append(fit1)
            else:
                next_gen.append(candidate2)
                next_fit.append(fit2)

        
        return np.array(next_gen[:self.population_size]), np.array(next_fit[:self.population_size])


    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        
        #Sort by fitness
        sorted_data = combined[combined[:, -1].argsort()]
        
        #Crowding based diversity preservation 
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            distances = np.linalg.norm(sorted_data[i, :-1] - np.array([x[:-1] for x in new_archive]), axis=1)
            closest_index = np.argmin(distances)
            if distances[closest_index] > self.crowding_factor * (np.max(self.upper_bounds) - np.min(self.lower_bounds)):
                new_archive.append(sorted_data[i])
            elif sorted_data[i,-1] < new_archive[closest_index][-1]:
                new_archive[closest_index] = sorted_data[i]
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        #Adaptive parameter control based on exploration/exploitation balance
        #Monitor the diversity of the population. Lower diversity indicates a need for more exploration.
        diversity = np.std(population, axis=0).mean()
        diversity_threshold = 0.2 * (self.upper_bounds.mean() - self.lower_bounds.mean())
        if diversity < diversity_threshold:  # Threshold for low diversity
            self.exploration_rate = min(1, self.exploration_rate + 0.05)  # Increase exploration
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)  # Decrease exploration

        # Adjust F and CR based on success rate (exploitation)
        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))
2025-06-23 10:42:52 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:42:52 ERROR Can not run the algorithm
2025-06-23 10:42:53 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 10:42:53 INFO FeHistory: [201.76811638 204.54510006 213.69652279 159.08501148 166.88095085
 186.6668214  179.10522443 183.14858396 189.25263717 202.01479509
 177.01704946 170.76639058 175.76548629 204.18186141 193.16447794
 186.46783896 206.07352182 131.72915556 189.84573263 158.61241722
 216.13306447 201.40818617 189.70462612 194.31077344 213.50881504
 192.23768778 172.4697591  196.44649202 171.43734369 207.51656882
 164.88427794 166.04040065 184.29755195 161.58147877 192.38214783
 198.794844   203.73177968 186.07774028 184.48959705 229.27165616
 196.5861454  170.14172218 165.54598747 192.66621575 189.43210855
 176.35502854 188.39455465 206.56052991 171.19676085 168.31013484
 207.81729257 145.36770109 212.73829098 191.64818962 222.21303815
 193.73381895 196.8197117  188.99792293 155.23905985 146.2102453
 217.44014844 220.93101832 186.0285908  212.31144223 201.44284519
 204.39039269 178.58340121 156.21974287 194.8352771  201.8582723
 185.78028401 178.30757197 165.48454192 196.37501433 209.60186508
 206.62842274 179.38330126 195.16036669 195.13076698 207.03498143
 208.8308503  171.17385024 226.47537755 148.9966549  199.60653216
 187.521148   172.94783704 180.87980096 171.68662285 210.08204557
 188.07766292 182.5506351  176.9895877  181.04798711 180.72203762
 182.39297723 179.42169432 197.37043659 202.45524516 223.22998283
 177.39447196]
2025-06-23 10:42:53 INFO Expected Optimum FE: -100
2025-06-23 10:42:53 INFO Unimodal AOCC mean: 0.1752
2025-06-23 10:42:53 INFO Multimodal (single component) AOCC mean: 0.1004
2025-06-23 10:42:53 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:42:53 INFO AOCC mean: 0.0919
2025-06-23 10:42:53 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 10:42:53 ERROR Can not run the algorithm
2025-06-23 10:42:53 INFO Run function 2 complete. FEHistory len: 101, AOCC: 0.1757
2025-06-23 10:42:53 INFO FeHistory: [-701.3036911  -701.28163717 -701.32706487 -701.3050579  -701.32042315
 -701.29530204 -701.32108475 -701.29425806 -701.29122603 -701.3247774
 -701.34832846 -701.2917408  -701.29579382 -701.28843405 -701.29017409
 -701.27777398 -701.29868691 -701.31659357 -701.35490146 -701.28807438
 -701.29852589 -701.32800201 -701.28247992 -701.29992516 -701.26731492
 -701.31432376 -701.31681683 -701.34619603 -701.304999   -701.31601043
 -701.30556773 -701.3058235  -701.26275339 -701.31060792 -701.33022284
 -701.32162514 -701.31659731 -701.33692564 -701.31769586 -701.31892866
 -701.29962803 -701.28612769 -701.31916042 -701.27577153 -701.29972354
 -701.32022269 -701.30561979 -701.29264543 -701.32616151 -701.29023973
 -701.32011817 -701.30634181 -701.3043094  -701.29132561 -701.30561246
 -701.33728901 -701.33098405 -701.31876693 -701.3202472  -701.28067763
 -701.33182614 -701.31914008 -701.31199492 -701.30524688 -701.31769729
 -701.36030649 -701.3162358  -701.26269469 -701.31073862 -701.30585632
 -701.3270218  -701.304577   -701.30171101 -701.30640993 -701.29144449
 -701.31748934 -701.32124495 -701.34215889 -701.30664197 -701.35348829
 -701.38373836 -701.32215639 -701.32810677 -701.34248236 -701.29689596
 -701.31636668 -701.27380848 -701.32130974 -701.31909522 -701.34424154
 -701.28128677 -701.3048479  -701.30519397 -701.3324127  -701.31630966
 -701.32732571 -701.306879   -701.28826473 -701.30622874 -701.28847468
 -701.26558432]
2025-06-23 10:42:53 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 10:42:53 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithFitnessBasedArchiveAndDiversityControl
import numpy as np
from scipy.spatial.distance import pdist, squareform
from scipy.stats import qmc

# Name: AdaptiveDEwithFitnessBasedArchiveAndDiversityControl
# Description: Adaptive Differential Evolution with a fitness-based archive and diversity control for multimodal optimization.
# Code:
class AdaptiveDEwithFitnessBasedArchiveAndDiversityControl:
    """
    Combines adaptive Differential Evolution (DE) with a fitness-based archive and diversity control 
    to efficiently explore and exploit multimodal landscapes.  The algorithm adapts its parameters 
    based on the population's diversity and the success rate of the DE operations.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.archive = []
        self.diversity_threshold = 0.2 * (self.upper_bounds.mean() - self.lower_bounds.mean()) # Adjust as needed
        self.exploration_rate = 0.8


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample


    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:
                a, b, c = self._select_different_archive(i)
            else:
                a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _select_different_archive(self, exclude):
        candidates = list(range(len(self.archive)))
        if len(candidates) < 3:
            return self._select_different(exclude)
        np.random.shuffle(candidates)
        return [self.archive[i][:-1] for i in candidates[:3]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        sorted_data = combined[combined[:, -1].argsort()]
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            is_unique = True
            for j in range(len(new_archive)):
                if np.linalg.norm(sorted_data[i, :-1] - new_archive[j][:-1]) < 0.1:
                    is_unique = False
                    break
            if is_unique:
                new_archive.append(sorted_data[i])
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        diversity = np.std(population, axis=0).mean()
        if diversity < self.diversity_threshold:
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 10:42:53 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 10:42:53 ERROR Can not run the algorithm
2025-06-23 10:42:53 INFO Run function 15 complete. FEHistory len: 101, AOCC: 0.1020
2025-06-23 10:42:53 INFO FeHistory: [-220.84545703 -221.68897173 -223.32237206 -223.00626883 -221.83658547
 -222.0412154  -223.07170654 -220.57011407 -223.26719495 -222.03573881
 -224.07001828 -221.20037826 -222.2499238  -222.41264571 -222.14729543
 -224.72377082 -221.5691084  -220.73631327 -221.51546059 -221.34997967
 -221.88286435 -221.29101619 -223.47794487 -224.4597899  -222.39488821
 -223.75171551 -221.5535935  -221.80251033 -223.24760263 -221.10314485
 -221.05597945 -222.75761428 -223.8584861  -221.90778711 -221.50236778
 -221.71130663 -223.9601191  -221.46083962 -220.61332852 -222.1569904
 -220.89810477 -221.70008216 -222.93755665 -221.84492794 -221.64508053
 -221.52941081 -220.16288858 -222.68433739 -220.93172267 -221.46597136
 -222.75147831 -221.15476705 -220.71387557 -222.48465515 -221.18718695
 -220.97852079 -222.47181588 -220.289251   -223.13297556 -223.1867522
 -221.65336959 -223.76114086 -221.21649439 -222.01352014 -222.09163501
 -221.65016156 -223.9281487  -222.0650094  -221.89745395 -221.85168705
 -221.48850426 -222.44529711 -222.74574391 -221.48448335 -222.2319447
 -221.59290124 -221.57760979 -220.57959187 -221.96857831 -223.30267659
 -222.67161231 -220.4784197  -222.84221957 -223.8492374  -221.62116188
 -220.70650431 -222.79931131 -222.89365803 -222.06225546 -223.15304591
 -223.29510114 -221.72629092 -222.53276827 -223.95890189 -221.95154429
 -221.58705664 -221.58611691 -222.52313223 -220.70421317 -222.21192178
 -222.67655824]
2025-06-23 10:42:53 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 10:42:53 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithFitnessBasedArchiveAndDiversityControl
import numpy as np
from scipy.spatial.distance import pdist, squareform
from scipy.stats import qmc

# Name: AdaptiveDEwithFitnessBasedArchiveAndDiversityControl
# Description: Adaptive Differential Evolution with a fitness-based archive and diversity control for multimodal optimization.
# Code:
class AdaptiveDEwithFitnessBasedArchiveAndDiversityControl:
    """
    Combines adaptive Differential Evolution (DE) with a fitness-based archive and diversity control 
    to efficiently explore and exploit multimodal landscapes.  The algorithm adapts its parameters 
    based on the population's diversity and the success rate of the DE operations.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.5  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.archive = []
        self.diversity_threshold = 0.2 * (self.upper_bounds.mean() - self.lower_bounds.mean()) # Adjust as needed
        self.exploration_rate = 0.8


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=self.population_size)
        scaled_sample = qmc.scale(sample, self.lower_bounds, self.upper_bounds)
        return scaled_sample


    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.exploration_rate:
                a, b, c = self._select_different_archive(i)
            else:
                a, b, c = self._select_different(i)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_different(self, exclude):
        candidates = list(range(self.population_size))
        candidates.remove(exclude)
        np.random.shuffle(candidates)
        return candidates[:3]

    def _select_different_archive(self, exclude):
        candidates = list(range(len(self.archive)))
        if len(candidates) < 3:
            return self._select_different(exclude)
        np.random.shuffle(candidates)
        return [self.archive[i][:-1] for i in candidates[:3]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        sorted_data = combined[combined[:, -1].argsort()]
        new_archive = [sorted_data[0]]
        for i in range(1, len(sorted_data)):
            is_unique = True
            for j in range(len(new_archive)):
                if np.linalg.norm(sorted_data[i, :-1] - new_archive[j][:-1]) < 0.1:
                    is_unique = False
                    break
            if is_unique:
                new_archive.append(sorted_data[i])
        return np.array(new_archive[:min(len(new_archive), self.archive_size)])

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _adapt_parameters(self, population, fitness_values):
        diversity = np.std(population, axis=0).mean()
        if diversity < self.diversity_threshold:
            self.exploration_rate = min(1, self.exploration_rate + 0.05)
        else:
            self.exploration_rate = max(0.1, self.exploration_rate - 0.05)

        success_rate = np.mean(fitness_values[:self.population_size // 2] < fitness_values[self.population_size // 2:])
        self.F = max(0.1, min(1.0, self.F + 0.1 * (success_rate - 0.5)))
        self.CR = max(0.1, min(1.0, self.CR + 0.1 * (success_rate - 0.5)))

2025-06-23 10:42:53 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 10:42:53 ERROR Can not run the algorithm
2025-06-23 10:42:53 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-23 10:42:53 INFO FeHistory: [168.18933797 154.94061394 183.70018567 216.64397392 185.37551417
 208.26548902 160.33871205 199.37969223 197.58856789 199.93490007
 198.06977961 174.35096103 149.23272587 196.55883393 177.7105751
 192.09792954 179.58966285 167.93608749 214.08906386 213.48313588
 213.13514249 139.70123092 163.85515684 192.21219047 227.07788166
 154.17785999 179.20151479 174.27184446 159.45697339 211.27582173
 172.39000446 200.66802406 170.32412003 207.92255431 188.14649233
 206.68077545 198.0458254  175.69381701 193.78451458 206.54009448
 206.76540797 181.04838829 225.57029899 221.57916509 156.17741073
 161.02310488 203.87611425 161.68291665 157.10018758 194.07050419
 191.59614421 210.72739902 203.14735297 173.71753459 183.57835161
 183.06050999 208.56968257 182.20297513 178.06821077 224.879979
 182.06789827 173.59108982 194.04814606 184.67817683 199.54156199
 178.47965223 200.92092414 168.73879373 177.75411823 199.21537872
 172.08712115 201.23961571 188.31234994 156.64235663 186.41618342
 181.35233435 186.61429961 207.52091529 172.00661012 180.75061433
 166.2602041  189.90425519 185.26228451 183.33908152 206.99334242
 187.3877552  214.56675742 212.4576771  199.49968636 170.22891945
 213.54636851 212.39063303 183.24020938 227.35121099 205.40930672
 200.77798246 159.14370065 164.80882874 208.62846689 190.06776131
 185.87862673]
2025-06-23 10:42:53 INFO Expected Optimum FE: -100
2025-06-23 10:42:53 INFO Unimodal AOCC mean: 0.1757
2025-06-23 10:42:53 INFO Multimodal (single component) AOCC mean: 0.1020
2025-06-23 10:42:53 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 10:42:53 INFO AOCC mean: 0.0926
