2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:18 ERROR Can not run the algorithm
2025-06-23 14:26:18 INFO Run function 2 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-23 14:26:18 INFO FeHistory: []
2025-06-23 14:26:18 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:26:18 ERROR Can not run the algorithm
2025-06-23 14:26:18 INFO Run function 15 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-23 14:26:18 INFO FeHistory: []
2025-06-23 14:26:18 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:26:18 ERROR Can not run the algorithm
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:26:18 ERROR Can not run the algorithm
2025-06-23 14:26:18 INFO Run function 24 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-23 14:26:18 INFO FeHistory: []
2025-06-23 14:26:18 INFO Expected Optimum FE: -100
2025-06-23 14:26:18 INFO Unimodal AOCC mean: 0.0000
2025-06-23 14:26:18 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 14:26:18 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:26:18 INFO AOCC mean: 0.0000
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:18 ERROR Can not run the algorithm
2025-06-23 14:26:18 INFO Run function 2 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-23 14:26:18 INFO FeHistory: []
2025-06-23 14:26:18 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:26:18 ERROR Can not run the algorithm
2025-06-23 14:26:18 INFO Run function 15 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-23 14:26:18 INFO FeHistory: []
2025-06-23 14:26:18 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:26:18 ERROR Can not run the algorithm
2025-06-23 14:26:18 INFO Run function 24 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-23 14:26:18 INFO FeHistory: []
2025-06-23 14:26:18 INFO Expected Optimum FE: -100
2025-06-23 14:26:18 INFO Unimodal AOCC mean: 0.0000
2025-06-23 14:26:18 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-23 14:26:18 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:26:18 INFO AOCC mean: 0.0000
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:18 INFO Run function 2 complete. FEHistory len: 100, AOCC: 0.1748
2025-06-23 14:26:18 INFO FeHistory: [-701.32705266 -701.32353017 -701.30109629 -701.29125815 -701.29015527
 -701.28715837 -701.33446335 -701.29474622 -701.29929859 -701.29466532
 -701.34400892 -701.32957517 -701.29929535 -701.33592518 -701.30073251
 -701.26786147 -701.29486153 -701.33886738 -701.31213262 -701.28992179
 -701.29701506 -701.29487992 -701.29175749 -701.29925843 -701.33063965
 -701.33061472 -701.31778007 -701.32700818 -701.30964407 -701.30520904
 -701.31773358 -701.30023806 -701.34007702 -701.29322572 -701.28863162
 -701.3363352  -701.30248417 -701.3078197  -701.32263552 -701.28395312
 -701.29124768 -701.29513759 -701.31972143 -701.29568217 -701.30541796
 -701.31124385 -701.30929912 -701.32583421 -701.3183794  -701.3062607
 -701.31636764 -701.32044595 -701.34191698 -701.29140113 -701.3140486
 -701.29489063 -701.32310235 -701.32959732 -701.30891834 -701.32917443
 -701.3382112  -701.31334922 -701.29033217 -701.30390348 -701.30639251
 -701.31434246 -701.31786097 -701.29846432 -701.33210587 -701.31903895
 -701.2742485  -701.29694225 -701.3006987  -701.30985385 -701.32845715
 -701.27218462 -701.3102043  -701.31181495 -701.30421849 -701.3228571
 -701.30176648 -701.30725646 -701.31005869 -701.32323208 -701.32807909
 -701.31877455 -701.30064323 -701.28490771 -701.30601735 -701.30365856
 -701.2906931  -701.28985985 -701.31389602 -701.31861739 -701.34753318
 -701.29163045 -701.29783456 -701.29641413 -701.2890482  -701.32583763]
2025-06-23 14:26:18 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:26:18 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA_Enhanced
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA_Enhanced
# Description: Combines adaptive Gaussian mutation with an archive for multimodal optimization.
# Code:

class AdaptiveGaussianArchiveEA_Enhanced:
    """
    Combines adaptive Gaussian mutation with an archive to maintain diversity and escape local optima in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  # Increased archive size for better diversity
        self.archive = []
        self.sigma = 20.0  # Initial standard deviation for Gaussian mutation
        self.sigma_decay = 0.98 # Adjust decay rate for faster convergence near optima.


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(population)
        self.eval_count += self.population_size

        self.best_solution_overall, self.best_fitness_overall = self._find_best(population, fitness)
        self.archive = self._update_archive(self.archive, population, fitness, self.archive_size)


        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, self.sigma)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population = np.concatenate((population, offspring))
            fitness = np.concatenate((fitness, offspring_fitness))
            
            self.archive = self._update_archive(self.archive, population, fitness, self.archive_size)
            population, fitness = self._selection(population, fitness)
            
            self.best_solution_overall, self.best_fitness_overall = self._find_best(population, fitness)
            self.sigma *= self.sigma_decay # Adaptive sigma decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_offspring(self, population, sigma):
        offspring = np.copy(population)
        for i in range(self.population_size):
            offspring[i] += np.random.normal(0, sigma, self.dim)
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)
        return offspring
    
    def _update_archive(self, archive, population, fitness, max_size):
        combined = np.column_stack((population, fitness))
        combined = combined[combined[:, -1].argsort()] # Sort by fitness
        
        updated_archive = []
        for sol, fit in combined:
            if len(updated_archive) < max_size:
                updated_archive.append((sol,fit))
            else:
                break
        return updated_archive
    

    def _selection(self, population, fitness):
        combined = np.column_stack((population, fitness))
        sorted_pop = combined[combined[:, -1].argsort()]
        return sorted_pop[:, :-1][:self.population_size], sorted_pop[:, -1][:self.population_size]


    def _find_best(self, population, fitness):
        best_idx = np.argmin(fitness)
        best_solution = population[best_idx]
        best_fitness = fitness[best_idx]
        return best_solution, best_fitness

2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:26:18 ERROR Can not run the algorithm
2025-06-23 14:26:18 INFO Run function 15 complete. FEHistory len: 100, AOCC: 0.0990
2025-06-23 14:26:18 INFO FeHistory: [-223.55732884 -222.39428486 -222.81429093 -221.74575695 -221.01449645
 -222.85627825 -221.6538638  -221.63256588 -222.77272919 -222.12333613
 -222.84447032 -222.21751973 -222.21357579 -222.75381153 -222.27024009
 -221.22926079 -221.39164536 -221.93878949 -220.72024313 -222.36184029
 -222.73427178 -223.70310503 -222.23208813 -222.14003093 -221.28140827
 -222.76541258 -221.77190826 -221.53505554 -220.9867374  -220.3653392
 -222.50844643 -221.26859563 -221.04774686 -222.45393861 -222.48404484
 -222.50415628 -221.528767   -222.54614142 -222.10772231 -223.04086463
 -222.46472042 -223.51986265 -221.25627613 -221.72902863 -222.74379422
 -221.60804471 -221.17932846 -220.97525356 -221.56848679 -222.26286968
 -222.2951968  -221.22151537 -220.81116366 -220.87925943 -222.31880332
 -221.1595366  -221.56814375 -220.56010296 -222.76800986 -221.60796028
 -221.72800435 -219.57141158 -221.39920746 -222.59902524 -221.66011792
 -222.06330586 -222.9183344  -223.77500768 -222.21156403 -222.78617051
 -222.31554292 -222.67505754 -224.01737758 -220.289474   -220.573784
 -222.28026788 -221.06139542 -223.54617079 -222.45840206 -223.40901195
 -222.0447668  -221.58332148 -220.50616969 -221.78311947 -222.69386504
 -222.36776906 -222.21079072 -223.58168823 -221.16299696 -223.29989117
 -222.68857987 -221.62243591 -224.04908306 -220.78703996 -222.47088185
 -221.38967273 -221.74604681 -221.43890334 -222.05015094 -222.02223885]
2025-06-23 14:26:18 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:26:18 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:26:18 ERROR Can not run the algorithm
2025-06-23 14:26:19 INFO Run function 24 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-23 14:26:19 INFO FeHistory: [206.23888073 208.60948966 186.86747641 173.8039838  162.46674682
 166.73066633 202.55942049 210.52346747 219.15291634 237.1896686
 194.24530809 205.98657549 209.41219566 192.9257762  184.10998138
 185.29182118 169.87895654 159.67566504 188.64491304 190.00714187
 210.51047789 189.3269245  186.27902163 217.92103585 180.68222986
 185.16696149 208.51510447 213.23960134 160.58618878 186.20342085
 157.25322746 191.54434144 168.64655271 166.84263259 246.51534126
 190.17761516 145.99876239 181.88241797 148.90051303 164.97171893
 207.2772371  197.57191879 164.79089085 194.10333963 187.98828425
 192.51196965 178.75255179 197.13243102 177.86742493 181.80152105
 223.60974342 209.71686957 184.90873702 173.06845214 186.50726128
 199.30261886 146.51771559 180.62373393 170.72171325 168.15140937
 181.26672043 220.80851129 180.7988827  183.99711875 175.17657181
 194.82619887 214.46265373 202.63706934 166.82522997 204.26620369
 188.43494882 180.28466401 183.26773389 176.04662577 223.8693084
 181.67601193 206.67581317 179.11767166 210.65511389 180.80987612
 165.49906975 210.10664647 180.23990246 195.95683147 210.01000658
 151.74113625 148.96035882 184.22847637 216.55987183 182.91113026
 178.709138   188.18700037 244.69415819 209.54592938 222.59948548
 192.50591586 211.72284539 182.30039466 194.25599928 175.07067375]
2025-06-23 14:26:19 INFO Expected Optimum FE: -100
2025-06-23 14:26:19 INFO Unimodal AOCC mean: 0.1748
2025-06-23 14:26:19 INFO Multimodal (single component) AOCC mean: 0.0990
2025-06-23 14:26:19 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:26:19 INFO AOCC mean: 0.0913
2025-06-23 14:26:19 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:26:25 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1867
2025-06-23 14:26:25 INFO FeHistory: [-701.31425952 -701.34031366 -701.34894631 ... -701.97393922 -702.00115394
 -702.02702516]
2025-06-23 14:26:25 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:26:25 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveDE
import numpy as np
import random

# Name: ArchiveGuidedAdaptiveDE
# Description: Combines Differential Evolution with an adaptive archive for efficient multimodal optimization.
class ArchiveGuidedAdaptiveDE:
    """
    Combines Differential Evolution with an adaptive archive and Gaussian mutation for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim
        self.archive_size = 200  # Larger archive for diversity
        self.archive = []
        self.population = None
        self.F_scale = 0.5
        self.sigma = 0.1 * (self.upper_bounds - self.lower_bounds) # Initial Gaussian mutation scale


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.update_archive(offspring, offspring_fitness)

            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)
            #Adaptive Sigma decay
            self.sigma *= 0.98

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        self.F_scale = 0.5 + 0.3 * np.random.rand()

        for i in range(self.population_size):
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            #Gaussian Mutation
            offspring[i] += np.random.normal(0, self.sigma, self.dim)
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

2025-06-23 14:26:25 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:26:28 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1786
2025-06-23 14:26:28 INFO FeHistory: [-701.31675186 -701.28394665 -701.33475185 ... -701.49857187 -701.49857187
 -701.49857187]
2025-06-23 14:26:28 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:26:28 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveDE
import numpy as np
import random

# Name: ArchiveGuidedAdaptiveDE
# Description: Combines Differential Evolution with an adaptive archive for robust multimodal optimization.
class ArchiveGuidedAdaptiveDE:
    """
    Combines Differential Evolution (DE) with an adaptive archive to enhance exploration and exploitation in multimodal landscapes.  
    Uses adaptive scaling factor and archive management for better diversity and convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 200  # Increased archive size
        self.archive = []
        self.population = None
        self.F_scale = 0.5  # Initial scaling factor
        self.F_decay = 0.99 #decay rate for scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive (prioritize diversity and fitness)
            self.update_archive(offspring, offspring_fitness)

            # Select next generation (tournament selection)
            combined_population = np.vstack((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            
            #Tournament selection
            self.population, fitness = self.tournament_selection(combined_population, combined_fitness)


            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)
            self.F_scale *= self.F_decay #Adaptive scaling factor decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        
        for i in range(self.population_size):
            # Select pbest from archive (if available), otherwise use current best
            pbest = self.select_pbest(population, fitness)

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

        return offspring
    
    def select_pbest(self, population, fitness):
        if self.archive:
            #Select from archive with a bias towards better solutions
            archive_fitness = np.array([f for _, f in self.archive])
            probabilities = np.exp(-archive_fitness) / np.sum(np.exp(-archive_fitness))
            pbest_index = np.random.choice(len(self.archive), p=probabilities)
            return self.archive[pbest_index][0]
        else:
            return population[np.argmin(fitness)]

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Replace worst solution in archive if offspring is better or diverse enough.
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or self.is_diverse(offspring[i]):
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def is_diverse(self, solution):
         #Simple diversity check: Euclidean distance to existing archive solutions
        if not self.archive:
            return True
        distances = np.linalg.norm(np.array([s for s,_ in self.archive]) - solution, axis=1)
        return np.min(distances) > 0.1 * np.linalg.norm(self.upper_bounds - self.lower_bounds)


    def tournament_selection(self, population, fitness):
        tournament_size = 5
        selected_indices = []
        for _ in range(self.population_size):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness[tournament])]
            selected_indices.append(winner_index)
        return population[selected_indices], fitness[selected_indices]

2025-06-23 14:26:28 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:26:31 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1100
2025-06-23 14:26:31 INFO FeHistory: [-220.98826174 -221.22486631 -223.01767173 ... -224.4946184  -223.46928261
 -222.58384101]
2025-06-23 14:26:31 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:26:31 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveDE
import numpy as np
import random

# Name: ArchiveGuidedAdaptiveDE
# Description: Combines Differential Evolution with an adaptive archive for efficient multimodal optimization.
class ArchiveGuidedAdaptiveDE:
    """
    Combines Differential Evolution with an adaptive archive and Gaussian mutation for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim
        self.archive_size = 200  # Larger archive for diversity
        self.archive = []
        self.population = None
        self.F_scale = 0.5
        self.sigma = 0.1 * (self.upper_bounds - self.lower_bounds) # Initial Gaussian mutation scale


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.update_archive(offspring, offspring_fitness)

            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)
            #Adaptive Sigma decay
            self.sigma *= 0.98

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        self.F_scale = 0.5 + 0.3 * np.random.rand()

        for i in range(self.population_size):
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            #Gaussian Mutation
            offspring[i] += np.random.normal(0, self.sigma, self.dim)
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

2025-06-23 14:26:31 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:26:39 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1111
2025-06-23 14:26:39 INFO FeHistory: [-220.21218435 -221.02871854 -220.31655049 ... -226.59818112 -226.59818112
 -226.59818112]
2025-06-23 14:26:39 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:26:39 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveDE
import numpy as np
import random

# Name: ArchiveGuidedAdaptiveDE
# Description: Combines Differential Evolution with an adaptive archive for robust multimodal optimization.
class ArchiveGuidedAdaptiveDE:
    """
    Combines Differential Evolution (DE) with an adaptive archive to enhance exploration and exploitation in multimodal landscapes.  
    Uses adaptive scaling factor and archive management for better diversity and convergence.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 200  # Increased archive size
        self.archive = []
        self.population = None
        self.F_scale = 0.5  # Initial scaling factor
        self.F_decay = 0.99 #decay rate for scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive (prioritize diversity and fitness)
            self.update_archive(offspring, offspring_fitness)

            # Select next generation (tournament selection)
            combined_population = np.vstack((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            
            #Tournament selection
            self.population, fitness = self.tournament_selection(combined_population, combined_fitness)


            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)
            self.F_scale *= self.F_decay #Adaptive scaling factor decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        
        for i in range(self.population_size):
            # Select pbest from archive (if available), otherwise use current best
            pbest = self.select_pbest(population, fitness)

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

        return offspring
    
    def select_pbest(self, population, fitness):
        if self.archive:
            #Select from archive with a bias towards better solutions
            archive_fitness = np.array([f for _, f in self.archive])
            probabilities = np.exp(-archive_fitness) / np.sum(np.exp(-archive_fitness))
            pbest_index = np.random.choice(len(self.archive), p=probabilities)
            return self.archive[pbest_index][0]
        else:
            return population[np.argmin(fitness)]

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Replace worst solution in archive if offspring is better or diverse enough.
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or self.is_diverse(offspring[i]):
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def is_diverse(self, solution):
         #Simple diversity check: Euclidean distance to existing archive solutions
        if not self.archive:
            return True
        distances = np.linalg.norm(np.array([s for s,_ in self.archive]) - solution, axis=1)
        return np.min(distances) > 0.1 * np.linalg.norm(self.upper_bounds - self.lower_bounds)


    def tournament_selection(self, population, fitness):
        tournament_size = 5
        selected_indices = []
        for _ in range(self.population_size):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness[tournament])]
            selected_indices.append(winner_index)
        return population[selected_indices], fitness[selected_indices]

2025-06-23 14:26:39 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:26:50 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0198
2025-06-23 14:26:50 INFO FeHistory: [169.32746439 202.64182469 151.37883892 ... -47.96120437 -59.46570002
 -64.04550086]
2025-06-23 14:26:50 INFO Expected Optimum FE: -100
2025-06-23 14:26:50 INFO Unimodal AOCC mean: 0.1867
2025-06-23 14:26:50 INFO Multimodal (single component) AOCC mean: 0.1100
2025-06-23 14:26:50 INFO Multimodal (multiple components) AOCC mean: 0.0198
2025-06-23 14:26:50 INFO AOCC mean: 0.1055
2025-06-23 14:27:02 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0008
2025-06-23 14:27:02 INFO FeHistory: [168.88147014 176.2145545  198.94574735 ...  -2.05348273  -2.05348273
  -2.05348273]
2025-06-23 14:27:02 INFO Expected Optimum FE: -100
2025-06-23 14:27:02 INFO Unimodal AOCC mean: 0.1786
2025-06-23 14:27:02 INFO Multimodal (single component) AOCC mean: 0.1111
2025-06-23 14:27:02 INFO Multimodal (multiple components) AOCC mean: 0.0008
2025-06-23 14:27:02 INFO AOCC mean: 0.0968
2025-06-23 14:28:15 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1796
2025-06-23 14:28:15 INFO FeHistory: [-701.32534666 -701.31791564 -701.3575464  ... -701.50260765 -701.52928305
 -701.51559869]
2025-06-23 14:28:15 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:28:15 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchiveAndGaussianPerturbation
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndGaussianPerturbation
# Description: Combines Differential Evolution, an archive for diversity, and Gaussian perturbation to escape local optima.
class AdaptiveDEwithArchiveAndGaussianPerturbation:
    """
    Combines Differential Evolution (DE), an archive to maintain diversity, and Gaussian perturbation to escape local optima in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
        self.archive = self._update_archive(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                
                # Gaussian perturbation
                mutant += np.random.normal(0, self.mutation_scale, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
            self.archive = self._update_archive(np.vstack((self.population,self.population)), np.concatenate((self.fitness_values, self.fitness_values)))
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 14:28:15 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:29:54 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1757
2025-06-23 14:29:54 INFO FeHistory: [-701.35329832 -701.26404972 -701.26762668 ... -701.31774785 -701.31774785
 -701.30316583]
2025-06-23 14:29:54 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:29:54 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEAwithNiching
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEAwithNiching
# Description: Improves exploration and exploitation in multimodal landscapes using adaptive Gaussian sampling, an archive, and niching.
# Code:
class AdaptiveGaussianArchiveEAwithNiching:
    """
    Combines adaptive Gaussian sampling with an archive and niching to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.98
        self.archive = []
        self.niche_radius = 0.25 * np.linalg.norm(self.upper_bounds - self.lower_bounds) #Adaptive niche radius


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._niche_preservation_selection(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay
            self.niche_radius *= self.sigma_decay #Adapt niche radius


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)


    def _niche_preservation_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        next_gen = []
        next_fit = []
        
        for i in range(self.population_size):
            best_individual = None
            best_fitness = float('inf')
            for j in range(len(combined_pop)):
                distance = np.linalg.norm(combined_pop[j] - (population[i] if i < len(population) else np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim))) #Distance to existing niche
                if distance < self.niche_radius and combined_fit[j] < best_fitness:
                    best_individual = combined_pop[j]
                    best_fitness = combined_fit[j]
            if best_individual is not None:
                next_gen.append(best_individual)
                next_fit.append(best_fitness)

        next_gen = np.array(next_gen)
        next_fit = np.array(next_fit)
        
        return next_gen, next_fit


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 14:29:54 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:30:08 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1080
2025-06-23 14:30:08 INFO FeHistory: [-220.81891388 -223.28781446 -221.88423779 ... -222.06330515 -221.25501802
 -222.46188996]
2025-06-23 14:30:08 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:30:08 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchiveAndGaussianPerturbation
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndGaussianPerturbation
# Description: Combines Differential Evolution, an archive for diversity, and Gaussian perturbation to escape local optima.
class AdaptiveDEwithArchiveAndGaussianPerturbation:
    """
    Combines Differential Evolution (DE), an archive to maintain diversity, and Gaussian perturbation to escape local optima in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
        self.archive = self._update_archive(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                
                # Gaussian perturbation
                mutant += np.random.normal(0, self.mutation_scale, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
            self.archive = self._update_archive(np.vstack((self.population,self.population)), np.concatenate((self.fitness_values, self.fitness_values)))
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 14:30:08 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:32:13 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:32:13 INFO FeHistory: [190.60224863 175.54181223 189.38785606 ...  75.87187379  92.73493516
  66.8146169 ]
2025-06-23 14:32:13 INFO Expected Optimum FE: -100
2025-06-23 14:32:13 INFO Unimodal AOCC mean: 0.1796
2025-06-23 14:32:13 INFO Multimodal (single component) AOCC mean: 0.1080
2025-06-23 14:32:13 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:32:13 INFO AOCC mean: 0.0959
2025-06-23 14:33:11 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1057
2025-06-23 14:33:11 INFO FeHistory: [-221.85392935 -224.07534579 -222.54040589 ... -222.29060866 -222.29060866
 -222.38538431]
2025-06-23 14:33:11 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:33:11 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEAwithNiching
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEAwithNiching
# Description: Improves exploration and exploitation in multimodal landscapes using adaptive Gaussian sampling, an archive, and niching.
# Code:
class AdaptiveGaussianArchiveEAwithNiching:
    """
    Combines adaptive Gaussian sampling with an archive and niching to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.98
        self.archive = []
        self.niche_radius = 0.25 * np.linalg.norm(self.upper_bounds - self.lower_bounds) #Adaptive niche radius


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._niche_preservation_selection(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay
            self.niche_radius *= self.sigma_decay #Adapt niche radius


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)


    def _niche_preservation_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        next_gen = []
        next_fit = []
        
        for i in range(self.population_size):
            best_individual = None
            best_fitness = float('inf')
            for j in range(len(combined_pop)):
                distance = np.linalg.norm(combined_pop[j] - (population[i] if i < len(population) else np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim))) #Distance to existing niche
                if distance < self.niche_radius and combined_fit[j] < best_fitness:
                    best_individual = combined_pop[j]
                    best_fitness = combined_fit[j]
            if best_individual is not None:
                next_gen.append(best_individual)
                next_fit.append(best_fitness)

        next_gen = np.array(next_gen)
        next_fit = np.array(next_fit)
        
        return next_gen, next_fit


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 14:33:11 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:36:49 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:36:49 INFO FeHistory: [163.0352194  202.72009757 170.9749356  ... 186.53419358 186.53419358
 171.90651498]
2025-06-23 14:36:49 INFO Expected Optimum FE: -100
2025-06-23 14:36:49 INFO Unimodal AOCC mean: 0.1757
2025-06-23 14:36:49 INFO Multimodal (single component) AOCC mean: 0.1057
2025-06-23 14:36:49 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:36:49 INFO AOCC mean: 0.0938
2025-06-23 14:37:38 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:38:46 INFO Run function 2 complete. FEHistory len: 70000, AOCC: 0.1878
2025-06-23 14:38:46 INFO FeHistory: [-701.29982587 -701.32211297 -701.28549207 ... -701.83067137 -701.83067137
 -701.83067137]
2025-06-23 14:38:46 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:38:46 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
from scipy.stats import norm

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A differential evolution algorithm enhanced with adaptive mutation and clustering for efficient multimodal optimization.

class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.archive = []  # Archive of good solutions
        self.archive_size = 200
        self.cluster_threshold = 0.5 #Adjust this parameter based on problem characteristics.


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values) #Adaptive parameter tuning

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_mutants(i, population)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_mutants(self, i, population):
        indices = np.random.choice(self.population_size, 3, replace=False)
        while i in indices:
            indices = np.random.choice(self.population_size, 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.where(np.random.rand(self.dim) < self.CR, v, x)
        trial[jrand] = v[jrand]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        idx = np.argsort(combined_fit)
        return combined_pop[idx[:self.population_size]], combined_fit[idx[:self.population_size]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptive mechanism: Adjust F and CR based on convergence
        mean_fitness = np.mean(fitness_values)
        if mean_fitness < 0.1 * self.best_fitness_overall: #If convergence is fast, reduce exploration
          self.F *= 0.9
          self.CR *= 0.95
        else: #If convergence is slow, increase exploration
          self.F *= 1.1
          self.CR *= 1.05
        self.F = np.clip(self.F, 0.1, 1.0)
        self.CR = np.clip(self.CR, 0.1, 1.0)

    def _distance(self, x, y):
        return np.linalg.norm(x - y)


    def _cluster_solutions(self, population):
        clusters = []
        for i, sol in enumerate(population):
            assigned = False
            for j, cluster in enumerate(clusters):
                if self._distance(sol, cluster[0]) < self.cluster_threshold:
                    clusters[j].append(sol)
                    assigned = True
                    break
            if not assigned:
                clusters.append([sol])
        return clusters
2025-06-23 14:38:46 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:39:37 INFO Run function 15 complete. FEHistory len: 70000, AOCC: 0.1173
2025-06-23 14:39:37 INFO FeHistory: [-220.51748466 -221.55634886 -221.71275076 ... -227.76842849 -227.76842849
 -227.76842849]
2025-06-23 14:39:37 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:39:37 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
from scipy.stats import norm

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A differential evolution algorithm enhanced with adaptive mutation and clustering for efficient multimodal optimization.

class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.archive = []  # Archive of good solutions
        self.archive_size = 200
        self.cluster_threshold = 0.5 #Adjust this parameter based on problem characteristics.


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values) #Adaptive parameter tuning

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_mutants(i, population)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_mutants(self, i, population):
        indices = np.random.choice(self.population_size, 3, replace=False)
        while i in indices:
            indices = np.random.choice(self.population_size, 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
        jrand = np.random.randint(self.dim)
        trial = np.where(np.random.rand(self.dim) < self.CR, v, x)
        trial[jrand] = v[jrand]
        return trial

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        idx = np.argsort(combined_fit)
        return combined_pop[idx[:self.population_size]], combined_fit[idx[:self.population_size]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptive mechanism: Adjust F and CR based on convergence
        mean_fitness = np.mean(fitness_values)
        if mean_fitness < 0.1 * self.best_fitness_overall: #If convergence is fast, reduce exploration
          self.F *= 0.9
          self.CR *= 0.95
        else: #If convergence is slow, increase exploration
          self.F *= 1.1
          self.CR *= 1.05
        self.F = np.clip(self.F, 0.1, 1.0)
        self.CR = np.clip(self.CR, 0.1, 1.0)

    def _distance(self, x, y):
        return np.linalg.norm(x - y)


    def _cluster_solutions(self, population):
        clusters = []
        for i, sol in enumerate(population):
            assigned = False
            for j, cluster in enumerate(clusters):
                if self._distance(sol, cluster[0]) < self.cluster_threshold:
                    clusters[j].append(sol)
                    assigned = True
                    break
            if not assigned:
                clusters.append([sol])
        return clusters
2025-06-23 14:39:37 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:42:50 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-23 14:42:50 INFO FeHistory: [209.53129049 225.66547236 212.10781276 ...  49.46931483  48.40627097
  46.91538811]
2025-06-23 14:42:50 INFO Expected Optimum FE: -100
2025-06-23 14:42:50 INFO Unimodal AOCC mean: 0.1878
2025-06-23 14:42:50 INFO Multimodal (single component) AOCC mean: 0.1173
2025-06-23 14:42:50 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:42:50 INFO AOCC mean: 0.1017
