2025-06-22 20:44:31 INFO Initializing first population
2025-06-22 20:44:31 INFO Initializing population from 7 seed files...
2025-06-22 20:44:31 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:44:36 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1734
2025-06-22 20:44:36 INFO FeHistory: [-333.97720574 -333.91860195 -334.03500197 ... -336.11597249 -336.11597522
 -336.11597552]
2025-06-22 20:44:36 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:44:36 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Standard Deviation for Gaussian Sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Adaptive Gaussian Sampling
            parents = self.tournament_selection(fitness_values, k=5)  # Tournament Selection
            offspring = self.gaussian_mutation(parents, self.sigma)

            # Bounds handling
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update population and best solution
            self.population = np.concatenate((self.population, offspring))
            fitness_values = np.concatenate((fitness_values, offspring_fitness))

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            # Adaptive Sigma
            self.sigma *= 0.99  # Gradually reduce sigma for finer search later.

            # Elitism
            sorted_pop = self.population[np.argsort(fitness_values)]
            self.population = sorted_pop[:self.population_size]
            fitness_values = fitness_values[np.argsort(fitness_values)][:self.population_size]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2  # Select half the population as parents
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

2025-06-22 20:44:36 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:44:40 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.2667
2025-06-22 20:44:40 INFO FeHistory: [361283.94363952 276112.3186285  101807.38577261 ...  -1002.47784284
  -1002.47784238  -1002.47784261]
2025-06-22 20:44:40 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:44:40 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Standard Deviation for Gaussian Sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Adaptive Gaussian Sampling
            parents = self.tournament_selection(fitness_values, k=5)  # Tournament Selection
            offspring = self.gaussian_mutation(parents, self.sigma)

            # Bounds handling
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update population and best solution
            self.population = np.concatenate((self.population, offspring))
            fitness_values = np.concatenate((fitness_values, offspring_fitness))

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            # Adaptive Sigma
            self.sigma *= 0.99  # Gradually reduce sigma for finer search later.

            # Elitism
            sorted_pop = self.population[np.argsort(fitness_values)]
            self.population = sorted_pop[:self.population_size]
            fitness_values = fitness_values[np.argsort(fitness_values)][:self.population_size]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2  # Select half the population as parents
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

2025-06-22 20:44:40 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:44:58 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2457
2025-06-22 20:44:58 INFO FeHistory: [ 16.13753683  23.6695745   28.25300527 ... -99.99808894 -99.99825589
 -99.99804753]
2025-06-22 20:44:58 INFO Expected Optimum FE: -100
2025-06-22 20:44:58 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Standard Deviation for Gaussian Sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Adaptive Gaussian Sampling
            parents = self.tournament_selection(fitness_values, k=5)  # Tournament Selection
            offspring = self.gaussian_mutation(parents, self.sigma)

            # Bounds handling
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update population and best solution
            self.population = np.concatenate((self.population, offspring))
            fitness_values = np.concatenate((fitness_values, offspring_fitness))

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            # Adaptive Sigma
            self.sigma *= 0.99  # Gradually reduce sigma for finer search later.

            # Elitism
            sorted_pop = self.population[np.argsort(fitness_values)]
            self.population = sorted_pop[:self.population_size]
            fitness_values = fitness_values[np.argsort(fitness_values)][:self.population_size]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2  # Select half the population as parents
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

2025-06-22 20:44:58 INFO Unimodal AOCC mean: 0.1734
2025-06-22 20:44:58 INFO Multimodal (single component) AOCC mean: 0.2667
2025-06-22 20:44:58 INFO Multimodal (multiple components) AOCC mean: 0.2457
2025-06-22 20:44:58 INFO AOCC mean: 0.2286
2025-06-22 20:44:58 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:45:04 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1765
2025-06-22 20:45:04 INFO FeHistory: [-333.98519148 -333.99834764 -333.98571387 ... -336.26980274 -336.26936366
 -336.26964702]
2025-06-22 20:45:04 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:45:04 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-22 20:45:04 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:45:11 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.1565
2025-06-22 20:45:11 INFO FeHistory: [507621.77161489 226147.46128977 172080.72271628 ...  -1002.25300444
  -1002.21605943  -1002.26029353]
2025-06-22 20:45:11 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:45:11 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-22 20:45:11 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:45:29 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.1661
2025-06-22 20:45:29 INFO FeHistory: [ 22.3038118    5.69779053  11.21295907 ... -99.3172901  -99.4634826
 -99.43839622]
2025-06-22 20:45:29 INFO Expected Optimum FE: -100
2025-06-22 20:45:29 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-22 20:45:29 INFO Unimodal AOCC mean: 0.1765
2025-06-22 20:45:29 INFO Multimodal (single component) AOCC mean: 0.1565
2025-06-22 20:45:29 INFO Multimodal (multiple components) AOCC mean: 0.1661
2025-06-22 20:45:29 INFO AOCC mean: 0.1664
2025-06-22 20:45:29 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:45:35 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1761
2025-06-22 20:45:35 INFO FeHistory: [-333.96504633 -333.94220218 -333.96574711 ... -336.27726269 -336.27725724
 -336.27724953]
2025-06-22 20:45:35 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:45:35 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-22 20:45:35 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:45:40 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.3745
2025-06-22 20:45:40 INFO FeHistory: [170971.03116528 243363.54112662 166759.67693997 ...  -1002.47907863
  -1002.47907859  -1002.47907864]
2025-06-22 20:45:40 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:45:40 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-22 20:45:40 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:45:57 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2612
2025-06-22 20:45:57 INFO FeHistory: [ 46.13913456  35.60091093  46.14797938 ... -99.99862758 -99.99872498
 -99.99890264]
2025-06-22 20:45:57 INFO Expected Optimum FE: -100
2025-06-22 20:45:57 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-22 20:45:57 INFO Unimodal AOCC mean: 0.1761
2025-06-22 20:45:57 INFO Multimodal (single component) AOCC mean: 0.3745
2025-06-22 20:45:57 INFO Multimodal (multiple components) AOCC mean: 0.2612
2025-06-22 20:45:57 INFO AOCC mean: 0.2706
2025-06-22 20:45:57 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:46:03 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1899
2025-06-22 20:46:03 INFO FeHistory: [-334.01190785 -334.0238796  -334.01401808 ... -336.23513722 -336.23513722
 -336.23513722]
2025-06-22 20:46:03 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:46:03 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithEnhancedInitialization
import numpy as np
from scipy.optimize import minimize

class AdaptiveDifferentialEvolutionWithEnhancedInitialization:
    """
    Combines Differential Evolution with enhanced initialization near known optima and local search for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], known_optimum=None):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.known_optimum = known_optimum  # Allow for None if no known optimum

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.local_search_freq = 5 # Perform local search every 5 generations

    def initialize_population(self, num_samples):
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(num_samples, self.dim))
        
        if self.known_optimum is not None:
            num_near_optimum = int(0.3 * num_samples) # 30% near the optimum
            noise_scale = 20 # Adjust noise scale as needed. Experiment with this!
            noise = np.random.normal(scale=noise_scale, size=(num_near_optimum, self.dim))
            population[:num_near_optimum, :] = self.known_optimum + noise
            population[:num_near_optimum, :] = np.clip(population[:num_near_optimum, :], self.lower_bounds, self.upper_bounds)

        return population

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self.initialize_population(self.population_size)
        fitness = objective_function(population)
        self.eval_count += len(fitness)
        best_solution = population[np.argmin(fitness)]
        best_fitness = np.min(fitness)
        self.best_solution_overall = best_solution
        self.best_fitness_overall = best_fitness

        generation = 0
        while self.eval_count < self.budget:
            # Differential Evolution
            new_population = np.zeros_like(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if trial_fitness[0] < fitness[i]:
                    new_population[i] = trial
                    fitness[i] = trial_fitness[0]
                else:
                    new_population[i] = population[i]

            population = new_population
            best_solution = population[np.argmin(fitness)]
            best_fitness = np.min(fitness)

            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            # Local Search
            if generation % self.local_search_freq == 0:
                result = minimize(objective_function, best_solution, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
                if result.fun < best_fitness:
                    best_fitness = result.fun
                    best_solution = result.x
                    self.eval_count += result.nfev

                    if best_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = best_fitness
                        self.best_solution_overall = best_solution

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info









2025-06-22 20:46:03 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:46:09 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 20:46:09 INFO FeHistory: [202255.89088946 331858.91512843 187047.98416736 ... 102463.21037033
 184279.35090506  66101.7229297 ]
2025-06-22 20:46:09 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:46:09 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:46:27 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0393
2025-06-22 20:46:27 INFO FeHistory: [ 55.16813402  14.9897455   40.69711573 ... -64.05881563 -64.05881563
 -64.05881563]
2025-06-22 20:46:27 INFO Expected Optimum FE: -100
2025-06-22 20:46:27 INFO Unimodal AOCC mean: 0.1899
2025-06-22 20:46:27 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 20:46:27 INFO Multimodal (multiple components) AOCC mean: 0.0393
2025-06-22 20:46:27 INFO AOCC mean: 0.0764
2025-06-22 20:46:27 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:46:37 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1721
2025-06-22 20:46:37 INFO FeHistory: [-334.00747994 -334.01718933 -334.00747994 ... -334.76507624 -335.82828901
 -334.87071462]
2025-06-22 20:46:37 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:46:37 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-22 20:46:37 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:46:46 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 20:46:46 INFO FeHistory: [195634.10491285 214033.59312566 195634.10491285 ... 120345.54124761
  43539.79105414  48677.85730207]
2025-06-22 20:46:46 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:46:46 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:47:19 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0141
2025-06-22 20:47:19 INFO FeHistory: [ 26.25401998  22.07935523  26.25401998 ... -26.44788752 -28.00679936
   2.20716259]
2025-06-22 20:47:19 INFO Expected Optimum FE: -100
2025-06-22 20:47:19 INFO Unimodal AOCC mean: 0.1721
2025-06-22 20:47:19 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 20:47:19 INFO Multimodal (multiple components) AOCC mean: 0.0141
2025-06-22 20:47:19 INFO AOCC mean: 0.0621
2025-06-22 20:47:19 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:47:25 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1686
2025-06-22 20:47:25 INFO FeHistory: [-333.97116916 -333.95013831 -333.96733772 ... -336.23785711 -336.20822476
 -336.21788094]
2025-06-22 20:47:25 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:47:25 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-22 20:47:25 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:47:30 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0543
2025-06-22 20:47:30 INFO FeHistory: [187477.49157154 247413.36772958 177941.98277451 ...  -1000.93608658
  -1001.79181686  -1001.31988531]
2025-06-22 20:47:30 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:47:30 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:47:47 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.1160
2025-06-22 20:47:47 INFO FeHistory: [ 23.08793735  11.84160873  41.84275127 ... -98.75929062 -98.74321256
 -98.92002506]
2025-06-22 20:47:47 INFO Expected Optimum FE: -100
2025-06-22 20:47:47 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-22 20:47:47 INFO Unimodal AOCC mean: 0.1686
2025-06-22 20:47:47 INFO Multimodal (single component) AOCC mean: 0.0543
2025-06-22 20:47:47 INFO Multimodal (multiple components) AOCC mean: 0.1160
2025-06-22 20:47:47 INFO AOCC mean: 0.1129
2025-06-22 20:47:47 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:48:47 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 20:50:32 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1762
2025-06-22 20:50:32 INFO FeHistory: [-334.03574522 -333.94753235 -333.92303297 ... -336.27258099 -336.27258689
 -336.27259284]
2025-06-22 20:50:32 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:50:32 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 20:50:33 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:53:23 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.3744
2025-06-22 20:53:23 INFO FeHistory: [238217.21949813 391533.44562815 285957.73427899 ...  -1002.4790786
  -1002.47907859  -1002.47907862]
2025-06-22 20:53:23 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:53:23 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 20:53:23 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:56:17 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2613
2025-06-22 20:56:17 INFO FeHistory: [ 24.60922142   7.9174554   15.85520057 ... -99.99811198 -99.99830577
 -99.9987603 ]
2025-06-22 20:56:17 INFO Expected Optimum FE: -100
2025-06-22 20:56:17 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 20:56:17 INFO Unimodal AOCC mean: 0.1762
2025-06-22 20:56:17 INFO Multimodal (single component) AOCC mean: 0.3744
2025-06-22 20:56:17 INFO Multimodal (multiple components) AOCC mean: 0.2613
2025-06-22 20:56:17 INFO AOCC mean: 0.2706
2025-06-22 20:58:07 INFO Started evolutionary loop, best so far: 0.27062966512643344
2025-06-22 20:58:07 INFO Population length is: 15
2025-06-22 20:58:07 INFO --- Performing Long-Term Reflection at Generation 1 ---
2025-06-22 20:58:14 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianSamplingEAwithArchive` vs (worst) `AdaptiveMultimodalEvolutionaryStrategy`, we see that archiving improves performance, especially on multimodal problems.  The archive allows the algorithm to retain diverse solutions and prevents premature convergence, as seen in the improved AOCC scores across all GNBG groups.  `AdaptiveGaussianSamplingEAwithArchive` also benefits from adaptive Gaussian sampling to adjust exploration and exploitation efficiently.

(second best) `AdaptiveGaussianSamplingEA` vs (second worst) `AdaptiveDifferentialEvolutionWithEnhancedInitialization`: The inclusion of an archive in `AdaptiveGaussianSamplingEAwithArchive` significantly boosts its performance compared to `AdaptiveGaussianSamplingEA`. The archive acts as a memory, storing diverse solutions and enhancing exploration, particularly important for complex multimodal functions. The adaptive mutation and tournament selection also prove crucial for effective search.  `AdaptiveDifferentialEvolutionWithEnhancedInitialization`, despite enhanced initialization, struggles with multimodal components, indicating the importance of a well-designed recombination/mutation mechanism.

Comparing (1st) `AdaptiveGaussianSamplingEAwithArchive` vs (2nd) `AdaptiveGaussianSamplingEA`, we see that the archive significantly improves the algorithm's ability to handle multi-component multimodal problems. The archive helps to maintain diversity and prevent premature convergence to local optima.  The difference is less pronounced in unimodal problems, suggesting the archive's primary benefit is in exploration rather than exploitation.

(3rd) `AdaptiveGaussianSamplingEA` vs (4th) `AdaptiveGaussianMutationDE`: Both use Gaussian mutation, but the adaptive approach in the top performer provides a more refined search, leading to better performance.  `AdaptiveGaussianMutationDE`, while incorporating DE, lacks the effective balance of exploration and exploitation found in the higher-ranked algorithms.


Comparing (second worst) `AdaptiveDifferentialEvolutionWithEnhancedInitialization` vs (worst) `AdaptiveMultimodalEvolutionaryStrategy`, we observe that incorporating local search (`AdaptiveDifferentialEvolutionWithEnhancedInitialization`) can be beneficial, although not as much as adaptive sampling and archiving.  `AdaptiveMultimodalEvolutionaryStrategy`, lacking these critical features, shows relatively poor performance on multimodal problems.

Overall: Adaptive Gaussian sampling coupled with archiving consistently shows the best performance across the benchmark functions. Adaptive mutation and a robust selection strategy are key for balancing exploration and exploitation. Incorporating DE or local search provides some benefits but isn't sufficient on its own for high-dimensional multimodal landscapes.


**Experience:**

Effective high-dimensional optimization requires a balance between exploration and exploitation.  Archiving diverse solutions, adaptive Gaussian sampling, and tournament selection prove crucial for navigating complex multimodal landscapes.  Simpler adaptive mutation strategies are outperformed by more sophisticated approaches.

2025-06-22 20:58:16 INFO Full response text: * **Keywords:** High-dimensional optimization, exploration-exploitation, multimodal landscapes, adaptive sampling, advanced mutation.

* **Advice:** Focus on designing heuristics that dynamically adjust exploration/exploitation balance based on the optimization landscape's characteristics (e.g., using Bayesian Optimization or similar adaptive methods).  Prioritize sophisticated mutation strategies over simpler ones. Consider incorporating mechanisms for handling high dimensionality, like dimensionality reduction techniques or manifold learning.

* **Avoid:**  Simple, static heuristics; reliance on only one exploration/exploitation strategy; ignoring the landscape's structure; neglecting the effects of high dimensionality.

* **Explanation:**  Sophisticated heuristics are needed to handle the complexities of high-dimensional multimodal problems. Static strategies will fail to adapt, while ignoring dimensionality leads to poor performance.  Dynamic adaptation is key to efficiently finding global optima.

2025-06-22 20:58:16 INFO Generating offspring via Crossover...
2025-06-22 20:58:24 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:58:30 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1510
2025-06-22 20:58:30 INFO FeHistory: [-334.04418864 -333.97933496 -333.99233878 ... -334.43589449 -334.43589446
 -334.43589449]
2025-06-22 20:58:30 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:58:30 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchiveDE
import numpy as np
import random

class AdaptiveGaussianSamplingEAwithArchiveDE:
    """
    Combines adaptive Gaussian sampling, archiving, and differential evolution for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.sigma = 10.0 #Initial Gaussian standard deviation
        self.F = 0.8 #Differential Evolution scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.update_archive(offspring, offspring_fitness)

            combined_population = np.vstack((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            sorted_indices = np.argsort(combined_fitness)
            self.population = combined_population[sorted_indices[:self.population_size]]
            fitness = combined_fitness[sorted_indices[:self.population_size]]

            best_solution, best_fitness = self.population[np.argmin(fitness)], np.min(fitness)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness

            #Adaptive sigma (reduce exploration as we converge)
            self.sigma *= 0.98 #Example adaptive strategy


        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        for i in range(self.population_size):
            #Gaussian mutation with adaptive sigma
            parent = population[i]
            offspring[i] = parent + np.random.normal(0, self.sigma, self.dim)
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)

            #DE component
            if self.archive:
                a,b = random.sample(range(len(self.archive)), 2)
                v = self.archive[a][0] + self.F * (self.archive[b][0] - parent)
                v = np.clip(v, self.lower_bounds, self.upper_bounds)
                offspring[i] = (offspring[i] + v)/2 #Blend offspring

        return offspring


    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-22 20:58:30 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:58:35 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 20:58:35 INFO FeHistory: [317752.30522397 180123.71921346 212419.40811291 ...  11028.48057165
  11028.48057167  11028.48057164]
2025-06-22 20:58:35 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:58:35 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:58:52 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0371
2025-06-22 20:58:52 INFO FeHistory: [ 26.01379303  26.68149456   7.39086793 ... -59.02484135 -59.02484135
 -59.02484135]
2025-06-22 20:58:52 INFO Expected Optimum FE: -100
2025-06-22 20:58:52 INFO Unimodal AOCC mean: 0.1510
2025-06-22 20:58:52 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 20:58:52 INFO Multimodal (multiple components) AOCC mean: 0.0371
2025-06-22 20:58:52 INFO AOCC mean: 0.0627
2025-06-22 20:59:01 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:59:01 ERROR Can not run the algorithm
2025-06-22 20:59:01 INFO Run function 5 complete. FEHistory len: 1500, AOCC: 0.1481
2025-06-22 20:59:01 INFO FeHistory: [-334.00594589 -334.05113987 -333.92304245 ... -333.9429879  -334.04890434
 -334.08318228]
2025-06-22 20:59:01 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:59:01 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchiveAndLocalSearch
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchiveAndLocalSearch
# Description: Combines adaptive Gaussian sampling, an archive, and local search for efficient multimodal optimization.
# Code:
class AdaptiveGaussianSamplingEAwithArchiveAndLocalSearch:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Size of archive
        self.archive = []
        self.mutation_strength = 5.0
        self.mutation_rate = 0.1
        self.local_search_iterations = 5 #Number of local search iterations

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.archive = []
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        while self.eval_count < self.budget:
            fitness_values = objective_function(self.population)
            self.eval_count += self.population_size

            # Update archive and best solution
            for i, (solution, fitness) in enumerate(zip(self.population, fitness_values)):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solution
                self._update_archive(solution, fitness)

            # Adaptive Mutation
            if self.best_fitness_overall < 1e-5:
                self.mutation_strength *= 0.95
            else:
                self.mutation_strength *= 1.05

            # Generate next generation
            next_generation = []
            for _ in range(self.population_size):
                parent = self._select_parent()
                child = self._mutate(parent)
                child = self._local_search(child, objective_function) #Apply local search
                next_generation.append(child)

            self.population = np.array(next_generation)

        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _update_archive(self, solution, fitness):
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_solution, worst_fitness = max(self.archive, key=lambda x: x[1])
            if fitness < worst_fitness:
                self.archive.remove((worst_solution, worst_fitness))
                self.archive.append((solution, fitness))

    def _select_parent(self):
        if random.random() < 0.5:  # 50% chance to select from archive
            return random.choice(self.archive)[0]
        else:
            return self.population[np.random.randint(0, self.population_size)]

    def _mutate(self, parent):
        child = parent + np.random.normal(0, self.mutation_strength * self.mutation_rate, self.dim)
        child = np.clip(child, self.lower_bounds, self.upper_bounds)
        return child

    def _local_search(self, solution, objective_function):
        current_solution = solution.copy()
        current_fitness = objective_function(current_solution.reshape(1,-1))[0]
        for _ in range(self.local_search_iterations):
            neighbor = current_solution + np.random.normal(0, 0.1 * (self.upper_bounds-self.lower_bounds), self.dim)
            neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
            neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
            self.eval_count += 1
            if neighbor_fitness < current_fitness:
                current_solution = neighbor
                current_fitness = neighbor_fitness
        return current_solution

2025-06-22 20:59:01 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:59:01 ERROR Can not run the algorithm
2025-06-22 20:59:01 INFO Run function 12 complete. FEHistory len: 1500, AOCC: 0.0000
2025-06-22 20:59:01 INFO FeHistory: [103194.8739159  120997.92872822 184243.63566248 ... 124431.44011713
 155316.07176041 145005.20934981]
2025-06-22 20:59:01 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:59:01 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:59:02 ERROR Can not run the algorithm
2025-06-22 20:59:02 INFO Run function 23 complete. FEHistory len: 1500, AOCC: 0.0053
2025-06-22 20:59:02 INFO FeHistory: [20.85734542 14.75097796 37.93882709 ... 10.42037719 32.89125021
 27.62701913]
2025-06-22 20:59:02 INFO Expected Optimum FE: -100
2025-06-22 20:59:02 INFO Unimodal AOCC mean: 0.1481
2025-06-22 20:59:02 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 20:59:02 INFO Multimodal (multiple components) AOCC mean: 0.0053
2025-06-22 20:59:02 INFO AOCC mean: 0.0511
2025-06-22 20:59:13 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:59:18 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1739
2025-06-22 20:59:18 INFO FeHistory: [-334.01012219 -333.92950634 -333.9600007  ... -336.09590788 -336.09590782
 -336.09590799]
2025-06-22 20:59:18 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:59:18 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50 # Size of the archive
        self.archive = []
        self.archive_fitness = []
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Adaptive Mutation
            offspring = self._adaptive_mutation(offspring)

            # Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self._update_archive(offspring, offspring_fitness)

            # Selection for next generation (combining population, offspring, and archive)
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            
            # Update best solution
            self._update_best(population, fitness_values)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append(offspring[i])
                self.archive_fitness.append(offspring_fitness[i])
            else:
                worst_index = np.argmax(self.archive_fitness)
                if offspring_fitness[i] < self.archive_fitness[worst_index]:
                    self.archive[worst_index] = offspring[i]
                    self.archive_fitness[worst_index] = offspring_fitness[i]


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring, np.array(self.archive)))
        combined_fit = np.concatenate((fitness_values, offspring_fitness, np.array(self.archive_fitness)))

        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
2025-06-22 20:59:18 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 20:59:24 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.3663
2025-06-22 20:59:24 INFO FeHistory: [247929.78664819 140164.85381215 315259.71873299 ...  -1002.47907863
  -1002.47907862  -1002.47907866]
2025-06-22 20:59:24 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 20:59:24 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50 # Size of the archive
        self.archive = []
        self.archive_fitness = []
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Adaptive Mutation
            offspring = self._adaptive_mutation(offspring)

            # Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self._update_archive(offspring, offspring_fitness)

            # Selection for next generation (combining population, offspring, and archive)
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            
            # Update best solution
            self._update_best(population, fitness_values)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append(offspring[i])
                self.archive_fitness.append(offspring_fitness[i])
            else:
                worst_index = np.argmax(self.archive_fitness)
                if offspring_fitness[i] < self.archive_fitness[worst_index]:
                    self.archive[worst_index] = offspring[i]
                    self.archive_fitness[worst_index] = offspring_fitness[i]


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring, np.array(self.archive)))
        combined_fit = np.concatenate((fitness_values, offspring_fitness, np.array(self.archive_fitness)))

        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
2025-06-22 20:59:24 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 20:59:41 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2173
2025-06-22 20:59:41 INFO FeHistory: [ 35.93945136  17.9430016    3.25078671 ... -99.92284341 -99.92276659
 -99.92276802]
2025-06-22 20:59:41 INFO Expected Optimum FE: -100
2025-06-22 20:59:41 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.

class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50 # Size of the archive
        self.archive = []
        self.archive_fitness = []
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Adaptive Mutation
            offspring = self._adaptive_mutation(offspring)

            # Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self._update_archive(offspring, offspring_fitness)

            # Selection for next generation (combining population, offspring, and archive)
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)
            
            # Update best solution
            self._update_best(population, fitness_values)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append(offspring[i])
                self.archive_fitness.append(offspring_fitness[i])
            else:
                worst_index = np.argmax(self.archive_fitness)
                if offspring_fitness[i] < self.archive_fitness[worst_index]:
                    self.archive[worst_index] = offspring[i]
                    self.archive_fitness[worst_index] = offspring_fitness[i]


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring, np.array(self.archive)))
        combined_fit = np.concatenate((fitness_values, offspring_fitness, np.array(self.archive_fitness)))

        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
2025-06-22 20:59:41 INFO Unimodal AOCC mean: 0.1739
2025-06-22 20:59:41 INFO Multimodal (single component) AOCC mean: 0.3663
2025-06-22 20:59:41 INFO Multimodal (multiple components) AOCC mean: 0.2173
2025-06-22 20:59:41 INFO AOCC mean: 0.2525
2025-06-22 20:59:51 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 20:59:57 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1752
2025-06-22 20:59:57 INFO FeHistory: [-333.97452715 -333.93361351 -333.94809656 ... -336.18715821 -336.18715665
 -336.1871541 ]
2025-06-22 20:59:57 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 20:59:57 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: Combines adaptive Gaussian sampling with an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #Size of archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1
        self.archive.append((self.best_solution_overall, self.best_fitness_overall))


        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            #Adaptive Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self._update_archive(offspring, offspring_fitness)

            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population


    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, offspring, offspring_fitness):
        for i, (solution, fitness) in enumerate(zip(offspring, offspring_fitness)):
            if len(self.archive) < self.archive_size :
                self.archive.append((solution, fitness))
            else:
                worst_index = np.argmax([f for s,f in self.archive])
                if fitness < self.archive[worst_index][1]:
                    self.archive[worst_index] = (solution, fitness)

2025-06-22 20:59:57 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:00:03 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.3746
2025-06-22 21:00:03 INFO FeHistory: [259895.10376052 454540.93269747 286753.31946533 ...  -1002.47907862
  -1002.47907861  -1002.47907862]
2025-06-22 21:00:03 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:00:03 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: Combines adaptive Gaussian sampling with an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #Size of archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1
        self.archive.append((self.best_solution_overall, self.best_fitness_overall))


        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            #Adaptive Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self._update_archive(offspring, offspring_fitness)

            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population


    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, offspring, offspring_fitness):
        for i, (solution, fitness) in enumerate(zip(offspring, offspring_fitness)):
            if len(self.archive) < self.archive_size :
                self.archive.append((solution, fitness))
            else:
                worst_index = np.argmax([f for s,f in self.archive])
                if fitness < self.archive[worst_index][1]:
                    self.archive[worst_index] = (solution, fitness)

2025-06-22 21:00:03 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:00:22 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.2618
2025-06-22 21:00:22 INFO FeHistory: [ 28.26464401  33.12969126  16.05567975 ... -99.99891526 -99.99890004
 -99.99870334]
2025-06-22 21:00:22 INFO Expected Optimum FE: -100
2025-06-22 21:00:22 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: Combines adaptive Gaussian sampling with an archive to efficiently explore multimodal landscapes.
# Code:
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to efficiently explore multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200 #Size of archive
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1
        self.archive.append((self.best_solution_overall, self.best_fitness_overall))


        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            #Adaptive Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self._update_archive(offspring, offspring_fitness)

            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population


    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2
        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, offspring, offspring_fitness):
        for i, (solution, fitness) in enumerate(zip(offspring, offspring_fitness)):
            if len(self.archive) < self.archive_size :
                self.archive.append((solution, fitness))
            else:
                worst_index = np.argmax([f for s,f in self.archive])
                if fitness < self.archive[worst_index][1]:
                    self.archive[worst_index] = (solution, fitness)

2025-06-22 21:00:22 INFO Unimodal AOCC mean: 0.1752
2025-06-22 21:00:22 INFO Multimodal (single component) AOCC mean: 0.3746
2025-06-22 21:00:22 INFO Multimodal (multiple components) AOCC mean: 0.2618
2025-06-22 21:00:22 INFO AOCC mean: 0.2705
2025-06-22 21:00:29 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:00:39 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1498
2025-06-22 21:00:39 INFO FeHistory: [-334.04492434 -334.0227031  -334.05614453 ... -334.35487815 -334.35487898
 -334.35487602]
2025-06-22 21:00:39 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:00:39 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithEnhancedNiching
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithEnhancedNiching
# Description: Combines adaptive Gaussian sampling with enhanced niching for multimodal optimization.
# Code:
class AdaptiveGaussianSamplingEAwithEnhancedNiching:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50  # Size of the archive
        self.archive = []
        self.mutation_rate = 0.5  # Initial mutation rate
        self.mutation_decay = 0.99
        self.niching_radius = 10 # Adaptive Niching Parameter


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.archive = []

        while self.eval_count < self.budget:
            fitness_values = objective_function(self.population)
            self.eval_count += self.population_size

            # Update best solution
            for i, fitness in enumerate(fitness_values):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = self.population[i]

            # Archive Management (enhanced niching)
            for i, (solution, fitness) in enumerate(zip(self.population, fitness_values)):
                is_new_niche = True
                for archived_solution, archived_fitness in self.archive:
                    distance = np.linalg.norm(solution - archived_solution)
                    if distance < self.niching_radius and fitness > archived_fitness:  # Enhanced niching condition: only add if better fitness
                        is_new_niche = False
                        break
                if is_new_niche and len(self.archive) < self.archive_size:
                    self.archive.append((solution, fitness))


            # Adaptive Gaussian Mutation
            self.mutation_rate *= self.mutation_decay
            offspring = []
            for i in range(self.population_size):
                # Tournament Selection
                parent_indices = random.sample(range(self.population_size), 5)
                parent_index = parent_indices[np.argmin(fitness_values[parent_indices])]
                parent = self.population[parent_index]

                # Adaptive Gaussian Mutation with archive influence
                if random.random() < 0.2 and len(self.archive) > 0:
                    archive_index = random.randint(0, len(self.archive) - 1)
                    parent = (parent + self.archive[archive_index][0]) / 2  # Blend with archive member

                mutation = np.random.normal(0, self.mutation_rate, self.dim)
                child = parent + mutation
                offspring.append(child)

            # Clip to bounds
            self.population = np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 21:00:39 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:00:48 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:00:48 INFO FeHistory: [210855.11447575 113220.68870017 179697.04831375 ...  23982.03634387
  23982.03495524  23982.03750438]
2025-06-22 21:00:48 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:00:48 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:01:10 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0355
2025-06-22 21:01:10 INFO FeHistory: [ 32.26072642  26.57625233  12.61714267 ... -56.6163435  -56.61634298
 -56.61634116]
2025-06-22 21:01:10 INFO Expected Optimum FE: -100
2025-06-22 21:01:10 INFO Unimodal AOCC mean: 0.1498
2025-06-22 21:01:10 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:01:10 INFO Multimodal (multiple components) AOCC mean: 0.0355
2025-06-22 21:01:10 INFO AOCC mean: 0.0618
2025-06-22 21:01:18 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:01:18 ERROR Can not run the algorithm
2025-06-22 21:01:18 INFO Run function 5 complete. FEHistory len: 200, AOCC: 0.1475
2025-06-22 21:01:18 INFO FeHistory: [-334.02211008 -333.97481322 -334.00803869 -334.00688235 -333.97121812
 -334.05036716 -334.01918421 -333.96725745 -334.00900484 -334.01561512
 -334.01425807 -334.01582259 -334.01543278 -333.95796866 -334.00174658
 -333.92992382 -334.03782231 -333.94541361 -333.90287637 -333.98747131
 -334.01276725 -333.98060848 -333.97789145 -334.12906741 -333.95486029
 -334.15628712 -334.01806043 -333.94896148 -333.97806395 -333.9691
 -334.03821966 -334.04856938 -333.97102605 -333.96289915 -333.92054815
 -333.93877841 -333.93867347 -333.98142903 -333.9796821  -333.96926928
 -333.99233609 -333.98888842 -334.0061806  -334.00284615 -334.00962836
 -334.00241557 -334.04060525 -333.92062996 -334.06422776 -333.94993623
 -334.01987806 -333.9777936  -333.99785572 -334.02993276 -334.0282003
 -333.94967346 -333.99620751 -334.01331151 -334.05894974 -334.02585846
 -334.05307366 -333.96719748 -334.00934535 -333.97156924 -333.98523915
 -333.97132609 -333.96649678 -334.03975105 -333.97034451 -334.01354791
 -333.92633741 -333.97908294 -333.97455236 -334.01583883 -334.01699293
 -333.98458985 -333.957826   -334.02020604 -333.99962616 -334.01407915
 -334.05843509 -333.98724501 -333.88382589 -333.98594393 -334.07163338
 -333.95872988 -333.9584794  -333.97071487 -333.98274703 -333.96613901
 -333.97830534 -333.88483659 -334.04765427 -333.97825058 -333.99107931
 -333.94205507 -333.96162377 -333.95315328 -334.0272571  -334.07028067
 -333.91745684 -333.86247265 -334.03162895 -334.00854152 -333.96617129
 -333.88574853 -333.95880341 -333.93463843 -333.87070307 -333.92537907
 -333.95226824 -334.00711022 -333.93747207 -333.96847352 -333.90175463
 -334.00542218 -333.89573542 -333.92109342 -333.98048753 -333.92004543
 -333.97524488 -333.95197337 -333.99584256 -333.95095942 -333.86875416
 -333.90419905 -333.93241322 -333.92337511 -333.94704718 -333.93676274
 -333.98726975 -333.95849071 -333.94495943 -333.96207776 -333.93860701
 -333.92353173 -334.00636906 -333.93883507 -333.9958158  -333.92145276
 -333.96149498 -333.96673808 -333.96457265 -334.00032434 -333.93142831
 -333.93819651 -333.99421046 -333.91985136 -333.97154452 -333.96923068
 -333.89580728 -334.03487337 -334.02561046 -333.98809411 -333.86111856
 -333.87715518 -333.907465   -333.95064476 -333.8924731  -334.03808648
 -333.99484528 -334.04876734 -333.99425239 -333.94686552 -333.90461078
 -333.99840981 -333.94126346 -333.91827891 -333.91348119 -333.87805317
 -333.9266681  -333.98054545 -333.97541148 -333.99191295 -333.89052495
 -333.91127447 -333.98382237 -333.91114537 -333.90551301 -333.94660831
 -333.93731615 -333.91580298 -333.88423821 -333.95142634 -333.94486027
 -333.96782922 -333.92807877 -333.97726034 -333.9487857  -333.90225762
 -333.92285627 -333.91525935 -333.9616925  -334.00957151 -333.97070312
 -333.95285259 -334.0055022  -333.99496626 -333.9492049  -333.96867818]
2025-06-22 21:01:18 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:01:18 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchiveAndDE
import numpy as np
import random

class AdaptiveGaussianSamplingEAwithArchiveAndDE:
    """
    Combines adaptive Gaussian sampling, an archive, and differential evolution for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50  # Size of the archive
        self.archive = []
        self.sigma = 0.5  # Initial mutation strength
        self.sigma_decay = 0.99
        self.mutation_scale = 0.8 #DE Mutation scale
        self.mutation_scale_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            offspring = []
            for i in range(self.population_size):
                # Differential Evolution Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                # Adaptive Gaussian perturbation
                mutant += np.random.normal(0, self.sigma, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                offspring.append(mutant)

            offspring = np.array(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((self.fitness_values, offspring_fitness))

            # Selection with Archive
            self._update_archive(combined_population, combined_fitness)
            self.population, self.fitness_values = self._select_population(self.archive)

            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

            self.sigma *= self.sigma_decay
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, population, fitness_values):
        combined = list(zip(population, fitness_values))
        combined.sort(key=lambda item: item[1]) #Sort by fitness
        self.archive = combined[:self.archive_size]


    def _select_population(self, archive):
      population = [x for x,y in archive]
      fitness_values = [y for x,y in archive]
      return np.array(population), np.array(fitness_values)
2025-06-22 21:01:18 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:01:18 ERROR Can not run the algorithm
2025-06-22 21:01:18 INFO Run function 12 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-22 21:01:18 INFO FeHistory: [238887.15343984 160978.57520243 444797.26812698  78611.46743575
 288564.01294057 516067.17151543 291347.03023961 165904.80767207
 346801.79806003 177661.28892627 189881.07576773 273327.8722204
 155145.30388564 175133.59496416 123223.59724201 270908.39567594
 203854.42705537 164875.30569646 235574.83612694 203680.04773618
 377985.11484216 217231.71194354 194652.57908433 250636.95932252
 136355.68414849 406143.10753436 157312.29731731 250949.56012948
 189491.82115308 307590.56418037 147699.17820612 316810.24409857
 187428.7644464  216150.19649456 254721.18607624 217540.1129813
 198465.42818952 156533.8037829  216451.80436637 154319.99204874
 263918.78761445 207175.08453322 187595.20710684 223094.70797852
 252451.06488719 202161.38043914 289627.67200789 254754.19373083
 247948.22889527 167705.14819045 209616.32089962 365766.4214035
 197496.27750733 255480.43484898 159904.1294301  306990.33881036
 243276.47040279 206132.47572381  79222.35229048 155139.91037138
 343525.18674477 207496.11461976 328147.06254752 191145.87668591
 337803.77811218 140340.68106705 184484.49536853 531008.28707803
 188370.98518288 183615.61698596 290497.7127332  204377.74825505
 182350.46746472 357787.27150938 375633.5492703  263166.12730674
 163039.46894042 241423.77757052 163749.21661194 165108.06564861
 294794.58139667 111622.04382018 104277.98492644 152639.09311627
 131017.50263059 285321.69525449 249497.257133   213137.59221611
 141180.841433   324975.6005172  189623.65711498 151398.05702304
 139740.48545247 274161.55274817 158292.92179223 225936.83509791
 161126.22626486 342776.8628157  259180.42282891 379879.91034983
 315412.23793391 264298.86565753 226788.28340654 385362.75504174
 203022.07231899 286551.31446506 356062.63267207 391336.60288193
 290805.06390674 393282.73882653 285958.69501243 298169.85172854
 122152.53695988 170436.36629098 291037.68113685 174442.10398993
 420957.81122603 369781.96372925 354319.95842587 202620.20273912
 293746.963828   235251.7264388  695691.75298259 240478.75525574
 292343.1657121  317562.78073907 403685.45009448 360293.58389046
 268845.16617388 131587.43350734 339903.22306632 343008.71984394
 255793.26524148 257288.50399519 394903.32548304 347961.53494404
 274029.98053682 189741.62895221 432167.14436182 263822.47199662
 128702.68659448 197511.15176947 227502.33021383 157667.58755388
 405030.91900393 339593.25500453 424993.7705779  296042.72146218
 340803.93502141 219304.26307294 392980.50887371 335455.28343806
 316797.52020911 135878.64000257 448953.07259023 195143.0001582
 215852.74769829 259484.81051474 377923.40857399 452688.57660653
 194691.02085325 185728.56611992 236645.53364314 380053.79481873
 296205.45369336 469003.52925081 340009.19975977 274131.99708692
 280126.60321553 415125.89893649 239078.98954507 139596.28259013
 339930.20442029 299030.07011424 327879.93370356 513101.31349381
 307611.34552844 328779.61559418 241562.0159157  533238.16300444
 310629.86555093 417355.20088084 196029.05000533 352395.49414721
 181971.23978374 257351.82306829 276060.69856491 517694.99044058
 311891.66039    311005.65538998 373070.23097012 201978.35278298
 221143.52851538 242593.22496657 307108.59853918 332538.64053649
 305337.36041071 293488.76617961 223574.07244626 296887.90356646]
2025-06-22 21:01:18 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:01:18 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:01:18 ERROR Can not run the algorithm
2025-06-22 21:01:18 INFO Run function 23 complete. FEHistory len: 200, AOCC: 0.0047
2025-06-22 21:01:18 INFO FeHistory: [  6.60268756  48.38488959  59.87721191  34.80742788  18.80673547
  35.01947662  20.9139752   23.2261286   43.97677315   8.86506685
  22.48549807  49.42052321  14.77952298   0.24337364  16.83354741
  38.27692354  20.96511567  41.19489293  30.10044853  33.27052236
  18.74674852  29.90892835  27.9633101   40.08650373  -2.32631743
   6.49583131  40.02658471  24.40261786  16.69352704  21.26765262
  41.19801401  40.40992258  17.7740293   26.1083154   55.6085457
  20.3854798   43.07539115  19.40888037  44.94424347  35.56385427
  33.6884208   33.47082617  13.37287372  37.56821232  11.1739652
  39.50539933 -10.31916315  31.06801832  50.80265287  38.94149171
   7.84059628  55.64778318  37.84461604  19.17286364  11.91797879
  32.11159309  13.94313138   8.86054482  27.3863302   21.14887017
  20.49113273  41.67472808  33.69525046  20.16980632  46.95899813
  32.69434223  45.77071053  14.32606873  17.67179302  14.94277562
  41.37205795  46.65363638  38.85905598  11.38464992  23.1093702
  29.39529406   9.51791731  47.53834056  24.1752962   11.2366506
  12.96729156  35.42153771  52.83754296  19.11299688  30.31804085
  29.48474538  39.16046162  45.76115589  19.84416397  22.02313531
  21.78558756  29.99758576  32.21019424  33.85752281  20.8993459
  14.55117662  33.92447544  19.10113121   0.93792637  22.0759197
  46.80990977  49.9588625   37.88817767  72.55811736  42.75701031
   2.45101749  11.20133367  40.85927498  37.53627782  39.29659185
  53.11474693  26.9704426   39.58981755  41.21066592  37.37334904
  26.58452073  51.95430597   7.94588439  50.26166623  32.22862613
  37.25400354  62.71590112  58.20094143  19.73725724  18.11474372
  40.94072438  52.2082646   32.15473448  42.44938756  43.17558772
  19.45871945  36.97422386  47.56601347  26.68248074  43.14565761
  52.8354024   46.1018246   23.12553798  19.30535781  36.68630316
  70.3496494   20.94723397  47.99625323  18.98171447  31.83866557
  69.62994427  48.68264843  45.97396032  35.25017925  36.19428041
  39.52527503  37.75162198  58.57977404  22.93949037  -7.93447025
  35.51637233  50.05212265  11.1968635   11.72848159  44.49921397
  21.49204014  64.56274939  49.94335207  38.19523526   4.39108767
  64.51101396  46.16591034  47.02579738  29.60481629  15.25109015
  41.22966292  -0.24235669  51.19019825  44.80928366  18.92165472
  65.50637841  23.87369247  66.21212381  24.80430703  41.06219706
   3.46117793  34.19488983  19.28971852  17.57496586  37.87726422
  57.05500774  50.91750223  29.1041152   14.85280113  46.63452905
  55.29094874  36.6199618   40.5301076   43.68050787  48.59470123
  39.4510619   41.11865458  38.94340882  32.62924704  29.63345092]
2025-06-22 21:01:18 INFO Expected Optimum FE: -100
2025-06-22 21:01:18 INFO Unimodal AOCC mean: 0.1475
2025-06-22 21:01:18 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:01:18 INFO Multimodal (multiple components) AOCC mean: 0.0047
2025-06-22 21:01:18 INFO AOCC mean: 0.0507
2025-06-22 21:01:28 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:02:28 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 21:04:21 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1518
2025-06-22 21:04:21 INFO FeHistory: [-333.91224218 -333.94688337 -333.97366153 ... -334.41515963 -334.42826137
 -334.43782276]
2025-06-22 21:04:21 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:04:21 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingDEWithArchive
import numpy as np
import random

class AdaptiveGaussianSamplingDEWithArchive:
    """
    Combines adaptive Gaussian sampling, differential evolution, and an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.F = 0.8 #Differential Evolution Parameter
        self.CR = 0.9 #Differential Evolution Parameter
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
            self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
            self.eval_count += 1
        else:
            self.best_solution_overall = np.array([])
            self.best_fitness_overall = 0

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _generate_offspring(self, population, fitness_values):
        offspring = []
        for i in range(self.population_size):
            #Differential Evolution
            a, b, c = np.random.choice(range(self.population_size), size=3, replace=False)
            v = population[a] + self.F * (population[b] - population[c])
            v = np.clip(v, self.lower_bounds, self.upper_bounds)
            u = np.where(np.random.rand(self.dim) < self.CR, v, population[i])
            
            #Gaussian Mutation
            u = u + np.random.normal(0, self.sigma, size=self.dim)
            u = np.clip(u, self.lower_bounds, self.upper_bounds)
            offspring.append(u)
        return np.array(offspring)


    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:04:21 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:07:14 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:07:14 INFO FeHistory: [229399.15292943 359405.9379128  216872.60718595 ...  58917.12024777
 100227.72508341  43060.86973841]
2025-06-22 21:07:14 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:07:14 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:10:21 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0316
2025-06-22 21:10:21 INFO FeHistory: [ 23.97744053  25.43454884  19.0219734  ... -43.87170446 -15.1844793
 -25.87670774]
2025-06-22 21:10:21 INFO Expected Optimum FE: -100
2025-06-22 21:10:21 INFO Unimodal AOCC mean: 0.1518
2025-06-22 21:10:21 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:10:21 INFO Multimodal (multiple components) AOCC mean: 0.0316
2025-06-22 21:10:21 INFO AOCC mean: 0.0612
2025-06-22 21:10:30 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:10:36 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1607
2025-06-22 21:10:36 INFO FeHistory: [-334.02233877 -334.08201011 -334.03129002 ... -334.93218355 -334.84600174
 -335.41992067]
2025-06-22 21:10:36 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:10:36 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchiveAndLocalSearch
import numpy as np
import random

# Name: AdaptiveGaussianSamplingEAwithArchiveAndLocalSearch
# Description: Combines adaptive Gaussian sampling, an archive, and local search for efficient multimodal optimization.
# Code:
class AdaptiveGaussianSamplingEAwithArchiveAndLocalSearch:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.archive_size = 100  # Size of the archive
        self.archive = []
        self.population_size = 50
        self.mutation_scale = 0.8
        self.mutation_decay = 0.99
        self.local_search_iterations = 5


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
        
        while self.eval_count < self.budget:
            offspring = []
            offspring_fitness = []
            for i in range(self.population_size):
                parent = self.population[i]
                mutant = self._mutate(parent)
                
                #Local Search
                mutant = self._local_search(mutant, objective_function)

                offspring.append(mutant)
                fitness = objective_function(mutant.reshape(1,-1))[0]
                offspring_fitness.append(fitness)
                self.eval_count +=1

            self.population = self._select(self.population, self.fitness_values, offspring, offspring_fitness)
            self.fitness_values = self._evaluate_population(objective_function, self.population)
            best_solution, best_fitness = self._find_best(self.population, self.fitness_values)
            
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            self._update_archive(self.population, self.fitness_values)
            self.mutation_scale *= self.mutation_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function, population = None):
        if population is None:
            population = self.population
        population_reshaped = population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += len(population)
        return fitness

    def _mutate(self, individual):
        mutation = np.random.normal(0, self.mutation_scale, self.dim)
        mutant = individual + mutation
        mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
        return mutant
    
    def _local_search(self, solution, objective_function):
        current_solution = solution.copy()
        current_fitness = objective_function(current_solution.reshape(1, -1))[0]
        for _ in range(self.local_search_iterations):
            neighbor = self._generate_neighbor(current_solution)
            neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
            self.eval_count += 1
            if neighbor_fitness < current_fitness:
                current_solution = neighbor
                current_fitness = neighbor_fitness
        return current_solution

    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _select(self, population, fitness_values, offspring, offspring_fitness):
        combined_population = np.vstack((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness))
        indices = np.argsort(combined_fitness)
        return combined_population[indices[:self.population_size]]

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
    
    def _update_archive(self, population, fitness_values):
        combined = list(zip(population, fitness_values))
        combined.sort(key=lambda item: item[1])
        self.archive.extend(combined[:min(len(combined), self.archive_size)])

2025-06-22 21:10:36 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:10:42 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:10:42 INFO FeHistory: [247176.41668357 213400.59072684 253275.41158632 ...  15074.01977068
  14788.95154294  14654.358913  ]
2025-06-22 21:10:42 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:10:42 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:11:01 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0275
2025-06-22 21:11:01 INFO FeHistory: [ 16.88726369  39.28196032 -16.02431632 ... -28.14316133 -39.70936065
 -50.76498824]
2025-06-22 21:11:01 INFO Expected Optimum FE: -100
2025-06-22 21:11:01 INFO Unimodal AOCC mean: 0.1607
2025-06-22 21:11:01 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:11:01 INFO Multimodal (multiple components) AOCC mean: 0.0275
2025-06-22 21:11:01 INFO AOCC mean: 0.0627
2025-06-22 21:11:10 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:11:15 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1657
2025-06-22 21:11:15 INFO FeHistory: [-333.98660786 -333.90129533 -333.98687601 ... -335.81042744 -335.82683863
 -335.83972489]
2025-06-22 21:11:15 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:11:15 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchiveAndDE
import numpy as np
import random

class AdaptiveGaussianSamplingEAwithArchiveAndDE:
    """
    Combines adaptive Gaussian sampling, an archive to maintain diversity, and Differential Evolution for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50
        self.archive = []
        self.sigma = 10.0  # Initial standard deviation for Gaussian sampling
        self.sigma_decay = 0.98 # Decay rate for sigma
        self.F = 0.8 # DE scaling factor
        self.CR = 0.9 # DE crossover rate


    def evaluate(self, objective_function, solutions):
        fitness = objective_function(solutions)
        self.eval_count += len(fitness)
        return fitness

    def update_archive(self, solutions, fitnesses):
        combined = np.column_stack((solutions, fitnesses))
        combined = sorted(combined, key=lambda x: x[-1]) #sort by fitness
        
        if len(self.archive) < self.archive_size:
            self.archive.extend(combined[:min(len(combined),self.archive_size-len(self.archive))])
        else:
            self.archive.extend(combined[:min(len(combined), self.archive_size)])
            self.archive = sorted(self.archive, key=lambda x: x[-1])[:self.archive_size]


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = self.evaluate(objective_function, population)
        self.update_archive(population, fitness)

        for i in range(len(fitness)):
            if fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[i]
                self.best_solution_overall = population[i]

        while self.eval_count < self.budget:
            #Adaptive Gaussian Sampling
            new_population = np.zeros_like(population)
            for i in range(self.population_size):
                if len(self.archive) > 0:
                    parent = random.choice(self.archive)[:-1]
                    new_individual = np.clip(np.random.normal(parent, self.sigma, size=self.dim), self.lower_bounds, self.upper_bounds)
                else:
                    new_individual = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
                new_population[i] = new_individual

            #Differential Evolution
            for i in range(self.population_size):
                a,b,c = random.sample(range(len(population)),3)
                mutant = population[a] + self.F * (population[b]-population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, new_population[i])
                trial_fitness = self.evaluate(objective_function, trial.reshape(1,-1))
                if trial_fitness[0] < fitness[i]:
                    new_population[i] = trial
                    fitness[i] = trial_fitness[0]


            population = new_population
            fitness = self.evaluate(objective_function, population)
            self.update_archive(population, fitness)

            for i in range(len(fitness)):
                if fitness[i] < self.best_fitness_overall:
                    self.best_fitness_overall = fitness[i]
                    self.best_solution_overall = population[i]
            self.sigma *= self.sigma_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 21:11:15 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:11:20 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:11:20 INFO FeHistory: [339982.66144033 215276.15267792 213281.2882039  ...   2954.05109562
   2953.97371961   2953.83176354]
2025-06-22 21:11:20 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:11:20 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:11:38 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.1497
2025-06-22 21:11:38 INFO FeHistory: [ 22.99237669  21.47588123  21.30112458 ... -99.71063582 -99.6839227
 -99.6718774 ]
2025-06-22 21:11:38 INFO Expected Optimum FE: -100
2025-06-22 21:11:38 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchiveAndDE
import numpy as np
import random

class AdaptiveGaussianSamplingEAwithArchiveAndDE:
    """
    Combines adaptive Gaussian sampling, an archive to maintain diversity, and Differential Evolution for robust multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 50
        self.archive = []
        self.sigma = 10.0  # Initial standard deviation for Gaussian sampling
        self.sigma_decay = 0.98 # Decay rate for sigma
        self.F = 0.8 # DE scaling factor
        self.CR = 0.9 # DE crossover rate


    def evaluate(self, objective_function, solutions):
        fitness = objective_function(solutions)
        self.eval_count += len(fitness)
        return fitness

    def update_archive(self, solutions, fitnesses):
        combined = np.column_stack((solutions, fitnesses))
        combined = sorted(combined, key=lambda x: x[-1]) #sort by fitness
        
        if len(self.archive) < self.archive_size:
            self.archive.extend(combined[:min(len(combined),self.archive_size-len(self.archive))])
        else:
            self.archive.extend(combined[:min(len(combined), self.archive_size)])
            self.archive = sorted(self.archive, key=lambda x: x[-1])[:self.archive_size]


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = self.evaluate(objective_function, population)
        self.update_archive(population, fitness)

        for i in range(len(fitness)):
            if fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[i]
                self.best_solution_overall = population[i]

        while self.eval_count < self.budget:
            #Adaptive Gaussian Sampling
            new_population = np.zeros_like(population)
            for i in range(self.population_size):
                if len(self.archive) > 0:
                    parent = random.choice(self.archive)[:-1]
                    new_individual = np.clip(np.random.normal(parent, self.sigma, size=self.dim), self.lower_bounds, self.upper_bounds)
                else:
                    new_individual = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
                new_population[i] = new_individual

            #Differential Evolution
            for i in range(self.population_size):
                a,b,c = random.sample(range(len(population)),3)
                mutant = population[a] + self.F * (population[b]-population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, new_population[i])
                trial_fitness = self.evaluate(objective_function, trial.reshape(1,-1))
                if trial_fitness[0] < fitness[i]:
                    new_population[i] = trial
                    fitness[i] = trial_fitness[0]


            population = new_population
            fitness = self.evaluate(objective_function, population)
            self.update_archive(population, fitness)

            for i in range(len(fitness)):
                if fitness[i] < self.best_fitness_overall:
                    self.best_fitness_overall = fitness[i]
                    self.best_solution_overall = population[i]
            self.sigma *= self.sigma_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 21:11:38 INFO Unimodal AOCC mean: 0.1657
2025-06-22 21:11:38 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:11:38 INFO Multimodal (multiple components) AOCC mean: 0.1497
2025-06-22 21:11:38 INFO AOCC mean: 0.1051
2025-06-22 21:11:49 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:12:49 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 21:14:57 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1552
2025-06-22 21:14:57 INFO FeHistory: [-333.98992078 -333.95809967 -333.93838294 ... -334.96206582 -335.04433279
 -334.91418374]
2025-06-22 21:14:57 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:14:57 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingWithArchiveAndDifferentialEvolution
import numpy as np
import random

class AdaptiveGaussianSamplingWithArchiveAndDifferentialEvolution:
    """
    Combines adaptive Gaussian sampling with an archive and differential evolution for enhanced exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []
        self.F = 0.8 #Differential Evolution scaling factor
        self.CR = 0.9 #Differential Evolution crossover rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
            self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
            self.eval_count += 1
        else:
            self.best_solution_overall = np.array([])
            self.best_fitness_overall = 0.0


        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            #Differential Evolution
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Gaussian sampling for diversity
            gaussian_offspring = self._gaussian_sampling(population)
            gaussian_offspring_fitness = objective_function(gaussian_offspring)
            self.eval_count += len(gaussian_offspring)

            population, fitness_values = self._select_next_generation(
                np.vstack((population, offspring, gaussian_offspring)),
                np.concatenate((fitness_values, offspring_fitness, gaussian_offspring_fitness))
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring, gaussian_offspring)),
                np.concatenate((fitness_values, offspring_fitness, gaussian_offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self._update_best(gaussian_offspring, gaussian_offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _differential_evolution(self, population, fitness_values):
        offspring = []
        for i in range(self.population_size):
            a, b, c = self._select_three_different(population, i)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring.append(trial)
        return np.array(offspring)

    def _select_three_different(self, population, i):
        indices = list(range(self.population_size))
        indices.remove(i)
        a, b, c = random.sample(indices, 3)
        return population[a], population[b], population[c]

    def _crossover(self, parent, mutant):
        cross_points = np.random.rand(self.dim) < self.CR
        child = np.where(cross_points, mutant, parent)
        return child


    def _gaussian_sampling(self, population):
        offspring = []
        for i in range(self.population_size):
            offspring.append(population[i] + np.random.normal(0, self.sigma, self.dim))
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _select_next_generation(self, population, fitness_values):
        sorted_indices = np.argsort(fitness_values)
        next_gen = population[sorted_indices[:self.population_size]]
        next_fit = fitness_values[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:14:57 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:18:06 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0001
2025-06-22 21:18:06 INFO FeHistory: [270097.38163513 284944.57830538 222365.99554506 ...   -341.94285004
   -682.28112429   -517.55439383]
2025-06-22 21:18:06 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:18:06 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:21:24 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0681
2025-06-22 21:21:24 INFO FeHistory: [ 31.92764254  19.09126346  30.4044357  ... -88.60721031 -88.89554771
 -88.97914248]
2025-06-22 21:21:24 INFO Expected Optimum FE: -100
2025-06-22 21:21:24 INFO Unimodal AOCC mean: 0.1552
2025-06-22 21:21:24 INFO Multimodal (single component) AOCC mean: 0.0001
2025-06-22 21:21:24 INFO Multimodal (multiple components) AOCC mean: 0.0681
2025-06-22 21:21:24 INFO AOCC mean: 0.0745
2025-06-22 21:21:24 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

### Better code
AdaptiveMultimodalEvolutionaryStrategy
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.population_size = 100  # Adjust as needed
        self.mutation_strength = 0.5 # Initial mutation strength
        self.mutation_decay = 0.99 # Decay factor for mutation strength
        self.niche_radius = 5.0 # Radius for niche creation. Adjust based on problem scale


        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness_values = np.zeros(self.population_size)
        self.niches = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.niches = []


        for i in range(self.population_size):
            self.fitness_values[i] = objective_function(self.population[i:i+1])[0]
            self.eval_count += 1
            if self.fitness_values[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness_values[i]
                self.best_solution_overall = self.population[i]

        while self.eval_count < self.budget:
            offspring = []
            for i in range(self.population_size):
                # Mutation with adaptive strength
                mutant = self.population[i] + np.random.normal(0, self.mutation_strength, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds) # Keep within bounds
                offspring.append(mutant)

            offspring_fitness = objective_function(np.array(offspring))
            self.eval_count += self.population_size


            #Selection and Niching
            combined_population = np.concatenate((self.population, np.array(offspring)))
            combined_fitness = np.concatenate((self.fitness_values, offspring_fitness))


            sorted_indices = np.argsort(combined_fitness)
            selected_indices = sorted_indices[:self.population_size]
            self.population = combined_population[selected_indices]
            self.fitness_values = combined_fitness[selected_indices]

            #Update best solution
            for i in range(self.population_size):
                if self.fitness_values[i] < self.best_fitness_overall:
                    self.best_fitness_overall = self.fitness_values[i]
                    self.best_solution_overall = self.population[i]

            #Adapt mutation strength. Reduce if converging, increase if exploring diverse areas.
            self.mutation_strength *= self.mutation_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_strength_final': self.mutation_strength
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


### Worse code
AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Analyze & experience
- Comparing (best) `AdaptiveGaussianSamplingEAwithArchive` vs (worst) `AdaptiveMultimodalEvolutionaryStrategy`, we see that archiving improves performance, especially on multimodal problems.  The archive allows the algorithm to retain diverse solutions and prevents premature convergence, as seen in the improved AOCC scores across all GNBG groups.  `AdaptiveGaussianSamplingEAwithArchive` also benefits from adaptive Gaussian sampling to adjust exploration and exploitation efficiently.

(second best) `AdaptiveGaussianSamplingEA` vs (second worst) `AdaptiveDifferentialEvolutionWithEnhancedInitialization`: The inclusion of an archive in `AdaptiveGaussianSamplingEAwithArchive` significantly boosts its performance compared to `AdaptiveGaussianSamplingEA`. The archive acts as a memory, storing diverse solutions and enhancing exploration, particularly important for complex multimodal functions. The adaptive mutation and tournament selection also prove crucial for effective search.  `AdaptiveDifferentialEvolutionWithEnhancedInitialization`, despite enhanced initialization, struggles with multimodal components, indicating the importance of a well-designed recombination/mutation mechanism.

Comparing (1st) `AdaptiveGaussianSamplingEAwithArchive` vs (2nd) `AdaptiveGaussianSamplingEA`, we see that the archive significantly improves the algorithm's ability to handle multi-component multimodal problems. The archive helps to maintain diversity and prevent premature convergence to local optima.  The difference is less pronounced in unimodal problems, suggesting the archive's primary benefit is in exploration rather than exploitation.

(3rd) `AdaptiveGaussianSamplingEA` vs (4th) `AdaptiveGaussianMutationDE`: Both use Gaussian mutation, but the adaptive approach in the top performer provides a more refined search, leading to better performance.  `AdaptiveGaussianMutationDE`, while incorporating DE, lacks the effective balance of exploration and exploitation found in the higher-ranked algorithms.


Comparing (second worst) `AdaptiveDifferentialEvolutionWithEnhancedInitialization` vs (worst) `AdaptiveMultimodalEvolutionaryStrategy`, we observe that incorporating local search (`AdaptiveDifferentialEvolutionWithEnhancedInitialization`) can be beneficial, although not as much as adaptive sampling and archiving.  `AdaptiveMultimodalEvolutionaryStrategy`, lacking these critical features, shows relatively poor performance on multimodal problems.

Overall: Adaptive Gaussian sampling coupled with archiving consistently shows the best performance across the benchmark functions. Adaptive mutation and a robust selection strategy are key for balancing exploration and exploitation. Incorporating DE or local search provides some benefits but isn't sufficient on its own for high-dimensional multimodal landscapes.
- * **Keywords:** High-dimensional optimization, exploration-exploitation, multimodal landscapes, adaptive sampling, advanced mutation.

* **Advice:** Focus on designing heuristics that dynamically adjust exploration/exploitation balance based on the optimization landscape's characteristics (e.g., using Bayesian Optimization or similar adaptive methods).  Prioritize sophisticated mutation strategies over simpler ones. Consider incorporating mechanisms for handling high dimensionality, like dimensionality reduction techniques or manifold learning.

* **Avoid:**  Simple, static heuristics; reliance on only one exploration/exploitation strategy; ignoring the landscape's structure; neglecting the effects of high dimensionality.

* **Explanation:**  Sophisticated heuristics are needed to handle the complexities of high-dimensional multimodal problems. Static strategies will fail to adapt, while ignoring dimensionality leads to poor performance.  Dynamic adaptation is key to efficiently finding global optima.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-22 21:21:24 INFO Mutation prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

Current heuristics:
AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- * **Keywords:** High-dimensional optimization, exploration-exploitation, multimodal landscapes, adaptive sampling, advanced mutation.

* **Advice:** Focus on designing heuristics that dynamically adjust exploration/exploitation balance based on the optimization landscape's characteristics (e.g., using Bayesian Optimization or similar adaptive methods).  Prioritize sophisticated mutation strategies over simpler ones. Consider incorporating mechanisms for handling high dimensionality, like dimensionality reduction techniques or manifold learning.

* **Avoid:**  Simple, static heuristics; reliance on only one exploration/exploitation strategy; ignoring the landscape's structure; neglecting the effects of high dimensionality.

* **Explanation:**  Sophisticated heuristics are needed to handle the complexities of high-dimensional multimodal problems. Static strategies will fail to adapt, while ignoring dimensionality leads to poor performance.  Dynamic adaptation is key to efficiently finding global optima.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-22 21:22:13 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:22:38 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1495
2025-06-22 21:22:38 INFO FeHistory: [-334.01514759 -333.99709959 -334.05750269 ... -334.31210869 -334.31210869
 -334.31210869]
2025-06-22 21:22:38 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:22:38 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A differential evolution algorithm enhanced with clustering and adaptive mutation for efficient multimodal optimization.
# Code:

class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []
        self.archive_size = 200
        self.cluster_threshold = 0.1 # Adjust as needed


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adaptive_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different(population, i)
            mutant = a + self.F * (b - c)
            trial = self._crossover(population[i], mutant)
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring

    def _select_different(self, population, i):
        indices = list(range(self.population_size))
        indices.remove(i)
        random.shuffle(indices)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
        cross_points = np.random.rand(self.dim) < self.CR
        return np.where(cross_points, v, x)

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adaptive_parameters(self, population, fitness_values):
        #Simple adaptive strategy: reduce F and increase CR if convergence is detected
        #Consider more sophisticated methods based on landscape characteristics for better adaptation.
        avg_fitness_diff = np.mean(np.diff(np.sort(fitness_values)))
        if avg_fitness_diff < 0.01 : # Adjust threshold as needed
            self.F *= 0.9
            self.CR *= 1.1
            self.F = max(0.1, self.F)  #Keep F within bounds
            self.CR = min(1.0, self.CR)


2025-06-22 21:22:38 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:23:13 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 21:25:30 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:25:30 INFO FeHistory: [401937.45466651 180261.24430995 278458.77674994 ...  31328.49336454
  46160.22587017  77169.67896711]
2025-06-22 21:25:30 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:25:30 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:28:35 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0381
2025-06-22 21:28:35 INFO FeHistory: [ 28.46855049   9.62176761  35.95877614 ... -50.09236517 -11.0628051
 -41.34012445]
2025-06-22 21:28:35 INFO Expected Optimum FE: -100
2025-06-22 21:28:35 INFO Unimodal AOCC mean: 0.1495
2025-06-22 21:28:35 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:28:35 INFO Multimodal (multiple components) AOCC mean: 0.0381
2025-06-22 21:28:35 INFO AOCC mean: 0.0625
2025-06-22 21:28:35 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:29:08 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1590
2025-06-22 21:29:08 INFO FeHistory: [-333.97454514 -333.94718187 -334.07404281 ... -334.97631858 -334.97631858
 -334.97631858]
2025-06-22 21:29:08 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:29:08 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A differential evolution algorithm enhanced with adaptive mutation and clustering to escape local optima in multimodal high-dimensional landscapes.
# Code:
class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []
        self.archive_size = 200
        self.cluster_threshold = 0.1 # Adjust this based on problem difficulty

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adaptive_parameters(population, fitness_values)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different_individuals(population, i)
            mutant = a + self.F * (b - c)
            trial = self._crossover(population[i], mutant)
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring

    def _select_different_individuals(self, population, i):
        indices = list(range(self.population_size))
        indices.remove(i)
        a, b, c = random.sample(indices, 3)
        return population[a], population[b], population[c]

    def _crossover(self, x, v):
        u = np.copy(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                u[j] = v[j]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adaptive_parameters(self, population, fitness_values):
      #Example adaptive strategy -  adjust based on performance
        mean_fitness = np.mean(fitness_values)
        if mean_fitness < self.best_fitness_overall * 0.9:  #Convergence check
            self.F *= 0.95 #Reduce exploration if converging
            self.CR *= 0.95
        else:
            self.F = min(1.2, self.F * 1.05) # Increase exploration if not converging
            self.CR = min(1.0, self.CR * 1.05)


2025-06-22 21:29:08 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:29:35 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 21:31:21 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:31:21 INFO FeHistory: [284049.26945975 252205.65867084 178543.63789088 ...  36405.5569442
  36405.5569442   36405.5569442 ]
2025-06-22 21:31:21 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:31:21 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:34:25 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0153
2025-06-22 21:34:25 INFO FeHistory: [ 35.98992716  28.13652589  28.51222311 ... -17.88197736   9.89610484
   2.35793089]
2025-06-22 21:34:25 INFO Expected Optimum FE: -100
2025-06-22 21:34:25 INFO Unimodal AOCC mean: 0.1590
2025-06-22 21:34:25 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:34:25 INFO Multimodal (multiple components) AOCC mean: 0.0153
2025-06-22 21:34:25 INFO AOCC mean: 0.0581
2025-06-22 21:34:25 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:35:25 INFO [TIMEOUT] Evaluation exceeded 60 seconds and was skipped.
2025-06-22 21:37:04 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1498
2025-06-22 21:37:04 INFO FeHistory: [-334.0340564  -333.91731294 -333.99523106 ... -334.37869424 -334.41366279
 -334.375371  ]
2025-06-22 21:37:04 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:37:04 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionwithClustering
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionwithClustering
# Description: A differential evolution algorithm enhanced with clustering and adaptive mutation for efficient multimodal optimization.
# Code:

class AdaptiveDifferentialEvolutionwithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.cluster_threshold = 0.5 # Adjust as needed
        self.archive = []
        self.archive_size = 200

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            
            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_individuals(i, population)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(population[i], mutant)
        return offspring

    def _select_individuals(self, i, population):
        indices = np.random.choice(self.population_size, 3, replace=False)
        while i in indices:
            indices = np.random.choice(self.population_size, 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
        jrand = random.randint(0, self.dim - 1)
        y = np.copy(x)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                y[j] = v[j]
        return y

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
                
    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        #Example adaptive mechanism (can be improved)
        avg_fitness = np.mean(fitness_values)
        if avg_fitness < self.best_fitness_overall * 0.8: #Explore more
            self.F = min(1.0, self.F + 0.1)
            self.CR = min(1.0, self.CR + 0.1)
        else: #Exploit more
            self.F = max(0.1, self.F - 0.1)
            self.CR = max(0.1, self.CR - 0.1)

2025-06-22 21:37:04 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:37:58 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:37:58 INFO FeHistory: [255607.68479888 142815.95819125 370343.34783589 ...   4038.35198559
   4038.35198559   4038.35198559]
2025-06-22 21:37:58 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:37:58 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:40:30 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0538
2025-06-22 21:40:30 INFO FeHistory: [ 28.38380984  50.83090788  33.42607787 ... -73.7873762  -73.7873762
 -73.7873762 ]
2025-06-22 21:40:30 INFO Expected Optimum FE: -100
2025-06-22 21:40:30 INFO Unimodal AOCC mean: 0.1498
2025-06-22 21:40:30 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:40:30 INFO Multimodal (multiple components) AOCC mean: 0.0538
2025-06-22 21:40:30 INFO AOCC mean: 0.0679
2025-06-22 21:40:30 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:40:30 ERROR Can not run the algorithm
2025-06-22 21:40:30 INFO Run function 5 complete. FEHistory len: 201, AOCC: 0.1468
2025-06-22 21:40:30 INFO FeHistory: [-334.02104564 -333.96513454 -334.01416162 -334.0651445  -334.02486204
 -333.98143339 -334.00288953 -334.01012978 -333.99994721 -333.98996081
 -334.10377438 -334.00687101 -334.03369445 -334.0169596  -334.02940125
 -333.99871792 -333.98286215 -333.91840858 -333.96413344 -333.9936417
 -334.08282839 -333.96461936 -334.00005302 -333.98562418 -334.03145676
 -334.02226171 -334.01310557 -333.99298191 -334.03477757 -333.96842214
 -334.00989273 -334.05290714 -334.0258857  -334.08287287 -334.03792812
 -334.10544486 -333.98148425 -333.99247912 -334.00692435 -333.92703668
 -334.04440652 -334.08003443 -333.90062943 -333.99087912 -333.92732898
 -334.03654459 -333.99009376 -333.93562405 -334.01704205 -333.93593243
 -333.98320312 -334.0793548  -334.01632833 -334.026185   -333.97850174
 -333.94728466 -333.94753868 -334.01282481 -334.00728042 -334.01789537
 -333.9598148  -334.00978717 -333.97211807 -333.94671028 -333.92498673
 -333.93876341 -333.90633306 -333.97211055 -333.98105205 -333.97302357
 -334.027212   -333.93081686 -333.89718392 -333.98915464 -333.99690762
 -333.94109931 -333.97546418 -334.07280618 -333.96545787 -333.99086911
 -333.98152464 -334.08122423 -333.94603494 -334.06550401 -334.00700685
 -333.96538931 -333.99878093 -333.94794255 -334.00488146 -333.98860511
 -333.96884197 -333.98169197 -334.0989585  -334.01472707 -334.02690262
 -333.96812594 -333.94924779 -333.97728183 -333.95914069 -334.03278803
 -333.97483753 -334.00444262 -333.96833107 -333.96961012 -333.99523971
 -333.90774125 -333.98498077 -333.96072149 -333.9786875  -334.02615123
 -333.95775978 -333.92665815 -333.96263992 -333.99467328 -333.93510695
 -333.96209694 -333.94840412 -333.98313314 -333.98874602 -333.93136513
 -334.01711695 -333.99987928 -333.9602098  -333.95944484 -333.95190498
 -333.92903574 -333.92840587 -333.93993866 -333.98797083 -333.92554805
 -333.94279945 -333.93983905 -333.98618947 -333.96098057 -333.9773795
 -333.91481964 -333.92933954 -333.92870764 -333.88779308 -333.9626098
 -333.93013373 -333.9467628  -333.98698773 -333.93642464 -333.97292312
 -333.93802331 -334.02328665 -333.98442732 -334.03207219 -333.96523272
 -333.95560892 -333.99693564 -333.9455206  -333.95553364 -333.92828243
 -333.89895446 -333.91801123 -333.96531686 -333.92498877 -333.96203595
 -333.92575174 -334.0069811  -333.91206568 -333.98826118 -333.94138188
 -333.9268392  -333.97021868 -333.90178913 -334.01209154 -333.96974296
 -333.9105597  -333.96747726 -333.99955066 -333.91951246 -333.9861102
 -333.96971916 -334.0008976  -333.98137417 -334.00535075 -334.01231918
 -333.93884528 -333.95197305 -333.96854626 -334.0229785  -333.93724331
 -333.91888438 -334.00167368 -334.00899949 -333.89334786 -334.04457312
 -333.93721501 -333.95074981 -333.90410363 -334.03377005 -333.94264313
 -333.90297012 -333.87175653 -333.94687303 -333.93148273 -333.95600891
 -333.94288505]
2025-06-22 21:40:30 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:40:30 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionwithClustering
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionwithClustering
# Description: Uses differential evolution with adaptive mutation and clustering to escape local optima in multimodal landscapes.

class AdaptiveDifferentialEvolutionwithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.cluster_threshold = 0.1 # Distance threshold for clustering
        self.archive = []
        self.archive_size = 200


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self._update_best(offspring, offspring_fitness)
            self._update_archive(population, fitness_values)
            self._adaptive_parameters()

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_distinct(i, self.population_size)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)  # Ensure bounds
            offspring[i] = self._crossover(population[i], mutant)
        return offspring

    def _select_distinct(self, i, pop_size):
        indices = random.sample(range(pop_size), 3)
        while i in indices:
            indices = random.sample(range(pop_size), 3)
        return indices

    def _crossover(self, parent, mutant):
        cross_points = np.random.rand(self.dim) < self.CR
        child = np.where(cross_points, mutant, parent)
        return child

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
      combined = np.column_stack((population, fitness_values))
      self.archive.extend(combined)
      self.archive.sort(key=lambda x: x[-1])
      self.archive = self.archive[:self.archive_size]
      

    def _adaptive_parameters(self):
      #Adaptive F and CR based on the diversity of the population.  A simple example.
      diversity = np.mean(np.std(self.archive[:,:-1], axis=0)) #Average standard deviation across dimensions
      if diversity < 0.1: #Reduce exploration if solutions are clustered
          self.F *= 0.9
          self.CR *= 0.9
      else: # Increase exploration
          self.F *= 1.1
          self.CR *= 1.1
          self.F = min(self.F,1.2) # Cap F for stability
          self.CR = min(self.CR, 1.0) #Cap CR

2025-06-22 21:40:30 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:40:30 ERROR Can not run the algorithm
2025-06-22 21:40:31 INFO Run function 12 complete. FEHistory len: 201, AOCC: 0.0000
2025-06-22 21:40:31 INFO FeHistory: [222577.51786369 208035.19095574 200599.88442132 155271.32113415
 181956.42628426 171614.18541179 157458.11008509 164835.56497712
 250524.41593142 217912.53703242 244511.84315587  80780.06530292
 150573.03371593 482302.28989822 292445.73655443 251002.97020971
 168764.89298916 270872.43399325 167695.89631972 361023.40236408
 141301.6405838  291365.81505238 188235.95625148 155155.47494757
 219820.68349689 291255.45546482 320211.00298805 187251.37060067
 388241.88606092 282144.24094086 409698.94331563 291417.60176087
 200471.35001574 219721.49497293 271235.80197366 201959.88198143
 446967.23377253 133987.41777279  95074.91328613 154075.37711625
 323759.77708068 208187.52964048 174631.83543294 144031.52285406
 411404.84513721 255530.10195504 338468.73394447 247685.02950845
 312964.24012414 221803.21989315 250599.50978282 192010.93507085
 263545.79418383 166799.9062715  315701.90575685 210225.31314952
 151209.7931201  154890.52446445 217393.26026239 140266.28879608
 197137.03325158 107624.19073013 247641.21083014 112335.01101089
 272175.43620824 195163.18486985 206238.66433428 212977.29034077
 268080.86372798 166085.6811758  205353.13748048 173121.80628565
 245868.03825919 315605.86097278 377017.37510242 191111.07608287
 257671.29142377 298673.91602123 208266.79412381  96391.96126603
 271475.91388932 198829.98636945 107392.50320063 280417.4903921
 365174.85187565 412088.1629974  222592.63009589 187070.49494802
 390912.25883528 296452.97435115 151409.16606569 177333.68242558
 398292.4544129  484253.60890056 146871.59537762 174992.34002135
 218211.40048105 448885.28241606 232966.09398566 303995.10662314
 114225.12757991 150041.67522041 161556.63882295 290457.00570998
 135429.62036187 213585.84454951 214733.17426099 308111.52649257
 163312.04317178 273271.00132768 161621.84357769 217588.04588674
 293322.70360063 256042.27665592 286388.46010128 321371.77971543
 473310.50418524 337149.34973502 225923.00499572 232368.73088932
 350074.28433936 153505.07150331 340034.43203413 182419.06206074
 366544.19029061 255229.96479875 203582.70693652 238200.21176007
 213585.60763708 341597.19800754 219864.87659145 438029.00534592
 255304.3273306  316103.22740523 248458.26344928 312508.73256313
 324323.44201573 307464.57799788 168782.64622539 513953.16115961
 318400.97945676 304639.29118562 240610.06952462 404770.81760502
 274864.28101748 340656.44591849 269634.61020216 100541.89145079
 259287.19366038 433167.65094825 139401.97653309 242111.31968499
 203568.81445798 199479.29648056 431274.45680696 251392.64943656
 133321.19180716 309320.11114566 386590.5663846  213990.84956608
 338912.2133605  266651.25821865 189690.84127689 285136.76719103
 495548.01294354 245188.17442972 185746.8301667  334741.96421088
 294501.74502965 346895.21107235 165952.62341245 156531.80487734
 486320.06571322 122055.88239034 257574.46630089 236712.52333853
 194576.36442996 346879.82028148 234072.26914538 271680.27643028
 446932.75728405 229234.82004216 537413.1345249  211167.36502434
 248012.41778722 128961.5974831  198459.90494816 208931.640178
 272243.4577696  483126.90143697 428461.10195434 258010.98836431
 323519.35391076 254531.62930719 224967.42155298 292011.53801091
 435862.06334569 358355.33840323 459102.73697474 309049.37845404
 438522.40268567]
2025-06-22 21:40:31 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:40:31 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:40:31 ERROR Can not run the algorithm
2025-06-22 21:40:31 INFO Run function 23 complete. FEHistory len: 201, AOCC: 0.0018
2025-06-22 21:40:31 INFO FeHistory: [31.07486955 41.65878439 11.66181231 13.83144204 29.66362796 48.58100296
 19.16014644 46.10856633 22.96510781 21.48938558 53.13223168 25.29528234
 44.78139735 -4.04040834 46.48318665  3.47681547 13.20880845 42.35204226
 21.44774836 27.40571765 31.84614683 14.03610869  7.74497824 34.15943659
  4.020478   37.20948062 -2.39716341 22.89507152 27.97507849 31.72290342
 12.06809975 27.40009028 41.79385509 31.44750244 44.49800375 35.96686281
 23.71024908 16.43687636 57.71792539 18.1070767  36.24074708 32.44705899
 24.83762604 21.16569208 32.19776988 27.84071251 30.37133362 -0.85137058
 22.22047477 16.2724612   9.68183301 35.81693557 35.56997731  5.81185207
 34.65616554 17.0698833   8.13070518 26.79520314 16.90964322 20.26033771
 20.19804269 33.42553537 21.6051062   2.28183895 21.42288348 14.22418822
  7.61824864  3.07464376 41.72588947 23.17131353 12.13073731 19.71711219
 34.72574472 10.93782875 11.58071947 41.91105017 26.43882011 32.8106878
 14.93782164  4.20924753 30.01882642 26.91150863 18.51615265 23.99972273
 12.64894571  9.69301053 -2.6443406  36.79862755 37.79883346 34.03462739
 37.20527155 30.4546821  23.76133123 19.66699999 27.57223383 28.42030945
 27.90214283 48.90152589 38.67633196 30.55707114 34.85981165 42.15006434
 22.89611369 23.00778063 32.73586429 68.54415997 34.1043237  48.34001405
 44.82344945 70.72484062 46.52447882 41.73100293 60.95465782 59.78565974
 58.76768978 34.99774339 54.80902769 44.03781319 17.47918911 31.77065592
 43.83637823 51.34966546 18.01293843 45.31826469 38.56629289 24.66986868
 16.93772395 36.32934415 55.3418589  52.59754143 18.54382798 18.1122678
 34.54025361 27.67829058 62.5158315  35.23488296 59.18039732 39.96983127
 39.22928335 17.49402779 62.8199286  15.97383375 21.51098083 13.81093879
 67.22950314 32.38063966 38.5917007  32.8906571  46.444079   13.34203631
 34.17678197 14.88056239 36.79971939 30.81262582 38.08503679 51.90566388
 28.47638312 48.29912557 13.7613451  56.25010963 44.49218881 17.79459102
 44.11937493 30.84343697 51.67818362 29.36757236 17.83172138 68.32896411
 28.10934306 40.47861353 36.84766714 60.20016804 53.6062259  51.90215978
 32.52699182 33.30434526 30.32040224  1.92953689 27.42499575 32.36610243
 47.92992216 18.15281819 25.29333165 46.03891906 20.74615451 52.70925661
 45.69332794 36.40247379 41.51726634 30.52393589 23.99487234 40.4159377
 35.01805837 20.94553581 38.01331406 15.82195028 50.10416724 24.85272693
 25.27405025 14.63356098 26.83041337]
2025-06-22 21:40:31 INFO Expected Optimum FE: -100
2025-06-22 21:40:31 INFO Unimodal AOCC mean: 0.1468
2025-06-22 21:40:31 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:40:31 INFO Multimodal (multiple components) AOCC mean: 0.0018
2025-06-22 21:40:31 INFO AOCC mean: 0.0495
2025-06-22 21:40:31 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:40:37 INFO Run function 5 complete. FEHistory len: 70000, AOCC: 0.1523
2025-06-22 21:40:37 INFO FeHistory: [-333.95092235 -334.01827059 -334.00361558 ... -334.45030309 -334.48699424
 -334.48059513]
2025-06-22 21:40:37 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:40:37 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A differential evolution algorithm enhanced with adaptive mutation and clustering to escape local optima in high-dimensional multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = [] # for clustering
        self.archive_size = 200
        self.cluster_threshold = 0.1 # Adjust as needed


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self._update_best(offspring, offspring_fitness)
            self._update_archive(population, fitness_values)
            self._adaptive_parameters()


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different_individuals(i, population)
            mutant = a + self.F * (b - c)
            offspring[i] = self._crossover(population[i], mutant)
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)
        return offspring

    def _select_different_individuals(self, i, population):
        candidates = list(range(self.population_size))
        candidates.remove(i)
        a, b, c = random.sample(candidates, 3)
        return population[a], population[b], population[c]

    def _crossover(self, x, v):
      
        crossover_mask = np.random.rand(self.dim) < self.CR
        return np.where(crossover_mask, v, x)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        for sol in combined:
            self._add_to_archive(sol)
        self._purge_archive()


    def _add_to_archive(self, solution):
        if len(self.archive) < self.archive_size:
            self.archive.append(solution)
        else:
             #Simple replacement for demonstration.  Consider more sophisticated archive management.
            worst_index = np.argmax([s[-1] for s in self.archive])
            if solution[-1] < self.archive[worst_index][-1]:
                self.archive[worst_index] = solution


    def _purge_archive(self):
        if len(self.archive) > self.archive_size:
             self.archive = sorted(self.archive, key=lambda x: x[-1])[:self.archive_size]



    def _adaptive_parameters(self):
        #Example adaptive strategy.  Replace with more sophisticated method if needed.
        if self.eval_count % (self.budget // 10) == 0 and self.eval_count > 0:
            self.F = max(0.1, self.F * 0.95 + 0.05 * random.random()) # slowly decrease F
            self.CR = max(0.1, self.CR * 0.95 + 0.05 * random.random()) # slowly decrease CR



2025-06-22 21:40:37 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:40:42 INFO Run function 12 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-22 21:40:42 INFO FeHistory: [258592.4301561  294483.73946651 283814.10616548 ...  57025.8173442
  47569.43863845 125489.54931777]
2025-06-22 21:40:42 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:40:42 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:00 INFO Run function 23 complete. FEHistory len: 70000, AOCC: 0.0360
2025-06-22 21:41:00 INFO FeHistory: [ 50.99804522  31.82519379  39.18488804 ... -34.96384106 -49.11282074
 -43.39721869]
2025-06-22 21:41:00 INFO Expected Optimum FE: -100
2025-06-22 21:41:00 INFO Unimodal AOCC mean: 0.1523
2025-06-22 21:41:00 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:00 INFO Multimodal (multiple components) AOCC mean: 0.0360
2025-06-22 21:41:00 INFO AOCC mean: 0.0628
2025-06-22 21:41:11 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:11 ERROR Can not run the algorithm
2025-06-22 21:41:12 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1450
2025-06-22 21:41:12 INFO FeHistory: [-333.95762099]
2025-06-22 21:41:12 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:12 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 198.60252851314704, archive_size: int = 853.563972424138, sigma_decay: float = 0.8924746834557005,
                 sigma_init_factor: float = 0.25295084020713854, tournament_size:int = 96.95366762360766):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:12 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:12 ERROR Can not run the algorithm
2025-06-22 21:41:12 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:12 INFO FeHistory: [307765.82566247]
2025-06-22 21:41:12 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:12 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:12 ERROR Can not run the algorithm
2025-06-22 21:41:12 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:12 INFO FeHistory: [50.78397919]
2025-06-22 21:41:12 INFO Expected Optimum FE: -100
2025-06-22 21:41:12 INFO Unimodal AOCC mean: 0.1450
2025-06-22 21:41:12 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:12 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:12 INFO AOCC mean: 0.0483
2025-06-22 21:41:12 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:12 ERROR Can not run the algorithm
2025-06-22 21:41:12 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1458
2025-06-22 21:41:12 INFO FeHistory: [-334.02661895]
2025-06-22 21:41:12 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:12 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 641.4900863388792, archive_size: int = 875.9950511961149, sigma_decay: float = 0.969151785920393,
                 sigma_init_factor: float = 0.08945673639359805, tournament_size:int = 43.537860462771846):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:12 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:12 ERROR Can not run the algorithm
2025-06-22 21:41:12 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:12 INFO FeHistory: [104321.40131198]
2025-06-22 21:41:12 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:12 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:12 ERROR Can not run the algorithm
2025-06-22 21:41:12 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:12 INFO FeHistory: [5.00178689]
2025-06-22 21:41:12 INFO Expected Optimum FE: -100
2025-06-22 21:41:12 INFO Unimodal AOCC mean: 0.1458
2025-06-22 21:41:12 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:12 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:12 INFO AOCC mean: 0.0486
2025-06-22 21:41:12 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:12 ERROR Can not run the algorithm
2025-06-22 21:41:13 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1451
2025-06-22 21:41:13 INFO FeHistory: [-333.96894749]
2025-06-22 21:41:13 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:13 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 578.0505042747243, archive_size: int = 411.30016246450566, sigma_decay: float = 0.9358207108468349,
                 sigma_init_factor: float = 0.09899729977708487, tournament_size:int = 24.10716698859145):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:13 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:13 ERROR Can not run the algorithm
2025-06-22 21:41:13 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:13 INFO FeHistory: [140830.96246887]
2025-06-22 21:41:13 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:13 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:13 ERROR Can not run the algorithm
2025-06-22 21:41:13 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:13 INFO FeHistory: [30.1472137]
2025-06-22 21:41:13 INFO Expected Optimum FE: -100
2025-06-22 21:41:13 INFO Unimodal AOCC mean: 0.1451
2025-06-22 21:41:13 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:13 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:13 INFO AOCC mean: 0.0484
2025-06-22 21:41:13 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:13 ERROR Can not run the algorithm
2025-06-22 21:41:13 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1454
2025-06-22 21:41:13 INFO FeHistory: [-333.9924952]
2025-06-22 21:41:13 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:13 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 770.6640656929376, archive_size: int = 120.36226418523991, sigma_decay: float = 0.9189893224248791,
                 sigma_init_factor: float = 0.3705116156243071, tournament_size:int = 77.5879383461802):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:13 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:13 ERROR Can not run the algorithm
2025-06-22 21:41:13 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:13 INFO FeHistory: [126825.67017151]
2025-06-22 21:41:13 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:13 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:13 ERROR Can not run the algorithm
2025-06-22 21:41:13 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:13 INFO FeHistory: [22.62852788]
2025-06-22 21:41:13 INFO Expected Optimum FE: -100
2025-06-22 21:41:13 INFO Unimodal AOCC mean: 0.1454
2025-06-22 21:41:13 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:13 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:13 INFO AOCC mean: 0.0485
2025-06-22 21:41:13 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:13 ERROR Can not run the algorithm
2025-06-22 21:41:14 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1459
2025-06-22 21:41:14 INFO FeHistory: [-334.03614175]
2025-06-22 21:41:14 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:14 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 433.8123121965694, archive_size: int = 952.0192507637354, sigma_decay: float = 0.8488939017849905,
                 sigma_init_factor: float = 0.14836209288676408, tournament_size:int = 36.977647654478766):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:14 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:14 ERROR Can not run the algorithm
2025-06-22 21:41:14 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:14 INFO FeHistory: [332394.40984781]
2025-06-22 21:41:14 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:14 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:14 ERROR Can not run the algorithm
2025-06-22 21:41:14 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:14 INFO FeHistory: [40.06505894]
2025-06-22 21:41:14 INFO Expected Optimum FE: -100
2025-06-22 21:41:14 INFO Unimodal AOCC mean: 0.1459
2025-06-22 21:41:14 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:14 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:14 INFO AOCC mean: 0.0486
2025-06-22 21:41:14 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:14 ERROR Can not run the algorithm
2025-06-22 21:41:14 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1445
2025-06-22 21:41:14 INFO FeHistory: [-333.92141592]
2025-06-22 21:41:14 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:14 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 565.4030873902162, archive_size: int = 853.563972424138, sigma_decay: float = 0.8488939017849905,
                 sigma_init_factor: float = 0.8729017397835304, tournament_size:int = 96.95366762360766):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:14 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:14 ERROR Can not run the algorithm
2025-06-22 21:41:14 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:14 INFO FeHistory: [122429.39186062]
2025-06-22 21:41:14 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:14 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:14 ERROR Can not run the algorithm
2025-06-22 21:41:14 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:14 INFO FeHistory: [53.50776092]
2025-06-22 21:41:14 INFO Expected Optimum FE: -100
2025-06-22 21:41:14 INFO Unimodal AOCC mean: 0.1445
2025-06-22 21:41:14 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:14 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:14 INFO AOCC mean: 0.0482
2025-06-22 21:41:14 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:14 ERROR Can not run the algorithm
2025-06-22 21:41:15 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1448
2025-06-22 21:41:15 INFO FeHistory: [-333.94451194]
2025-06-22 21:41:15 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:15 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 554.368252433074, archive_size: int = 43.133364728108745, sigma_decay: float = 0.8488939017849905,
                 sigma_init_factor: float = 0.7575061078865825, tournament_size:int = 24.10716698859145):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:15 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:15 ERROR Can not run the algorithm
2025-06-22 21:41:15 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:15 INFO FeHistory: [231584.75790926]
2025-06-22 21:41:15 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:15 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:15 ERROR Can not run the algorithm
2025-06-22 21:41:15 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:15 INFO FeHistory: [9.06341051]
2025-06-22 21:41:15 INFO Expected Optimum FE: -100
2025-06-22 21:41:15 INFO Unimodal AOCC mean: 0.1448
2025-06-22 21:41:15 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:15 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:15 INFO AOCC mean: 0.0483
2025-06-22 21:41:15 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:15 ERROR Can not run the algorithm
2025-06-22 21:41:15 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1453
2025-06-22 21:41:15 INFO FeHistory: [-333.98316558]
2025-06-22 21:41:15 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:15 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 770.6640656929376, archive_size: int = 952.0192507637354, sigma_decay: float = 0.9563816580001688,
                 sigma_init_factor: float = 0.7762491975427006, tournament_size:int = 96.95366762360766):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:15 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:15 ERROR Can not run the algorithm
2025-06-22 21:41:15 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:15 INFO FeHistory: [329873.77208135]
2025-06-22 21:41:15 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:15 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:15 ERROR Can not run the algorithm
2025-06-22 21:41:15 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:15 INFO FeHistory: [35.39374927]
2025-06-22 21:41:15 INFO Expected Optimum FE: -100
2025-06-22 21:41:15 INFO Unimodal AOCC mean: 0.1453
2025-06-22 21:41:15 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:15 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:15 INFO AOCC mean: 0.0484
2025-06-22 21:41:15 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:15 ERROR Can not run the algorithm
2025-06-22 21:41:16 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1455
2025-06-22 21:41:16 INFO FeHistory: [-334.00033147]
2025-06-22 21:41:16 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:16 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 226.2526068878272, archive_size: int = 952.0192507637354, sigma_decay: float = 0.8488939017849905,
                 sigma_init_factor: float = 0.7677873893883531, tournament_size:int = 100.77958273882862):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:16 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:16 ERROR Can not run the algorithm
2025-06-22 21:41:16 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:16 INFO FeHistory: [283543.24576362]
2025-06-22 21:41:16 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:16 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:16 ERROR Can not run the algorithm
2025-06-22 21:41:16 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:16 INFO FeHistory: [7.46992126]
2025-06-22 21:41:16 INFO Expected Optimum FE: -100
2025-06-22 21:41:16 INFO Unimodal AOCC mean: 0.1455
2025-06-22 21:41:16 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:16 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:16 INFO AOCC mean: 0.0485
2025-06-22 21:41:16 INFO --- GNBG Problem Parameters for f5 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -337.508998
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 21:41:16 ERROR Can not run the algorithm
2025-06-22 21:41:16 INFO Run function 5 complete. FEHistory len: 1, AOCC: 0.1451
2025-06-22 21:41:16 INFO FeHistory: [-333.96573739]
2025-06-22 21:41:16 INFO Expected Optimum FE: -337.50899809752036
2025-06-22 21:41:16 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEAwithArchive
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 521.0558987088907, archive_size: int = 126.58488565662634, sigma_decay: float = 0.8488939017849905,
                 sigma_init_factor: float = 0.7677873893883531, tournament_size:int = 91.35915441206022):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_init_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-22 21:41:16 INFO --- GNBG Problem Parameters for f12 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1002.479079
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.5]
----------------------------------------
2025-06-22 21:41:16 ERROR Can not run the algorithm
2025-06-22 21:41:16 INFO Run function 12 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:16 INFO FeHistory: [171134.74689486]
2025-06-22 21:41:16 INFO Expected Optimum FE: -1002.4790787013411
2025-06-22 21:41:16 INFO --- GNBG Problem Parameters for f23 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.4 0.4 0.4 0.4 0.4]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-22 21:41:16 ERROR Can not run the algorithm
2025-06-22 21:41:16 INFO Run function 23 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-22 21:41:16 INFO FeHistory: [32.28689221]
2025-06-22 21:41:16 INFO Expected Optimum FE: -100
2025-06-22 21:41:16 INFO Unimodal AOCC mean: 0.1451
2025-06-22 21:41:16 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-22 21:41:16 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-22 21:41:16 INFO AOCC mean: 0.0484
