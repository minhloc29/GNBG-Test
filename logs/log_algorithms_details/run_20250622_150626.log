2025-06-22 15:06:27 INFO Initializing first population
2025-06-22 15:06:27 INFO Initializing population from 7 seed files...
2025-06-22 15:06:27 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:06:28 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:06:28 INFO FeHistory: [222019.07920674 164164.72687428 152565.82386116 ...   2936.6102311
   2260.51456109   2095.13690216]
2025-06-22 15:06:28 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:06:28 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:06:28 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:06:28 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:06:28 INFO AOCC mean: 0.0000
2025-06-22 15:06:28 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:06:29 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:06:29 INFO FeHistory: [154161.82347732 156885.85967055 118430.84497995 ...   1029.86904579
    440.15977575    663.10387596]
2025-06-22 15:06:29 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:06:29 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:06:29 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:06:29 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:06:29 INFO AOCC mean: 0.0000
2025-06-22 15:06:29 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:06:29 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:06:29 INFO FeHistory: [221775.22397329 265416.8160739  218978.50205535 ...    717.30452885
    285.50965175   1505.62178394]
2025-06-22 15:06:29 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:06:29 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:06:29 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:06:29 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:06:29 INFO AOCC mean: 0.0000
2025-06-22 15:06:29 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:06:30 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.9731
2025-06-22 15:06:30 INFO FeHistory: [173154.09687475 132955.85672148 172074.12761019 ...  62949.33316839
  90456.05900923  70270.61249405]
2025-06-22 15:06:30 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:06:30 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithEnhancedInitialization
import numpy as np
from scipy.optimize import minimize

class AdaptiveDifferentialEvolutionWithEnhancedInitialization:
    """
    Combines Differential Evolution with enhanced initialization near known optima and local search for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], known_optimum=None):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.known_optimum = known_optimum  # Allow for None if no known optimum

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.local_search_freq = 5 # Perform local search every 5 generations

    def initialize_population(self, num_samples):
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(num_samples, self.dim))
        
        if self.known_optimum is not None:
            num_near_optimum = int(0.3 * num_samples) # 30% near the optimum
            noise_scale = 20 # Adjust noise scale as needed. Experiment with this!
            noise = np.random.normal(scale=noise_scale, size=(num_near_optimum, self.dim))
            population[:num_near_optimum, :] = self.known_optimum + noise
            population[:num_near_optimum, :] = np.clip(population[:num_near_optimum, :], self.lower_bounds, self.upper_bounds)

        return population

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self.initialize_population(self.population_size)
        fitness = objective_function(population)
        self.eval_count += len(fitness)
        best_solution = population[np.argmin(fitness)]
        best_fitness = np.min(fitness)
        self.best_solution_overall = best_solution
        self.best_fitness_overall = best_fitness

        generation = 0
        while self.eval_count < self.budget:
            # Differential Evolution
            new_population = np.zeros_like(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if trial_fitness[0] < fitness[i]:
                    new_population[i] = trial
                    fitness[i] = trial_fitness[0]
                else:
                    new_population[i] = population[i]

            population = new_population
            best_solution = population[np.argmin(fitness)]
            best_fitness = np.min(fitness)

            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            # Local Search
            if generation % self.local_search_freq == 0:
                result = minimize(objective_function, best_solution, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
                if result.fun < best_fitness:
                    best_fitness = result.fun
                    best_solution = result.x
                    self.eval_count += result.nfev

                    if best_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = best_fitness
                        self.best_solution_overall = best_solution

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info









2025-06-22 15:06:30 INFO Unimodal AOCC mean: 0.9731
2025-06-22 15:06:30 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:06:30 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:06:30 INFO AOCC mean: 0.9731
2025-06-22 15:06:30 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:06:31 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.1119
2025-06-22 15:06:31 INFO FeHistory: [172878.40498366 171489.4308047  172878.40498366 ...  -1058.50914057
  -1080.42849385   -910.14016269]
2025-06-22 15:06:31 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:06:31 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-22 15:06:31 INFO Unimodal AOCC mean: 0.1119
2025-06-22 15:06:31 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:06:31 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:06:31 INFO AOCC mean: 0.1119
2025-06-22 15:06:31 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:06:32 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:06:32 INFO FeHistory: [117531.90020228 140444.84110714 124804.55590804 ...  10461.57671764
  12846.23905591   8390.01384594]
2025-06-22 15:06:32 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:06:32 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:06:32 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:06:32 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:06:32 INFO AOCC mean: 0.0000
2025-06-22 15:06:32 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:06:57 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:06:57 INFO FeHistory: [164282.54307335 131927.94937284 137594.2962093  ...   1039.11130145
    431.62429487    403.05617947]
2025-06-22 15:06:57 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:06:57 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:06:57 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:06:57 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:06:57 INFO AOCC mean: 0.0000
2025-06-22 15:07:44 INFO Started evolutionary loop, best so far: 0.9731018369220096
2025-06-22 15:07:44 INFO Population length is: 20
2025-06-22 15:07:44 INFO --- Performing Long-Term Reflection at Generation 1 ---
2025-06-22 15:07:50 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveDifferentialEvolutionWithEnhancedInitialization` vs (worst) `AdaptiveMultimodalEvolutionaryStrategy` (Rank 10), we see that the best-performing algorithm incorporates enhanced initialization near known optima and a local search step.  This significantly improves its ability to find the global optimum, especially in unimodal and ill-conditioned landscapes. The worst algorithm lacks these features, leading to poor performance.

`(second best)` `AdaptiveMultimodalOptimizer` vs (second worst) `AdaptiveGaussianSamplingEA` (Rank 4), shows the benefit of incorporating CMA-ES and local search.  CMA-ES provides effective exploration in high dimensions, and local search refines solutions.  The AdaptiveGaussianSamplingEA relies on simpler mutation and selection, lacking the sophisticated exploration of CMA-ES.

Comparing (1st) vs (2nd), we see that targeted initialization near potential optima provides a substantial advantage over a purely CMA-ES based approach, especially in ill-conditioned landscapes. The incorporation of a local search in both enhances their capability to escape local optima.

(3rd) `AdaptiveMultimodalOptimizerImproved` vs (4th) shows that Simulated Annealing, while aiming for multimodal optimization, fails to achieve the same performance as DE or CMA-ES based methods in high-dimensional landscapes. The adaptive parameter tuning improves exploration but does not compensate for the algorithm's fundamental limitations.

Comparing (second worst) `AdaptiveGaussianSamplingEA` vs (worst) `AdaptiveMultimodalEvolutionaryStrategy`, we observe that even a simple DE implementation with adaptive parameters outperforms the Evolutionary Strategy with adaptive mutation. The niching mechanism in the worst algorithm does not seem to fully counteract its simpler mutation strategy.  The adaptive mutation in the worst algorithm is not as effective as the combination of enhanced initialization and local search seen in the best performing algorithm.

Overall: The top-performing algorithms share characteristics like adaptive parameter tuning, effective exploration mechanisms (DE, CMA-ES), and local search to refine solutions in complex landscapes.  Simple mutation strategies and a lack of informed initialization are key weaknesses of the lower-performing algorithms.

**Experience:**

Combining exploration strategies like DE or CMA-ES with targeted initialization and local search is crucial for high-dimensional multimodal optimization.  Adaptive parameter tuning improves robustness, but a well-chosen base algorithm is paramount for success.  Niching can help diversify the population, but it's not a substitute for strong exploration.

2025-06-22 15:07:52 INFO Full response text: * **Keywords:**  High-dimensional optimization, multimodal functions, exploration-exploitation balance, adaptive algorithms, niching methods.

* **Advice:** Focus on designing hybrid algorithms combining global exploration (e.g., DE, CMA-ES) with efficient local search (e.g., gradient descent, Nelder-Mead).  Prioritize adaptive mechanisms that dynamically adjust the exploration-exploitation balance based on problem characteristics.  Consider theoretical analysis to understand algorithm behavior.

* **Avoid:**  Over-reliance on niching as a primary exploration strategy; neglecting theoretical underpinnings;  using fixed parameters without adaptive tuning; ignoring the base algorithm's inherent strengths and weaknesses.

* **Explanation:** Effective heuristics require a deep understanding of the optimization landscape.  Combining diverse techniques with adaptive control, guided by theoretical analysis, leads to more robust and efficient solutions than simply relying on a single method or heuristic.  The goal is to create algorithms that are both effective at finding good solutions and computationally efficient.

2025-06-22 15:07:52 INFO Generating offspring via Crossover...
2025-06-22 15:07:58 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:07:59 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.8937
2025-06-22 15:07:59 INFO FeHistory: [96915.58946492 96536.86683717 96995.87641641 ... -1081.9837994
 -1081.9837994  -1081.9837994 ]
2025-06-22 15:07:59 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:07:59 INFO Good algorithm:
Algorithm Name: AdaptiveCMAESwithLocalSearch
import numpy as np
from scipy.optimize import minimize

# Name: AdaptiveCMAESwithLocalSearch
# Description: Hybrid CMA-ES with local search for escaping local optima in multimodal landscapes.

class AdaptiveCMAESwithLocalSearch:
    """
    Combines CMA-ES for global exploration with local search for exploitation in multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.cma = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = float('inf')

        #Initial evaluation
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        self.cma = CMAEvolutionStrategy(self.best_solution_overall, 1.0, {'popsize': 10}) # Adjust popsize as needed


        while self.eval_count < self.budget:
            #CMA-ES iterations
            solutions = self.cma.ask()
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            self.cma.tell(solutions, fitnesses)

            #Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solutions[i]

            #Local search triggered by improvement slowdown.
            if self.cma.result.fbest < 1e-2 and self.eval_count > self.budget//10: #Adjust threshold and condition as needed.
                self.local_search(objective_function)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'cma_result': self.cma.result
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def local_search(self, objective_function):
        """Performs a local search around the current best solution using SciPy's minimize."""
        res = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, 
                       method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
        if res.fun < self.best_fitness_overall:
            self.best_fitness_overall = res.fun
            self.best_solution_overall = res.x
        self.eval_count += res.nfev #Increment evaluation count by number of function evaluations during local search.

from cma import CMAEvolutionStrategy

2025-06-22 15:07:59 INFO Unimodal AOCC mean: 0.8937
2025-06-22 15:07:59 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:07:59 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:07:59 INFO AOCC mean: 0.8937
2025-06-22 15:08:07 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:08:19 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:08:19 INFO FeHistory: [139500.51979606 138675.61228704 140188.52531394 ... 100748.48063792
 103728.99803248 100420.28837461]
2025-06-22 15:08:19 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:08:19 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:08:19 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:08:19 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:08:19 INFO AOCC mean: 0.0000
2025-06-22 15:08:27 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:08:27 ERROR Can not run the algorithm
2025-06-22 15:08:27 INFO Run function 1 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-22 15:08:27 INFO FeHistory: [192257.37119769 163143.99908521 146506.90485782 138376.44332966
 191120.78281252 201999.01580892 190654.99418566 156774.87383821
 199514.9481691  212962.57650508 159828.14615232 152126.43960654
 129720.69963081 199000.17691681 121349.39485699 127850.25638352
 148641.35971567 183875.11857358 118420.53719871 183407.16210194
 190971.55817108 169501.62493939 159041.48396889 143928.06043466
 194715.0294265  214708.6968927  184475.8468655  202029.98882445
 182282.92955753 177665.6996488  220703.59130752 136580.07999996
 142971.59411984 200761.17594995 183459.64111017 130863.06613833
 188528.85228965 117304.78520199 221397.15237276 112564.8155376
 205200.26079373 177956.0277111  174725.02970475 154054.32802134
 195727.09279896 179532.75999365 188057.24491678 193646.1286316
 211967.49780658 163013.680216   166762.83468669 181042.47227984
 129713.8637139  179811.01678718 155207.55161459 173428.33543015
 189232.18653686 211249.43221317 185640.62059064 168097.20754292
 111166.60520981 169146.39600423 121705.49502987 172773.18509061
 194350.34082669 185263.13563577 160320.66581691 121390.72061547
 137074.29603822 197769.59778944 115647.83053654 142937.73549249
 208260.35536034 141878.70851338 167192.6995581  157673.89989418
 175184.92383008  90925.59027532 108282.61223645 177901.3749589
 134258.75954896 153023.57686253 194497.37317967 167752.40716276
 175872.570734   137842.17496448 147133.55206901 122330.37924993
  93537.64177248 174938.71660554 183611.25117534 189304.68314896
 180878.17918745 135239.93754242 165361.29224888 218320.27812986
 117279.15704024 202188.21994213 154604.93063067 123554.67339532
 241636.64027685 173402.19310887 160731.56517965 208678.43806946
 213238.06856136 145330.58073122 186802.49075589 272183.93383353
 197186.6602523  221863.94401458 203049.89035089 137798.78277816
 207812.27527143 164299.07349292 140803.17603489 165271.21469955
 230984.22451039 231695.40727017 288165.51792121 158200.66491127
 259204.20411555 289279.36868016 221628.75274367 233283.90908225
 227309.63000525 297364.78826486 135653.70100232 190034.33889153
 224973.97521916 131852.3924975  196957.78746583 209203.34600677
 187151.94917394 196884.96631208 219264.01921479 200331.86188626
 231502.75271389 273733.40734182 137897.28339407 198221.03734586
 177450.93053252 177073.35393001 238213.43262092 268343.87312796
 176095.93555634 176498.61040663 199829.22921621 134660.86468859
 187195.31022561 160721.98358645 211002.44626406 100053.17341618
 210907.70142389 200493.07009771 235757.70063589 194998.03955441
 229603.2896621  221858.17236752 159196.16443366 174570.77657623
 269980.24727435 223398.4612972  205003.25214557 208282.77899327
 200311.00958359 156553.12793351 252642.99608797 171486.18719332
 188171.96629637 215860.31305121 201067.25747014 210184.64369581
 192748.40340676 229085.91989489 159702.05355803 223250.59762379
 166382.9047726  154021.15909287 182797.9206107  249623.9235229
 196571.49172915 246129.99185063 252103.95635791 161128.61969416
 163995.07454562 218847.41032778 220725.07010305 187144.16701986
 203088.95975016 153302.86381632 212005.19983432 178795.71186263
 185474.85302709 203969.14422652 256083.82992699 236726.25569253
 228525.15689695 281368.01978641 197879.20560191 141558.54582764]
2025-06-22 15:08:27 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:08:27 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:08:27 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:08:27 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:08:27 INFO AOCC mean: 0.0000
2025-06-22 15:08:36 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:08:36 INFO Run function 1 complete. FEHistory len: 185, AOCC: 0.9872
2025-06-22 15:08:36 INFO FeHistory: [182756.77083084 182821.69779263 182817.26918483 182820.23328646
 183748.83050815 183663.66002522 182601.85798383 183724.67053534
 184136.82323544 182846.85697415 183119.20259028 271577.69321188
 157037.53220797 264796.80348013 148743.97977543 149853.78624175
 192780.45674075 207834.64619768 219049.45734255 179483.7512741
 203263.53485467 106432.69778578 269356.81536413 187406.30595729
 188761.59061995 196044.84925387 263753.47491153 212172.86846091
 210381.16507169 179627.28231944 197347.78009337 140326.85034191
 279993.99482529 232249.19637865 192337.05486643 210323.66596525
 174286.10704586 232294.08797339 170472.51648453 148844.52156349
 228286.03848207 179153.5492296  169501.37525592 144069.74141963
 203096.21113714 142503.39549372 163933.34757192 190588.59452831
 181844.2170849  149255.73280745 235997.74298839 264420.30378538
 253162.23528716 254222.92239331 256412.57407798 247439.42438718
 260029.85299364 136876.24703304 236453.9872588  234042.76861561
 189541.6370163  106432.69778578 106432.69778404 106432.69778532
 106432.69778724 106432.69778684 106432.69778609 106432.69778618
 106432.69778523 106432.69778427 106432.69778605 106432.69778546
 106432.69778486 106432.69778395 106432.69778589 106432.6977839
 106432.69778638 106432.6977866  106432.69778422 106432.69778704
 106432.69778691 106432.69778712 106432.69778622 106432.69778547
 106432.69778696 106432.69778568 106432.6977861  106432.69778383
 106432.69778748 106432.69778527 106432.69778581 106432.69778279
  52350.21927498  52350.21927671  52350.21927544  52350.21927458
  52350.21927392  52350.21927467  52350.21927458  52350.21927553
  52350.21927649  52350.21927471  52350.2192753   52350.2192759
  52350.21927357  52350.21927486  52350.21927419  52350.21927449
  52350.21927416  52350.21927654  52350.21927372  52350.21927413
  52350.21927395  52350.21927454  52350.21927529  52350.2192738
  52350.21927508  52350.21927466  52350.21927424  52350.21927431
  52350.21927548  52350.21927495  52350.21927419  -1081.98379885
  -1081.98379885  -1081.98379885  -1081.98379885  -1081.98379885
  -1081.98379885  -1081.98379885  -1081.98379885  -1081.98379885
  -1081.98379885  -1081.98379885  -1081.98379885  -1081.98379885
  -1081.98379885  -1081.98379885  -1081.98379885  -1081.98379885
  -1081.98379885  -1081.98379885  -1081.98379885  -1081.98379885
  -1081.98379885  -1081.98379885  -1081.98379885  -1081.98379885
  -1081.98379885  -1081.98379885  -1081.98379885  -1081.98379885
  -1081.98379885  -1081.98379885  -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994 ]
2025-06-22 15:08:36 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:08:36 INFO Good algorithm:
Algorithm Name: AdaptiveCMAESDEwithLocalSearch
import numpy as np
from scipy.optimize import minimize

# Name: AdaptiveCMAESDEwithLocalSearch
# Description: Hybrid CMA-ES and DE with local search for escaping local optima in multimodal landscapes.
class AdaptiveCMAESDEwithLocalSearch:
    """
    Combines CMA-ES for global exploration, DE for local exploitation, and local search for escaping local optima.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10  # CMA-ES population size
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.cma = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        # Initialize CMA-ES
        self.cma = CMAEvolutionStrategy(self.best_solution_overall, 1.0, {'popsize': self.population_size})

        while self.eval_count < self.budget:
            # CMA-ES exploration phase
            solutions = self.cma.ask()
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            self.cma.tell(solutions, fitnesses)

            # Update best solution from CMA-ES
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solutions[i]

            # DE local exploitation and local search
            if self.eval_count < self.budget * 0.8: # Switch to DE after 80% of budget
                for _ in range(5): #Number of DE iterations per CMA-ES iteration
                  new_solution = self._differential_evolution(objective_function)
                  if new_solution[1] < self.best_fitness_overall:
                      self.best_fitness_overall = new_solution[1]
                      self.best_solution_overall = new_solution[0]

            # Local search refinement
            res = minimize(objective_function, self.best_solution_overall, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
            self.eval_count += res.nfev
            if res.fun < self.best_fitness_overall:
                self.best_fitness_overall = res.fun
                self.best_solution_overall = res.x
            if self.best_fitness_overall < acceptance_threshold:
                break

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'cma_result': self.cma.result
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _differential_evolution(self, objective_function):
        population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        best_solution = None
        best_fitness = float('inf')
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = np.copy(population[i])
            j_rand = random.randint(0, self.dim - 1)
            for j in range(self.dim):
                if random.random() < self.CR or j == j_rand:
                    trial[j] = mutant[j]
            trial_fitness = objective_function(trial.reshape(1, -1))[0]
            self.eval_count +=1
            if trial_fitness < best_fitness:
                best_fitness = trial_fitness
                best_solution = trial
        return best_solution, best_fitness

import random
from cma import CMAEvolutionStrategy
2025-06-22 15:08:36 INFO Unimodal AOCC mean: 0.9872
2025-06-22 15:08:36 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:08:36 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:08:36 INFO AOCC mean: 0.9872
2025-06-22 15:08:42 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:08:45 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.9919
2025-06-22 15:08:45 INFO FeHistory: [182418.99544772 182267.24378405 182579.75421201 ...  -1081.9837994
  -1081.9837994   -1081.9837994 ]
2025-06-22 15:08:45 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:08:45 INFO Good algorithm:
Algorithm Name: AdaptiveCMAESwithLocalSearch
import numpy as np
from scipy.optimize import minimize

# Name: AdaptiveCMAESwithLocalSearch
# Description: Hybrid CMA-ES with local search for escaping local optima in multimodal landscapes.
class AdaptiveCMAESwithLocalSearch:
    """
    Combines CMA-ES for global exploration with local search for exploitation.  Adapts sigma based on progress.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.sigma = 0.5  # Initial step size for CMA-ES
        self.cma = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initialize CMA-ES
        self.cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10, 'tolfun':1e-10})

        while self.eval_count < self.budget:
            # Generate solutions with CMA-ES
            solutions = self.cma.ask()
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            self.cma.tell(solutions, fitnesses)

            # Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solutions[i]

            # Adaptive Sigma Adjustment
            if self.cma.result.fbest < 1e-2: # Condition for local search
                self.sigma *= 0.9 # Reduce sigma if close to optimum
            else:
                self.sigma *= 1.1 # Increase otherwise

            # Local search around best solution
            result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
            if result.fun < self.best_fitness_overall:
                self.best_fitness_overall = result.fun
                self.best_solution_overall = result.x

            self.cma.sigma = self.sigma #Update CMA-ES sigma


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'cma_result': self.cma.result
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

from cma import CMAEvolutionStrategy

2025-06-22 15:08:45 INFO Unimodal AOCC mean: 0.9919
2025-06-22 15:08:45 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:08:45 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:08:45 INFO AOCC mean: 0.9919
2025-06-22 15:08:53 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:09:00 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:09:00 INFO FeHistory: [167927.5755379  166592.31710603 135805.49487133 ... 103656.08993361
 111422.89320621 101145.66264588]
2025-06-22 15:09:00 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:09:00 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:09:00 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:09:00 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:09:00 INFO AOCC mean: 0.0000
2025-06-22 15:09:09 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:09:10 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:09:10 INFO FeHistory: [183379.03903401 219962.52328421 154797.14026857 ... 193968.20282767
 194024.32603667 193968.20282767]
2025-06-22 15:09:10 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:09:10 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:09:10 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:09:10 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:09:10 INFO AOCC mean: 0.0000
2025-06-22 15:09:18 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:10:15 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:10:15 INFO FeHistory: [142125.27924483 175001.3089496  133768.42658039 ... 143442.32181381
 143723.68055619 143442.32181381]
2025-06-22 15:10:15 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:10:15 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:10:15 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:10:15 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:10:15 INFO AOCC mean: 0.0000
2025-06-22 15:10:22 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:10:22 ERROR Can not run the algorithm
2025-06-22 15:10:22 INFO Run function 1 complete. FEHistory len: 11, AOCC: 0.0000
2025-06-22 15:10:22 INFO FeHistory: [193257.91177905 193529.26999069 192294.93043974 193083.33766442
 191525.65181916 192789.73783333 193658.87496444 193688.61242786
 191656.65425258 195654.37183194 192012.7694    ]
2025-06-22 15:10:22 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:10:22 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:10:22 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:10:22 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:10:22 INFO AOCC mean: 0.0000
2025-06-22 15:10:32 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:10:32 INFO Run function 1 complete. FEHistory len: 8630, AOCC: 0.0000
2025-06-22 15:10:32 INFO FeHistory: [ 1.28523695e+05  1.45953765e+05  1.55309855e+05 ... -1.12062979e+02
  4.52554933e+02  3.18546517e+02]
2025-06-22 15:10:32 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:10:32 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:10:32 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:10:32 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:10:32 INFO AOCC mean: 0.0000
2025-06-22 15:10:32 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal & Ill-Conditioned Landscapes (like f1-f6): These have a single optimum, but it may be hidden in a very long, narrow, rotated valley. A good strategy needs to effectively sample the central region of the search space and adapt its search direction.

Rugged Single-Basin Landscapes (like f7-f15): These are filled with numerous, deep local optima. The algorithm must have a robust escape mechanism to avoid getting trapped.

Deceptive Multi-Component Landscapes (like f16-f24): These problems have multiple, separate basins of attraction. The global optimum may be in a small, remote basin, while a larger, more attractive basin leads to a suboptimal solution. A good strategy must have aggressive global exploration capabilities.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).



### Better code
AdaptiveMultimodalEvolutionaryAlgorithm
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryAlgorithm:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate
        self.mutation_adaptation_factor = 0.95 #Factor to decrease mutation after stagnation

        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        #Initial fitness evaluation
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        best_index = np.argmin(fitness_values)
        self.best_solution_overall = self.population[best_index].copy()
        self.best_fitness_overall = fitness_values[best_index]

        stagnation_count = 0
        
        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                # Differential Evolution
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                
                # Boundary handling (clipping)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                
                trial = np.copy(self.population[i])
                j_rand = random.randint(0, self.dim - 1)
                for j in range(self.dim):
                    if random.random() < self.CR or j == j_rand:
                        trial[j] = mutant[j]
                
                #Adaptive mutation to escape local optima
                if stagnation_count > 10 :
                  trial += np.random.normal(0,self.mutation_rate * (self.upper_bounds - self.lower_bounds), self.dim)
                  trial = np.clip(trial, self.lower_bounds, self.upper_bounds) #Clip values to avoid overflow

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values[i] = trial_fitness
                    if trial_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = trial_fitness
                        self.best_solution_overall = trial.copy()
                        stagnation_count = 0 #Reset stagnation counter
                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)

            #Adaptive mutation rate
            if self.best_fitness_overall < acceptance_threshold :
                break
            if stagnation_count > 10 :
                self.mutation_rate *= self.mutation_adaptation_factor
            stagnation_count += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


### Worse code
AdaptiveMultimodalOptimizer
# Name: AdaptiveMultimodalOptimizer
# Description: Hybrid evolutionary algorithm combining CMA-ES with a local search for escaping local optima and handling multimodal landscapes.

import numpy as np
from cma import CMAEvolutionStrategy

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.cma = None  # Initialize CMA-ES object later

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        # Initialize CMA-ES
        self.cma = CMAEvolutionStrategy(self.best_solution_overall, 1.0, {'popsize': 10}) # Adjust popsize as needed

        while self.eval_count < self.budget:
            # Generate solutions with CMA-ES
            solutions = self.cma.ask()
            
            # Handle bounds: clip solutions to stay within bounds
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)

            # Evaluate solutions
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            
            self.cma.tell(solutions, fitnesses)

            # Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solutions[i]
                    
            # Local Search (optional, enhances escape from local optima):
            if self.cma.result.fbest < 1e-2: #adjust based on problem difficulty
                #Simple local search by perturbing the best solution
                for _ in range(5): # Number of local search iterations
                    perturbation = np.random.normal(0, 0.1, self.dim) # Adjust perturbation scale
                    perturbed_solution = np.clip(self.best_solution_overall + perturbation, self.lower_bounds, self.upper_bounds)
                    perturbed_fitness = objective_function(perturbed_solution.reshape(1,-1))[0]
                    if perturbed_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = perturbed_fitness
                        self.best_solution_overall = perturbed_solution
                        

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'cma_result': self.cma.result # Include CMA-ES's results for diagnostics
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


### Analyze & experience
- Comparing (best) `AdaptiveDifferentialEvolutionWithEnhancedInitialization` vs (worst) `AdaptiveMultimodalEvolutionaryStrategy` (Rank 10), we see that the best-performing algorithm incorporates enhanced initialization near known optima and a local search step.  This significantly improves its ability to find the global optimum, especially in unimodal and ill-conditioned landscapes. The worst algorithm lacks these features, leading to poor performance.

`(second best)` `AdaptiveMultimodalOptimizer` vs (second worst) `AdaptiveGaussianSamplingEA` (Rank 4), shows the benefit of incorporating CMA-ES and local search.  CMA-ES provides effective exploration in high dimensions, and local search refines solutions.  The AdaptiveGaussianSamplingEA relies on simpler mutation and selection, lacking the sophisticated exploration of CMA-ES.

Comparing (1st) vs (2nd), we see that targeted initialization near potential optima provides a substantial advantage over a purely CMA-ES based approach, especially in ill-conditioned landscapes. The incorporation of a local search in both enhances their capability to escape local optima.

(3rd) `AdaptiveMultimodalOptimizerImproved` vs (4th) shows that Simulated Annealing, while aiming for multimodal optimization, fails to achieve the same performance as DE or CMA-ES based methods in high-dimensional landscapes. The adaptive parameter tuning improves exploration but does not compensate for the algorithm's fundamental limitations.

Comparing (second worst) `AdaptiveGaussianSamplingEA` vs (worst) `AdaptiveMultimodalEvolutionaryStrategy`, we observe that even a simple DE implementation with adaptive parameters outperforms the Evolutionary Strategy with adaptive mutation. The niching mechanism in the worst algorithm does not seem to fully counteract its simpler mutation strategy.  The adaptive mutation in the worst algorithm is not as effective as the combination of enhanced initialization and local search seen in the best performing algorithm.

Overall: The top-performing algorithms share characteristics like adaptive parameter tuning, effective exploration mechanisms (DE, CMA-ES), and local search to refine solutions in complex landscapes.  Simple mutation strategies and a lack of informed initialization are key weaknesses of the lower-performing algorithms.
- * **Keywords:**  High-dimensional optimization, multimodal functions, exploration-exploitation balance, adaptive algorithms, niching methods.

* **Advice:** Focus on designing hybrid algorithms combining global exploration (e.g., DE, CMA-ES) with efficient local search (e.g., gradient descent, Nelder-Mead).  Prioritize adaptive mechanisms that dynamically adjust the exploration-exploitation balance based on problem characteristics.  Consider theoretical analysis to understand algorithm behavior.

* **Avoid:**  Over-reliance on niching as a primary exploration strategy; neglecting theoretical underpinnings;  using fixed parameters without adaptive tuning; ignoring the base algorithm's inherent strengths and weaknesses.

* **Explanation:** Effective heuristics require a deep understanding of the optimization landscape.  Combining diverse techniques with adaptive control, guided by theoretical analysis, leads to more robust and efficient solutions than simply relying on a single method or heuristic.  The goal is to create algorithms that are both effective at finding good solutions and computationally efficient.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-22 15:10:32 INFO Mutation prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal & Ill-Conditioned Landscapes (like f1-f6): These have a single optimum, but it may be hidden in a very long, narrow, rotated valley. A good strategy needs to effectively sample the central region of the search space and adapt its search direction.

Rugged Single-Basin Landscapes (like f7-f15): These are filled with numerous, deep local optima. The algorithm must have a robust escape mechanism to avoid getting trapped.

Deceptive Multi-Component Landscapes (like f16-f24): These problems have multiple, separate basins of attraction. The global optimum may be in a small, remote basin, while a larger, more attractive basin leads to a suboptimal solution. A good strategy must have aggressive global exploration capabilities.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).



Current heuristics:
AdaptiveCMAESwithLocalSearch
import numpy as np
from scipy.optimize import minimize

# Name: AdaptiveCMAESwithLocalSearch
# Description: Hybrid CMA-ES with local search for escaping local optima in multimodal landscapes.
class AdaptiveCMAESwithLocalSearch:
    """
    Combines CMA-ES for global exploration with local search for exploitation.  Adapts sigma based on progress.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.sigma = 0.5  # Initial step size for CMA-ES
        self.cma = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initialize CMA-ES
        self.cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10, 'tolfun':1e-10})

        while self.eval_count < self.budget:
            # Generate solutions with CMA-ES
            solutions = self.cma.ask()
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            self.cma.tell(solutions, fitnesses)

            # Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solutions[i]

            # Adaptive Sigma Adjustment
            if self.cma.result.fbest < 1e-2: # Condition for local search
                self.sigma *= 0.9 # Reduce sigma if close to optimum
            else:
                self.sigma *= 1.1 # Increase otherwise

            # Local search around best solution
            result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
            if result.fun < self.best_fitness_overall:
                self.best_fitness_overall = result.fun
                self.best_solution_overall = result.x

            self.cma.sigma = self.sigma #Update CMA-ES sigma


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'cma_result': self.cma.result
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

from cma import CMAEvolutionStrategy


Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- * **Keywords:**  High-dimensional optimization, multimodal functions, exploration-exploitation balance, adaptive algorithms, niching methods.

* **Advice:** Focus on designing hybrid algorithms combining global exploration (e.g., DE, CMA-ES) with efficient local search (e.g., gradient descent, Nelder-Mead).  Prioritize adaptive mechanisms that dynamically adjust the exploration-exploitation balance based on problem characteristics.  Consider theoretical analysis to understand algorithm behavior.

* **Avoid:**  Over-reliance on niching as a primary exploration strategy; neglecting theoretical underpinnings;  using fixed parameters without adaptive tuning; ignoring the base algorithm's inherent strengths and weaknesses.

* **Explanation:** Effective heuristics require a deep understanding of the optimization landscape.  Combining diverse techniques with adaptive control, guided by theoretical analysis, leads to more robust and efficient solutions than simply relying on a single method or heuristic.  The goal is to create algorithms that are both effective at finding good solutions and computationally efficient.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-22 15:11:14 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:11:18 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.4835
2025-06-22 15:11:18 INFO FeHistory: [144419.81447319 159737.36607314 253563.31602107 ...   7516.42152799
   7516.42152805   7516.42152802]
2025-06-22 15:11:18 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:11:18 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
from scipy.optimize import minimize
from scipy.spatial.distance import cdist

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: Hybrid Differential Evolution with clustering for escaping local optima and exploring diverse regions in multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 #Adaptive parameter, can be tuned
        self.F = 0.8 #Differential weight
        self.CR = 0.9 #Crossover rate
        self.k = 5  # Number of clusters for K-means

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitnesses = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall = self.population[np.argmin(fitnesses)]
        self.best_fitness_overall = np.min(fitnesses)

        while self.eval_count < self.budget:
            # Differential Evolution step
            offspring = self.generate_offspring()
            offspring_fitnesses = objective_function(offspring)
            self.eval_count += len(offspring_fitnesses)

            # Selection
            for i in range(self.population_size):
                if offspring_fitnesses[i] < fitnesses[i]:
                    self.population[i] = offspring[i]
                    fitnesses[i] = offspring_fitnesses[i]

            # Update best solution
            best_index = np.argmin(fitnesses)
            if fitnesses[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitnesses[best_index]

            # Clustering and Adaptive Strategy
            if self.eval_count > self.budget / 2 : #Start clustering later for initial exploration.
                if self.best_fitness_overall < 1e-2:
                    self.F *=0.9 #Reduce exploration near optimum
                    self.CR *= 0.9
                else:
                    self.F *= 1.1
                    self.CR *= 1.1

                centroids = self.kmeans_clustering(self.population, self.k)
                for centroid in centroids:
                    # Local search around each centroid
                    result = minimize(lambda x: objective_function(x.reshape(1, -1))[0], centroid, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                    if result.fun < self.best_fitness_overall:
                        self.best_fitness_overall = result.fun
                        self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self):
        offspring = np.zeros_like(self.population)
        for i in range(self.population_size):
            a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
            v = self.population[a] + self.F * (self.population[b] - self.population[c])
            v = np.clip(v, self.lower_bounds, self.upper_bounds)
            jrand = np.random.randint(0, self.dim)
            for j in range(self.dim):
                if np.random.rand() < self.CR or j == jrand:
                    offspring[i, j] = v[j]
                else:
                    offspring[i, j] = self.population[i, j]
        return offspring


    def kmeans_clustering(self, data, k):
        #Simple K-means implementation
        centroids = data[np.random.choice(len(data), k, replace=False)]
        while True:
            distances = cdist(data, centroids)
            labels = np.argmin(distances, axis=1)
            new_centroids = np.array([data[labels == i].mean(axis=0) for i in range(k)])
            if np.allclose(centroids, new_centroids):
                break
            centroids = new_centroids
        return centroids
2025-06-22 15:11:18 INFO Unimodal AOCC mean: 0.4835
2025-06-22 15:11:18 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:11:18 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:11:18 INFO AOCC mean: 0.4835
2025-06-22 15:11:18 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:11:18 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0833
2025-06-22 15:11:18 INFO FeHistory: [159162.26076397 227345.33455688 215728.3010538  ...   2907.95696189
   1359.62258119   1421.37441657]
2025-06-22 15:11:18 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:11:18 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES
import numpy as np
from scipy.optimize import minimize
from scipy.spatial.distance import cdist

# Name: AdaptiveHybridDE_CMAES
# Description: Hybrid Differential Evolution and CMA-ES with adaptive local search for multimodal landscapes.
# Code:
class AdaptiveHybridDE_CMAES:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.floor(dim/2)) #Adaptive population size
        self.F = 0.8 #Differential evolution scaling factor
        self.CR = 0.9 #Differential evolution crossover rate
        self.cma = None
        self.local_search_trigger = 1e-3 #Fitness threshold for triggering local search

        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        #Initial Evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1,-1))[0]
            self.eval_count +=1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()


        while self.eval_count < self.budget:
            #Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                v = self.population[a] + self.F * (self.population[b] - self.population[c])
                u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]),self.lower_bounds, self.upper_bounds)
                offspring[i] = u

            #Evaluate offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            #Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < self.fitness[i]:
                    self.fitness[i] = offspring_fitness[i]
                    self.population[i] = offspring[i]
                    if self.fitness[i] < self.best_fitness_overall:
                        self.best_fitness_overall = self.fitness[i]
                        self.best_solution_overall = self.population[i].copy()

            #Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger:
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10}) #Initiate CMA-ES
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]

                #Local Search
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

from cma import CMAEvolutionStrategy
2025-06-22 15:11:18 INFO Unimodal AOCC mean: 0.0833
2025-06-22 15:11:18 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:11:18 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:11:18 INFO AOCC mean: 0.0833
2025-06-22 15:11:18 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:11:19 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:11:19 INFO FeHistory: [175567.64404012 172443.52436197 173567.95943304 ...  14656.25101646
  14225.7432961   24318.58280948]
2025-06-22 15:11:19 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:11:19 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:11:19 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:11:19 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:11:19 INFO AOCC mean: 0.0000
2025-06-22 15:11:19 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:11:20 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:11:20 INFO FeHistory: [131326.2135658  171020.09563402 167856.76444743 ...  71614.07392549
  71614.07392549  71614.07392549]
2025-06-22 15:11:20 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:11:20 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:11:20 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:11:20 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:11:20 INFO AOCC mean: 0.0000
2025-06-22 15:11:20 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:11:21 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.9331
2025-06-22 15:11:21 INFO FeHistory: [157356.2894417  238643.22804891 117775.45512634 ... 181511.83672561
 112935.5479483   85462.50154975]
2025-06-22 15:11:21 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:11:21 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithLocalSearch
import numpy as np
from scipy.optimize import minimize

# Name: AdaptiveDifferentialEvolutionWithLocalSearch
# Description: Hybrid Differential Evolution with local search for escaping local optima in multimodal landscapes, dynamically adjusting mutation strategy.
class AdaptiveDifferentialEvolutionWithLocalSearch:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * self.dim  # Adjust as needed
        self.population = None
        self.F = 0.8 #Differential Evolution scaling factor
        self.CR = 0.9 #Crossover rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitnesses = objective_function(self.population)
        self.eval_count += self.population_size


        while self.eval_count < self.budget:
            # Differential Evolution
            new_population = np.zeros_like(self.population)
            for i in range(self.population_size):
                # Mutation
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                
                #Adaptive Mutation: Increase F if stuck in local minima
                if self.eval_count > self.budget / 2 and self.best_fitness_overall > 1e-4:
                  self.F = min(1.2, self.F + 0.1)

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                jrand = np.random.randint(0, self.dim)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])
                trial[jrand] = mutant[jrand]

                #Selection
                trial_fitness = objective_function(trial.reshape(1,-1))[0]
                self.eval_count += 1
                if trial_fitness < fitnesses[i]:
                    new_population[i] = trial
                    fitnesses[i] = trial_fitness
                else:
                    new_population[i] = self.population[i]

            self.population = new_population

            # Update best solution
            best_index = np.argmin(fitnesses)
            if fitnesses[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitnesses[best_index]
                self.best_solution_overall = self.population[best_index]
                
            #Local Search
            result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
            if result.fun < self.best_fitness_overall:
                self.best_fitness_overall = result.fun
                self.best_solution_overall = result.x
                self.eval_count += result.nfev

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:11:21 INFO Unimodal AOCC mean: 0.9331
2025-06-22 15:11:21 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:11:21 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:11:21 INFO AOCC mean: 0.9331
2025-06-22 15:11:28 WARNING No replacements made in template string. Returning None.
2025-06-22 15:11:35 WARNING No replacements made in template string. Returning None.
2025-06-22 15:11:42 WARNING No replacements made in template string. Returning None.
2025-06-22 15:11:42 INFO Generation 2, best so far: 0.9919197902331691
2025-06-22 15:11:42 INFO Population length is: 11
2025-06-22 15:11:42 INFO --- Performing Long-Term Reflection at Generation 2 ---
2025-06-22 15:11:48 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveHybridDE_CMAES` vs (worst) `AdaptiveDifferentialEvolutionWithLocalSearch` (last one), we see that the best-performing algorithm uses a hybrid approach combining Differential Evolution (DE) and CMA-ES, incorporating adaptive population size and a local search trigger based on fitness. The worst-performing algorithm uses only DE with a simple local search, lacking the sophisticated adaptive mechanisms and global exploration capabilities of CMA-ES.  `AdaptiveHybridDE_CMAES` also features more sophisticated adaptive mechanisms for parameters `F` and `CR` compared to its simpler counterparts.

`(second best)` `AdaptiveHybridCMAESDE` vs (second worst) `AdaptiveCMAESwithLocalSearch`: The second-best algorithm effectively integrates CMA-ES and DE, strategically using CMA-ES for global exploration and DE with local search for exploitation. In contrast, `AdaptiveCMAESwithLocalSearch` relies solely on CMA-ES with a basic local search triggered by stagnation, limiting its ability to handle complex multimodal landscapes. The second best shows a superior strategy of alternating between exploration and exploitation.

Comparing (1st) vs (2nd), we see that both utilize a hybrid approach. However, `AdaptiveHybridDE_CMAES` demonstrates a more nuanced adaptive strategy by adjusting the population size and employing both DE and CMA-ES in a more integrated manner.  `AdaptiveHybridCMAESDE` runs CMA-ES and DE more separately.

(3rd) `AdaptiveDifferentialEvolutionWithCMAESAndLocalSearch` vs (4th) `AdaptiveDifferentialEvolutionWithLocalSearch`:  The third-ranked algorithm combines DE, CMA-ES, and local search, offering a more robust approach than the fourth, which uses only DE and a local search. The third-ranked algorithm strategically applies CMA-ES for exploration and integrates local search more effectively. The fourth ranked algorithm relies entirely on DE, failing to benefit from the global exploration of CMA-ES.

Comparing (second worst) vs (worst), we see that `AdaptiveCMAESwithLocalSearch` at least attempts a hybrid approach but lacks the adaptive parameter control and sophisticated integration of DE found in the higher-ranked algorithms.  The worst algorithm utilizes a simple DE with a basic local search and lacks any adaptive parameter mechanisms resulting in poor performance across the GNBG benchmarks.

Overall: The higher-ranked algorithms demonstrate the advantages of hybrid approaches, adaptive parameter control, and strategic integration of global exploration (CMA-ES) and local exploitation (DE and local search) for effectively tackling the diverse challenges presented by the GNBG benchmark functions.  The less successful algorithms rely on simpler methods and lack the sophistication required for navigating complex multimodal landscapes.  The use of adaptive population size and local search triggering are key components of success.


**Experience:**

Hybrid approaches combining global exploration and local exploitation, coupled with adaptive parameter tuning and well-timed local searches, are crucial for effectively optimizing high-dimensional, multimodal landscapes.  Adaptive population sizing also greatly enhances performance.

2025-06-22 15:11:50 INFO Full response text: * **Keywords:**  Adaptive, Exploration/Exploitation, Multimodal, High-Dimensional, Niching, Parameter Tuning, Local Search.

* **Advice:** Focus on hybrid algorithms blending global and local search methods.  Incorporate adaptive mechanisms for population size and parameter tuning. Carefully select base optimization algorithms known for their strengths in high-dimensional, multimodal spaces. Experiment with various niching techniques to maintain diversity but avoid over-reliance.

* **Avoid:**  Premature convergence, stagnation in local optima, neglecting the base algorithm's impact, overly complex niching that slows down computation.

* **Explanation:** Effective heuristics require a balance.  Global search prevents premature convergence, while local search refines solutions. Adaptive components ensure robustness across diverse problem landscapes.  Sophisticated niching can improve exploration, but it should complement, not replace, a strong foundation.

2025-06-22 15:11:50 INFO Generating offspring via Crossover...
2025-06-22 15:11:59 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:12:00 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.6800
2025-06-22 15:12:00 INFO FeHistory: [150003.67104339 223586.95142364 157306.23769207 ...  -1081.98379552
  -1081.98379541  -1081.98379628]
2025-06-22 15:12:00 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:12:00 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDE_CMAES
# Description: Hybrid DE/CMA-ES with adaptive parameters and local search for multimodal optimization.
class AdaptiveHybridDE_CMAES:
    """
    Combines Differential Evolution (DE) and CMA-ES for global exploration and local exploitation, 
    adapting parameters and incorporating local search to efficiently handle multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(self.dim / 2)  # Adaptive population size
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.cma = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitnesses = objective_function(population)
        self.eval_count += self.population_size

        best_index = np.argmin(fitnesses)
        self.best_solution_overall = population[best_index]
        self.best_fitness_overall = fitnesses[best_index]

        # Initialize CMA-ES after initial DE evaluation
        self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 10 + int(self.dim/4), 'tolfun':1e-10})


        while self.eval_count < self.budget:
            #Hybrid Approach: Alternate between DE and CMA-ES
            if self.eval_count % 10 < 5 : #DE phase (more frequent initially)
                new_population = np.zeros_like(population)
                for i in range(self.population_size):
                    a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                    mutant = population[a] + self.F * (population[b] - population[c])

                    #Adaptive F and CR
                    if self.eval_count < self.budget * 0.2:
                        self.F = 1.0
                        self.CR = 0.8
                    else:
                        self.F = 0.5
                        self.CR = 0.95

                    trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                    trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
                    trial_fitness = objective_function(trial.reshape(1, -1))[0]
                    self.eval_count += 1

                    if trial_fitness < fitnesses[i]:
                        new_population[i] = trial
                        fitnesses[i] = trial_fitness
                        if trial_fitness < self.best_fitness_overall:
                            self.best_fitness_overall = trial_fitness
                            self.best_solution_overall = trial
                population = new_population
            else: #CMA-ES phase
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses_cma = objective_function(solutions)
                self.eval_count += len(fitnesses_cma)
                self.cma.tell(solutions, fitnesses_cma)

                for i, fitness in enumerate(fitnesses_cma):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]


            # Local Search (periodically)
            if self.eval_count % (self.population_size * 5) == 0:
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:12:00 INFO Unimodal AOCC mean: 0.6800
2025-06-22 15:12:00 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:12:00 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:12:00 INFO AOCC mean: 0.6800
2025-06-22 15:12:11 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:12:12 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:12:12 INFO FeHistory: [ 92133.5326142  180242.73303041 168540.95997961 ... 211118.90416062
 190273.75548883 197027.94059934]
2025-06-22 15:12:12 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:12:12 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:12:12 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:12:12 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:12:12 INFO AOCC mean: 0.0000
2025-06-22 15:12:21 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:12:21 ERROR Can not run the algorithm
2025-06-22 15:12:21 INFO Run function 1 complete. FEHistory len: 241, AOCC: 0.0000
2025-06-22 15:12:21 INFO FeHistory: [152487.14790224 152311.37636663 151932.19374635 152157.31630931
 153116.2491876  152749.04552452 152583.48885635 152390.81440648
 152056.64265141 152594.68164262 151978.41483067 151644.51183114
 152243.57654015 152047.30350068 152324.84525656 152764.77030055
 152493.29378245 153205.46921854 152720.46705254 151876.41389615
 151885.14177617 153223.77840666 152924.53453487 153198.80996499
 152463.45246856 152427.95384284 151644.51183114 152243.57654015
 152047.30350068 152324.84525656 152764.77030055 151562.75575246
 151539.60012401 151824.39982446 152034.75153375 151079.63728081
 151520.39019904 151134.56351887 152917.75630959 151957.79486772
 151937.66045313 150299.30756412 152200.40596496 152515.59721043
 153356.33503845 151737.40885646 152741.87024912 152214.06083022
 151562.75575246 151539.60012401 151824.39982446 152034.75153375
 151079.63728081 150087.07526872 150486.04931248 149556.174678
 150139.53959179 149874.9979612  150954.54860247 153214.56740405
 152339.95065977 151402.91281536 151654.92283666 151393.95886947
 152561.26483663 152438.00773834 152400.91272927 150304.42001998
 152116.78997536 152474.25405595 152380.22639131 151951.54909158
 150087.07526872 150486.04931248 149556.174678   150139.53959179
 149874.9979612  149883.80749935 149401.9221039  149928.41769699
 149507.97947245 149404.5465211  152984.46754781 152198.43674567
 152486.50183328 153390.23345322 154037.94105121 154175.89678821
 150876.71280831 152828.87091637 150563.05000055 152775.46299307
 153156.52805915 152369.59138456 152633.64826675 153648.92541721
 152773.92153866 150901.32189563 149883.80749935 149401.9221039
 149928.41769699 149507.97947245 149404.5465211  149623.88534944
 149937.65276848 150100.40979215 149420.1293347  148738.87325327
 155141.99227358 149041.99659589 151172.97262409 156190.80934856
 153741.94634391 152745.37022152 156351.02998807 151581.50042239
 151309.42928635 152311.66716256 152201.72017016 150475.4663999
 153673.3718666  152024.03189739 149303.47218361 154025.69906846
 155711.14352954 147039.61526567 152283.51940087 149623.88534944
 149937.65276848 150100.40979215 149420.1293347  148738.87325327
 147120.03248823 147329.22387889 146856.48079308 147286.28692775
 147251.3641239  149641.99040567 146058.16370237 149271.15329539
 144933.18038748 150642.10310293 151610.58168633 160795.68603166
 152922.71665681 151922.61864313 155284.59214427 150410.09099508
 153563.22716271 150724.80750021 161217.33966037 151538.56513053
 146624.5649476  151375.75602955 149362.84767502 146100.73404712
 149663.47605461 145750.49896853 153944.51810261 147120.03248823
 147329.22387889 146856.48079308 147286.28692775 147251.3641239
 144920.90025826 144887.83911477 144836.08442894 144476.1969581
 145140.06919738 157437.4054073  150756.0311715  166815.37803581
 145939.44835719 145789.60387739 162590.13304922 153656.55529486
 147108.22897356 147734.7998106  162961.91802964 149752.51897055
 148142.32537287 156975.55067469 144561.43999607 149583.75301263
 142530.87486505 139676.71475032 152720.01805897 147113.13231003
 148225.43412066 157973.31270546 152070.6065604  140593.3213344
 146097.52780631 147891.0008022  153916.81153693 144920.90025826
 144887.83911477 144836.08442894 144476.1969581  145140.06919738
 139756.75845738 139808.08004938 140224.51753878 139911.20857357
 139648.11373068 145941.75103807 144265.95575925 155768.92107052
 142967.8385787  135946.85532875 156595.39167076 149212.16807102
 150053.53504171 141067.54751323 167616.46712608 147613.18167889
 148155.69927892 145952.79450678 148399.21094387 152592.01413483
 150219.69137605 147338.82299063 146020.60751403 138543.01049263
 145140.94574589 131983.32787094 154261.55949517 137923.99573673
 138369.34330022 148380.59467748 155588.84832169 140610.94502718
 140247.23523462 148430.78693535 147654.96181779 154699.71312902
 139756.75845738 139808.08004938 140224.51753878 139911.20857357
 139648.11373068]
2025-06-22 15:12:21 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:12:21 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:12:21 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:12:21 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:12:21 INFO AOCC mean: 0.0000
2025-06-22 15:12:31 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:12:32 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.2808
2025-06-22 15:12:32 INFO FeHistory: [261007.17839558 115532.63304343 194192.52264556 ...  -1081.9837994
  -1081.9837994   -1081.9837994 ]
2025-06-22 15:12:32 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:12:32 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_Improved
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDE_CMAES_Improved
# Description: Hybrid DE & CMA-ES with adaptive parameters and local search for multimodal optimization.
class AdaptiveHybridDE_CMAES_Improved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(dim / 2)
        self.F = 0.8
        self.CR = 0.9
        self.cma = None
        self.local_search_trigger = 1e-3  # Adjust as needed
        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        # Initial evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1, -1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        while self.eval_count < self.budget:
            # Adaptive parameters
            if self.eval_count < self.budget * 0.3:
                self.F = 1.0
                self.CR = 0.8
            elif self.eval_count < self.budget * 0.7:
                self.F = 0.7
                self.CR = 0.9
            else:
                self.F = 0.5
                self.CR = 0.95
            
            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                v = self.population[a] + self.F * (self.population[b] - self.population[c])
                u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                offspring[i] = u

            # Evaluate offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < self.fitness[i]:
                    self.fitness[i] = offspring_fitness[i]
                    self.population[i] = offspring[i]
                    if self.fitness[i] < self.best_fitness_overall:
                        self.best_fitness_overall = self.fitness[i]
                        self.best_solution_overall = self.population[i].copy()

            # Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger and self.cma is None:
                self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})

            if self.cma is not None and self.eval_count < self.budget:
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                #Local Search after CMA-ES update
                result = minimize(lambda x: objective_function(x.reshape(1, -1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:12:32 INFO Unimodal AOCC mean: 0.2808
2025-06-22 15:12:32 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:12:32 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:12:32 INFO AOCC mean: 0.2808
2025-06-22 15:12:42 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:12:42 ERROR Can not run the algorithm
2025-06-22 15:12:43 INFO Run function 1 complete. FEHistory len: 2161, AOCC: 0.8930
2025-06-22 15:12:43 INFO FeHistory: [139447.83477965 159237.08274178 163416.84127552 ...  30388.78114047
  30426.70399964  30456.42313734]
2025-06-22 15:12:43 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:12:43 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_Enhanced
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

class AdaptiveHybridDE_CMAES_Enhanced:
    """
    Combines Differential Evolution (DE) and CMA-ES with adaptive parameters and local search for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.floor(dim / 2))  # Adaptive population size
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.cma = None
        self.local_search_trigger = 1e-3  # Fitness threshold for triggering local search
        self.stagnation_counter = 0
        self.stagnation_limit = 10 # Number of iterations without improvement before triggering CMA-ES


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)

        # Initial evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1, -1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        last_best_fitness = self.best_fitness_overall
        while self.eval_count < self.budget:
            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                v = self.population[a] + self.F * (self.population[b] - self.population[c])
                u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                offspring[i] = u

            # Evaluate offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < self.fitness[i]:
                    self.fitness[i] = offspring_fitness[i]
                    self.population[i] = offspring[i]
                    if self.fitness[i] < self.best_fitness_overall:
                        self.best_fitness_overall = self.fitness[i]
                        self.best_solution_overall = self.population[i].copy()
                        self.stagnation_counter = 0
                
            #Adaptive Parameter Control (Example)
            if self.eval_count > self.budget / 10: #Adjust parameters after 10% of budget.
                 self.F *= 0.95
                 self.CR *= 0.95



            #Adaptive CMA-ES and Local Search
            if self.stagnation_counter >= self.stagnation_limit: #Trigger CMA-ES and local search
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                        self.stagnation_counter = 0


                # Local Search (L-BFGS-B)
                result = minimize(lambda x: objective_function(x.reshape(1, -1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x
                    self.stagnation_counter = 0

            if self.best_fitness_overall == last_best_fitness:
                self.stagnation_counter += 1
            else:
                last_best_fitness = self.best_fitness_overall
                
        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
2025-06-22 15:12:43 INFO Unimodal AOCC mean: 0.8930
2025-06-22 15:12:43 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:12:43 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:12:43 INFO AOCC mean: 0.8930
2025-06-22 15:12:52 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:12:54 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.6033
2025-06-22 15:12:54 INFO FeHistory: [172219.83769037 208342.60721861 181330.42620699 ...  -1081.9837994
  -1081.9837994   -1081.9837994 ]
2025-06-22 15:12:54 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:12:54 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_Improved
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDE_CMAES_Improved
# Description: Hybrid DE & CMA-ES with adaptive parameters and local search for multimodal optimization.
# Code:
class AdaptiveHybridDE_CMAES_Improved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.ceil(self.dim / 2))  #Adaptive
        self.F = 0.8
        self.CR = 0.9
        self.cma = None
        self.local_search_trigger = 1e-3 # Adjust as needed
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        #Initial Evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1,-1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        while self.eval_count < self.budget:
            # Adaptive Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                v = self.population[a] + self.F * (self.population[b] - self.population[c])
                u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                offspring[i] = u

            #Evaluate offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            #Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < self.fitness[i]:
                    self.fitness[i] = offspring_fitness[i]
                    self.population[i] = offspring[i]
                    if self.fitness[i] < self.best_fitness_overall:
                        self.best_fitness_overall = self.fitness[i]
                        self.best_solution_overall = self.population[i].copy()

            #Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger :
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})
                
                #Adaptive Parameter Tuning for CMA-ES based on progress
                if self.eval_count > self.budget * 0.5: #Reduce step size in later stages
                    self.cma.sigma *= 0.8

                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)

                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]

                #Local Search (periodically)
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

            #Adaptive F and CR for DE
            if self.eval_count < self.budget * 0.2:
                self.F = 1.0
                self.CR = 0.8
            else:
                self.F = 0.5
                self.CR = 0.95


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
2025-06-22 15:12:54 INFO Unimodal AOCC mean: 0.6033
2025-06-22 15:12:54 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:12:54 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:12:54 INFO AOCC mean: 0.6033
2025-06-22 15:13:03 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:13:04 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.1393
2025-06-22 15:13:04 INFO FeHistory: [ 1.56261239e+05  1.61401084e+05  1.89097376e+05 ...  2.15709751e+02
 -6.77480992e+01  4.91280223e+02]
2025-06-22 15:13:04 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:13:04 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDECMAESwithAdaptiveLocalSearch
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveHybridDECMAESwithAdaptiveLocalSearch
# Description: Hybrid DE-CMA-ES with adaptive local search and population size for multimodal optimization.
# Code:
class AdaptiveHybridDECMAESwithAdaptiveLocalSearch:
    """
    Combines Differential Evolution (DE) and CMA-ES with adaptive local search 
    and population size for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.floor(dim/2)) #Adaptive population size
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.cma = None
        self.local_search_trigger = 1e-3 # Fitness threshold for triggering local search
        self.de_iterations = 5 # Number of DE iterations before CMA-ES
        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        # Initial evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1,-1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        while self.eval_count < self.budget:
            # Differential Evolution phase
            for _ in range(self.de_iterations):
                offspring = np.zeros_like(self.population)
                for i in range(self.population_size):
                    a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                    v = self.population[a] + self.F * (self.population[b] - self.population[c])
                    u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                    offspring[i] = u

                offspring_fitness = objective_function(offspring)
                self.eval_count += self.population_size

                for i in range(self.population_size):
                    if offspring_fitness[i] < self.fitness[i]:
                        self.fitness[i] = offspring_fitness[i]
                        self.population[i] = offspring[i]
                        if self.fitness[i] < self.best_fitness_overall:
                            self.best_fitness_overall = self.fitness[i]
                            self.best_solution_overall = self.population[i].copy()
                if self.eval_count >= self.budget:
                    break

            #Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger and self.eval_count < self.budget:
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]

                #Local Search
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:13:04 INFO Unimodal AOCC mean: 0.1393
2025-06-22 15:13:04 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:13:04 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:13:04 INFO AOCC mean: 0.1393
2025-06-22 15:13:13 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:13:14 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.9822
2025-06-22 15:13:14 INFO FeHistory: [266804.72475814 154092.16258357 192721.45488749 ...  20613.60261041
  12409.50765174  18966.4705138 ]
2025-06-22 15:13:14 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:13:14 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_with_LocalSearch
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDE_CMAES_with_LocalSearch
# Description: Hybrid DE, CMA-ES, and local search for robust multimodal optimization with adaptive parameters.
# Code:
class AdaptiveHybridDE_CMAES_with_LocalSearch:
    """
    Combines Differential Evolution (DE), CMA-ES, and local search for robust multimodal optimization.
    Adapts DE parameters (F, CR) and CMA-ES population size based on progress.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size_de = 50  # Initial DE population size
        self.population_size_cma = 10 # Initial CMA-ES population size
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.cma = None
        self.local_search_freq = 5


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size_de, self.dim))
        fitness = objective_function(population)
        self.eval_count += self.population_size_de
        self.best_solution_overall = population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)


        generation = 0
        while self.eval_count < self.budget:
            # Differential Evolution
            new_population = np.zeros_like(population)
            for i in range(self.population_size_de):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size_de), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if trial_fitness[0] < fitness[i]:
                    new_population[i] = trial
                    fitness[i] = trial_fitness[0]
                else:
                    new_population[i] = population[i]
            population = new_population
            best_solution_de = population[np.argmin(fitness)]
            best_fitness_de = np.min(fitness)

            if best_fitness_de < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness_de
                self.best_solution_overall = best_solution_de


            # CMA-ES
            if self.cma is None:
                self.cma = CMAEvolutionStrategy(best_solution_de, 20, {'popsize': self.population_size_cma}) #Start wide!

            solutions = self.cma.ask()
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            self.cma.tell(solutions, fitnesses)

            if np.min(fitnesses) < self.best_fitness_overall:
                self.best_fitness_overall = np.min(fitnesses)
                self.best_solution_overall = solutions[np.argmin(fitnesses)]

            #Adaptive parameter adjustment
            if self.cma.result.fbest < 1e-2: # Condition for local search
                self.F *= 0.9
                self.CR *= 0.9
                self.population_size_cma = max(5, int(self.population_size_cma * 0.9))
            else:
                self.F *= 1.1
                self.CR *= 1.1
                self.population_size_cma = min(100, int(self.population_size_cma * 1.1))

            # Local Search
            if generation % self.local_search_freq == 0:
                result = minimize(objective_function, self.best_solution_overall, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x
                    self.eval_count += result.nfev

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
2025-06-22 15:13:14 INFO Unimodal AOCC mean: 0.9822
2025-06-22 15:13:14 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:13:14 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:13:14 INFO AOCC mean: 0.9822
2025-06-22 15:13:24 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:13:25 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.9881
2025-06-22 15:13:25 INFO FeHistory: [244022.33294263 186498.43603602 189199.59324752 ...   5155.42135182
   2422.74640639   3539.67101511]
2025-06-22 15:13:25 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:13:25 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_Improved
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy
import random

# Name: AdaptiveHybridDE_CMAES_Improved
# Description: Hybrid DE & CMA-ES with adaptive local search and population resizing for multimodal optimization.
# Code:
class AdaptiveHybridDE_CMAES_Improved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.floor(dim / 2))  # Adaptive population size
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.cma = None
        self.local_search_trigger = 1e-3  # Fitness threshold for triggering local search
        self.population = None
        self.fitness = None
        self.cma_iterations = 0

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)
        self.cma_iterations = 0

        # Initial Evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1, -1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        while self.eval_count < self.budget:
            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                v = self.population[a] + self.F * (self.population[b] - self.population[c])
                u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                offspring[i] = u

            # Evaluate offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection
            combined_population = np.vstack((self.population, offspring))
            combined_fitness = np.concatenate((self.fitness, offspring_fitness))
            sorted_indices = np.argsort(combined_fitness)
            self.population = combined_population[sorted_indices[:self.population_size]]
            self.fitness = combined_fitness[sorted_indices[:self.population_size]]

            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[best_index]
                self.best_solution_overall = self.population[best_index].copy()


            # Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger and self.cma is None and self.eval_count < self.budget * 0.8:  #Adjust 80% budget
                self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})
            if self.cma is not None and self.cma_iterations < 100: # Limit CMA-ES iterations to prevent excessive calls
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                self.cma_iterations +=1

            #Local Search (less frequent for efficiency)
            if self.eval_count % (self.population_size * 2) == 0:
                result = minimize(lambda x: objective_function(x.reshape(1, -1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:13:25 INFO Unimodal AOCC mean: 0.9881
2025-06-22 15:13:25 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:13:25 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:13:25 INFO AOCC mean: 0.9881
2025-06-22 15:13:34 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:13:35 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.2165
2025-06-22 15:13:35 INFO FeHistory: [165923.30925657 132251.60127834 153316.57029263 ...  -1081.9837991
  -1081.9837992   -1081.98379917]
2025-06-22 15:13:35 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:13:35 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDE_CMAES
# Description: Hybrid DE and CMA-ES with adaptive population size and local search for multimodal landscapes.
class AdaptiveHybridDE_CMAES:
    """
    Combines Differential Evolution (DE) and CMA-ES with adaptive population size and local search 
    for efficiently exploring and exploiting multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(self.dim / 2) # Adaptive population size
        self.F = 0.8  # DE differential weight
        self.CR = 0.9  # DE crossover rate
        self.cma = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitnesses = objective_function(population)
        self.eval_count += self.population_size

        best_index = np.argmin(fitnesses)
        self.best_solution_overall = population[best_index]
        self.best_fitness_overall = fitnesses[best_index]

        #Adaptive phase switching between DE and CMA-ES
        phase = 0 # 0: DE, 1: CMA-ES. Initially DE for exploration

        while self.eval_count < self.budget:
            if phase == 0: #Differential Evolution phase
                new_population = np.zeros_like(population)
                for i in range(self.population_size):
                    a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                    mutant = population[a] + self.F * (population[b] - population[c])
                    trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                    trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
                    trial_fitness = objective_function(trial.reshape(1, -1))[0]
                    self.eval_count += 1
                    if trial_fitness < fitnesses[i]:
                        new_population[i] = trial
                        fitnesses[i] = trial_fitness
                        if trial_fitness < self.best_fitness_overall:
                            self.best_fitness_overall = trial_fitness
                            self.best_solution_overall = trial
                population = new_population
                if self.eval_count > self.budget * 0.3: # Switch to CMA-ES after 30% of the budget
                    phase = 1
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': self.population_size})

            elif phase == 1: #CMA-ES phase
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses_cma = objective_function(solutions)
                self.eval_count += len(fitnesses_cma)
                self.cma.tell(solutions, fitnesses_cma)
                for i, fitness in enumerate(fitnesses_cma):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]

            # Local Search (periodically)
            if self.eval_count % (self.population_size * 5) == 0:
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:13:35 INFO Unimodal AOCC mean: 0.2165
2025-06-22 15:13:35 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:13:35 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:13:35 INFO AOCC mean: 0.2165
2025-06-22 15:13:35 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal & Ill-Conditioned Landscapes (like f1-f6): These have a single optimum, but it may be hidden in a very long, narrow, rotated valley. A good strategy needs to effectively sample the central region of the search space and adapt its search direction.

Rugged Single-Basin Landscapes (like f7-f15): These are filled with numerous, deep local optima. The algorithm must have a robust escape mechanism to avoid getting trapped.

Deceptive Multi-Component Landscapes (like f16-f24): These problems have multiple, separate basins of attraction. The global optimum may be in a small, remote basin, while a larger, more attractive basin leads to a suboptimal solution. A good strategy must have aggressive global exploration capabilities.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).



### Better code
AdaptiveDifferentialEvolutionWithLocalSearch
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveDifferentialEvolutionWithLocalSearch
# Description: Hybrid Differential Evolution with local search and adaptive mutation for escaping local optima in multimodal landscapes.
class AdaptiveDifferentialEvolutionWithLocalSearch:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(self.dim/2) #Adaptive population size
        self.F = 0.8 # Differential weight
        self.CR = 0.9 # Crossover rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitnesses = objective_function(population)
        self.eval_count += self.population_size

        best_index = np.argmin(fitnesses)
        self.best_solution_overall = population[best_index]
        self.best_fitness_overall = fitnesses[best_index]

        while self.eval_count < self.budget:
            new_population = np.zeros_like(population)
            for i in range(self.population_size):
                # Differential Evolution Mutation
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])

                #Adaptive F and CR
                if self.eval_count < self.budget * 0.2 : #Explore more in early stages
                    self.F = 1.0
                    self.CR = 0.8
                else:
                    self.F = 0.5 #Exploit better in late stages
                    self.CR = 0.95

                #Crossover
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1

                #Selection
                if trial_fitness < fitnesses[i]:
                    new_population[i] = trial
                    fitnesses[i] = trial_fitness
                    if trial_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = trial_fitness
                        self.best_solution_overall = trial

            population = new_population

            # Local Search (periodically)
            if self.eval_count % (self.population_size * 5) == 0:
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


### Worse code
AdaptiveCMAESwithLocalSearch
import numpy as np
from scipy.optimize import minimize

# Name: AdaptiveCMAESwithLocalSearch
# Description: Hybrid CMA-ES with local search for escaping local optima in multimodal landscapes.
class AdaptiveCMAESwithLocalSearch:
    """
    Combines CMA-ES for global exploration with local search for exploitation.  Adapts sigma based on progress.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.sigma = 0.5  # Initial step size for CMA-ES
        self.cma = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initialize CMA-ES
        self.cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10, 'tolfun':1e-10})

        while self.eval_count < self.budget:
            # Generate solutions with CMA-ES
            solutions = self.cma.ask()
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            self.cma.tell(solutions, fitnesses)

            # Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solutions[i]

            # Adaptive Sigma Adjustment
            if self.cma.result.fbest < 1e-2: # Condition for local search
                self.sigma *= 0.9 # Reduce sigma if close to optimum
            else:
                self.sigma *= 1.1 # Increase otherwise

            # Local search around best solution
            result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
            if result.fun < self.best_fitness_overall:
                self.best_fitness_overall = result.fun
                self.best_solution_overall = result.x

            self.cma.sigma = self.sigma #Update CMA-ES sigma


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'cma_result': self.cma.result
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

from cma import CMAEvolutionStrategy


### Analyze & experience
- Comparing (best) `AdaptiveHybridDE_CMAES` vs (worst) `AdaptiveDifferentialEvolutionWithLocalSearch` (last one), we see that the best-performing algorithm uses a hybrid approach combining Differential Evolution (DE) and CMA-ES, incorporating adaptive population size and a local search trigger based on fitness. The worst-performing algorithm uses only DE with a simple local search, lacking the sophisticated adaptive mechanisms and global exploration capabilities of CMA-ES.  `AdaptiveHybridDE_CMAES` also features more sophisticated adaptive mechanisms for parameters `F` and `CR` compared to its simpler counterparts.

`(second best)` `AdaptiveHybridCMAESDE` vs (second worst) `AdaptiveCMAESwithLocalSearch`: The second-best algorithm effectively integrates CMA-ES and DE, strategically using CMA-ES for global exploration and DE with local search for exploitation. In contrast, `AdaptiveCMAESwithLocalSearch` relies solely on CMA-ES with a basic local search triggered by stagnation, limiting its ability to handle complex multimodal landscapes. The second best shows a superior strategy of alternating between exploration and exploitation.

Comparing (1st) vs (2nd), we see that both utilize a hybrid approach. However, `AdaptiveHybridDE_CMAES` demonstrates a more nuanced adaptive strategy by adjusting the population size and employing both DE and CMA-ES in a more integrated manner.  `AdaptiveHybridCMAESDE` runs CMA-ES and DE more separately.

(3rd) `AdaptiveDifferentialEvolutionWithCMAESAndLocalSearch` vs (4th) `AdaptiveDifferentialEvolutionWithLocalSearch`:  The third-ranked algorithm combines DE, CMA-ES, and local search, offering a more robust approach than the fourth, which uses only DE and a local search. The third-ranked algorithm strategically applies CMA-ES for exploration and integrates local search more effectively. The fourth ranked algorithm relies entirely on DE, failing to benefit from the global exploration of CMA-ES.

Comparing (second worst) vs (worst), we see that `AdaptiveCMAESwithLocalSearch` at least attempts a hybrid approach but lacks the adaptive parameter control and sophisticated integration of DE found in the higher-ranked algorithms.  The worst algorithm utilizes a simple DE with a basic local search and lacks any adaptive parameter mechanisms resulting in poor performance across the GNBG benchmarks.

Overall: The higher-ranked algorithms demonstrate the advantages of hybrid approaches, adaptive parameter control, and strategic integration of global exploration (CMA-ES) and local exploitation (DE and local search) for effectively tackling the diverse challenges presented by the GNBG benchmark functions.  The less successful algorithms rely on simpler methods and lack the sophistication required for navigating complex multimodal landscapes.  The use of adaptive population size and local search triggering are key components of success.
- * **Keywords:**  Adaptive, Exploration/Exploitation, Multimodal, High-Dimensional, Niching, Parameter Tuning, Local Search.

* **Advice:** Focus on hybrid algorithms blending global and local search methods.  Incorporate adaptive mechanisms for population size and parameter tuning. Carefully select base optimization algorithms known for their strengths in high-dimensional, multimodal spaces. Experiment with various niching techniques to maintain diversity but avoid over-reliance.

* **Avoid:**  Premature convergence, stagnation in local optima, neglecting the base algorithm's impact, overly complex niching that slows down computation.

* **Explanation:** Effective heuristics require a balance.  Global search prevents premature convergence, while local search refines solutions. Adaptive components ensure robustness across diverse problem landscapes.  Sophisticated niching can improve exploration, but it should complement, not replace, a strong foundation.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-22 15:13:35 INFO Mutation prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal & Ill-Conditioned Landscapes (like f1-f6): These have a single optimum, but it may be hidden in a very long, narrow, rotated valley. A good strategy needs to effectively sample the central region of the search space and adapt its search direction.

Rugged Single-Basin Landscapes (like f7-f15): These are filled with numerous, deep local optima. The algorithm must have a robust escape mechanism to avoid getting trapped.

Deceptive Multi-Component Landscapes (like f16-f24): These problems have multiple, separate basins of attraction. The global optimum may be in a small, remote basin, while a larger, more attractive basin leads to a suboptimal solution. A good strategy must have aggressive global exploration capabilities.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).



Current heuristics:
AdaptiveCMAESwithLocalSearch
import numpy as np
from scipy.optimize import minimize

# Name: AdaptiveCMAESwithLocalSearch
# Description: Hybrid CMA-ES with local search for escaping local optima in multimodal landscapes.
class AdaptiveCMAESwithLocalSearch:
    """
    Combines CMA-ES for global exploration with local search for exploitation.  Adapts sigma based on progress.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.sigma = 0.5  # Initial step size for CMA-ES
        self.cma = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initialize CMA-ES
        self.cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10, 'tolfun':1e-10})

        while self.eval_count < self.budget:
            # Generate solutions with CMA-ES
            solutions = self.cma.ask()
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            self.cma.tell(solutions, fitnesses)

            # Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solutions[i]

            # Adaptive Sigma Adjustment
            if self.cma.result.fbest < 1e-2: # Condition for local search
                self.sigma *= 0.9 # Reduce sigma if close to optimum
            else:
                self.sigma *= 1.1 # Increase otherwise

            # Local search around best solution
            result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
            if result.fun < self.best_fitness_overall:
                self.best_fitness_overall = result.fun
                self.best_solution_overall = result.x

            self.cma.sigma = self.sigma #Update CMA-ES sigma


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'cma_result': self.cma.result
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

from cma import CMAEvolutionStrategy


Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- * **Keywords:**  Adaptive, Exploration/Exploitation, Multimodal, High-Dimensional, Niching, Parameter Tuning, Local Search.

* **Advice:** Focus on hybrid algorithms blending global and local search methods.  Incorporate adaptive mechanisms for population size and parameter tuning. Carefully select base optimization algorithms known for their strengths in high-dimensional, multimodal spaces. Experiment with various niching techniques to maintain diversity but avoid over-reliance.

* **Avoid:**  Premature convergence, stagnation in local optima, neglecting the base algorithm's impact, overly complex niching that slows down computation.

* **Explanation:** Effective heuristics require a balance.  Global search prevents premature convergence, while local search refines solutions. Adaptive components ensure robustness across diverse problem landscapes.  Sophisticated niching can improve exploration, but it should complement, not replace, a strong foundation.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-22 15:14:15 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:14:15 ERROR Can not run the algorithm
2025-06-22 15:14:15 INFO Run function 1 complete. FEHistory len: 208, AOCC: 0.9908
2025-06-22 15:14:15 INFO FeHistory: [164520.20612099 205109.62072453 169418.60253232 206155.7469716
 214877.59238633 218900.94723972 134135.45982183 146820.89918693
 171081.05642774 181889.20605137 163905.15768207 191591.32629805
 262614.84785671 203904.43076845 269622.74849642 169609.44379704
 252780.38585099 253053.02064157 205870.04490193 198545.1601969
 225069.24186748 134135.45982183 134135.4598201  134135.45982143
 134135.45982191 134135.45982436 134135.45982385 134135.45982208
 134135.45982201 134135.45982316 134135.4598209  134135.45982111
 134135.45982262 134135.45982154 134135.45982125 134135.45982106
 134135.45982351 134135.45982302 134135.45982114 134135.45982168
 134135.45982231 134135.45982311 134135.45982107 134135.45982035
 134135.45982426 134135.45981936 134135.45982191 134135.45981897
 134135.45982425 134135.4598219  134135.45982215 134135.45982186
  48746.98551155  48746.98551327  48746.98551195  48746.98551147
  48746.98551018  48746.98551068  48746.9855113   48746.98551137
  48746.98551022  48746.98551248  48746.98551227  48746.98551075
  48746.98551184  48746.98551213  48746.98551232  48746.98551106
  48746.98551036  48746.98551224  48746.9855117   48746.98551107
  48746.98551052  48746.98551231  48746.98551105  48746.98551015
  48746.98551017  48746.98551147  48746.98551081  48746.98551088
  48746.98551148  48746.98551123  48746.98551152  -1081.98379376
  -1081.98379376  -1081.98379376  -1081.98379376  -1081.98379376
  -1081.98379376  -1081.98379376  -1081.98379376  -1081.98379376
  -1081.98379376  -1081.98379376  -1081.98379376  -1081.98379376
  -1081.98379376  -1081.98379376  -1081.98379376  -1081.98379376
  -1081.98379376  -1081.98379376  -1081.98379376  -1081.98379376
  -1081.98379376  -1081.98379376  -1081.98379376  -1081.98379376
  -1081.98379376  -1081.98379376  -1081.98379376  -1081.98379376
  -1081.98379376  -1081.98379376  -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994  209980.76241174]
2025-06-22 15:14:15 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:14:15 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveHybridDE_CMAES
# Description: A hybrid Differential Evolution and CMA-ES algorithm with adaptive population size and local search for efficiently solving multimodal, high-dimensional optimization problems.

class AdaptiveHybridDE_CMAES:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.popsize = 10  # Initial population size for DE
        self.F = 0.8 # DE scaling factor
        self.CR = 0.9 # DE crossover rate
        self.cma = None # CMA-ES object

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.popsize, self.dim))
        fitnesses = objective_function(population)
        self.eval_count += self.popsize


        while self.eval_count < self.budget:
            # Differential Evolution
            for i in range(self.popsize):
                a, b, c = np.random.choice(np.delete(np.arange(self.popsize), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1,-1))[0]
                self.eval_count += 1
                if trial_fitness < fitnesses[i]:
                    population[i] = trial
                    fitnesses[i] = trial_fitness

            # Update best solution from DE
            best_index_de = np.argmin(fitnesses)
            if fitnesses[best_index_de] < self.best_fitness_overall:
                self.best_fitness_overall = fitnesses[best_index_de]
                self.best_solution_overall = population[best_index_de]

            # Adaptive switch to CMA-ES for fine-tuning
            if self.best_fitness_overall < 1e-2 and self.cma is None:
                self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 10, 'tolfun': 1e-12})


            if self.cma:
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses_cma = objective_function(solutions)
                self.eval_count += len(fitnesses_cma)
                self.cma.tell(solutions, fitnesses_cma)

                best_index_cma = np.argmin(fitnesses_cma)
                if fitnesses_cma[best_index_cma] < self.best_fitness_overall:
                    self.best_fitness_overall = fitnesses_cma[best_index_cma]
                    self.best_solution_overall = solutions[best_index_cma]


            #Adaptive population size (increase for exploration, decrease for exploitation)
            if self.best_fitness_overall > 1e-1:
                self.popsize = int(min(self.popsize * 1.1, 50)) #cap at 50
            else:
                self.popsize = int(max(self.popsize * 0.9, 5)) #cap at 5

            #Local Search
            result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
            if result.fun < self.best_fitness_overall:
                self.best_fitness_overall = result.fun
                self.best_solution_overall = result.x
                

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:14:15 INFO Unimodal AOCC mean: 0.9908
2025-06-22 15:14:15 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:14:15 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:14:15 INFO AOCC mean: 0.9908
2025-06-22 15:14:15 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:14:15 ERROR Can not run the algorithm
2025-06-22 15:14:15 INFO Run function 1 complete. FEHistory len: 3824, AOCC: 0.8902
2025-06-22 15:14:15 INFO FeHistory: [148388.08335127 182812.1466592  152595.99053699 ...   5984.54788678
   4250.2765752    3705.72288113]
2025-06-22 15:14:15 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:14:15 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionCMAES
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveDifferentialEvolutionCMAES
# Description: Hybrid Differential Evolution and CMA-ES with adaptive local search for escaping local optima in multimodal landscapes.
class AdaptiveDifferentialEvolutionCMAES:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(3*np.log(self.dim)) # Adaptive population size
        self.F = 0.8 #Differential Evolution scaling factor
        self.CR = 0.9 #Differential Evolution crossover rate
        self.cma = None
        self.local_search_trigger = 0.1 # Fraction of budget before triggering local search


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitnesses = objective_function(population)
        self.eval_count += self.population_size


        for i in range(self.population_size):
            if fitnesses[i] < self.best_fitness_overall:
                self.best_fitness_overall = fitnesses[i]
                self.best_solution_overall = population[i]

        #Main loop
        while self.eval_count < self.budget:
            #Differential Evolution
            offspring = np.zeros_like(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size),i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1,-1))[0]
                self.eval_count += 1
                if trial_fitness < fitnesses[i]:
                    offspring[i] = trial
                    fitnesses[i] = trial_fitness
                else:
                    offspring[i] = population[i]

            population = offspring

            # Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = population[i]

            # Adaptive CMA-ES and Local Search
            if self.eval_count > self.budget * self.local_search_trigger:
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5})
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                cma_fitnesses = objective_function(solutions)
                self.eval_count += len(cma_fitnesses)
                self.cma.tell(solutions, cma_fitnesses)
                
                #Local Search
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:14:15 INFO Unimodal AOCC mean: 0.8902
2025-06-22 15:14:15 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:14:15 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:14:15 INFO AOCC mean: 0.8902
2025-06-22 15:14:15 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:14:15 ERROR Can not run the algorithm
2025-06-22 15:14:15 INFO Run function 1 complete. FEHistory len: 149, AOCC: 0.9911
2025-06-22 15:14:15 INFO FeHistory: [192554.15671078 191087.26239303 141890.61672301 232988.65197889
 221635.99252854 188851.60822325 187297.37834232 230611.15026626
 102012.21558419 180254.21258621 157698.53402239 249641.38158558
 261933.16942843 183877.93000863 211902.17625076 211538.25472955
 253361.19677176 233348.08336429 219058.49069325 177494.09321356
 201646.16990511 102012.21558419 102012.21558504 102012.21558166
 102012.21558619 102012.21558435 102012.21558632 102012.21558407
 102012.21558589 102012.21558532 102012.21558319 102012.21558341
 102012.21558409 102012.21558266 102012.21558345 102012.21558341
 102012.21558481 102012.21558497 102012.21558383 102012.21558532
 102012.21558591 102012.21558443 102012.21558399 102012.21558465
 102012.215585   102012.21558319 102012.21558392 102012.21558369
 102012.21558567 102012.21558657 102012.21558395 102012.21558342
  41496.86226419  41496.86226333  41496.86226289  41496.86226379
  41496.86226403  41496.86226332  41496.86226431  41496.86226319
  41496.86226306  41496.86226519  41496.86226497  41496.86226428
  41496.86226278  41496.86226493  41496.86226496  41496.8622637
  41496.86226341  41496.86226455  41496.86226305  41496.86226334
  41496.86226395  41496.86226439  41496.86226373  41496.86226338
  41496.86226519  41496.86226446  41496.86226469  41496.86226352
  41496.86226366  41496.86226442  41496.86226496  -1081.98379858
  -1081.98379858  -1081.98379858  -1081.98379858  -1081.98379858
  -1081.98379858  -1081.98379858  -1081.98379858  -1081.98379858
  -1081.98379858  -1081.98379858  -1081.98379858  -1081.98379858
  -1081.98379858  -1081.98379858  -1081.98379858  -1081.98379858
  -1081.98379858  -1081.98379858  -1081.98379858  -1081.98379858
  -1081.98379858  -1081.98379858  -1081.98379858  -1081.98379858
  -1081.98379858  -1081.98379858  -1081.98379858  -1081.98379858
  -1081.98379858  -1081.98379858  -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994  167201.09424275 141489.4094269   73527.74236584
 183018.91576016]
2025-06-22 15:14:15 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:14:15 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionCMAES
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveDifferentialEvolutionCMAES
# Description: Hybrid Differential Evolution and CMA-ES with adaptive population size and local search for robust multimodal optimization.
class AdaptiveDifferentialEvolutionCMAES:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.popsize = 10  # Initial population size
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Differential Evolution crossover rate
        self.sigma = 0.5 #Initial CMA-ES step size

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.popsize, self.dim))
        fitnesses = objective_function(population)
        self.eval_count += self.popsize

        best_index = np.argmin(fitnesses)
        self.best_solution_overall = population[best_index]
        self.best_fitness_overall = fitnesses[best_index]


        while self.eval_count < self.budget:
            #Differential Evolution
            offspring = np.zeros_like(population)
            for i in range(self.popsize):
                a, b, c = np.random.choice(np.delete(np.arange(self.popsize), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1,-1))[0]
                self.eval_count += 1
                if trial_fitness < fitnesses[i]:
                    offspring[i] = trial
                    fitnesses[i] = trial_fitness
                    if trial_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = trial_fitness
                        self.best_solution_overall = trial

            population = offspring


            #Adaptive Population Size
            if self.best_fitness_overall < acceptance_threshold:
                self.popsize = int(max(3, self.popsize*0.8))
            else:
                self.popsize = int(min(100, self.popsize*1.1)) #Upper bound on popsize



            #CMA-ES for local refinement near the end
            if self.eval_count > 0.8*self.budget:  # Switch to CMA-ES for local refinement
                cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10, 'tolfun':1e-10})
                for _ in range(min(self.budget - self.eval_count, 50)): #limit the number of cma iterations
                    solutions = cma.ask()
                    solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                    fitnesses_cma = objective_function(solutions)
                    self.eval_count += len(fitnesses_cma)
                    cma.tell(solutions, fitnesses_cma)
                    for i, fitness in enumerate(fitnesses_cma):
                        if fitness < self.best_fitness_overall:
                            self.best_fitness_overall = fitness
                            self.best_solution_overall = solutions[i]

            
            #Local Search
            result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
            if result.fun < self.best_fitness_overall:
                self.best_fitness_overall = result.fun
                self.best_solution_overall = result.x


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'population_size_history': [self.popsize] #add history of popsize
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:14:15 INFO Unimodal AOCC mean: 0.9911
2025-06-22 15:14:15 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:14:15 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:14:15 INFO AOCC mean: 0.9911
2025-06-22 15:14:15 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:14:16 ERROR Can not run the algorithm
2025-06-22 15:14:16 INFO Run function 1 complete. FEHistory len: 8375, AOCC: 0.1685
2025-06-22 15:14:16 INFO FeHistory: [235447.71265797 172652.99277834 189921.39300917 ...  -1081.9837994
  -1081.9837994   -1081.9837994 ]
2025-06-22 15:14:16 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:14:16 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
from scipy.optimize import differential_evolution

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A hybrid differential evolution algorithm with clustering for escaping local optima and enhanced exploration in multimodal landscapes.
class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.ceil(self.dim/2)) #Adaptive population size


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        bounds = list(zip(self.lower_bounds, self.upper_bounds))
        
        while self.eval_count < self.budget:
            #Differential Evolution
            result = differential_evolution(lambda x: objective_function(x.reshape(1,-1))[0], bounds, maxiter=10, popsize=self.population_size,seed=self.eval_count)
            
            if result.fun < self.best_fitness_overall:
                self.best_fitness_overall = result.fun
                self.best_solution_overall = result.x

            self.eval_count += result.nfev


            #Adaptive Population Size
            if result.nit < 5 : #Increase population if not much progress
                self.population_size = min(self.population_size * 2, 100)
            elif self.best_fitness_overall < 1e-2: # Decrease if close to optimum
                self.population_size = max(self.population_size /2, 10)
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'differential_evolution_result':result
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:14:16 INFO Unimodal AOCC mean: 0.1685
2025-06-22 15:14:16 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:14:16 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:14:16 INFO AOCC mean: 0.1685
2025-06-22 15:14:16 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:14:17 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:14:17 INFO FeHistory: [140270.55385884 165277.34808328 223951.94328701 ... 188430.25466596
 178557.47677905 237790.21936801]
2025-06-22 15:14:17 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:14:17 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:14:17 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:14:17 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:14:17 INFO AOCC mean: 0.0000
2025-06-22 15:14:24 WARNING No replacements made in template string. Returning None.
2025-06-22 15:14:31 WARNING No replacements made in template string. Returning None.
2025-06-22 15:14:39 WARNING No replacements made in template string. Returning None.
2025-06-22 15:14:39 INFO Generation 3, best so far: 0.9919197902331691
2025-06-22 15:14:39 INFO Population length is: 11
2025-06-22 15:14:39 INFO --- Performing Long-Term Reflection at Generation 3 ---
2025-06-22 15:14:43 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveDifferentialEvolutionCMAES vs (worst) AdaptiveHybridPSO_CMAES, we see that the best utilizes a hybrid approach combining Differential Evolution (DE) and CMA-ES more effectively, adapting population size based on dimensionality. The worst uses PSO and CMA-ES, lacking the adaptive mechanisms for parameter tuning and population size adjustment seen in the best.  (second best) AdaptiveHybridDE_CMAES vs (second worst) AdaptiveHybridDECMAESwithAdaptiveLocalSearch shows similar hybrid approaches, but the second best incorporates more sophisticated adaptive parameter tuning (F and CR in DE) and a more robust local search. Comparing (1st) vs (2nd), we see that the top performer directly incorporates a local search within its main loop while the second-best uses it periodically. (3rd) AdaptiveHybridDE_CMAES_Improved vs (4th) AdaptiveHybridDE_CMAES_Improved (same name, different code) reveals almost identical code with slight differences in parameter adaptation and local search trigger, but the performance discrepancy suggests that these small differences can affect the results substantially. Comparing (second worst) AdaptiveHybridDECMAESwithAdaptiveLocalSearch vs (worst) AdaptiveHybridPSO_CMAES, we see that the second-worst uses a more appropriate hybrid DE-CMA-ES approach whereas the worst algorithm's PSO-CMA-ES combination seems less effective for this benchmark. Overall: The best-performing algorithms effectively integrate DE and CMA-ES, utilizing adaptive parameter tuning and local search strategies tailored to the specific challenges posed by the GNBG benchmark functions. The simpler hybrid approaches and those relying on PSO show significantly lower performance.


**Experience:**

Effective hybrid algorithms require careful consideration of algorithm selection and parameter adaptation.  Adaptive population sizing and strategically timed local searches are crucial for high-dimensional, multimodal optimization problems.  The choice of local search method also significantly impacts performance.

2025-06-22 15:14:44 INFO Full response text: **Keywords:** Heuristic design,  adaptive algorithms,  multimodal optimization, exploration-exploitation tradeoff,  parameter control.

**Advice:** Focus on specific algorithm weaknesses.  Quantify exploration/exploitation balance.  Design adaptive mechanisms with clear triggers (e.g., fitness stagnation, diversity loss).  Systematically compare different local search methods and their parameterizations.

**Avoid:** Vague generalizations ("crucial," "greatly enhances").  Overreliance on adaptive mechanisms without a strong base algorithm.  Ignoring the computational cost of adaptation.  Failing to empirically validate design choices.

**Explanation:**  The ineffective reflection lacks concrete action.  Effective heuristics require detailed analysis, controlled experiments, and rigorous evaluation of specific components, not just general assertions about hybrid approaches.  The improved reflection guides the design process by identifying areas needing precise attention and measurement.

2025-06-22 15:14:44 INFO Generating offspring via Crossover...
2025-06-22 15:14:56 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:14:56 ERROR Can not run the algorithm
2025-06-22 15:14:56 INFO Run function 1 complete. FEHistory len: 1616, AOCC: 0.0000
2025-06-22 15:14:56 INFO FeHistory: [115537.8565244  115742.83324604 115946.21198811 ...  66062.39212129
  63641.87797236  65371.91278994]
2025-06-22 15:14:56 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:14:56 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:14:56 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:14:56 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:14:56 INFO AOCC mean: 0.0000
2025-06-22 15:15:05 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:15:06 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.3843
2025-06-22 15:15:06 INFO FeHistory: [218579.17255086 155527.88520217 138209.19823247 ...  -1081.9837994
  -1081.9837994   -1081.9837994 ]
2025-06-22 15:15:06 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:15:06 INFO Good algorithm:
Algorithm Name: AdaptiveDE_CMAES_withLocalSearch
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveDE_CMAES_withLocalSearch
# Description: Hybrid DE & CMA-ES with adaptive parameters and local search for multimodal optimization.
# Code:
class AdaptiveDE_CMAES_withLocalSearch:
    """Combines Differential Evolution (DE) for global exploration and CMA-ES for local exploitation, 
       adapting parameters and incorporating local search for efficient multimodal optimization."""
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.ceil(self.dim / 2))
        self.F = 0.8
        self.CR = 0.9
        self.cma = None
        self.local_search_trigger = 1e-3
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        #Initial Evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1,-1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        while self.eval_count < self.budget:
            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                v = self.population[a] + self.F * (self.population[b] - self.population[c])
                u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                offspring[i] = u

            #Evaluate offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            #Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < self.fitness[i]:
                    self.fitness[i] = offspring_fitness[i]
                    self.population[i] = offspring[i]
                    if self.fitness[i] < self.best_fitness_overall:
                        self.best_fitness_overall = self.fitness[i]
                        self.best_solution_overall = self.population[i].copy()

            # Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger:
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})

                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)

                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]

                #Local Search
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

            #Adaptive F and CR for DE
            if self.eval_count < self.budget * 0.2:
                self.F = 1.0
                self.CR = 0.8
            else:
                self.F = 0.5
                self.CR = 0.95


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:15:06 INFO Unimodal AOCC mean: 0.3843
2025-06-22 15:15:06 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:15:06 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:15:06 INFO AOCC mean: 0.3843
2025-06-22 15:15:16 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:15:16 ERROR Can not run the algorithm
2025-06-22 15:15:16 INFO Run function 1 complete. FEHistory len: 198, AOCC: 0.9914
2025-06-22 15:15:16 INFO FeHistory: [147639.5753264  147659.12523969 147623.95994214 147459.18151299
 147446.05193112 147980.24888482 147763.98140346 147235.31513077
 148348.56794482 147303.22186759 148228.74684275 147474.1932495
 147388.52023152 147658.36522937 147091.98950605 147368.79005799
 147091.98950605 147091.9895052  147091.98950353 147091.98950668
 147091.98950468 147091.98950648 147091.98950644 147091.98950854
 147091.98950651 147091.98950653 147091.98950638 147091.98950697
 147091.98950369 147091.98950743 147091.9895044  147091.98950686
 147091.98950439 147091.98950747 147091.98950436 147091.98950846
 147091.98950754 147091.98950672 147091.98950645 147091.98950719
 147091.9895067  147091.98950559 147091.98950354 147091.98950586
 147091.98950642 147091.98950743 147091.98950393  60589.48123961
  60589.48124045  60589.48123831  60589.48123921  60589.48124097
  60589.48123918  60589.48123922  60589.48123861  60589.48123915
  60589.48123912  60589.48123928  60589.48123869  60589.4812382
  60589.48123855  60589.48123882  60589.48123912  60589.48123815
  60589.48123818  60589.4812413   60589.48123876  60589.48123857
  60589.48123893  60589.48123921  60589.48123846  60589.48123896
  60589.48124006  60589.48123887  60589.48123979  60589.48123924
  60589.48123866  60589.48123882  -1081.9837966   -1081.9837966
  -1081.9837966   -1081.9837966   -1081.9837966   -1081.9837966
  -1081.9837966   -1081.9837966   -1081.9837966   -1081.9837966
  -1081.9837966   -1081.9837966   -1081.9837966   -1081.9837966
  -1081.9837966   -1081.9837966   -1081.9837966   -1081.9837966
  -1081.9837966   -1081.9837966   -1081.9837966   -1081.9837966
  -1081.9837966   -1081.9837966   -1081.9837966   -1081.9837966
  -1081.9837966   -1081.9837966   -1081.9837966   -1081.9837966
  -1081.9837966   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
 147287.55309911 147346.89221628 147166.66189019 147492.60150992
 146845.73941539 147639.79009808 147055.03941602 147973.06984833
 147828.35463072 148170.78718011 147474.1932495  147388.52023152
 147658.36522937 147091.98950605 147368.79005799 146695.64757017
 147866.48238793 147460.02535488 147092.47833158 147400.2618047
 147850.19585561 148072.54376921 147447.27457927 146927.49672165
 147245.05848747 147315.57617001 147903.05210842 146749.85119843
 148140.43888238 147253.90363806 146695.64757017 147866.48238793
 147460.02535488 147092.47833158 147400.2618047  146159.77315962
 146789.44872386 146944.38515046 147097.41032049 146027.2782298
 148687.28118897 147481.43277807 147345.44113879 147466.38869057
 148066.17060825 147518.88154105 146461.95875207 148217.41580646
 146159.77315962 146789.44872386 146944.38515046 147097.41032049
 146027.2782298  145864.55451152 145415.96970469 145915.33382096
 146386.77671411 145805.88824695]
2025-06-22 15:15:16 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:15:16 INFO Good algorithm:
Algorithm Name: AdaptiveHybridCMAESDEwithLocalSearch
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridCMAESDEwithLocalSearch
# Description: Hybrid CMA-ES and DE with adaptive population size and local search for multimodal optimization.
class AdaptiveHybridCMAESDEwithLocalSearch:
    """
    Combines CMA-ES for global exploration and DE for local exploitation with adaptive population size and local search.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.popsize = 10 #Initial population size
        self.F = 0.8 # DE scaling factor
        self.CR = 0.9 # DE crossover rate
        self.local_search_freq = 5 # Perform local search every 5 iterations

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.tile(self.best_solution_overall, (self.popsize, 1)) + np.random.normal(0, 0.5, (self.popsize, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)

        iteration = 0
        while self.eval_count < self.budget:
            fitnesses = objective_function(population)
            self.eval_count += self.popsize

            # Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = population[i]

            #Differential Evolution Step
            offspring = self.differential_evolution(population, fitnesses)
            
            #CMA-ES Step (Global exploration)
            if iteration == 0: #Initialize CMA-ES only once.
                self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5})
            cma_solutions = self.cma.ask()
            cma_solutions = np.clip(cma_solutions, self.lower_bounds, self.upper_bounds)
            cma_fitnesses = objective_function(cma_solutions)
            self.eval_count += len(cma_fitnesses)
            self.cma.tell(cma_solutions, cma_fitnesses)

            for i, fitness in enumerate(cma_fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = cma_solutions[i]


            #Adaptive Population Size
            if self.cma.result.fbest < self.best_fitness_overall * 0.9 : #Check for progress; adjust popsize
              self.popsize = int(self.popsize * 1.2)
            elif self.cma.result.fbest > self.best_fitness_overall * 1.1:
              self.popsize = int(self.popsize * 0.8)
              self.popsize = max(5, self.popsize) # Minimum popsize

            population = np.concatenate((offspring, cma_solutions))
            population = np.clip(population, self.lower_bounds, self.upper_bounds)
            
            #Local Search
            if iteration % self.local_search_freq == 0:
                self.local_search(objective_function)
            
            iteration += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'popsize_history' : self.popsize
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def differential_evolution(self, population, fitnesses):
        offspring = []
        for i in range(self.popsize):
            a, b, c = np.random.choice(np.delete(np.arange(self.popsize), i), 3, replace=False)
            v = population[a] + self.F * (population[b] - population[c])
            u = np.clip(self.crossover(population[i], v), self.lower_bounds, self.upper_bounds)
            if objective_function(u.reshape(1, -1))[0] < fitnesses[i]:
                offspring.append(u)
            else:
                offspring.append(population[i])
        return np.array(offspring)
            

    def crossover(self, x, v):
        u = np.copy(x)
        indices = np.random.rand(self.dim) < self.CR
        u[indices] = v[indices]
        return u
    
    def local_search(self, objective_function):
        result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
        if result.fun < self.best_fitness_overall:
            self.best_fitness_overall = result.fun
            self.best_solution_overall = result.x

def objective_function(x):
    #Example objective function for testing: Sphere function
    return np.sum(x**2, axis=1)
2025-06-22 15:15:16 INFO Unimodal AOCC mean: 0.9914
2025-06-22 15:15:16 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:15:16 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:15:16 INFO AOCC mean: 0.9914
2025-06-22 15:15:27 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:15:27 ERROR Can not run the algorithm
2025-06-22 15:15:27 INFO Run function 1 complete. FEHistory len: 601, AOCC: 0.0000
2025-06-22 15:15:27 INFO FeHistory: [186879.36180001 186528.42525634 187152.77206496 187010.77139781
 187727.09160841 186402.92690988 186746.70966934 186827.35867694
 186572.02917856 186836.36703933 187417.16496847 186757.49827419
 186514.55515794 186077.43501025 186283.71624879 186056.52838989
 187511.33664364 186518.39750179 186278.16816839 186908.91935894
 186862.40072245 186487.97987955 186891.68556371 187775.17375698
 187832.0144616  186773.7891703  186757.49827419 186514.55515794
 186077.43501025 186283.71624879 186056.52838989 185840.84776576
 186011.15906015 186411.00904    185534.34986237 186142.49823988
 186715.78179072 186641.08152449 186657.80013362 187093.0138139
 187882.61157187 187910.76263179 186296.05346905 187050.04283551
 186338.05754882 187588.98343288 185840.84776576 186011.15906015
 186411.00904    185534.34986237 186142.49823988 185305.13620729
 185554.25960581 185875.90389018 185888.4456519  185240.02556826
 187247.77249497 187200.7879712  186950.95463848 187452.51421808
 186979.07777173 187814.44032199 186631.07041663 188149.50974043
 187607.78103718 186081.52542798 185305.13620729 185554.25960581
 185875.90389018 185888.4456519  185240.02556826 184994.84327288
 184938.59225413 185225.00872442 185425.11150538 185591.92375609
 188204.35405426 185993.50657049 187027.01712091 185966.41892995
 188237.79702555 187872.46387129 188601.76382015 186871.65316745
 186771.15242294 188577.75638028 184994.84327288 184938.59225413
 185225.00872442 185425.11150538 185591.92375609 184623.9514346
 184579.05421536 185105.31091813 185413.76386329 185106.55635203
 184034.01687161 184704.60131113 187301.33699189 184778.02498634
 186105.76990184 187994.85057173 186784.48244883 187743.37187909
 189769.72607265 186854.39344993 184623.9514346  184579.05421536
 185105.31091813 185413.76386329 185106.55635203 184240.8803353
 184288.86743538 185202.15583874 184370.6806972  184355.89182601
 188776.48438976 190890.01841755 184571.75480087 188790.60474972
 185470.32501778 183910.28829546 183699.67644794 187390.44415721
 184707.96903749 185665.71557959 184240.8803353  184288.86743538
 185202.15583874 184370.6806972  184355.89182601 183644.71754533
 184344.28536876 184665.66830401 184218.05688624 184349.27574255
 184701.33267419 189358.95754795 195381.5758859  182094.73962757
 187713.89005211 189617.98798113 188681.33020726 185102.21900889
 191111.01369085 185880.46284464 183644.71754533 184344.28536876
 184665.66830401 184218.05688624 184349.27574255 183390.61751021
 183972.6816671  184615.57375685 183849.17658718 183802.17173263
 186670.72171853 191649.50659744 202165.61376571 180650.54358074
 186174.15842914 188126.60693578 183395.38915275 187047.8941929
 182137.41717063 189700.21916812 183390.61751021 183972.6816671
 184615.57375685 183849.17658718 183802.17173263 182809.81226177
 183513.0262758  183554.28842268 183656.74588228 183699.9946209
 181799.66480855 198015.35613475 185281.48397148 184015.8806323
 188315.26925863 184440.99861372 194656.50229804 200232.03792581
 182630.42478838 200215.38622817 182809.81226177 183513.0262758
 183554.28842268 183656.74588228 183699.9946209  182804.1055645
 183165.13108441 182679.67860802 183009.51312114 182691.05376058
 200944.83669684 217636.09174459 186323.22216079 199451.23009868
 187192.50090243 211069.57731477 192671.29018111 197594.33033121
 202704.1389537  201078.92305436 182804.1055645  183165.13108441
 182679.67860802 183009.51312114 182691.05376058 182555.36726982
 182119.27915962 182533.70550356 182846.80513399 182675.94068589
 209336.88595848 198460.27225285 197780.46709305 200147.62900402
 195881.24742472 213818.29898766 194203.10076561 196153.27518717
 227885.07275908 178396.49081783 182555.36726982 182119.27915962
 182533.70550356 182846.80513399 182675.94068589 182073.04619158
 182819.77266999 181850.57815424 182845.83189206 182597.25184626
 189848.23635513 192725.72652575 197026.16222268 201324.27491197
 242228.99337103 231655.03301443 244532.024562   236914.19393094
 220039.33675219 199515.59841242 182073.04619158 182819.77266999
 181850.57815424 182845.83189206 182597.25184626 181300.28223856
 181735.53541361 182208.34760863 181547.24935284 181696.52727768
 180100.43928109 269306.04511047 232458.57056626 192924.18111989
 228772.97727318 234098.10935528 202772.1422003  213543.15400351
 232145.58593464 191276.33742198 181300.28223856 181735.53541361
 182208.34760863 181547.24935284 181696.52727768 181074.17347501
 181289.2685426  180928.65510111 181654.70364867 181134.14479122
 247113.14313185 238373.23490745 191015.57865662 274611.02228018
 290747.14013806 226531.48976403 216793.18259935 216079.77416458
 216961.79999517 243064.96935139 181074.17347501 181289.2685426
 180928.65510111 181654.70364867 181134.14479122 180683.57560278
 181095.35833621 180552.87750495 180831.01390125 180994.55502098
 203934.11398645 249985.24204832 251218.9078642  257830.52608405
 263547.40297181 281573.04550918 240410.8861144  218896.80832453
 239393.9997885  263524.22295805 180683.57560278 181095.35833621
 180552.87750495 180831.01390125 180994.55502098 180465.2440962
 180944.81181612 180295.53364076 180911.28554033 180234.21212785
 303530.28910293 195691.61490682 253736.82660238 215301.78991512
 300857.74267916 275303.90647116 221522.41404161 248174.65105856
 281952.6785762  237985.6134607  180465.2440962  180944.81181612
 180295.53364076 180911.28554033 180234.21212785 179902.09132502
 180355.45663399 179881.23418463 180431.09717712 180205.87140315
 275017.12262529 255767.10572308 204667.15716855 248098.63738631
 228842.5799678  140369.3443455  228549.02329235 263625.88227005
 223237.49158492 214171.79899896 179902.09132502 180355.45663399
 179881.23418463 180431.09717712 180205.87140315 179744.53081496
 179552.22421517 179888.28764014 178935.41550377 179577.92791665
 217363.37115579 290038.91555594 298277.655062   144090.08191929
 282966.13519558 198755.09123552 199974.10665336 199430.6439895
 179744.53081496 179552.22421517 179888.28764014 178935.41550377
 179577.92791665 179108.18621382 178195.82361673 179393.81999793
 179134.67389206 179160.22479685 285226.09777554 239163.54592947
 168193.85017688 144090.08191929 255551.82903827 198755.09123552
 273830.61403911 316391.91054219 179108.18621382 178195.82361673
 179393.81999793 179134.67389206 179160.22479685 178157.20926936
 178345.63406086 178250.69698188 178894.21155426 177522.7274058
 311367.19143812 343384.87421334 168193.85017688 144090.08191929
 293076.18590617 335656.00711951 190835.02609482 188868.26545983
 178157.20926936 178345.63406086 178250.69698188 178894.21155426
 177522.7274058  177245.45094942 178338.83190054 177465.3767018
 177412.87289734 179688.36220542 194322.32495132 209438.17977144
 168193.85017688 144090.08191929 171112.73446933 210097.35915084
 317757.56026872 269348.07128871 177245.45094942 178338.83190054
 177465.3767018  177412.87289734 179688.36220542 175509.34211301
 177272.74212489 176989.40671242 176636.52670412 177167.13719992
 157380.54824344 212101.63602451 233815.58711263 144090.08191929
 171112.73446933 207496.5360009  203157.12891195 305294.04170354
 175509.34211301 177272.74212489 176989.40671242 176636.52670412
 177167.13719992 175844.6840299  176153.49108181 175261.57177227
 176439.70850766 175528.16947469 202909.77429322 169087.10456699
 216566.45250149 227736.94178071 171112.73446933 202533.95060079
 178629.66882081 201142.90484598 175509.34211301 175844.6840299
 176153.49108181 175261.57177227 176439.70850766 175528.16947469
 174721.98601293 175126.57168029 176345.0497592  174702.87394012
 174808.57218564 211847.53441281 225425.5299403  132281.75535717
 172838.19344011 233879.94543839 268804.47689791 209240.98323285
 159165.03587316 220976.050428   174721.98601293 175126.57168029
 176345.0497592  174702.87394012 174808.57218564 173713.07984524
 175190.0616657  174210.77389206 173663.05592428 173581.16504467
 251988.37146934 142601.38465076 132281.75535717 172838.19344011
 212268.67401498 279176.19333541 215672.40363788 159165.03587316
 282730.01014894 173713.07984524 175190.0616657  174210.77389206
 173663.05592428 173581.16504467 173155.24407279 173702.31814615
 172604.18094102 172775.49164892 173732.18877055 183628.92431722
 142601.38465076 132281.75535717 241713.50998927 213701.3065685
 252440.52834892 170348.59103304 204216.48743755 182251.54715398
 173155.24407279 173702.31814615 172604.18094102 172775.49164892
 173732.18877055 172541.39299749 173443.01711278 172373.77264499
 172053.9420642  173563.88460349 183628.92431722 142601.38465076
 132281.75535717 237968.3113154  208680.41553754 219303.8265855
 175561.10829536 187652.92235013 241239.69385164 172541.39299749
 173443.01711278 172373.77264499 172053.9420642  173563.88460349
 171243.07346888 172440.98974414 171792.82464566 172836.0708258
 173222.27522945 198937.64711924 142601.38465076 155444.4627835
 217163.2635906  263925.11921321 219303.8265855  175561.10829536
 200450.89279045 127283.88829314 154610.2305096  171243.07346888
 172440.98974414 171792.82464566 172836.0708258  173222.27522945
 170276.03402236 170615.44900369 171943.29881939 170926.23937759
 173739.37250959 241196.03865538 231534.21214516 155444.4627835
 136897.42279285 315227.26745565 229203.73863079 209371.90091862
 160588.82880564 182498.15258599 155773.92694376 170276.03402236
 170615.44900369 171943.29881939 170926.23937759 173739.37250959
 168469.07893991 170138.04692953 170133.60734855 171461.95835149
 172354.57716379 295767.4465947  205385.35311176 155444.4627835
 136897.42279285 162468.27120869 268602.6544965  198807.87104501
 238159.89214232 207751.36583661 155773.92694376 168469.07893991
 170138.04692953 170133.60734855 171461.95835149 172354.57716379
 166985.21544043 168599.30306939 166231.70444373 168716.71578546
 168825.65416981]
2025-06-22 15:15:27 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:15:27 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:15:27 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:15:27 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:15:27 INFO AOCC mean: 0.0000
2025-06-22 15:15:39 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:15:39 ERROR Can not run the algorithm
2025-06-22 15:15:39 INFO Run function 1 complete. FEHistory len: 1165, AOCC: 0.0000
2025-06-22 15:15:39 INFO FeHistory: [238080.82205362 237572.22210223 236822.31112815 ... 235398.1038927
 235743.85529274 235446.0956667 ]
2025-06-22 15:15:39 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:15:39 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:15:39 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:15:39 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:15:39 INFO AOCC mean: 0.0000
2025-06-22 15:15:49 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:15:51 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:15:51 INFO FeHistory: [148246.97435881 220812.56278049 139186.69390297 ... 164130.06429466
 257109.82795713 150118.62320559]
2025-06-22 15:15:51 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:15:51 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:15:51 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:15:51 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:15:51 INFO AOCC mean: 0.0000
2025-06-22 15:16:01 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:16:01 ERROR Can not run the algorithm
2025-06-22 15:16:01 INFO Run function 1 complete. FEHistory len: 980, AOCC: 0.9651
2025-06-22 15:16:01 INFO FeHistory: [217280.56482243 177228.00647475 214436.89391134 175365.51460202
 167560.94365967 178282.74965408 142274.08298628 139985.80234204
 144910.85081114 175792.60614057 164797.83971899 145622.39399069
 176199.84125223 195987.87709765 186650.03808436 185902.03451634
 210033.6679631  159360.41270226 170112.31903308 153248.39200248
 203367.31266147 177665.729871   193009.87152986 167137.62080185
 137836.75013088 182348.45381794 203963.84168681 209693.50216077
 280220.75467808 185545.6672247  266286.7934063  219613.49736138
 216602.65822564 199106.11563203 229511.45029829 157142.51625627
 180083.59198257 191673.61034803 161127.5029268  227175.41537692
 256609.74853595 191715.68942358 230911.77766569 194590.97012995
 177077.53486308 205336.41076137 246390.29559171 266051.60320532
 218281.19793933 207330.28507044 180542.78911086 132525.20458134
 222211.15771865 261652.34893655 157072.21748521 192880.79251603
 194006.34954121 187689.89775807 211425.33261011 181534.27004207
 223038.31786237 231294.29700777 212720.38261202 214300.51732278
 210891.54924815 163472.57912931 226431.21715767 143380.30629148
 135677.85756263 239050.29979659 182309.16393108 217246.9137176
 194987.24830944 216612.05210412 245298.34452475 245801.46380551
 165360.44431098 153694.75157476 169381.59000589 249385.36470108
 210070.49573468 202368.42453825 201890.71647859 193728.14459017
 232497.33233727 192998.51332589 179900.4609325  182700.58387392
 246494.87551688 187477.46501452 236021.77495056 253358.43412158
 221973.33453745 203612.14748274 191746.05227283 236744.41072836
 229668.141585   234140.89829806 265617.16574087 209599.81203039
 185686.56212304 193910.79771446 217827.2475741  236365.96680877
 186083.83864077 180767.69711087 171618.63411565 188949.97298252
 168165.88391489 192148.76033738 134547.38270228 165790.7939876
 226753.87417493 222689.82161022 166296.49684219 207808.77983758
 169249.55055012 213440.76106869 220065.81858254 239690.74512374
 187035.16713444 188126.75454899 234726.74646578 184894.42030665
 216537.96760258 213018.56195388 218478.87561077 203169.50545058
 223099.26665837 178754.76587549 172363.47800931 267328.16046387
 174255.97433202 189421.02914414 172388.16279048 207545.01775044
 133716.14223733 225294.6208923  221319.56716813 227626.32187322
 212537.17146773 215976.7485808  176529.15329077 125058.75746533
 231912.47949987 207646.20880653 158035.30005071 226713.8747458
 252263.0670622  263061.88278131 189593.66565177 215160.49335666
 199134.20722231 182006.37851411 212601.55060458 208045.8468878
 224174.71124469 230703.0658589  235914.20147804 189943.43762711
 179853.5947857  163824.74559088 215339.87569544 135615.45976177
 235868.08671445 210990.33590151 171031.52426636 111940.73193897
 197100.38917388 266307.92951221 229365.90634653 198704.15353589
 192769.02835612 224863.14794679 227078.30453471 124100.71256594
 238238.48723635 187956.67890924 194010.8376647  268375.65260826
 191666.01799952 215844.13383212 185484.32688789 220661.0622037
 214842.28088746 187208.58006144 176363.10892992 213382.68829585
 223389.86257296 149865.56940968 200125.34399844 178295.72281244
 261026.23558657 166469.44458783 198606.57908507 175164.20008893
 208363.95709917 232354.94693977 220991.54935775 202703.75485473
 235699.22231991 187243.32931084 222258.63217831 126514.97137877
 168765.15566567 199036.39007753 215397.64853778 115459.25069104
 196674.80607182  95545.75504152 239531.03433468 196441.97677009
 124716.96733443 160396.70956423 154063.40105627 214155.35177405
 219164.41572312 177049.15655191 138422.1733886  135969.29509145
 188276.35350615 284758.05593007 208418.12797715 164650.56861705
 151241.03162272 141354.48045348 156743.68433266 164183.59870802
 166306.24156014 206537.7118251  138062.8930803  157794.80913793
 145009.28940047 211477.67532261 181691.52499131 176790.91866712
 133198.56293209 159606.06891082 159736.35385017 199948.25689231
 236019.68496241 163964.18424343 140226.37519775 252215.31366532
 165352.14689661 203367.31950252 131968.33986983 185177.45952306
 172438.32521948 194920.7430251  195653.91303716 186697.29182091
 210856.05302431 134870.59162734 168001.79461842 166316.56022071
 206920.03955981 178747.14783239 142493.9579865  205586.57228992
 180353.44702175 222751.36997717 159070.15844725 211576.48654492
 137852.80248121 205733.4652993  187024.15223948 203773.3732361
 139329.07674476 238826.98289492 116920.13947979 165610.91830592
 233746.6436481  160481.91606039 135815.10581946  95185.68798368
  95345.46436244  95689.03772958  94596.87919933  95619.23931997
  94596.87919933  94596.87920103  94596.87920005  94596.87920174
  94596.87920042  94596.87920063  94596.87919937  94596.87920078
  94596.87920121  94596.87919845  94596.87920086  94596.87919947
  94596.87919747  94596.87920042  94596.87919753  94596.8791992
  94596.87919958  94596.87919838  94596.87919861  94596.87919927
  94596.8791997   94596.8791997   94596.87919795  94596.87919998
  94596.87919949  94596.87919854  94596.87919822  94596.87920039
  94596.87919986  94596.87920042  94596.87919835  52642.39504013
  52642.39503843  52642.39503942  52642.39503973  52642.39503904
  52642.39503926  52642.3950401   52642.39503913  52642.39503862
  52642.39504101  52642.39503861  52642.39504     52642.39503872
  52642.39503908  52642.39503934  52642.39504026  52642.39503988
  52642.39504108  52642.39504085  52642.39504019  52642.39503976
  52642.39503976  52642.39503963  52642.39503949  52642.39503997
  52642.39503933  52642.39503939  52642.39503946  52642.39503961
  52642.39503919  52642.39503934  -1081.98379793  -1081.98379793
  -1081.98379793  -1081.98379793  -1081.98379793  -1081.98379793
  -1081.98379793  -1081.98379793  -1081.98379793  -1081.98379793
  -1081.98379793  -1081.98379793  -1081.98379793  -1081.98379793
  -1081.98379793  -1081.98379793  -1081.98379793  -1081.98379793
  -1081.98379793  -1081.98379793  -1081.98379793  -1081.98379793
  -1081.98379793  -1081.98379793  -1081.98379793  -1081.98379793
  -1081.98379793  -1081.98379793  -1081.98379793  -1081.98379793
  -1081.98379793  -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
 172109.70893788 235483.90118021 139006.10976927 177940.66581428
 178604.57341728 202148.8461328  182768.79683477 165280.67693933
 227147.0890347  203491.34354996 153742.00372492 206681.588367
 210719.70393963 194494.47565774 267745.8807949  186594.60389463
 166789.17413923 163900.64278147 177987.61698845 198981.23319129
 166893.00617769 129946.1604419  177468.79422574 234293.02608409
 138501.96921908 138806.80772103 171618.12127613 138618.834055
 155058.18764275 240469.32231837 194357.35210089 137118.05164859
 224779.29119798 201242.5825221  147721.08917876 214815.82467117
 185713.56037362 190204.72537612 203236.86445278 185643.62050213
 301023.70531076 149951.81086489 150143.79929784 179800.44916059
 131760.43977104 180227.00856036 135970.2719584  178120.46919088
 164416.2475081  141323.1077899  155223.33322122 160381.97469239
 166137.89445974 218514.28124295 190728.22858385 163473.88183261
 180444.76879224 265816.66332857 159342.39889181 207921.42160513
 137264.9225707  164377.91289001 186429.15562686 119878.15397334
 128736.04344766 119907.20764642 194544.28599507 119420.03809769
 162637.1077009  152404.16495417 252997.73044945 179906.32519474
 199462.300893   173915.28275209 224421.66459646 123180.56819521
 121288.28904543 164281.03394985 188194.35628203 150852.83978217
 146579.77791408 158088.14151632 172644.63206236 208000.15845911
 179297.15068132 147554.33401224 160011.96089337 100957.79507133
 198111.43488202 182161.67903835 132919.15094198 111914.36948336
 178043.86467837 193148.1799629  140900.30543458 197534.692917
 148062.96234374 153483.90517011 190404.51253938 158831.39059761
 108440.02181293 160233.09750133 200059.11672972 163983.11753514
 151336.06776599 143293.37027587 164948.8716399  181605.89929033
 160740.02942801 159354.67927204 132819.22256485 190126.58555698
 143909.57195425 127760.2342562  186975.80233609 152979.262457
 185168.50266644 171413.76732344 128388.9112653  191706.758098
 103381.87399874 201233.45834459 179192.84973995 127860.4628458
 149909.9280691   94646.74666505  94237.56922283  94463.01179884
  94958.18936324  94067.03213551  -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994  199798.32616372
 133039.86869047 143295.85851959 146838.64144663 221273.2499446
 225791.22752959 173886.46015989 170381.89315849 119800.88377087
  84235.172911   154674.73233993 149815.13145204 179501.20163895
 190153.84445614 102333.29289996 134806.63642303 150967.8524183
 221237.09682579 127653.42914853 171152.51809705 240249.1667472
 201898.29668758 112128.93131537 173678.88047281 123393.30363071
 162856.6193204  159189.81964312 166792.25731381 198544.03992906
  99204.7612263  184716.57247436 163777.63909987 148525.72765723
 161460.11828183 122695.5370387  151466.9492969  155751.27381917
 141477.95092748 211113.82567083 215197.70477196 200391.93286361
 187511.2984909   86261.08755745 171699.53282407 145710.11419211
 106900.94405504 156015.25502702 167413.27473005  88266.88684332
 154876.72576539 214806.41044449 134661.57324329 220278.66067239
 136706.11857374 154103.96977095  98445.6072117  107229.7527126
 159623.59146283 149334.08442943  96901.40242731 172575.47268617
 149357.71555279 150826.25794597 177308.76803224 145301.75815696
 161561.63802283 148649.50805409 198820.73439908 150064.70973708
 194252.70886525 156913.74051873 160039.73742619 177155.4112623
  97887.76213993  98335.21693605 113414.87390425 149445.41215793
 160838.30215371 172919.52045792 120855.93304984 187353.55850988
 181467.44514436 113841.60319533 106425.52273332 132132.88605769
 166374.05750937 180003.14310253 154744.87682598 122327.1257386
 149074.80225604 126325.26523024 163079.30609618 172855.94056027
 170476.23129855 182176.99680148 183763.41513833 168240.17622454
 128939.2600947  195005.02120377 131951.58618659 137152.2162284
 162852.1636717  102484.05312121 164223.57941146 119013.98569792
 133362.21955656 207291.29899053 185576.147779   130770.99717581
  98257.51551489 163557.38095843  86831.74172049 204839.06384077
 150663.05607627 102666.07958116 162336.78366419 142102.10540228
 132405.47262247 143665.55441061 137772.47969702 183939.85959806
 161632.78252622 170326.24350619 169437.74428122 154634.8151615
  93937.65487962  94564.83451756  94005.69729993  94176.40146535
  93921.85180177  -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.98379939
  -1081.98379939  -1081.98379939  -1081.98379939  -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994   -1081.9837994   -1081.9837994
  -1081.9837994   -1081.9837994  153206.95813681 120932.66982356
 161287.62010078 155123.50685942 113813.76210052 151406.79875496
 123600.63946641 136574.26615252 101997.01013557 198527.26034846
 176633.79384316 120818.81233335 140494.7988836  137889.42780688
  92002.98688587 127526.37066744 121685.14146556 206495.73661795
 112434.17540213 191825.8167667  121009.31466797 122604.67377318
 105873.54495938 103722.33230689 188229.86232474 184961.17008474
 146110.39090658 175476.29308247 122302.94578176 114344.474623
 143710.74452544 158525.02259325 182054.21215816 187196.08990124
 131458.28857327 129169.61698574 130809.70593815 101675.99244911
 131315.90163567 118553.99652746 154727.97965071 137675.18325247
  97954.88265437 164447.02275064 135413.38237572 108503.50994214
 147156.78449308  75378.15360331 127424.99186104 104817.67277682
 142291.24803012 116847.17869161 150457.19981914 146682.57158124
 166321.3027131  106585.69460283 142260.95998205 147845.09401788
 131987.5568845  101273.0218224  121692.18165641 103582.932938
 161039.98765678 163283.00236029 151425.84579023 149970.70571239
 140885.76192437 177188.90036964 192287.03079313  80119.96399023
 123965.08537692 133820.66908983 139455.08656576 161269.50644601
 134549.4206625  191882.28046543 183748.82748236  96380.38970751
 173790.94703729 110291.18976543  90801.74949557 115942.69138364
  80586.19830941 139674.85144898 156885.27659601  79730.81645873
 138700.74565803 134193.9261381  117911.26695016 126157.44357668
 116025.52902937 172168.94412223 115147.4637865  134113.97598253
 175288.88024672 127482.70018501 122070.5149259  125739.66793776
 183983.96275677 103693.28170701 147775.54140768 128533.67503658
 156200.81622417  72019.39921786 116205.52865245 145935.66139692
  99044.57462951 145998.77383041 121491.01564321 125281.45406219
 145394.95644635 119315.29275995 160973.28266609 143629.4583653
 147088.939899   116956.63431906 117290.00569673 132241.99303093
 157414.35761044 157130.39011916  83303.46734205 142202.74965273
 143635.55220903 112468.70879173 121936.12493927  93575.2193812
  94107.39982975  93718.25950339  94416.94215525  94284.87018297]
2025-06-22 15:16:01 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:16:01 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDECMAESwithImprovedLocalSearch
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDECMAESwithImprovedLocalSearch
# Description: Hybrid DE-CMA-ES with adaptive local search and population size for multimodal optimization.
# Code:
class AdaptiveHybridDECMAESwithImprovedLocalSearch:
    """
    Combines Differential Evolution (DE) and CMA-ES with adaptive local search 
    and population size for efficient multimodal optimization.  Improves upon previous versions by 
    incorporating a more robust local search trigger and adaptive parameter control.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.floor(dim/2)) #Adaptive population size
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.cma = None
        self.local_search_trigger = 0.1 #Adaptive trigger for local search, starts higher then decreases
        self.de_iterations = 5 # Number of DE iterations before CMA-ES
        self.stagnation_threshold = 10 #Number of iterations without improvement before local search
        self.stagnation_count = 0
        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.stagnation_count = 0

        # Initial evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1,-1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()
                self.stagnation_count = 0
            else:
                self.stagnation_count += 1


        while self.eval_count < self.budget:
            # Differential Evolution phase
            for _ in range(self.de_iterations):
                offspring = np.zeros_like(self.population)
                for i in range(self.population_size):
                    a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                    v = self.population[a] + self.F * (self.population[b] - self.population[c])
                    u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                    offspring[i] = u

                offspring_fitness = objective_function(offspring)
                self.eval_count += self.population_size

                for i in range(self.population_size):
                    if offspring_fitness[i] < self.fitness[i]:
                        self.fitness[i] = offspring_fitness[i]
                        self.population[i] = offspring[i]
                        if self.fitness[i] < self.best_fitness_overall:
                            self.best_fitness_overall = self.fitness[i]
                            self.best_solution_overall = self.population[i].copy()
                            self.stagnation_count = 0
                            self.local_search_trigger *= 0.9 #Decrease trigger after improvement
                        else:
                            self.stagnation_count += 1
                    else:
                        self.stagnation_count +=1

                if self.eval_count >= self.budget:
                    break

            #Adaptive CMA-ES and Local Search
            if self.stagnation_count >= self.stagnation_threshold and self.eval_count < self.budget:
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                        self.stagnation_count = 0
                        self.local_search_trigger *= 0.9

                #Local Search
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x
                    self.stagnation_count = 0
                    self.local_search_trigger *= 0.9

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
2025-06-22 15:16:01 INFO Unimodal AOCC mean: 0.9651
2025-06-22 15:16:01 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:16:01 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:16:01 INFO AOCC mean: 0.9651
2025-06-22 15:16:12 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:16:13 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.9857
2025-06-22 15:16:13 INFO FeHistory: [145537.16884707 163044.51908332 111202.44493494 ...  71614.07392549
  71614.07392549  71614.07392549]
2025-06-22 15:16:13 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:16:13 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDECMAESwithImprovedLocalSearch
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDECMAESwithImprovedLocalSearch
# Description: Hybrid DE/CMA-ES with adaptive parameters and improved local search for multimodal optimization.

class AdaptiveHybridDECMAESwithImprovedLocalSearch:
    """
    Combines Differential Evolution (DE) and CMA-ES for global exploration and local exploitation, 
    adapting parameters and incorporating a robust local search to efficiently handle multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(self.dim / 2)  # Adaptive population size
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.cma = None
        self.stagnation_count = 0 # Counter for fitness stagnation
        self.stagnation_threshold = 10 # Number of iterations before triggering local search


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitnesses = objective_function(population)
        self.eval_count += self.population_size

        best_index = np.argmin(fitnesses)
        self.best_solution_overall = population[best_index]
        self.best_fitness_overall = fitnesses[best_index]

        previous_best_fitness = self.best_fitness_overall

        # Initialize CMA-ES after initial DE evaluation
        #self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 10 + int(self.dim/4), 'tolfun':1e-10})


        while self.eval_count < self.budget:
            #DE phase
            new_population = np.zeros_like(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])

                #Adaptive F and CR
                if self.eval_count < self.budget * 0.2:
                    self.F = 1.0
                    self.CR = 0.8
                else:
                    self.F = 0.5
                    self.CR = 0.95

                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1

                if trial_fitness < fitnesses[i]:
                    new_population[i] = trial
                    fitnesses[i] = trial_fitness
                    if trial_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = trial_fitness
                        self.best_solution_overall = trial
                        self.stagnation_count = 0 #reset stagnation counter
                    else:
                        self.stagnation_count +=1

            population = new_population
            
            #Local Search Triggered by Stagnation
            if self.stagnation_count >= self.stagnation_threshold:
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x
                self.stagnation_count = 0 # Reset after local search

            #CMA-ES phase (less frequent)
            if self.eval_count % (self.population_size * 3) == 0 and self.eval_count < self.budget * 0.8: #only run CMA-ES for a fraction of the budget
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 10 + int(self.dim/4), 'tolfun':1e-10})
                cma_solutions = self.cma.ask()
                cma_solutions = np.clip(cma_solutions, self.lower_bounds, self.upper_bounds)
                cma_fitnesses = objective_function(cma_solutions)
                self.eval_count += len(cma_fitnesses)
                self.cma.tell(cma_solutions, cma_fitnesses)

                for i, fitness in enumerate(cma_fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = cma_solutions[i]
                        self.stagnation_count = 0 #reset stagnation counter

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
2025-06-22 15:16:13 INFO Unimodal AOCC mean: 0.9857
2025-06-22 15:16:13 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:16:13 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:16:13 INFO AOCC mean: 0.9857
2025-06-22 15:16:24 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:16:25 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.3268
2025-06-22 15:16:25 INFO FeHistory: [129655.23620418 204724.28000026 140532.83940547 ...  -1081.9837994
  -1081.9837994   -1081.9837994 ]
2025-06-22 15:16:25 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:16:25 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_Enhanced
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDE_CMAES_Enhanced
# Description: Hybrid DE & CMA-ES with adaptive parameters, local search, and population diversity control for multimodal optimization.
class AdaptiveHybridDE_CMAES_Enhanced:
    """
    Combines Differential Evolution (DE) and CMA-ES, adapting parameters and 
    incorporating local search and diversity control to efficiently handle 
    multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(self.dim / 2)  
        self.F = 0.8  
        self.CR = 0.9  
        self.cma = None
        self.local_search_trigger = 1e-3  
        self.diversity_threshold = 0.1 #parameter to control population diversity
        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        # Initial evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1, -1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        while self.eval_count < self.budget:
            # Adaptive parameters
            if self.eval_count < self.budget * 0.3:
                self.F = 1.0
                self.CR = 0.8
            elif self.eval_count < self.budget * 0.7:
                self.F = 0.7
                self.CR = 0.9
            else:
                self.F = 0.5
                self.CR = 0.95

            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                v = self.population[a] + self.F * (self.population[b] - self.population[c])
                u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                offspring[i] = u

            # Evaluate offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection and diversity check
            for i in range(self.population_size):
                if offspring_fitness[i] < self.fitness[i]:
                    self.fitness[i] = offspring_fitness[i]
                    self.population[i] = offspring[i]
                    if self.fitness[i] < self.best_fitness_overall:
                        self.best_fitness_overall = self.fitness[i]
                        self.best_solution_overall = self.population[i].copy()
            
            #Maintain Diversity
            if np.std(self.fitness) < self.diversity_threshold:
                self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(int(self.population_size/2), self.dim))
                
                temp_fitness = objective_function(self.population)
                self.eval_count += len(temp_fitness)
                self.fitness = np.concatenate((self.fitness,temp_fitness))


            # Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger and self.cma is None:
                self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})

            if self.cma is not None and self.eval_count < self.budget:
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                #Local Search after CMA-ES update
                result = minimize(lambda x: objective_function(x.reshape(1, -1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:16:25 INFO Unimodal AOCC mean: 0.3268
2025-06-22 15:16:25 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:16:25 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:16:25 INFO AOCC mean: 0.3268
2025-06-22 15:16:37 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:16:37 ERROR Can not run the algorithm
2025-06-22 15:16:37 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.9777
2025-06-22 15:16:37 INFO FeHistory: [214486.73521014 146318.19707767 190990.78249592 ... 163358.10704575
 105267.68411131 107631.50243108]
2025-06-22 15:16:37 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:16:37 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDECMAESwithImprovedLocalSearch
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveHybridDECMAESwithImprovedLocalSearch
# Description: Hybrid DE-CMA-ES with adaptive local search and population size for multimodal optimization.
# Code:
class AdaptiveHybridDECMAESwithImprovedLocalSearch:
    """
    Combines Differential Evolution (DE) and CMA-ES with adaptive local search 
    and population size for efficient multimodal optimization.  Improves upon previous 
    versions by using a more robust local search strategy and adaptive parameter control.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.floor(dim/2)) #Adaptive population size
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.cma = None
        self.local_search_trigger = 0.1 # Fraction of budget before triggering local search
        self.de_iterations = 5 # Number of DE iterations before CMA-ES
        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)
        self.stagnation_count = 0 # Counter for fitness stagnation
        self.stagnation_threshold = 10 # Number of iterations with no improvement before triggering local search

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.stagnation_count = 0

        # Initial evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1,-1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()
                self.stagnation_count = 0 #Reset stagnation counter
            else:
                self.stagnation_count +=1


        while self.eval_count < self.budget:
            # Differential Evolution phase
            for _ in range(self.de_iterations):
                offspring = np.zeros_like(self.population)
                for i in range(self.population_size):
                    a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                    v = self.population[a] + self.F * (self.population[b] - self.population[c])
                    u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                    offspring[i] = u

                offspring_fitness = objective_function(offspring)
                self.eval_count += self.population_size

                for i in range(self.population_size):
                    if offspring_fitness[i] < self.fitness[i]:
                        self.fitness[i] = offspring_fitness[i]
                        self.population[i] = offspring[i]
                        if self.fitness[i] < self.best_fitness_overall:
                            self.best_fitness_overall = self.fitness[i]
                            self.best_solution_overall = self.population[i].copy()
                            self.stagnation_count = 0 #Reset stagnation counter
                        else:
                            self.stagnation_count += 1
                    else:
                         self.stagnation_count += 1

                if self.eval_count >= self.budget:
                    break

            #Adaptive CMA-ES and Local Search
            if self.stagnation_count > self.stagnation_threshold and self.eval_count < self.budget:
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                        self.stagnation_count = 0 #Reset stagnation counter
                    else:
                         self.stagnation_count += 1

                #Improved Local Search using multiple methods
                result_bfgs = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                result_neldermead = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='Nelder-Mead')

                best_result = result_bfgs if result_bfgs.fun < result_neldermead.fun else result_neldermead
                if best_result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = best_result.fun
                    self.best_solution_overall = best_result.x
                    self.stagnation_count = 0 #Reset stagnation counter
                else:
                    self.stagnation_count += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:16:37 INFO Unimodal AOCC mean: 0.9777
2025-06-22 15:16:37 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:16:37 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:16:37 INFO AOCC mean: 0.9777
2025-06-22 15:16:37 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal & Ill-Conditioned Landscapes (like f1-f6): These have a single optimum, but it may be hidden in a very long, narrow, rotated valley. A good strategy needs to effectively sample the central region of the search space and adapt its search direction.

Rugged Single-Basin Landscapes (like f7-f15): These are filled with numerous, deep local optima. The algorithm must have a robust escape mechanism to avoid getting trapped.

Deceptive Multi-Component Landscapes (like f16-f24): These problems have multiple, separate basins of attraction. The global optimum may be in a small, remote basin, while a larger, more attractive basin leads to a suboptimal solution. A good strategy must have aggressive global exploration capabilities.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).



### Better code
AdaptiveHybridDECMAESwithAdaptiveLocalSearch
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveHybridDECMAESwithAdaptiveLocalSearch
# Description: Hybrid DE-CMA-ES with adaptive local search and population size for multimodal optimization.
# Code:
class AdaptiveHybridDECMAESwithAdaptiveLocalSearch:
    """
    Combines Differential Evolution (DE) and CMA-ES with adaptive local search 
    and population size for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(np.floor(dim/2)) #Adaptive population size
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.cma = None
        self.local_search_trigger = 1e-3 # Fitness threshold for triggering local search
        self.de_iterations = 5 # Number of DE iterations before CMA-ES
        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        # Initial evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1,-1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        while self.eval_count < self.budget:
            # Differential Evolution phase
            for _ in range(self.de_iterations):
                offspring = np.zeros_like(self.population)
                for i in range(self.population_size):
                    a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                    v = self.population[a] + self.F * (self.population[b] - self.population[c])
                    u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                    offspring[i] = u

                offspring_fitness = objective_function(offspring)
                self.eval_count += self.population_size

                for i in range(self.population_size):
                    if offspring_fitness[i] < self.fitness[i]:
                        self.fitness[i] = offspring_fitness[i]
                        self.population[i] = offspring[i]
                        if self.fitness[i] < self.best_fitness_overall:
                            self.best_fitness_overall = self.fitness[i]
                            self.best_solution_overall = self.population[i].copy()
                if self.eval_count >= self.budget:
                    break

            #Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger and self.eval_count < self.budget:
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]

                #Local Search
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


### Worse code
AdaptiveDifferentialEvolutionCMAES
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveDifferentialEvolutionCMAES
# Description: Hybrid Differential Evolution and CMA-ES with adaptive local search for escaping local optima in multimodal landscapes.
class AdaptiveDifferentialEvolutionCMAES:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(3*np.log(self.dim)) # Adaptive population size
        self.F = 0.8 #Differential Evolution scaling factor
        self.CR = 0.9 #Differential Evolution crossover rate
        self.cma = None
        self.local_search_trigger = 0.1 # Fraction of budget before triggering local search


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitnesses = objective_function(population)
        self.eval_count += self.population_size


        for i in range(self.population_size):
            if fitnesses[i] < self.best_fitness_overall:
                self.best_fitness_overall = fitnesses[i]
                self.best_solution_overall = population[i]

        #Main loop
        while self.eval_count < self.budget:
            #Differential Evolution
            offspring = np.zeros_like(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size),i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1,-1))[0]
                self.eval_count += 1
                if trial_fitness < fitnesses[i]:
                    offspring[i] = trial
                    fitnesses[i] = trial_fitness
                else:
                    offspring[i] = population[i]

            population = offspring

            # Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = population[i]

            # Adaptive CMA-ES and Local Search
            if self.eval_count > self.budget * self.local_search_trigger:
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5})
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                cma_fitnesses = objective_function(solutions)
                self.eval_count += len(cma_fitnesses)
                self.cma.tell(solutions, cma_fitnesses)
                
                #Local Search
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


### Analyze & experience
- Comparing (best) AdaptiveDifferentialEvolutionCMAES vs (worst) AdaptiveHybridPSO_CMAES, we see that the best utilizes a hybrid approach combining Differential Evolution (DE) and CMA-ES more effectively, adapting population size based on dimensionality. The worst uses PSO and CMA-ES, lacking the adaptive mechanisms for parameter tuning and population size adjustment seen in the best.  (second best) AdaptiveHybridDE_CMAES vs (second worst) AdaptiveHybridDECMAESwithAdaptiveLocalSearch shows similar hybrid approaches, but the second best incorporates more sophisticated adaptive parameter tuning (F and CR in DE) and a more robust local search. Comparing (1st) vs (2nd), we see that the top performer directly incorporates a local search within its main loop while the second-best uses it periodically. (3rd) AdaptiveHybridDE_CMAES_Improved vs (4th) AdaptiveHybridDE_CMAES_Improved (same name, different code) reveals almost identical code with slight differences in parameter adaptation and local search trigger, but the performance discrepancy suggests that these small differences can affect the results substantially. Comparing (second worst) AdaptiveHybridDECMAESwithAdaptiveLocalSearch vs (worst) AdaptiveHybridPSO_CMAES, we see that the second-worst uses a more appropriate hybrid DE-CMA-ES approach whereas the worst algorithm's PSO-CMA-ES combination seems less effective for this benchmark. Overall: The best-performing algorithms effectively integrate DE and CMA-ES, utilizing adaptive parameter tuning and local search strategies tailored to the specific challenges posed by the GNBG benchmark functions. The simpler hybrid approaches and those relying on PSO show significantly lower performance.
- **Keywords:** Heuristic design,  adaptive algorithms,  multimodal optimization, exploration-exploitation tradeoff,  parameter control.

**Advice:** Focus on specific algorithm weaknesses.  Quantify exploration/exploitation balance.  Design adaptive mechanisms with clear triggers (e.g., fitness stagnation, diversity loss).  Systematically compare different local search methods and their parameterizations.

**Avoid:** Vague generalizations ("crucial," "greatly enhances").  Overreliance on adaptive mechanisms without a strong base algorithm.  Ignoring the computational cost of adaptation.  Failing to empirically validate design choices.

**Explanation:**  The ineffective reflection lacks concrete action.  Effective heuristics require detailed analysis, controlled experiments, and rigorous evaluation of specific components, not just general assertions about hybrid approaches.  The improved reflection guides the design process by identifying areas needing precise attention and measurement.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-22 15:16:37 INFO Mutation prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal & Ill-Conditioned Landscapes (like f1-f6): These have a single optimum, but it may be hidden in a very long, narrow, rotated valley. A good strategy needs to effectively sample the central region of the search space and adapt its search direction.

Rugged Single-Basin Landscapes (like f7-f15): These are filled with numerous, deep local optima. The algorithm must have a robust escape mechanism to avoid getting trapped.

Deceptive Multi-Component Landscapes (like f16-f24): These problems have multiple, separate basins of attraction. The global optimum may be in a small, remote basin, while a larger, more attractive basin leads to a suboptimal solution. A good strategy must have aggressive global exploration capabilities.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).



Current heuristics:
AdaptiveCMAESwithLocalSearch
import numpy as np
from scipy.optimize import minimize

# Name: AdaptiveCMAESwithLocalSearch
# Description: Hybrid CMA-ES with local search for escaping local optima in multimodal landscapes.
class AdaptiveCMAESwithLocalSearch:
    """
    Combines CMA-ES for global exploration with local search for exploitation.  Adapts sigma based on progress.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.sigma = 0.5  # Initial step size for CMA-ES
        self.cma = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initialize CMA-ES
        self.cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10, 'tolfun':1e-10})

        while self.eval_count < self.budget:
            # Generate solutions with CMA-ES
            solutions = self.cma.ask()
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            self.cma.tell(solutions, fitnesses)

            # Update best solution
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solutions[i]

            # Adaptive Sigma Adjustment
            if self.cma.result.fbest < 1e-2: # Condition for local search
                self.sigma *= 0.9 # Reduce sigma if close to optimum
            else:
                self.sigma *= 1.1 # Increase otherwise

            # Local search around best solution
            result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
            if result.fun < self.best_fitness_overall:
                self.best_fitness_overall = result.fun
                self.best_solution_overall = result.x

            self.cma.sigma = self.sigma #Update CMA-ES sigma


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'cma_result': self.cma.result
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

from cma import CMAEvolutionStrategy


Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- **Keywords:** Heuristic design,  adaptive algorithms,  multimodal optimization, exploration-exploitation tradeoff,  parameter control.

**Advice:** Focus on specific algorithm weaknesses.  Quantify exploration/exploitation balance.  Design adaptive mechanisms with clear triggers (e.g., fitness stagnation, diversity loss).  Systematically compare different local search methods and their parameterizations.

**Avoid:** Vague generalizations ("crucial," "greatly enhances").  Overreliance on adaptive mechanisms without a strong base algorithm.  Ignoring the computational cost of adaptation.  Failing to empirically validate design choices.

**Explanation:**  The ineffective reflection lacks concrete action.  Effective heuristics require detailed analysis, controlled experiments, and rigorous evaluation of specific components, not just general assertions about hybrid approaches.  The improved reflection guides the design process by identifying areas needing precise attention and measurement.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-22 15:17:22 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:17:22 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:17:22 INFO FeHistory: [176098.75144941 197476.93677306 138730.67997823 ... 199230.15971525
 232379.84064366 120099.32763592]
2025-06-22 15:17:22 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:17:22 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:17:22 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:17:22 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:17:22 INFO AOCC mean: 0.0000
2025-06-22 15:17:22 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:17:24 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:17:24 INFO FeHistory: [152616.763548   160520.92318234 155988.88664021 ...  95531.93916778
 176401.94348978 109933.13585924]
2025-06-22 15:17:24 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:17:24 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:17:24 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:17:24 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:17:24 INFO AOCC mean: 0.0000
2025-06-22 15:17:25 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:17:25 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:17:25 INFO FeHistory: [206354.72460449 122377.60590545 189603.89216302 ...  15534.19893109
  11715.37350625  11592.0508861 ]
2025-06-22 15:17:25 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:17:25 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:17:25 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:17:25 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:17:25 INFO AOCC mean: 0.0000
2025-06-22 15:17:25 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:17:26 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:17:26 INFO FeHistory: [197551.02762095  89676.83462045 233743.15533323 ...   8773.20596534
   8804.37990088   8632.67236571]
2025-06-22 15:17:26 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:17:26 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:17:26 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:17:26 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:17:26 INFO AOCC mean: 0.0000
2025-06-22 15:17:26 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:17:27 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:17:27 INFO FeHistory: [158792.90575244 180741.97540327 185638.12268602 ...    766.73942072
    807.68827904    797.76905281]
2025-06-22 15:17:27 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:17:27 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:17:27 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:17:27 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:17:27 INFO AOCC mean: 0.0000
2025-06-22 15:17:34 WARNING No replacements made in template string. Returning None.
2025-06-22 15:17:42 WARNING No replacements made in template string. Returning None.
2025-06-22 15:17:49 WARNING No replacements made in template string. Returning None.
2025-06-22 15:17:49 INFO Generation 4, best so far: 0.9919197902331691
2025-06-22 15:17:49 INFO Population length is: 11
2025-06-22 15:17:49 INFO --- Performing Long-Term Reflection at Generation 4 ---
2025-06-22 15:17:54 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveHybridDE_CMAES_Enhanced` vs (worst) `AdaptiveDifferentialEvolutionwithClustering`, we see that the best algorithm utilizes a hybrid approach combining Differential Evolution (DE) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), adapting parameters based on optimization progress.  It also incorporates local search and population diversity control for handling multimodal landscapes more effectively. The worst algorithm uses only DE with adaptive mutation and clustering, lacking the global exploration capabilities of CMA-ES and sophisticated diversity mechanisms.

(second best) `AdaptiveHybridCMAESDE_Improved` vs (second worst) `AdaptiveDifferentialEvolutionWithClustering`: The second-best algorithm also uses a hybrid CMA-ES and DE approach but with less sophisticated parameter adaptation and a simpler local search trigger. The second-worst algorithm only uses DE with clustering and adaptive mutation. The improved adaptive strategy in the second best algorithm leads to better performance.

Comparing (1st) vs (2nd), we see that `AdaptiveHybridDE_CMAES_Enhanced` has a more refined parameter adaptation strategy and a more robust diversity control mechanism, contributing to its superior performance.  `AdaptiveHybridCMAESDE_Improved` lacks these features, resulting in slightly less effective exploration and exploitation.

(3rd) vs (4th): Both are similar hybrid approaches.  The difference lies in implementation details and adaptive parameter adjustments. The slight variations in parameter adaptation and the introduction of a stagnation counter in the 4th-ranked algorithm do not significantly impact the overall AOCC score in this case.

Comparing (second worst) vs (worst), we see similar algorithms which use DE with clustering and adaptive mechanisms. They differ in the exact details of their adaptive strategy and clustering implementations and triggering local search only in case of stagnation. These fine differences don't make a major impact on performance.

Overall: The top-performing algorithms effectively combine global exploration (CMA-ES) with local exploitation (DE), adaptive parameter tuning, and diversity control mechanisms to handle the challenges posed by the GNBG benchmark functions.  Simpler algorithms relying solely on DE struggle to escape local optima and effectively explore high-dimensional spaces.


**Experience:**

Hybrid approaches combining global and local search methods, with adaptive parameter adjustments based on optimization progress and effective diversity maintenance strategies, are crucial for efficiently solving high-dimensional multimodal optimization problems like the GNBG benchmark.  Sophisticated adaptive mechanisms are key to overcoming the challenges presented by the varying landscape characteristics.

2025-06-22 15:17:55 INFO Full response text: **Keywords:**  Adaptive Parameter Control, Diversity Maintenance, Exploration-Exploitation Balance,  High-Dimensional Multimodal Optimization.

**Advice:** Focus on *mechanisms* for dynamically adjusting exploration/exploitation balance and maintaining population diversity (e.g.,  novel niching methods, adaptive mutation/crossover rates).  Prioritize robust base algorithms and sophisticated adaptive parameter control over simply adding more features.  Rigorous benchmarking on challenging problems is essential.

**Avoid:**  Vague terms like "effective," "well-timed," and "adaptively sized." Avoid over-reliance on adaptive population sizing as a primary solution.  Avoid simply combining existing algorithms without deep understanding of their interactions.

**Explanation:** The ineffective reflection focuses on generic statements lacking concrete implementation details. The improved reflection emphasizes *how* to achieve the stated goals, focusing on specific techniques and avoiding vague, potentially ineffective approaches.  The key is to shift from descriptive to prescriptive.  The $999K would be best spent on rigorous research and experimentation to develop and validate these mechanisms.

2025-06-22 15:17:55 INFO Generating offspring via Crossover...
2025-06-22 15:18:07 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:18:09 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.1051
2025-06-22 15:18:09 INFO FeHistory: [169369.83295393 208794.86516693 230066.69895746 ...  -1081.97206252
  -1081.97482077  -1081.97450186]
2025-06-22 15:18:09 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:18:09 INFO Good algorithm:
Algorithm Name: AdaptiveHybridCMAESDEwithNichingAndLocalSearch
import numpy as np
from cma import CMAEvolutionStrategy
import random

# Name: AdaptiveHybridCMAESDEwithNichingAndLocalSearch
# Description: Hybrid CMA-ES and DE with adaptive niching, local search, and improved parameter control for multimodal optimization.
# Code:

class AdaptiveHybridCMAESDEwithNichingAndLocalSearch:
    """
    Combines CMA-ES and DE, adapting niching radius and incorporating local search for multimodal problems.  Improves parameter adaptation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(dim / 2)
        self.F = 0.8
        self.CR = 0.9
        self.niching_radius = 0.5
        self.cma = None
        self.local_search_trigger = 1e-2
        self.archive = []  # archive for niching
        self.stagnation_counter = 0
        self.max_stagnation = 10 #parameter to control stagnation


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))

        while self.eval_count < self.budget:
            # DE Phase with Niching
            offspring = self._differential_evolution(population)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Adaptive Niching
            self.archive.extend(zip(offspring, offspring_fitness))
            population, fitnesses = self._niching(population, offspring, offspring_fitness)

            # Update best solution and stagnation counter
            best_fitness_this_gen = np.min(fitnesses)
            if best_fitness_this_gen < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness_this_gen
                self.best_solution_overall = population[np.argmin(fitnesses)]
                self.stagnation_counter = 0
            else:
                self.stagnation_counter += 1

            # Adaptive CMA-ES and Local Search
            if self.stagnation_counter > self.max_stagnation or self.best_fitness_overall < self.local_search_trigger:  # Trigger CMA-ES and local search based on stagnation or fitness
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 10, 'tolfun': 1e-10})
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                        self.stagnation_counter = 0  # Reset stagnation counter

                # Local Search (only if CMA-ES triggered)
                result = self._local_search(self.best_solution_overall)
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x
                    self.stagnation_counter = 0 # Reset stagnation counter

            # Adapt Niching Radius (example - adjust as needed)
            self.niching_radius = max(0.1, self.niching_radius * 0.95)  # slowly decrease

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _differential_evolution(self, population):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            mutant = population[a] + self.F * (population[b] - population[c])
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring

    def _niching(self, current_pop, offspring, offspring_fitness):
        combined_pop = np.concatenate((current_pop, offspring))
        combined_fitness = np.concatenate((objective_function(current_pop), offspring_fitness))

        unique_population = []
        unique_fitness = []
        for i in range(len(combined_pop)):
            is_unique = True
            for j in range(len(unique_population)):
                distance = np.linalg.norm(combined_pop[i] - unique_population[j])
                if distance < self.niching_radius:
                    is_unique = False
                    break
            if is_unique:
                unique_population.append(combined_pop[i])
                unique_fitness.append(combined_fitness[i])

        return np.array(unique_population[:self.population_size]), np.array(unique_fitness[:self.population_size])

    def _local_search(self, x0):
        from scipy.optimize import minimize
        return minimize(lambda x: objective_function(x.reshape(1,-1))[0], x0, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))

def objective_function(X):
    #Example objective function, replace with your actual function
    return np.sum(X**2, axis=1)

2025-06-22 15:18:09 INFO Unimodal AOCC mean: 0.1051
2025-06-22 15:18:09 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:18:09 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:18:09 INFO AOCC mean: 0.1051
2025-06-22 15:18:18 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:18:19 ERROR Can not run the algorithm
2025-06-22 15:18:19 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.2967
2025-06-22 15:18:19 INFO FeHistory: [ 88572.37553829 111215.15855529 178992.4701515  ...   -500.22032317
   -479.28905321   -433.29851081]
2025-06-22 15:18:19 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:18:19 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_Improved
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDE_CMAES_Improved
# Description: Hybrid DE & CMA-ES with adaptive parameters and local search for multimodal optimization.
class AdaptiveHybridDE_CMAES_Improved:
    """
    Combines Differential Evolution (DE) and CMA-ES, adapting parameters and 
    incorporating local search to efficiently handle multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(self.dim / 2)  
        self.F = 0.8  
        self.CR = 0.9  
        self.cma = None
        self.local_search_trigger = 1e-3  
        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        # Initial evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1, -1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        while self.eval_count < self.budget:
            # Adaptive parameters (simplified)
            if self.eval_count < self.budget * 0.5:
                self.F = 0.9
                self.CR = 0.8
            else:
                self.F = 0.5
                self.CR = 0.95

            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                v = self.population[a] + self.F * (self.population[b] - self.population[c])
                u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                offspring[i] = u

            # Evaluate offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < self.fitness[i]:
                    self.fitness[i] = offspring_fitness[i]
                    self.population[i] = offspring[i]
                    if self.fitness[i] < self.best_fitness_overall:
                        self.best_fitness_overall = self.fitness[i]
                        self.best_solution_overall = self.population[i].copy()

            # Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger and self.cma is None:
                self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 5, 'tolfun': 1e-10})

            if self.cma is not None and self.eval_count < self.budget:
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                #Local Search after CMA-ES update
                result = minimize(lambda x: objective_function(x.reshape(1, -1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
2025-06-22 15:18:19 INFO Unimodal AOCC mean: 0.2967
2025-06-22 15:18:19 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:18:19 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:18:19 INFO AOCC mean: 0.2967
2025-06-22 15:18:30 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:18:32 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:18:32 INFO FeHistory: [177696.81242789 195799.76413164 176625.8417723  ... 212641.06259746
 209350.78970109 215396.75445   ]
2025-06-22 15:18:32 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:18:32 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:18:32 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:18:32 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:18:32 INFO AOCC mean: 0.0000
2025-06-22 15:18:41 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:18:42 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.4587
2025-06-22 15:18:42 INFO FeHistory: [189803.57479925 213116.91344583 186238.37575306 ...  -1081.9837994
  -1081.9837994   -1081.9837994 ]
2025-06-22 15:18:42 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:18:42 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_Improved
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize

# Name: AdaptiveHybridDE_CMAES_Improved
# Description: Hybrid DE & CMA-ES with adaptive parameters and local search for multimodal landscapes.
class AdaptiveHybridDE_CMAES_Improved:
    """
    Combines Differential Evolution (DE) and CMA-ES, adapting parameters and 
    incorporating local search to efficiently handle multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(self.dim / 2)  # Adaptive population size
        self.F = 0.8  # DE scaling factor
        self.CR = 0.9  # DE crossover rate
        self.cma = None  # CMA-ES object
        self.local_search_trigger = 1e-3  # Fitness threshold for local search
        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = np.full(self.population_size, np.inf)


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        # Initial evaluation
        for i in range(self.population_size):
            self.fitness[i] = objective_function(self.population[i].reshape(1, -1))[0]
            self.eval_count += 1
            if self.fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.fitness[i]
                self.best_solution_overall = self.population[i].copy()

        while self.eval_count < self.budget:
            # Adaptive parameters (simplified for clarity)
            self.F = 0.8 * (1 - self.eval_count / self.budget) + 0.2 # Linear decrease
            self.CR = 0.9 + 0.1 * (self.eval_count / self.budget) # Linear increase

            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                v = self.population[a] + self.F * (self.population[b] - self.population[c])
                u = np.clip(np.where(np.random.rand(self.dim) < self.CR, v, self.population[i]), self.lower_bounds, self.upper_bounds)
                offspring[i] = u

            # Evaluate offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < self.fitness[i]:
                    self.fitness[i] = offspring_fitness[i]
                    self.population[i] = offspring[i]
                    if self.fitness[i] < self.best_fitness_overall:
                        self.best_fitness_overall = self.fitness[i]
                        self.best_solution_overall = self.population[i].copy()

            # Adaptive CMA-ES and Local Search
            if self.best_fitness_overall < self.local_search_trigger and self.cma is None:
                self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 10})

            if self.cma is not None and self.eval_count < self.budget:
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                # Local search after CMA-ES update
                result = minimize(lambda x: objective_function(x.reshape(1, -1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
2025-06-22 15:18:42 INFO Unimodal AOCC mean: 0.4587
2025-06-22 15:18:42 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:18:42 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:18:42 INFO AOCC mean: 0.4587
2025-06-22 15:18:51 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:18:52 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.9380
2025-06-22 15:18:52 INFO FeHistory: [113394.82489267 180241.90691923 157170.32882824 ...  -1029.89706265
   -978.92866971  -1025.49352467]
2025-06-22 15:18:52 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:18:52 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_Enhanced
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveHybridDE_CMAES_Enhanced
# Description: Hybrid DE/CMA-ES with adaptive parameters and local search for multimodal optimization.
# Code:
class AdaptiveHybridDE_CMAES_Enhanced:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 50  # Adjust as needed
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.sigma = 0.5 #Initial CMA-ES step size
        self.cma = None
        self.stagnation_counter = 0
        self.DE_iterations = 0 #counter for DE iterations



    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitnesses = objective_function(population)
        self.eval_count += self.population_size

        best_index = np.argmin(fitnesses)
        self.best_solution_overall = population[best_index]
        self.best_fitness_overall = fitnesses[best_index]


        while self.eval_count < self.budget:
            #Hybrid Approach: Switch between DE and CMA-ES based on stagnation
            if self.stagnation_counter < self.population_size * 0.2: #Use DE for initial exploration
                #Differential Evolution
                new_population = []
                for i in range(self.population_size):
                    a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), size=3, replace=False)
                    mutant = population[a] + self.F * (population[b] - population[c])
                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                    trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                    trial_fitness = objective_function(trial.reshape(1,-1))[0]
                    self.eval_count += 1
                    if trial_fitness < fitnesses[i]:
                        new_population.append(trial)
                        fitnesses[i] = trial_fitness
                        if trial_fitness < self.best_fitness_overall:
                            self.best_fitness_overall = trial_fitness
                            self.best_solution_overall = trial
                            self.stagnation_counter = 0
                    else:
                        new_population.append(population[i])
                population = np.array(new_population)
                self.DE_iterations += 1

            else: #Switch to CMA-ES for exploitation and fine-tuning

                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10})

                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses_cma = objective_function(solutions)
                self.eval_count += len(fitnesses_cma)
                self.cma.tell(solutions, fitnesses_cma)

                for i, fitness in enumerate(fitnesses_cma):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                        self.stagnation_counter = 0


            #Adaptive Parameter Control
            if self.stagnation_counter > self.population_size * 0.3 :
                self.F *= 0.9
                self.CR *= 0.9
                self.sigma *= 0.9 #Reduce step size for CMA-ES
                self.stagnation_counter = 0 #Reset Stagnation Counter

            self.stagnation_counter += 1

            # Local search (periodically)
            if self.DE_iterations % 10 == 0:
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x
                    self.stagnation_counter = 0


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'DE_iterations': self.DE_iterations
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

2025-06-22 15:18:52 INFO Unimodal AOCC mean: 0.9380
2025-06-22 15:18:52 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:18:52 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:18:52 INFO AOCC mean: 0.9380
2025-06-22 15:19:02 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:19:03 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.8682
2025-06-22 15:19:03 INFO FeHistory: [123151.26333628 138297.31097824 229725.50001112 ...   -903.36178719
   -902.4548506    -961.76840198]
2025-06-22 15:19:03 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:19:03 INFO Good algorithm:
Algorithm Name: AdaptiveHybridDE_CMAES_Enhanced
import numpy as np
from scipy.optimize import minimize
from cma import CMAEvolutionStrategy

# Name: AdaptiveHybridDE_CMAES_Enhanced
# Description: Hybrid DE/CMA-ES with adaptive parameters and local search for multimodal optimization.
# Code:
class AdaptiveHybridDE_CMAES_Enhanced:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 50  # Adjust as needed
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.sigma = 0.5 #Initial CMA-ES step size
        self.cma = None
        self.population = None
        self.fitnesses = None
        self.stagnation_counter = 0


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.stagnation_counter = 0

        # Initialize Population (hybrid start)
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.fitnesses = objective_function(self.population)
        self.eval_count += self.population_size
        
        best_index = np.argmin(self.fitnesses)
        self.best_solution_overall = self.population[best_index]
        self.best_fitness_overall = self.fitnesses[best_index]
        self.cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10, 'tolfun':1e-10})


        while self.eval_count < self.budget:
            #Differential Evolution Step
            new_population = []
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), size=3, replace=False)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1,-1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitnesses[i]:
                    new_population.append(trial)
                    self.fitnesses[i] = trial_fitness
                    if trial_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = trial_fitness
                        self.best_solution_overall = trial
                        self.stagnation_counter = 0
                        self.cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10, 'tolfun':1e-10}) #Restart CMA-ES
                else:
                    new_population.append(self.population[i])
            self.population = np.array(new_population)

            #CMA-ES Step (interleaved)
            solutions = self.cma.ask()
            solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
            fitnesses = objective_function(solutions)
            self.eval_count += len(fitnesses)
            self.cma.tell(solutions, fitnesses)
            for i, fitness in enumerate(fitnesses):
                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = solutions[i]
                    self.stagnation_counter = 0


            # Adaptive Parameter Control
            if self.stagnation_counter > self.population_size * 0.2: # 20% stagnation
                self.F *= 0.9
                self.CR *= 0.9
                self.sigma *= 1.1 #increase exploration
                self.stagnation_counter = 0


            #Local Search (periodically)
            if (self.eval_count / self.population_size) % 5 == 0:
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, bounds=list(zip(self.lower_bounds, self.upper_bounds)), method='L-BFGS-B')
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x
                    self.stagnation_counter = 0
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, self.sigma, {'popsize': 10, 'tolfun':1e-10}) #Restart CMA-ES

            self.stagnation_counter +=1


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info
2025-06-22 15:19:03 INFO Unimodal AOCC mean: 0.8682
2025-06-22 15:19:03 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:19:03 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:19:03 INFO AOCC mean: 0.8682
2025-06-22 15:19:13 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:19:13 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.0000
2025-06-22 15:19:13 INFO FeHistory: [137001.85515519 130103.02352799 189522.75238021 ...    578.67256959
    578.84445748    579.54456366]
2025-06-22 15:19:13 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:19:13 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:19:13 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:19:13 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:19:13 INFO AOCC mean: 0.0000
2025-06-22 15:19:23 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:19:23 ERROR Can not run the algorithm
2025-06-22 15:19:23 INFO Run function 1 complete. FEHistory len: 111, AOCC: 0.0000
2025-06-22 15:19:23 INFO FeHistory: [179386.91169048 222235.39591857 148498.83258462 163882.12981322
 189353.13463222 131454.39510699 157993.99726227 193634.86842612
 151864.31024968 148184.15688669 146064.85244695 149452.55624575
 184338.03891045 190795.87684468 202283.99978003 224659.1009999
 182060.02925768 178699.32143118 136135.04091792 190075.69659885
 194039.60578184 164172.4527939  181086.40409133 163358.55689002
 234754.0537652  127466.8907909  209135.12010916 192744.47923682
 160057.65440789 309973.59933872 134656.06730287 196131.75825626
 151331.07205795 186597.34063396 249042.74897338 177162.2069166
 155732.45199363 159113.38706604 185653.16296304 197851.67751162
 109577.6205594  123753.93777688 167427.48892073 204453.63629673
 227853.17368309 122514.92864481 159158.38209164 157829.75581566
 143406.97555395 222407.34653238 250834.28715176 235421.2169329
 185197.32893783 191942.28414258 201000.07307655 245545.97203982
 155875.11610192 266219.80317293  99420.7403087  154648.93883975
 190218.84234883 290026.14535037 136196.18148029 207241.81367123
 221752.80945673 303825.49026199 223582.65941732 210640.30076658
 214295.93984002 282604.43558777 249287.36979767 306242.17820933
 256567.02507794 144584.60168496 193503.85186473 214176.11370695
 261668.56061573 284688.47596545 166517.44681139 258873.45753688
 170463.22029111 240576.53327868 234831.92224142 191447.03986328
 284227.12564698 244823.89241935 236206.79830978 247975.3905638
 135038.92397953 182623.43185904 200112.42727385 304460.66027668
 134414.11727824 194134.38449338 224462.65730292 112141.83139164
 100561.3102672  244217.01003986 167860.03446544 229637.68345249
 109659.17537261 109323.32588332 109548.02832289 109980.88842989
 109576.159934   109744.67450236 110167.78658632 109921.59099137
 110210.51136534 109282.49096803 214385.27646894]
2025-06-22 15:19:23 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:19:23 INFO Unimodal AOCC mean: 0.0000
2025-06-22 15:19:23 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:19:23 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:19:23 INFO AOCC mean: 0.0000
2025-06-22 15:19:34 INFO --- GNBG Problem Parameters for f1 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -1081.983799
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-22 15:19:36 INFO Run function 1 complete. FEHistory len: 10000, AOCC: 0.9645
2025-06-22 15:19:36 INFO FeHistory: [146698.97304654 194642.3804899  193251.28322716 ...  -1081.9837994
  -1081.9837994  321272.14980751]
2025-06-22 15:19:36 INFO Expected Optimum FE: -1081.9837994003399
2025-06-22 15:19:36 INFO Good algorithm:
Algorithm Name: AdaptiveHybridCMAESDEwithImprovedNichingAndLocalSearch
import numpy as np
from cma import CMAEvolutionStrategy
from scipy.optimize import minimize
import random

# Name: AdaptiveHybridCMAESDEwithImprovedNichingAndLocalSearch
# Description: Hybrid CMA-ES and DE with adaptive niching, local search, and improved parameter adaptation for multimodal optimization.
# Code:

class AdaptiveHybridCMAESDEwithImprovedNichingAndLocalSearch:
    """
    Combines CMA-ES and DE, adapting niching radius and incorporating local search for multimodal problems.  Improves parameter adaptation and adds a stagnation counter.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 + int(dim / 2)
        self.F = 0.8
        self.CR = 0.9
        self.niching_radius = 0.5
        self.cma = None
        self.local_search_trigger = 1e-2
        self.archive = []  # archive for niching
        self.stagnation_counter = 0
        self.max_stagnation = 10


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))

        while self.eval_count < self.budget:
            # DE Phase with Niching
            offspring = self._differential_evolution(population)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Adaptive Niching
            self.archive.extend(zip(offspring, offspring_fitness))
            population, fitnesses = self._niching(population, offspring, offspring_fitness)

            # Update best solution and check for stagnation
            previous_best = self.best_fitness_overall
            for i, fit in enumerate(fitnesses):
                if fit < self.best_fitness_overall:
                    self.best_fitness_overall = fit
                    self.best_solution_overall = population[i]
                    self.stagnation_counter = 0  # Reset stagnation counter
            if previous_best == self.best_fitness_overall:
                self.stagnation_counter +=1

            # Adaptive CMA-ES and Local Search
            if self.stagnation_counter >= self.max_stagnation or self.best_fitness_overall < self.local_search_trigger:
                if self.cma is None:
                    self.cma = CMAEvolutionStrategy(self.best_solution_overall, 0.5, {'popsize': 10, 'tolfun': 1e-10})
                solutions = self.cma.ask()
                solutions = np.clip(solutions, self.lower_bounds, self.upper_bounds)
                fitnesses = objective_function(solutions)
                self.eval_count += len(fitnesses)
                self.cma.tell(solutions, fitnesses)
                for i, fitness in enumerate(fitnesses):
                    if fitness < self.best_fitness_overall:
                        self.best_fitness_overall = fitness
                        self.best_solution_overall = solutions[i]
                        self.stagnation_counter = 0 #Reset on improvement

                # Local Search (only after CMA-ES)
                result = minimize(lambda x: objective_function(x.reshape(1,-1))[0], self.best_solution_overall, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
                if result.fun < self.best_fitness_overall:
                    self.best_fitness_overall = result.fun
                    self.best_solution_overall = result.x
                    self.stagnation_counter = 0 #Reset on improvement

            # Adapt Niching Radius (example - adjust as needed)
            self.niching_radius = max(0.1, self.niching_radius * 0.95)  # slowly decrease

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _differential_evolution(self, population):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            mutant = population[a] + self.F * (population[b] - population[c])
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring

    def _niching(self, current_pop, offspring, offspring_fitness):
        combined_pop = np.concatenate((current_pop, offspring))
        combined_fitness = np.concatenate((objective_function(current_pop), offspring_fitness))

        unique_population = []
        unique_fitness = []
        for i in range(len(combined_pop)):
            is_unique = True
            for j in range(len(unique_population)):
                distance = np.linalg.norm(combined_pop[i] - unique_population[j])
                if distance < self.niching_radius:
                    is_unique = False
                    break
            if is_unique:
                unique_population.append(combined_pop[i])
                unique_fitness.append(combined_fitness[i])

        return np.array(unique_population[:self.population_size]), np.array(unique_fitness[:self.population_size])

def objective_function(x): #example objective function for testing
    return np.sum(x**2, axis=1)

2025-06-22 15:19:36 INFO Unimodal AOCC mean: 0.9645
2025-06-22 15:19:36 INFO Multimodal (single component) AOCC mean: nan
2025-06-22 15:19:36 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-22 15:19:36 INFO AOCC mean: 0.9645
