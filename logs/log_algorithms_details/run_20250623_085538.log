2025-06-23 08:55:39 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:55:39 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:55:39 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:55:39 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:55:39 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:55:39 ERROR Can not run the algorithm
2025-06-23 08:55:39 ERROR Can not run the algorithm
2025-06-23 08:55:39 INFO Run function 2 complete. FEHistory len: 300, AOCC: 0.1756
2025-06-23 08:55:39 INFO FeHistory: [-701.30030103 -701.30581169 -701.27033861 -701.31124605 -701.33438196
 -701.32865216 -701.3265568  -701.27144113 -701.35821012 -701.35120466
 -701.31740831 -701.31189044 -701.30223772 -701.28265206 -701.30440369
 -701.29641961 -701.31162011 -701.30887808 -701.31441214 -701.33247947
 -701.31728047 -701.30675787 -701.31627111 -701.31614837 -701.31566379
 -701.30214302 -701.3059276  -701.27630272 -701.30275175 -701.30061692
 -701.32780413 -701.29606386 -701.2775066  -701.30618314 -701.3351802
 -701.30645399 -701.33080201 -701.34882311 -701.297655   -701.3313851
 -701.30665763 -701.29006999 -701.31781094 -701.30571376 -701.28496967
 -701.27432436 -701.31395387 -701.32914796 -701.34168141 -701.2658158
 -701.32320514 -701.30874908 -701.31116698 -701.29045226 -701.32844099
 -701.31573673 -701.31332891 -701.32019668 -701.3331339  -701.30960385
 -701.29737978 -701.28474606 -701.32644772 -701.31610186 -701.2991807
 -701.32573092 -701.30846509 -701.37938434 -701.32918629 -701.3065735
 -701.31702381 -701.28858794 -701.30010347 -701.30418156 -701.28213716
 -701.31266366 -701.31881575 -701.30239676 -701.29167785 -701.29661797
 -701.28791524 -701.2908759  -701.29320523 -701.28227835 -701.30800784
 -701.30920776 -701.3107877  -701.32433797 -701.32300479 -701.33121244
 -701.26650508 -701.31298364 -701.30301458 -701.34029755 -701.2990625
 -701.29903787 -701.29527402 -701.30393685 -701.300145   -701.28197046
 -701.29262042 -701.33409876 -701.30542538 -701.3103862  -701.28104661
 -701.28834949 -701.29601089 -701.29327262 -701.30855242 -701.30368321
 -701.31465319 -701.32357208 -701.32727167 -701.27503844 -701.32990321
 -701.30878795 -701.31874139 -701.28704481 -701.29943893 -701.28058494
 -701.33173153 -701.32690037 -701.31899247 -701.28617312 -701.29358382
 -701.30487058 -701.28753784 -701.29769761 -701.31298045 -701.31768674
 -701.28510219 -701.30597106 -701.29509524 -701.32829806 -701.28803904
 -701.30542637 -701.29882781 -701.29762952 -701.33514315 -701.30381114
 -701.29490056 -701.32206252 -701.34090741 -701.29823746 -701.35117411
 -701.30686229 -701.30324318 -701.29954692 -701.3184277  -701.33888691
 -701.32149836 -701.29036422 -701.31706189 -701.32308808 -701.33111472
 -701.29273716 -701.29139226 -701.32918015 -701.31882485 -701.32839938
 -701.33957533 -701.31572791 -701.31867632 -701.35558671 -701.33715999
 -701.28964109 -701.27374944 -701.30525251 -701.27113755 -701.3222022
 -701.32780062 -701.33145305 -701.33142278 -701.28515679 -701.27810977
 -701.28460132 -701.32770934 -701.2857245  -701.3038165  -701.29527728
 -701.29852187 -701.31733866 -701.3278836  -701.31399363 -701.29727572
 -701.30973807 -701.28530153 -701.29689093 -701.30366279 -701.28807554
 -701.31515031 -701.29215598 -701.33345328 -701.3008901  -701.28733719
 -701.31817284 -701.30168102 -701.31176287 -701.29186708 -701.27349461
 -701.29938039 -701.28699361 -701.31878348 -701.33763314 -701.30733552
 -701.2982674  -701.29020341 -701.29282887 -701.33434397 -701.30330979
 -701.30005284 -701.30179492 -701.31727209 -701.29820359 -701.30181027
 -701.30320353 -701.28235023 -701.31401232 -701.30126566 -701.30117359
 -701.31280339 -701.26082701 -701.31521489 -701.31143868 -701.31129903
 -701.32398331 -701.33270284 -701.29221282 -701.31318614 -701.27498318
 -701.30507407 -701.28304652 -701.35082143 -701.32776911 -701.30829794
 -701.29957493 -701.32636509 -701.29918408 -701.29622146 -701.3126889
 -701.34471092 -701.30926211 -701.3030064  -701.29101635 -701.31276286
 -701.31082776 -701.28055811 -701.31762583 -701.33537354 -701.31254926
 -701.27465229 -701.33588879 -701.28778132 -701.34361626 -701.31684891
 -701.28982145 -701.32816209 -701.31385981 -701.31592133 -701.26885062
 -701.28910162 -701.31947824 -701.31437306 -701.31797974 -701.33371832
 -701.31540489 -701.32685216 -701.34042091 -701.30781375 -701.28700264
 -701.30580514 -701.31443629 -701.31107847 -701.29010341 -701.27474526
 -701.32976397 -701.30485106 -701.30410687 -701.33124104 -701.31115954
 -701.28889566 -701.26824425 -701.27425641 -701.28304615 -701.30588776
 -701.31072721 -701.30296152 -701.3170067  -701.35262429 -701.30387589
 -701.32345997 -701.31566942 -701.33748298 -701.31996073 -701.34005042
 -701.30087503 -701.26567371 -701.32388924 -701.29210631 -701.29778664]
2025-06-23 08:55:39 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:55:39 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveDE
import numpy as np
import random

# Name: ArchiveGuidedAdaptiveDE
# Description: Differential Evolution with adaptive scaling and archive-guided exploration for multimodal optimization.
class ArchiveGuidedAdaptiveDE:
    """
    Differential Evolution with adaptive scaling and archive-guided exploration for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * dim  # Heuristic: population size
        self.archive_size = 200  # Increased archive size for diversity
        self.archive = []
        self.F_scale = 0.5  # Initial scaling factor
        self.F_scale_delta = 0.1 # Adaptive parameter for F_scale adjustment
        self.CR = 0.9 # Crossover Rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population,fitness)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self._update_archive(offspring, offspring_fitness)
            self.population, fitness = self._select_next_generation(offspring,offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population,fitness)
            self._adapt_F_scale(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = np.zeros((self.population_size, self.dim))
        for i in range(self.population_size):
            pbest = self._select_pbest()
            a, b, c = self._select_different_indices(i)
            mutant = self.population[i] + self.F_scale * (pbest - self.population[i] + self.population[a] - self.population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(self.population[i], mutant)
        return offspring

    def _select_pbest(self):
        if self.archive:
            return random.choice(self.archive)[0]
        else:
            return self.population[np.argmin(np.array([f for _, f in self.archive]))]

    def _select_different_indices(self, i):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == i or b == i or c == i:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c
    
    def _crossover(self,x,v):
        return np.where(np.random.rand(self.dim) < self.CR, v, x)

    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def _select_next_generation(self, offspring, offspring_fitness):
        combined_population = np.concatenate((self.population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        return combined_population[indices[:self.population_size]], combined_fitness[indices[:self.population_size]]

    def _adapt_F_scale(self, fitness):
        if np.min(fitness) < self.best_fitness_overall:
            self.F_scale = min(1.0, self.F_scale + self.F_scale_delta)
        else:
            self.F_scale = max(0.0, self.F_scale - self.F_scale_delta)
    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]
        return self.best_solution_overall, self.best_fitness_overall
2025-06-23 08:55:39 INFO Unimodal AOCC mean: 0.1756
2025-06-23 08:55:39 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:55:39 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:55:39 INFO AOCC mean: 0.1756
2025-06-23 08:55:39 INFO Run function 2 complete. FEHistory len: 300, AOCC: 0.1756
2025-06-23 08:55:39 INFO FeHistory: [-701.30225781 -701.30827921 -701.31335237 -701.36166869 -701.34312833
 -701.30913578 -701.31143967 -701.30010784 -701.35732768 -701.34223321
 -701.28627362 -701.31464854 -701.32468787 -701.28322708 -701.26863974
 -701.33236015 -701.29326774 -701.27737224 -701.30679546 -701.32763447
 -701.29556397 -701.28962169 -701.30947589 -701.32045039 -701.31273724
 -701.31391593 -701.35983155 -701.35322126 -701.2901212  -701.30728372
 -701.29261444 -701.29027733 -701.29006625 -701.30229593 -701.35184694
 -701.32745249 -701.33011819 -701.29835928 -701.2859808  -701.28413812
 -701.33960038 -701.32506086 -701.32592247 -701.31214216 -701.3196703
 -701.30834936 -701.29091127 -701.33782952 -701.32712007 -701.29341854
 -701.3150532  -701.32128441 -701.34650079 -701.34939804 -701.31308769
 -701.31257237 -701.32946163 -701.29188856 -701.31071625 -701.32418034
 -701.33392809 -701.33259947 -701.28059774 -701.32707205 -701.32607936
 -701.31240922 -701.31161715 -701.31786692 -701.30074941 -701.32652838
 -701.31313141 -701.31406711 -701.28363943 -701.36666978 -701.31366682
 -701.31910868 -701.27220106 -701.31477549 -701.31365833 -701.27798579
 -701.31873436 -701.3023738  -701.2996892  -701.30749778 -701.32160644
 -701.28535022 -701.33709608 -701.29021084 -701.32462497 -701.31419847
 -701.29144528 -701.33488701 -701.32695742 -701.30154156 -701.31123726
 -701.30134528 -701.31764029 -701.29909421 -701.30732639 -701.29488059
 -701.32575832 -701.31432749 -701.29071242 -701.29126769 -701.31181003
 -701.32676225 -701.3057445  -701.31158933 -701.30313404 -701.3120344
 -701.30073204 -701.36893025 -701.28286796 -701.30793214 -701.30609862
 -701.33427327 -701.30566243 -701.3288622  -701.36423099 -701.30758431
 -701.29934828 -701.32523248 -701.2953747  -701.34560328 -701.31019978
 -701.28474788 -701.30614654 -701.31029517 -701.31709708 -701.32716487
 -701.33847781 -701.29435288 -701.28345293 -701.31300668 -701.31233299
 -701.30769113 -701.28722464 -701.2972493  -701.30075234 -701.32456282
 -701.33479353 -701.27483081 -701.30944991 -701.35613594 -701.37916302
 -701.32212175 -701.32455738 -701.308524   -701.35999261 -701.3119214
 -701.31148739 -701.29208135 -701.29335953 -701.33112147 -701.29740872
 -701.33417708 -701.33447637 -701.29664641 -701.3329042  -701.32624641
 -701.32357796 -701.31206677 -701.2967199  -701.29794169 -701.29468789
 -701.31203087 -701.31694497 -701.32489559 -701.2997342  -701.27395635
 -701.30272538 -701.34732591 -701.33686269 -701.30132234 -701.27223632
 -701.30329039 -701.31448324 -701.31549311 -701.28590371 -701.31592551
 -701.31311662 -701.29768749 -701.31592411 -701.31334316 -701.31457858
 -701.31548086 -701.28783929 -701.30917234 -701.29660749 -701.32531136
 -701.30523013 -701.3231691  -701.31422618 -701.32132409 -701.33696731
 -701.32152403 -701.34297723 -701.29662781 -701.35524655 -701.29571251
 -701.31632862 -701.34033071 -701.31928564 -701.31363597 -701.32415933
 -701.28861743 -701.28596942 -701.29812998 -701.26932599 -701.29024229
 -701.29337165 -701.2972124  -701.34348216 -701.29382915 -701.29498533
 -701.31860557 -701.33071922 -701.30017975 -701.31557713 -701.30622416
 -701.33528193 -701.31337349 -701.30931031 -701.31433372 -701.30660856
 -701.31177407 -701.32940443 -701.31740252 -701.31032477 -701.34227012
 -701.30596039 -701.33329309 -701.30746301 -701.28923468 -701.3182754
 -701.31909206 -701.33995309 -701.28336006 -701.32233299 -701.28604377
 -701.30959217 -701.31551114 -701.31534862 -701.28410178 -701.3109326
 -701.35935266 -701.3123845  -701.30178379 -701.29103047 -701.30056172
 -701.31656793 -701.29692993 -701.27098353 -701.32568716 -701.30158991
 -701.30918771 -701.32439521 -701.2910973  -701.30583337 -701.29934067
 -701.31706894 -701.29461383 -701.29447769 -701.30270841 -701.294326
 -701.30962319 -701.30010704 -701.32983068 -701.3121345  -701.31567567
 -701.30243392 -701.30689767 -701.31393244 -701.3269143  -701.28333389
 -701.33165782 -701.30660331 -701.29372877 -701.30932056 -701.3134543
 -701.29512048 -701.34080128 -701.33854411 -701.32289417 -701.320553
 -701.33001416 -701.30053288 -701.30083932 -701.2899042  -701.32409745
 -701.30780123 -701.30298154 -701.33060964 -701.36085249 -701.30339351
 -701.28823062 -701.30363473 -701.29160888 -701.3024184  -701.2982166 ]
2025-06-23 08:55:39 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:55:39 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveDE
import numpy as np
import random

# Name: ArchiveGuidedAdaptiveDE
# Description: Differential Evolution with adaptive scaling and archive-guided exploration for multimodal optimization.
class ArchiveGuidedAdaptiveDE:
    """
    Differential Evolution with adaptive scaling factor and archive-guided exploration for efficiently solving multimodal optimization problems.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * self.dim
        self.archive_size = 200  # Increased archive size for better diversity
        self.archive = []
        self.F_scale = 0.5
        self.CR = 0.9 # Crossover rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self.update_archive(offspring, offspring_fitness)
            self.population, fitness = self.selection(self.population, fitness, offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        self.F_scale = 0.5 + 0.3 * np.random.rand()  # Adaptive scaling factor
        for i in range(self.population_size):
            pbest = self._select_pbest()
            a, b, c = self._select_different(i, self.population_size)
            mutant = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(population[i], mutant, self.CR)

        return offspring


    def _select_pbest(self):
        if self.archive:
            return random.choice(self.archive)[0]
        else:
            return self.population[np.argmin(np.concatenate((self.best_fitness_overall,self.population)))]

    def _crossover(self, x, v, CR):
        mask = np.random.rand(self.dim) < CR
        return np.where(mask, v, x)

    def _select_different(self, index, population_size):
        a, b, c = random.sample(range(population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(population_size), 3)
        return a, b, c


    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def selection(self, population, fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        return combined_population[indices[:self.population_size]], combined_fitness[indices[:self.population_size]]

    def _find_best(self, population, fitness):
        best_index = np.argmin(fitness)
        return population[best_index], fitness[best_index]
2025-06-23 08:55:39 INFO Unimodal AOCC mean: 0.1756
2025-06-23 08:55:39 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:55:39 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:55:39 INFO AOCC mean: 0.1756
2025-06-23 08:55:40 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1762
2025-06-23 08:55:40 INFO FeHistory: [-701.29132622 -701.31218814 -701.25313088 ... -701.35422938 -701.35426747
 -701.35421585]
2025-06-23 08:55:40 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:55:40 INFO Good algorithm:
Algorithm Name: ArchiveGuidedAdaptiveDEwithLocalSearch
import numpy as np
import random

# Name: ArchiveGuidedAdaptiveDEwithLocalSearch
# Description: Combines Differential Evolution, adaptive archive, and local search for multimodal optimization.
# Code:
class ArchiveGuidedAdaptiveDEwithLocalSearch:
    """
    Combines Differential Evolution, adaptive archive, and local search for efficient multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim
        self.archive_size = 200  # Increased archive size for diversity
        self.archive = []
        self.population = None
        self.F_scale = 0.5
        self.CR = 0.9 # Crossover rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Local Search on offspring
            offspring, offspring_fitness = self.local_search(offspring, offspring_fitness, objective_function)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            #Select best
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        self.F_scale = 0.5 + 0.3 * np.random.rand()

        for i in range(self.population_size):
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            mutant = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            #Crossover
            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i] = np.where(cross_points, mutant, population[i])

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def local_search(self, offspring, offspring_fitness, objective_function):
        local_search_iterations = 5
        for i in range(len(offspring)):
            current = offspring[i]
            for _ in range(local_search_iterations):
                neighbor = current + np.random.normal(0, 0.1, self.dim)
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
                self.eval_count += 1
                if neighbor_fitness < offspring_fitness[i]:
                    current = neighbor
                    offspring_fitness[i] = neighbor_fitness
            offspring[i] = current
        return offspring, offspring_fitness
2025-06-23 08:55:40 INFO Unimodal AOCC mean: 0.1762
2025-06-23 08:55:40 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:55:40 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:55:40 INFO AOCC mean: 0.1762
2025-06-23 08:55:49 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1753
2025-06-23 08:55:49 INFO FeHistory: [-701.31532436 -701.29252411 -701.28013074 ... -701.29371004 -701.3223882
 -701.35108465]
2025-06-23 08:55:49 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:55:49 INFO Good algorithm:
Algorithm Name: AdaptiveArchiveSimulatedAnnealingEA
import numpy as np
import random

class AdaptiveArchiveSimulatedAnnealingEA:
    """
    Combines adaptive Gaussian sampling with an archive and simulated annealing to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []
        self.temperature = 1.0
        self.cooling_rate = 0.95

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            for i in range(self.population_size):
                solution = population[i]
                neighbor = self._generate_neighbor(solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1
                
                if self._accept(neighbor_fitness, fitness_values[i], self.temperature):
                    population[i] = neighbor
                    fitness_values[i] = neighbor_fitness

            self.archive = self._update_archive(population, fitness_values)
            self._update_best(population, fitness_values)
            self.sigma *= self.sigma_decay
            self.temperature *= self.cooling_rate

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _generate_neighbor(self, solution):
        neighbor = solution + np.random.normal(0, self.sigma, self.dim)
        return np.clip(neighbor, self.lower_bounds, self.upper_bounds)

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability

    def _update_best(self, population, fitness_values):
        for i, fitness in enumerate(fitness_values):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = population[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 08:55:49 INFO Unimodal AOCC mean: 0.1753
2025-06-23 08:55:49 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:55:49 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:55:49 INFO AOCC mean: 0.1753
2025-06-23 08:56:05 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1774
2025-06-23 08:56:05 INFO FeHistory: [-701.32133971 -701.30798552 -701.2995921  ... -701.39576336 -701.39053776
 -701.39188568]
2025-06-23 08:56:05 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:56:05 INFO Good algorithm:
Algorithm Name: AdaptiveArchiveGaussianDE
import numpy as np
import random

class AdaptiveArchiveGaussianDE:
    """
    Combines Differential Evolution with Gaussian mutation, adaptive sigma, and an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []
        self.F = 0.8 #Differential Evolution scaling factor
        self.CR = 0.9 #Differential Evolution crossover rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = float('inf') if self.dim > 0 else 0

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _differential_evolution(self, population, fitness_values):
        offspring = []
        for i in range(self.population_size):
            a, b, c = self._select_different_individuals(population, i)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring.append(trial)
        return np.array(offspring)
    
    def _select_different_individuals(self, population, i):
        indices = list(range(self.population_size))
        indices.remove(i)
        a,b,c = random.sample(indices,3)
        return population[a], population[b], population[c]

    def _crossover(self, x, v):
        jrand = random.randint(0,self.dim-1)
        y = np.copy(x)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                y[j] = v[j]
        return y

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

2025-06-23 08:56:05 INFO Unimodal AOCC mean: 0.1774
2025-06-23 08:56:05 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:56:05 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:56:05 INFO AOCC mean: 0.1774
2025-06-23 08:56:23 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:56:23 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:56:24 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1772
2025-06-23 08:56:24 INFO FeHistory: [-701.31794907 -701.29884161 -701.30512094 ... -701.54483101 -701.56088265
 -701.55430947]
2025-06-23 08:56:24 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:56:24 INFO Good algorithm:
Algorithm Name: AdaptiveArchiveOptimizer
import numpy as np
import random

# Name: AdaptiveArchiveOptimizer
# Description: A hybrid evolutionary algorithm employing adaptive parameter tuning, diverse exploration via multiple mutation strategies, and an archive-based search to effectively navigate multimodal landscapes.

class AdaptiveArchiveOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.archive = []  # Archive of diverse solutions
        self.archive_size = 100  # Maximum archive size
        self.population_size = 50
        self.mutation_rate = 0.2
        self.exploration_factor = 1.0 # Initially high exploration
        self.exploitation_factor = 0.1


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self._update_best(population, fitness_values)
        self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            self._update_best(offspring, offspring_fitness)
            self._update_archive(offspring, offspring_fitness)

            population = self._select_population(population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(population)
            self.eval_count += self.population_size

            #Adapt parameters based on search progress
            self._adapt_parameters()


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population):
        offspring = []
        for i in range(self.population_size):
            parent1 = population[random.randint(0, self.population_size -1)]
            parent2 = population[random.randint(0, self.population_size -1)]

            child = self._crossover(parent1, parent2)
            child = self._mutate(child)
            offspring.append(child)

        return np.array(offspring)


    def _crossover(self, parent1, parent2):
        crossover_point = random.randint(1, self.dim - 1)
        child = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))
        return child

    def _mutate(self, individual):
        for i in range(self.dim):
            if random.random() < self.mutation_rate:
                individual[i] += np.random.normal(0, self.exploration_factor * (self.upper_bounds[i] - self.lower_bounds[i]))
        individual = np.clip(individual, self.lower_bounds, self.upper_bounds)
        return individual

    def _update_best(self, population, fitness_values):
        for i, fitness in enumerate(fitness_values):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = population[i]

    def _update_archive(self, population, fitness_values):
        for i, solution in enumerate(population):
            self._add_to_archive(solution, fitness_values[i])
        #Maintaining diversity.  This is a rudimentary method, more sophisticated ones could be implemented.
        if len(self.archive) > self.archive_size:
            self.archive.sort(key=lambda x: x[1]) #Sort by fitness
            self.archive = self.archive[:self.archive_size]



    def _add_to_archive(self, solution, fitness):
        self.archive.append((solution, fitness))


    def _select_population(self, parent_population, parent_fitness, offspring_population, offspring_fitness):
        combined_population = np.concatenate((parent_population, offspring_population))
        combined_fitness = np.concatenate((parent_fitness, offspring_fitness))
        
        indices = np.argsort(combined_fitness)
        selected_population = combined_population[indices[:self.population_size]]
        
        return selected_population


    def _adapt_parameters(self):
        #Simple adaptive mechanism.  More sophisticated methods could be used here.
        if self.eval_count > self.budget * 0.7: #Shift towards exploitation later in the search
            self.exploration_factor *= 0.9
            self.exploitation_factor *= 1.1
        



2025-06-23 08:56:24 INFO Unimodal AOCC mean: 0.1772
2025-06-23 08:56:24 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:56:24 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:56:24 INFO AOCC mean: 0.1772
2025-06-23 08:56:24 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1914
2025-06-23 08:56:24 INFO FeHistory: [-701.29849232 -701.32569409 -701.31244142 ... -702.06360324 -702.03971962
 -702.05188382]
2025-06-23 08:56:24 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:56:24 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalExplorationOptimizer
import numpy as np
import random

# Name: AdaptiveMultimodalExplorationOptimizer
# Description: An adaptive evolutionary algorithm combining diverse exploration strategies and an archive for robust multimodal optimization.
# Code:
class AdaptiveMultimodalExplorationOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 50  # Initial population size
        self.archive = []  # Archive of diverse solutions
        self.archive_size = 100  # Maximum archive size
        self.mutation_rate = 0.1  # Initial mutation rate
        self.exploration_rate = 0.8  # Initial exploration rate (global vs. local search)
        self.diversity_threshold = 0.1

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self._initialize_population()
        fitness = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection: Tournament selection
            parents = self._tournament_selection(population, fitness)

            # Genetic Operators: Mutation & Crossover
            offspring = self._generate_offspring(parents)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)


            # Archive Management
            self._update_archive(offspring, offspring_fitness)

            # Adaptive Parameter Tuning
            self._adapt_parameters()

            # Combine population and offspring
            population = np.vstack((population, offspring))
            fitness = np.concatenate((fitness, offspring_fitness))

            # Select top individuals
            idx = np.argsort(fitness)
            population = population[idx[:self.population_size]]
            fitness = fitness[idx[:self.population_size]]
            

            # Update best solution
            best_index = np.argmin(fitness)
            if fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[best_index]
                self.best_solution_overall = population[best_index]


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive),
            'final_mutation_rate': self.mutation_rate,
            'final_exploration_rate': self.exploration_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))

    def _tournament_selection(self, population, fitness, tournament_size=5):
        selected_indices = np.random.choice(len(population), size=self.population_size, replace=True)
        fitness_selected = fitness[selected_indices]
        
        best_in_tournament = np.argmin(fitness_selected)
        best_index = selected_indices[best_in_tournament]
        return population[[best_index]]
        

    def _generate_offspring(self, parents):
        offspring = []
        for _ in range(self.population_size):
            parent = parents[0]
            offspring_individual = parent + np.random.normal(0, self.mutation_rate * (self.upper_bounds - self.lower_bounds), self.dim)
            offspring_individual = np.clip(offspring_individual, self.lower_bounds, self.upper_bounds)
            offspring.append(offspring_individual)

        return np.array(offspring)


    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                # Simple replacement strategy (replace worst in archive)
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def _adapt_parameters(self):
        if len(self.archive) > 0:
             diversity = np.std([np.linalg.norm(x) for x, _ in self.archive])
             if diversity < self.diversity_threshold :
                self.mutation_rate *= 1.1 #increase mutation rate
             else:
                 self.mutation_rate *= 0.9 #decrease mutation rate
                 


2025-06-23 08:56:24 INFO Unimodal AOCC mean: 0.1914
2025-06-23 08:56:24 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:56:24 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:56:24 INFO AOCC mean: 0.1914
2025-06-23 08:57:32 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:57:32 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:57:32 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:57:32 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:57:32 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:57:32 ERROR Can not run the algorithm
2025-06-23 08:57:32 ERROR Can not run the algorithm
2025-06-23 08:57:32 INFO Run function 2 complete. FEHistory len: 300, AOCC: 0.1755
2025-06-23 08:57:32 INFO FeHistory: [-701.28665494 -701.30596497 -701.30648758 -701.28728507 -701.28110716
 -701.29587711 -701.32682172 -701.30939503 -701.30642031 -701.30961607
 -701.30414795 -701.31677543 -701.29730688 -701.32560267 -701.2942763
 -701.29339311 -701.33095719 -701.30004731 -701.30311498 -701.32513366
 -701.27463391 -701.27453456 -701.28722347 -701.30116241 -701.33808402
 -701.30268409 -701.31755442 -701.27342565 -701.32832945 -701.30421291
 -701.28778301 -701.32184836 -701.31075221 -701.314604   -701.29476462
 -701.30172156 -701.2817708  -701.33706804 -701.33333063 -701.34254989
 -701.35050016 -701.29021569 -701.30161341 -701.30782671 -701.28942638
 -701.3386732  -701.30743371 -701.30811327 -701.3060619  -701.29291868
 -701.32854206 -701.28041191 -701.33517604 -701.35189173 -701.2840598
 -701.29961195 -701.32072828 -701.30212079 -701.2842629  -701.3534367
 -701.32936933 -701.30370007 -701.31367494 -701.30523606 -701.29324029
 -701.30726442 -701.32353923 -701.3133606  -701.29705194 -701.28865522
 -701.36788654 -701.32462923 -701.29487755 -701.34755835 -701.29311697
 -701.3537119  -701.29305543 -701.30971507 -701.27573553 -701.3095959
 -701.31207388 -701.32950904 -701.29969314 -701.29381256 -701.32841798
 -701.34706745 -701.3193715  -701.30669862 -701.33644983 -701.30305795
 -701.30420104 -701.31164941 -701.30003552 -701.30208418 -701.29922132
 -701.33525726 -701.32787547 -701.31135089 -701.28845899 -701.27259414
 -701.29557647 -701.2912592  -701.32270772 -701.3217889  -701.3084046
 -701.30868473 -701.32121487 -701.28649783 -701.34654082 -701.33629645
 -701.32474518 -701.30845023 -701.30994406 -701.31046006 -701.34551207
 -701.31250041 -701.28442414 -701.27995836 -701.30406991 -701.34722637
 -701.3156977  -701.32431665 -701.26677756 -701.31854428 -701.28845284
 -701.31935696 -701.32827616 -701.28718465 -701.32160524 -701.2935112
 -701.31842716 -701.3419933  -701.30531603 -701.31835135 -701.31187123
 -701.33815252 -701.33318865 -701.33596463 -701.29612856 -701.3004982
 -701.35267622 -701.30211343 -701.34178032 -701.30523596 -701.32041725
 -701.29477409 -701.27604874 -701.33385096 -701.30134473 -701.33372151
 -701.30772966 -701.33685895 -701.3316729  -701.35748418 -701.2831348
 -701.33975824 -701.30867187 -701.32016572 -701.32481429 -701.30539104
 -701.29395122 -701.31065898 -701.33906164 -701.32596462 -701.28875761
 -701.29198985 -701.318781   -701.29244471 -701.32045242 -701.32041846
 -701.29345066 -701.31795446 -701.28402596 -701.32682168 -701.35336442
 -701.27323425 -701.29935806 -701.30337963 -701.28821724 -701.30271166
 -701.32500402 -701.31461449 -701.31061885 -701.30200824 -701.29784769
 -701.28249424 -701.30188064 -701.30866146 -701.36142934 -701.33612952
 -701.2682059  -701.29589081 -701.34881661 -701.30406292 -701.2897709
 -701.3235104  -701.32226707 -701.33566432 -701.30441689 -701.28857818
 -701.3184984  -701.28464961 -701.30008208 -701.32795489 -701.30533572
 -701.32650923 -701.30396343 -701.32005602 -701.29302861 -701.30139204
 -701.30002498 -701.30661271 -701.3291683  -701.32263401 -701.29651699
 -701.31355774 -701.32218989 -701.32845918 -701.29434589 -701.30830298
 -701.30257196 -701.31983742 -701.29297033 -701.32975891 -701.3183652
 -701.2919472  -701.31019223 -701.29571549 -701.32995599 -701.31146175
 -701.34568073 -701.28765832 -701.30027653 -701.35398713 -701.31446839
 -701.31011579 -701.29066223 -701.30153789 -701.28191702 -701.32527366
 -701.32221472 -701.33757687 -701.33160569 -701.30144962 -701.27788849
 -701.31354579 -701.32969628 -701.2959738  -701.30941176 -701.29447919
 -701.31800723 -701.33625672 -701.27685778 -701.30846765 -701.27708689
 -701.24692242 -701.31703914 -701.31282342 -701.29088088 -701.2757513
 -701.30723802 -701.31582011 -701.29425281 -701.31764482 -701.28198673
 -701.28249255 -701.29690247 -701.30693038 -701.30246019 -701.33759945
 -701.34911771 -701.30276604 -701.31605715 -701.31004675 -701.30502891
 -701.33175147 -701.29683523 -701.2907609  -701.28497994 -701.34709353
 -701.30717747 -701.3082601  -701.30080004 -701.2915541  -701.31559273
 -701.29301493 -701.31725361 -701.32847325 -701.28886169 -701.37413731
 -701.32282765 -701.31471262 -701.32254416 -701.30052475 -701.28552223
 -701.31532205 -701.34281329 -701.3078378  -701.32489258 -701.3037252 ]
2025-06-23 08:57:32 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:57:32 INFO Good algorithm:
Algorithm Name: AdaptiveArchiveDEwithLocalSearch
import numpy as np
import random

# Name: AdaptiveArchiveDEwithLocalSearch
# Description: Differential Evolution with adaptive scaling, archive, and local search for multimodal optimization.
class AdaptiveArchiveDEwithLocalSearch:
    """
    Combines Differential Evolution with adaptive scaling, an archive for diversity, and local search to escape local optima in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * dim
        self.archive_size = 200
        self.archive = []
        self.F_scale = 0.5
        self.F_scale_delta = 0.1
        self.CR = 0.9
        self.local_search_iterations = 5 # Number of local search steps

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring = self._local_search(offspring, objective_function) # Apply local search
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self._update_archive(offspring, offspring_fitness)
            self.population, fitness = self._select_next_generation(offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness)
            self._adapt_F_scale(fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = np.zeros((self.population_size, self.dim))
        for i in range(self.population_size):
            pbest = self._select_pbest()
            a, b, c = self._select_different_indices(i)
            mutant = self.population[i] + self.F_scale * (pbest - self.population[i] + self.population[a] - self.population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(self.population[i], mutant)
        return offspring

    def _select_pbest(self):
        if self.archive:
            return random.choice(self.archive)[0]  # Randomly select from archive for diversity
        else:
            return self.population[np.argmin(objective_function(self.population))]

    def _select_different_indices(self, i):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == i or b == i or c == i:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _crossover(self, x, v):
        return np.where(np.random.rand(self.dim) < self.CR, v, x)

    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def _select_next_generation(self, offspring, offspring_fitness):
        combined_population = np.concatenate((self.population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        return combined_population[indices[:self.population_size]], combined_fitness[indices[:self.population_size]]

    def _adapt_F_scale(self, fitness):
        if np.min(fitness) < self.best_fitness_overall:
            self.F_scale = min(1.0, self.F_scale + self.F_scale_delta)
        else:
            self.F_scale = max(0.0, self.F_scale - self.F_scale_delta)

    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]
        return self.best_solution_overall, self.best_fitness_overall

    def _local_search(self, offspring, objective_function):
        for i in range(len(offspring)):
            current_solution = offspring[i].copy()
            current_fitness = objective_function(current_solution.reshape(1,-1))[0]
            for _ in range(self.local_search_iterations):
                neighbor = current_solution + np.random.normal(0, 0.1, self.dim) #Small perturbation
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
                if neighbor_fitness < current_fitness:
                    current_solution = neighbor
                    current_fitness = neighbor_fitness
            offspring[i] = current_solution
        return offspring

2025-06-23 08:57:32 INFO Unimodal AOCC mean: 0.1755
2025-06-23 08:57:32 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:57:32 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:57:32 INFO AOCC mean: 0.1755
2025-06-23 08:57:32 INFO Run function 2 complete. FEHistory len: 300, AOCC: 0.1753
2025-06-23 08:57:32 INFO FeHistory: [-701.26040537 -701.32954317 -701.32109037 -701.30635424 -701.30101742
 -701.30874862 -701.32062322 -701.34366906 -701.31335728 -701.32559633
 -701.30034719 -701.33527206 -701.29992662 -701.28736049 -701.34870083
 -701.31303954 -701.29580093 -701.33344388 -701.2913981  -701.29024767
 -701.2958358  -701.33906608 -701.31421746 -701.32639769 -701.32413335
 -701.31414854 -701.31345589 -701.29103173 -701.32014087 -701.31355955
 -701.3244653  -701.3244215  -701.30788157 -701.30721911 -701.32633729
 -701.34837353 -701.31172518 -701.30868989 -701.30362596 -701.35134204
 -701.32387845 -701.32965317 -701.29500917 -701.29131492 -701.31488244
 -701.36497534 -701.29914328 -701.30665707 -701.32211705 -701.30325292
 -701.3097127  -701.31650123 -701.31003033 -701.29623856 -701.31218978
 -701.33882917 -701.32335127 -701.2966405  -701.30451049 -701.28638783
 -701.2912505  -701.32777212 -701.34292473 -701.30595183 -701.27516713
 -701.31623599 -701.29542279 -701.29586433 -701.27738964 -701.32219056
 -701.31615853 -701.31857672 -701.33906855 -701.28797965 -701.31938445
 -701.29845536 -701.30876834 -701.29216342 -701.32451224 -701.29390433
 -701.30843094 -701.33254468 -701.33530543 -701.28184513 -701.29768733
 -701.29754798 -701.31136106 -701.29556977 -701.30379168 -701.32644959
 -701.30580307 -701.30088098 -701.29935631 -701.32546197 -701.3028461
 -701.31058149 -701.30026446 -701.36183172 -701.28258795 -701.32499427
 -701.30662386 -701.31424661 -701.35811618 -701.29245516 -701.31142366
 -701.29698443 -701.29119345 -701.33887115 -701.300814   -701.31145888
 -701.3107028  -701.32547438 -701.2899708  -701.28215144 -701.34159464
 -701.33252297 -701.2942517  -701.34710327 -701.32776077 -701.30476998
 -701.30508061 -701.31895844 -701.3104523  -701.31933515 -701.31343684
 -701.29976667 -701.31454578 -701.31595909 -701.31802143 -701.30209814
 -701.30946444 -701.30805396 -701.30013176 -701.34161755 -701.27568802
 -701.27564435 -701.3356692  -701.31768852 -701.28704642 -701.3033515
 -701.33351754 -701.28475108 -701.28223975 -701.28003016 -701.30747896
 -701.30710201 -701.29833963 -701.32290306 -701.29176194 -701.28008709
 -701.34603313 -701.30048248 -701.30556916 -701.27641244 -701.28189663
 -701.3085273  -701.29904801 -701.33634872 -701.30179907 -701.29837358
 -701.32300578 -701.30890245 -701.30389654 -701.32349729 -701.27966732
 -701.30585468 -701.29702165 -701.344216   -701.3230194  -701.31037135
 -701.29448075 -701.31232318 -701.32577811 -701.33101503 -701.28984647
 -701.3033543  -701.32474336 -701.29073384 -701.3038864  -701.27647461
 -701.32772603 -701.29523906 -701.27351039 -701.29274566 -701.33544194
 -701.32834267 -701.31963427 -701.34588566 -701.32678968 -701.30290236
 -701.34007661 -701.30748524 -701.28976718 -701.31326921 -701.29587377
 -701.29009842 -701.32562747 -701.33183146 -701.28659113 -701.30889913
 -701.28423325 -701.33189076 -701.3383408  -701.3056398  -701.32146104
 -701.27131686 -701.30929082 -701.30697473 -701.28141605 -701.31342142
 -701.27191715 -701.3063417  -701.2862937  -701.29775818 -701.32781123
 -701.3090068  -701.27893396 -701.34650821 -701.30758872 -701.3156253
 -701.28566551 -701.30732988 -701.29255486 -701.33376268 -701.28681191
 -701.32691538 -701.31997274 -701.30111715 -701.34930796 -701.32910794
 -701.30862577 -701.31389433 -701.27368447 -701.32110464 -701.34867478
 -701.31445181 -701.31514843 -701.29722603 -701.31362908 -701.30051073
 -701.31590502 -701.28690879 -701.29823773 -701.29546218 -701.28961224
 -701.28976659 -701.32119982 -701.29659284 -701.30889609 -701.31006165
 -701.32998386 -701.32505136 -701.26737937 -701.31071225 -701.32342776
 -701.30265627 -701.30470743 -701.31729782 -701.30785198 -701.32338382
 -701.31185805 -701.29767777 -701.29405968 -701.31425264 -701.33397223
 -701.30565279 -701.29581338 -701.29318736 -701.29766616 -701.29648169
 -701.29492516 -701.31813683 -701.29663077 -701.31356588 -701.28111841
 -701.28840246 -701.31712693 -701.31289175 -701.30229358 -701.32361059
 -701.31864805 -701.3158386  -701.30712934 -701.30231106 -701.30649877
 -701.32967578 -701.30782582 -701.28829497 -701.32408457 -701.31601554
 -701.29122554 -701.31929409 -701.28676923 -701.28507961 -701.29396248
 -701.31006188 -701.33914301 -701.31052875 -701.33318265 -701.33325499]
2025-06-23 08:57:32 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:57:32 INFO Good algorithm:
Algorithm Name: AdaptiveArchiveDEwithLocalSearch
# Name: AdaptiveArchiveDEwithLocalSearch
# Description: Differential Evolution with adaptive scaling, archive, and local search for multimodal optimization.

import numpy as np
import random

class AdaptiveArchiveDEwithLocalSearch:
    """
    Combines Differential Evolution (DE) with adaptive scaling, an archive for diversity, and local search to escape local optima in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * self.dim
        self.archive_size = 200
        self.archive = []
        self.F_scale = 0.5
        self.CR = 0.9
        self.local_search_iterations = 10 # Number of iterations for local search


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring = self.local_search(offspring, objective_function) #Apply local search
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self.update_archive(offspring, offspring_fitness)
            self.population, fitness = self.selection(self.population, fitness, offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        self.F_scale = 0.5 + 0.3 * np.random.rand()
        for i in range(self.population_size):
            pbest = self._select_pbest()
            a, b, c = self._select_different(i, self.population_size)
            mutant = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(population[i], mutant, self.CR)
        return offspring

    def _select_pbest(self):
        if self.archive:
            return random.choice(self.archive)[0]
        else:
            return self.population[np.argmin(self.population)] #Fallback to best in population

    def _crossover(self, x, v, CR):
        mask = np.random.rand(self.dim) < CR
        return np.where(mask, v, x)

    def _select_different(self, index, population_size):
        a, b, c = random.sample(range(population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(population_size), 3)
        return a, b, c

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def selection(self, population, fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        return combined_population[indices[:self.population_size]], combined_fitness[indices[:self.population_size]]

    def _find_best(self, population, fitness):
        best_index = np.argmin(fitness)
        return population[best_index], fitness[best_index]

    def local_search(self, population, objective_function):
        for i in range(len(population)):
            current_solution = population[i].copy()
            current_fitness = objective_function(current_solution.reshape(1,-1))[0]
            for _ in range(self.local_search_iterations):
                neighbor = current_solution + np.random.normal(0, 0.1, self.dim) #Small perturbation
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
                if neighbor_fitness < current_fitness:
                    current_solution = neighbor
                    current_fitness = neighbor_fitness
            population[i] = current_solution
        return population

2025-06-23 08:57:32 INFO Unimodal AOCC mean: 0.1753
2025-06-23 08:57:32 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:57:32 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:57:32 INFO AOCC mean: 0.1753
2025-06-23 08:57:33 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1765
2025-06-23 08:57:33 INFO FeHistory: [-701.34244118 -701.34054196 -701.31288207 ... -701.37974694 -701.37985174
 -701.37990419]
2025-06-23 08:57:33 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:57:33 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchiveAndLocalSearch
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndLocalSearch
# Description: Combines Differential Evolution, adaptive archive, and local search for multimodal optimization.
# Code:
class AdaptiveDEwithArchiveAndLocalSearch:
    """
    Combines Differential Evolution, an adaptive archive, and local search for efficient multimodal optimization.  Employs adaptive parameter control for robust exploration and exploitation.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim
        self.archive_size = 200
        self.archive = []
        self.population = None
        self.F_scale = 0.5
        self.CR = 0.9
        self.sigma_local = 0.1 #for local search
        self.local_search_iterations = 5

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Local Search on offspring
            offspring, offspring_fitness = self.local_search(offspring, offspring_fitness, objective_function)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Adaptive Parameter Control
            self.adapt_parameters(offspring_fitness)

            #Select best
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        
        for i in range(self.population_size):
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            mutant = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            #Crossover
            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i] = np.where(cross_points, mutant, population[i])

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def local_search(self, offspring, offspring_fitness, objective_function):
        for i in range(len(offspring)):
            current = offspring[i]
            for _ in range(self.local_search_iterations):
                neighbor = current + np.random.normal(0, self.sigma_local, self.dim)
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
                self.eval_count += 1
                if neighbor_fitness < offspring_fitness[i]:
                    current = neighbor
                    offspring_fitness[i] = neighbor_fitness
            offspring[i] = current
        return offspring, offspring_fitness

    def adapt_parameters(self, offspring_fitness):
        #Simple adaptive parameter control. Adjust based on performance.
        avg_fitness_improvement = np.mean(offspring_fitness)
        if avg_fitness_improvement < 0.1:
            self.F_scale *= 1.1 #Increase exploration if not improving much.
            self.CR *= 0.99 #Slightly decrease crossover rate
            self.sigma_local *= 1.05 #increase local search radius.
        elif avg_fitness_improvement > 0.5:
            self.F_scale *= 0.9 #Decrease exploration if improving rapidly
            self.CR *= 1.01 #increase crossover rate
            self.sigma_local *= 0.95 #decrease local search radius

        self.F_scale = np.clip(self.F_scale, 0.1, 1.0) #Keep parameters within bounds
        self.CR = np.clip(self.CR, 0.1, 1.0)
        self.sigma_local = np.clip(self.sigma_local, 0.01, 0.5)

2025-06-23 08:57:33 INFO Unimodal AOCC mean: 0.1765
2025-06-23 08:57:33 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:57:33 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:57:33 INFO AOCC mean: 0.1765
2025-06-23 08:57:33 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1764
2025-06-23 08:57:33 INFO FeHistory: [-701.28045168 -701.29175692 -701.31815095 ... -701.40449656 -701.40448987
 -701.40455524]
2025-06-23 08:57:33 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:57:33 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchiveAndLocalSearch
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndLocalSearch
# Description: Combines Differential Evolution, adaptive archive, and local search for multimodal optimization with adaptive parameters.
# Code:
class AdaptiveDEwithArchiveAndLocalSearch:
    """
    Combines Differential Evolution, an adaptive archive, and local search for efficient multimodal optimization.  Adapts mutation strength and crossover rate based on performance.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim
        self.archive_size = 200
        self.archive = []
        self.population = None
        self.F_scale = 0.5
        self.CR = 0.9
        self.F_adapt_rate = 0.1
        self.CR_adapt_rate = 0.05

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            #Local Search on offspring
            offspring, offspring_fitness = self.local_search(offspring, offspring_fitness, objective_function)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            #Select best
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution and adapt parameters
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)
            self.adapt_parameters(fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))

        for i in range(self.population_size):
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            mutant = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            #Crossover
            cross_points = np.random.rand(self.dim) < self.CR
            offspring[i] = np.where(cross_points, mutant, population[i])

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def local_search(self, offspring, offspring_fitness, objective_function):
        local_search_iterations = 5
        for i in range(len(offspring)):
            current = offspring[i]
            for _ in range(local_search_iterations):
                neighbor = current + np.random.normal(0, 0.1, self.dim)
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
                self.eval_count += 1
                if neighbor_fitness < offspring_fitness[i]:
                    current = neighbor
                    offspring_fitness[i] = neighbor_fitness
            offspring[i] = current
        return offspring, offspring_fitness

    def adapt_parameters(self, fitness):
        # Simple adaptive scheme based on average fitness improvement
        avg_fitness_improvement = np.mean(fitness) - self.best_fitness_overall
        if avg_fitness_improvement > 0 : # Improve
            self.F_scale = max(0.1, self.F_scale - self.F_adapt_rate)
            self.CR = max(0.1, self.CR - self.CR_adapt_rate)
        else: #Not Improve
            self.F_scale = min(1.0, self.F_scale + self.F_adapt_rate)
            self.CR = min(1.0, self.CR + self.CR_adapt_rate)

2025-06-23 08:57:33 INFO Unimodal AOCC mean: 0.1764
2025-06-23 08:57:33 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:57:33 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:57:33 INFO AOCC mean: 0.1764
2025-06-23 08:57:42 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1760
2025-06-23 08:57:42 INFO FeHistory: [-701.29558797 -701.29593836 -701.33440609 ... -701.3749103  -701.37480263
 -701.37477234]
2025-06-23 08:57:42 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:57:42 INFO Good algorithm:
Algorithm Name: AdaptiveArchiveDEwithLocalSearch
import numpy as np
import random

# Name: AdaptiveArchiveDEwithLocalSearch
# Description: Differential Evolution with adaptive parameters, archive, and local search for multimodal optimization.
class AdaptiveArchiveDEwithLocalSearch:
    """
    Combines Differential Evolution (DE) with adaptive parameters, an archive to maintain diversity, and local search to escape local optima.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.archive = []
        self.archive_size = 200
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.local_search_iterations = 5


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self._update_best(population, fitness_values)
        self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            #Local Search
            for i in range(len(offspring)):
                offspring[i] = self._local_search(offspring[i], objective_function)
            
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self._update_best(offspring, offspring_fitness)
            self._update_archive(offspring, offspring_fitness)

            population = self._select_population(population, fitness_values, offspring, offspring_fitness)
            fitness_values = objective_function(population)
            self.eval_count += self.population_size
            self._adapt_parameters()

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population):
        offspring = []
        for i in range(self.population_size):
            a, b, c = self._select_individuals(population, i)
            mutant = self._mutate(a, b, c)
            trial = self._crossover(population[i], mutant)
            offspring.append(trial)
        return np.array(offspring)

    def _select_individuals(self, population, i):
        indices = list(range(self.population_size))
        indices.remove(i)
        random.shuffle(indices)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _mutate(self, a, b, c):
        return np.clip(a + self.F * (b - c), self.lower_bounds, self.upper_bounds)

    def _crossover(self, x, v):
        trial = np.copy(x)
        jrand = random.randint(0, self.dim - 1)
        for j in range(self.dim):
            if random.random() < self.CR or j == jrand:
                trial[j] = v[j]
        return trial

    def _update_best(self, population, fitness_values):
        for i, fitness in enumerate(fitness_values):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = population[i]

    def _update_archive(self, population, fitness_values):
        for i, solution in enumerate(population):
            self._add_to_archive(solution, fitness_values[i])
        if len(self.archive) > self.archive_size:
            self.archive.sort(key=lambda x: x[1])
            self.archive = self.archive[:self.archive_size]


    def _add_to_archive(self, solution, fitness):
        already_present = any(np.allclose(solution, arch[0], atol=1e-6) for arch in self.archive)
        if not already_present:
            self.archive.append((solution, fitness))


    def _select_population(self, parent_population, parent_fitness, offspring_population, offspring_fitness):
        combined_population = np.concatenate((parent_population, offspring_population))
        combined_fitness = np.concatenate((parent_fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        selected_population = combined_population[indices[:self.population_size]]
        return selected_population

    def _adapt_parameters(self):
        if self.eval_count > self.budget * 0.7:
            self.F *= 0.9
            self.CR *= 0.95

    def _local_search(self, solution, objective_function):
        current_solution = np.copy(solution)
        current_fitness = objective_function(current_solution.reshape(1,-1))[0]
        for _ in range(self.local_search_iterations):
            neighbor = current_solution + np.random.normal(0, 0.1 * (self.upper_bounds - self.lower_bounds), self.dim)
            neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
            neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
            if neighbor_fitness < current_fitness:
                current_solution = neighbor
                current_fitness = neighbor_fitness
        return current_solution

2025-06-23 08:57:42 INFO Unimodal AOCC mean: 0.1760
2025-06-23 08:57:42 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:57:42 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:57:42 INFO AOCC mean: 0.1760
2025-06-23 08:58:04 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:58:04 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:58:04 ERROR Can not run the algorithm
2025-06-23 08:58:04 INFO Run function 2 complete. FEHistory len: 100, AOCC: 0.1753
2025-06-23 08:58:04 INFO FeHistory: [-701.33750734 -701.31430354 -701.29338118 -701.31461702 -701.30944436
 -701.3312824  -701.29931388 -701.30699    -701.28663027 -701.29851651
 -701.30762402 -701.30605321 -701.32777846 -701.30712922 -701.29000325
 -701.3147025  -701.34716377 -701.28519315 -701.28067105 -701.30270763
 -701.2967857  -701.32056128 -701.29455438 -701.29728346 -701.33666345
 -701.3035881  -701.3030994  -701.29512721 -701.33810246 -701.31067731
 -701.32758773 -701.32017261 -701.34730041 -701.31728727 -701.32194274
 -701.30017629 -701.30795161 -701.33373477 -701.30717438 -701.33273075
 -701.30143593 -701.34125492 -701.3248815  -701.31931041 -701.3324225
 -701.30132621 -701.28812723 -701.28237839 -701.2933644  -701.32082565
 -701.31776335 -701.32980585 -701.30071672 -701.30850607 -701.30911222
 -701.30187574 -701.29290698 -701.32143587 -701.26443741 -701.32826686
 -701.31057275 -701.30281031 -701.30495694 -701.32128043 -701.31349918
 -701.33915294 -701.28922295 -701.29915368 -701.27604836 -701.3045426
 -701.2839341  -701.30929949 -701.30074493 -701.33378953 -701.33155511
 -701.30688633 -701.26061737 -701.3080074  -701.3194646  -701.32771134
 -701.32538111 -701.28944508 -701.31295304 -701.31320242 -701.29728506
 -701.31954811 -701.32898839 -701.30715255 -701.30474408 -701.29743587
 -701.30701766 -701.32709365 -701.32031311 -701.36561491 -701.29866688
 -701.2932552  -701.34482846 -701.29161111 -701.33672018 -701.29660597]
2025-06-23 08:58:04 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:58:04 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerWithClustering
import numpy as np
import random

# Name: AdaptiveMultimodalOptimizerWithClustering
# Description: An adaptive EA using clustering to maintain diversity and guide exploration in multimodal landscapes.
# Code:
class AdaptiveMultimodalOptimizerWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 100
        self.archive = []  # Archive of diverse solutions
        self.archive_size = 200
        self.mutation_rate = 0.2
        self.crossover_rate = 0.7
        self.exploration_rate = 0.5  # Balance between global and local exploration
        self.clustering_threshold = 0.2 #parameter for determining clusters


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self._initialize_population()
        fitness = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection: Tournament selection with fitness scaling to emphasize diversity
            parents = self._tournament_selection(population, fitness)

            # Genetic Operators: Blend crossover and Gaussian mutation
            offspring = self._generate_offspring(parents)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Archive Management with Clustering
            self._update_archive(offspring, offspring_fitness)

            # Adaptive Parameter Tuning based on archive diversity and convergence
            self._adapt_parameters()

            # Combine population and offspring, then select the best
            population = np.vstack((population, offspring))
            fitness = np.concatenate((fitness, offspring_fitness))
            idx = np.argsort(fitness)
            population = population[idx[:self.population_size]]
            fitness = fitness[idx[:self.population_size]]


            # Update best solution
            best_index = np.argmin(fitness)
            if fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[best_index]
                self.best_solution_overall = population[best_index]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive),
            'final_mutation_rate': self.mutation_rate,
            'final_exploration_rate': self.exploration_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))

    def _tournament_selection(self, population, fitness, tournament_size=5):
        parents = []
        for _ in range(self.population_size // 2):  # Select pairs of parents
            tournament = np.random.choice(len(population), size=tournament_size, replace=False)
            #Fitness scaling for diversity
            scaled_fitness = fitness[tournament] / np.max(fitness[tournament])
            
            best1_index = tournament[np.argmin(scaled_fitness)]
            tournament = np.delete(tournament, np.argmin(scaled_fitness))
            best2_index = tournament[np.argmin(scaled_fitness)]
            parents.extend([population[best1_index], population[best2_index]])
        return np.array(parents)


    def _generate_offspring(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]

            if random.random() < self.crossover_rate:
                offspring1 = self._blend_crossover(parent1, parent2)
                offspring2 = self._blend_crossover(parent2, parent1)
            else:
                offspring1 = parent1.copy()
                offspring2 = parent2.copy()
                
            offspring1 += np.random.normal(0, self.mutation_rate * (self.upper_bounds - self.lower_bounds), self.dim)
            offspring2 += np.random.normal(0, self.mutation_rate * (self.upper_bounds - self.lower_bounds), self.dim)


            offspring1 = np.clip(offspring1, self.lower_bounds, self.upper_bounds)
            offspring2 = np.clip(offspring2, self.lower_bounds, self.upper_bounds)
            offspring.extend([offspring1, offspring2])

        return np.array(offspring)

    def _blend_crossover(self, parent1, parent2, alpha=0.5):
        return alpha * parent1 + (1 - alpha) * parent2

    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Smart replacement with clustering
                distances = np.array([np.linalg.norm(offspring[i] - x) for x, _ in self.archive])
                closest_index = np.argmin(distances)
                if offspring_fitness[i] < self.archive[closest_index][1] and distances[closest_index] > self.clustering_threshold:
                    self.archive[closest_index] = (offspring[i], offspring_fitness[i])


    def _adapt_parameters(self):
        if len(self.archive) > 0:
            avg_distance = np.mean([np.linalg.norm(x1 - x2) for (x1, _), (x2, _) in zip(self.archive, self.archive[1:])])
            
            if avg_distance < self.clustering_threshold:  # Population converging, increase exploration
                self.mutation_rate *= 1.1
                self.exploration_rate *= 1.1
            else:  # Population diverse, refine local search
                self.mutation_rate *= 0.9
                self.exploration_rate *= 0.9
                

2025-06-23 08:58:04 INFO Unimodal AOCC mean: 0.1753
2025-06-23 08:58:04 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:58:04 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:58:04 INFO AOCC mean: 0.1753
2025-06-23 08:58:08 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1768
2025-06-23 08:58:08 INFO FeHistory: [-701.33561611 -701.30902475 -701.32385386 ... -701.43102729 -701.43102738
 -701.43102721]
2025-06-23 08:58:08 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:58:08 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerWithClustering
import numpy as np
import random

# Name: AdaptiveMultimodalOptimizerWithClustering
# Description: An adaptive evolutionary algorithm using clustering to maintain diversity and escape local optima in multimodal landscapes.
# Code:
class AdaptiveMultimodalOptimizerWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 100
        self.archive = []
        self.archive_size = 200
        self.mutation_rate = 0.2
        self.crossover_rate = 0.7
        self.exploration_rate = 0.8  # Probability of global exploration
        self.clustering_threshold = 0.5 # Adjust as needed

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self._initialize_population()
        fitness = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection: Tournament selection with fitness scaling
            parents = self._tournament_selection(population, fitness)

            # Genetic Operators: Blend Crossover & Gaussian Mutation
            offspring = self._generate_offspring(parents)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Archive Management with Clustering
            self._update_archive(offspring, offspring_fitness)
            
            # Adaptive Parameter Tuning based on archive diversity
            self._adapt_parameters()

            # Combine and Resample
            population = np.vstack((population, offspring))
            fitness = np.concatenate((fitness, offspring_fitness))
            idx = np.argsort(fitness)
            population = population[idx[:self.population_size]]
            fitness = fitness[idx[:self.population_size]]


            # Update best solution
            best_index = np.argmin(fitness)
            if fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[best_index]
                self.best_solution_overall = population[best_index]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive),
            'final_mutation_rate': self.mutation_rate,
            'final_exploration_rate': self.exploration_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))

    def _tournament_selection(self, population, fitness, tournament_size=5):
        # Fitness scaling to improve selection pressure
        scaled_fitness = fitness - np.min(fitness)
        if np.max(scaled_fitness) > 0:
            scaled_fitness /= np.max(scaled_fitness)
        scaled_fitness = 1.0 - scaled_fitness # Minimize problem

        selected_indices = []
        for _ in range(self.population_size // 2): # Select pairs of parents
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner1 = tournament[np.argmin(scaled_fitness[tournament])]
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner2 = tournament[np.argmin(scaled_fitness[tournament])]
            selected_indices.extend([winner1, winner2])
            
        return population[selected_indices]

    def _generate_offspring(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            if random.random() < self.crossover_rate:
                child = self._blend_crossover(parent1, parent2)
            else:
                child = parent1
            child = self._mutate(child)
            offspring.append(child)
        return np.array(offspring)

    def _blend_crossover(self, parent1, parent2, alpha=0.5):
        return alpha * parent1 + (1 - alpha) * parent2

    def _mutate(self, individual):
        mutated = individual + np.random.normal(0, self.mutation_rate * (self.upper_bounds - self.lower_bounds), self.dim)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                # Clustering-based replacement:  Replace closest solution in archive
                closest_index = self._find_closest_in_archive(offspring[i])
                if offspring_fitness[i] < self.archive[closest_index][1] :
                    self.archive[closest_index] = (offspring[i], offspring_fitness[i])
                


    def _find_closest_in_archive(self, solution):
        min_dist = float('inf')
        closest_index = 0
        for i, (archived_sol, _) in enumerate(self.archive):
            dist = np.linalg.norm(solution - archived_sol)
            if dist < min_dist:
                min_dist = dist
                closest_index = i
        return closest_index


    def _adapt_parameters(self):
        if len(self.archive) > 0:
            # Calculate diversity using clustering
            archive_solutions = np.array([sol for sol, _ in self.archive])
            distances = np.linalg.norm(archive_solutions[:, np.newaxis, :] - archive_solutions[np.newaxis, :, :], axis=2)
            avg_distance = np.mean(distances[np.triu_indices(distances.shape[0], k=1)]) #average distance between points

            if avg_distance < self.clustering_threshold:
                self.mutation_rate *= 1.1
                self.exploration_rate *= 1.1 #increase exploration
            else:
                self.mutation_rate *= 0.9
                self.exploration_rate *= 0.9 #decrease exploration

2025-06-23 08:58:08 INFO Unimodal AOCC mean: 0.1768
2025-06-23 08:58:08 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:58:08 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:58:08 INFO AOCC mean: 0.1768
2025-06-23 08:59:26 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:59:26 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:59:26 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:59:26 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:59:26 ERROR Can not run the algorithm
2025-06-23 08:59:26 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:59:26 ERROR Can not run the algorithm
2025-06-23 08:59:26 ERROR Can not run the algorithm
2025-06-23 08:59:26 ERROR Can not run the algorithm
2025-06-23 08:59:26 ERROR Can not run the algorithm
2025-06-23 08:59:26 INFO Run function 2 complete. FEHistory len: 100, AOCC: 0.1755
2025-06-23 08:59:26 INFO FeHistory: [-701.33668019 -701.32678965 -701.32516114 -701.2985442  -701.31218318
 -701.28997768 -701.35178944 -701.33446415 -701.28344985 -701.30343755
 -701.28643056 -701.31896907 -701.31334218 -701.31244319 -701.34015768
 -701.32321833 -701.28543852 -701.33001955 -701.30908369 -701.29937545
 -701.31164625 -701.33272273 -701.28786585 -701.31633387 -701.29774418
 -701.32211282 -701.31084559 -701.32256091 -701.30894806 -701.30322405
 -701.32208293 -701.33101082 -701.27469284 -701.3178791  -701.29303475
 -701.33005968 -701.31560499 -701.30106445 -701.31915097 -701.31465947
 -701.2763983  -701.29207309 -701.32098054 -701.31287861 -701.31458344
 -701.31163316 -701.30911817 -701.31098577 -701.31167096 -701.3485743
 -701.32738884 -701.30352042 -701.28965657 -701.31910651 -701.3171694
 -701.32218675 -701.29539451 -701.31893125 -701.28724289 -701.29059584
 -701.28943398 -701.28822079 -701.30915577 -701.30399609 -701.30709107
 -701.31572738 -701.30124714 -701.28425396 -701.33140937 -701.29913441
 -701.2856638  -701.29301392 -701.31122261 -701.29153322 -701.29859475
 -701.32014003 -701.30024749 -701.33176598 -701.33991669 -701.29841085
 -701.28906831 -701.31130946 -701.3283171  -701.33139847 -701.30663884
 -701.35656948 -701.28785672 -701.27220547 -701.26858744 -701.34104933
 -701.3283825  -701.30718407 -701.33008155 -701.3254727  -701.28611046
 -701.30731089 -701.31407285 -701.28444901 -701.307957   -701.37443783]
2025-06-23 08:59:26 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:59:26 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchiveAndLocalSearchImproved
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndLocalSearchImproved
# Description: Adaptive Differential Evolution with archive and local search for multimodal optimization.
# Code:
class AdaptiveDEwithArchiveAndLocalSearchImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.5  # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.local_search_prob = 0.1 # Probability of performing local search


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self._initialize_population()
        fitness = objective_function(population)
        self.eval_count += self.population_size
        self.best_solution_overall = population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            #Combine and select
            combined_population = np.vstack((population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]


            #Update best
            best_index = np.argmin(fitness)
            if fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[best_index]
                self.best_solution_overall = population[best_index]

            # Archive management
            self._update_archive(population, fitness)

            # Adapt parameters (simple example - more sophisticated adaptation possible)
            self._adapt_parameters(population, fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive),
            'final_F': self.F,
            'final_CR': self.CR
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))

    def _generate_offspring(self, population, fitness):
        offspring = []
        for i in range(self.population_size):
            a, b, c = self._select_individuals(population, i)
            mutant = self._mutate(a, b, c)
            trial = self._crossover(population[i], mutant)
            
            #Local search with probability
            if random.random() < self.local_search_prob:
                trial = self._local_search(trial, objective_function)

            offspring.append(trial)
        return np.array(offspring)

    def _select_individuals(self, population, i):
        indices = np.random.choice(len(population), size=3, replace=False)
        while i in indices: # Ensure that the current individual is not selected as a parent.
            indices = np.random.choice(len(population), size=3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]
    

    def _mutate(self, a, b, c):
        return np.clip(a + self.F * (b - c), self.lower_bounds, self.upper_bounds)

    def _crossover(self, x, v):
        mask = np.random.rand(self.dim) < self.CR
        return np.where(mask, v, x)

    def _local_search(self, x, objective_function):
      # Simple local search (replace with a more sophisticated one if needed)
      step_size = 0.01 * (self.upper_bounds - self.lower_bounds)
      for _ in range(10): # 10 iterations
          for i in range(self.dim):
              temp_x = x.copy()
              temp_x[i] += random.uniform(-1, 1) * step_size[i]
              temp_x = np.clip(temp_x, self.lower_bounds, self.upper_bounds)
              temp_fitness = objective_function(temp_x.reshape(1, -1))[0]
              if temp_fitness < objective_function(x.reshape(1, -1))[0]:
                  x = temp_x
      return x


    def _update_archive(self, population, fitness):
        for i in range(len(population)):
            if len(self.archive) < self.archive_size:
                self.archive.append((population[i], fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (population[i], fitness[i])

    def _adapt_parameters(self, population, fitness):
        # Simple adaptation: Adjust F and CR based on convergence speed
        if np.std(fitness) < 0.1 * (self.best_fitness_overall): # Check for convergence
            self.F *= 0.9
            self.CR *= 0.9
        else:
            self.F *= 1.1
            self.CR *= 1.1
            self.F = np.clip(self.F, 0.1, 1.0)  #Keep F within bounds
            self.CR = np.clip(self.CR, 0.1, 1.0) #Keep CR within bounds
2025-06-23 08:59:26 INFO Unimodal AOCC mean: 0.1755
2025-06-23 08:59:26 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:59:26 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:59:26 INFO AOCC mean: 0.1755
2025-06-23 08:59:26 INFO Run function 2 complete. FEHistory len: 300, AOCC: 0.1756
2025-06-23 08:59:26 INFO FeHistory: [-701.30880512 -701.26992246 -701.31091425 -701.31865932 -701.28380753
 -701.34386423 -701.33318094 -701.27634831 -701.29101406 -701.30620324
 -701.33557076 -701.30044317 -701.35702339 -701.30185283 -701.31538071
 -701.26186639 -701.32548936 -701.31403823 -701.33995044 -701.30544033
 -701.29225061 -701.31687257 -701.31202896 -701.32839283 -701.30960248
 -701.30410151 -701.29075238 -701.3189447  -701.30636754 -701.34919684
 -701.30586013 -701.33649508 -701.28478866 -701.32208151 -701.30601208
 -701.3003775  -701.29980588 -701.29678139 -701.31945508 -701.2857782
 -701.30296857 -701.34782488 -701.33857587 -701.32940137 -701.28275112
 -701.28340438 -701.30945204 -701.28576632 -701.31462985 -701.2999745
 -701.33393256 -701.32385057 -701.31107831 -701.29190271 -701.35357911
 -701.30903328 -701.31600989 -701.31133915 -701.30660359 -701.31715321
 -701.31003223 -701.31469183 -701.3326632  -701.31838053 -701.29594707
 -701.31292568 -701.3153128  -701.33377746 -701.30740885 -701.3702687
 -701.29681849 -701.3237333  -701.29290602 -701.3392521  -701.30157422
 -701.34344318 -701.29814678 -701.30467223 -701.32971742 -701.33320283
 -701.29194181 -701.31776293 -701.27445432 -701.30530628 -701.31129532
 -701.35508655 -701.30544845 -701.28902711 -701.31486308 -701.29589106
 -701.32197866 -701.3217531  -701.31157525 -701.36800938 -701.31884629
 -701.31211095 -701.28254031 -701.27940368 -701.31416253 -701.30384286
 -701.31977726 -701.30161613 -701.32720972 -701.30355852 -701.31405056
 -701.33569156 -701.29337082 -701.26652118 -701.28021913 -701.31434145
 -701.30532187 -701.33202479 -701.33284845 -701.34027197 -701.34027649
 -701.27978998 -701.29653054 -701.28598971 -701.30935588 -701.3098967
 -701.31197568 -701.30278114 -701.30564019 -701.28378019 -701.3116533
 -701.29547621 -701.27788937 -701.31700374 -701.29610104 -701.31042347
 -701.29346816 -701.30557076 -701.29732214 -701.30353648 -701.30976865
 -701.35978925 -701.30291982 -701.29257032 -701.2980433  -701.30704473
 -701.32046216 -701.26736256 -701.2884838  -701.29305384 -701.28830908
 -701.33053633 -701.31304347 -701.28844806 -701.27467673 -701.35209165
 -701.31779655 -701.29653656 -701.31232543 -701.31821222 -701.29967109
 -701.26001397 -701.34025037 -701.30032048 -701.2979017  -701.29228178
 -701.28193294 -701.31677109 -701.31589479 -701.3379594  -701.30784606
 -701.28273605 -701.28334723 -701.32923592 -701.31427143 -701.32261919
 -701.2819509  -701.29168505 -701.30743953 -701.29416163 -701.30203673
 -701.31207768 -701.32469173 -701.30835409 -701.31786094 -701.32259886
 -701.31537013 -701.34548072 -701.28864034 -701.29403769 -701.34770407
 -701.28959345 -701.33668799 -701.31391981 -701.30084351 -701.31056871
 -701.29982869 -701.29695505 -701.3010446  -701.31138393 -701.32708069
 -701.28330507 -701.32898655 -701.32096866 -701.3053612  -701.31032451
 -701.34155395 -701.28492593 -701.33579667 -701.29464989 -701.3282842
 -701.31896437 -701.31444071 -701.28890195 -701.31737326 -701.31970289
 -701.30807324 -701.32070221 -701.3054688  -701.36012526 -701.31748886
 -701.31639789 -701.30041171 -701.29126894 -701.33938648 -701.31183958
 -701.29007647 -701.31499064 -701.31980731 -701.34094883 -701.31385439
 -701.29719405 -701.33941361 -701.32945015 -701.3013632  -701.29061891
 -701.32414492 -701.30879265 -701.32459745 -701.30314329 -701.30656808
 -701.3097879  -701.31611809 -701.28715623 -701.28873451 -701.33491384
 -701.29938982 -701.3803598  -701.29981919 -701.29073886 -701.34202751
 -701.33367844 -701.2714622  -701.27932491 -701.29221927 -701.30097322
 -701.28224473 -701.3100564  -701.30020482 -701.34526615 -701.30233027
 -701.29129986 -701.30220361 -701.29458692 -701.31092305 -701.29857691
 -701.31501672 -701.30655176 -701.3097451  -701.29530232 -701.31839429
 -701.2946311  -701.29641408 -701.32197437 -701.29365391 -701.35876844
 -701.30769415 -701.33003502 -701.29769237 -701.31219164 -701.28379709
 -701.32339658 -701.30183777 -701.30175054 -701.31368601 -701.32458452
 -701.30611846 -701.28926593 -701.28527984 -701.32287757 -701.3019421
 -701.30788834 -701.28548665 -701.31414216 -701.32770112 -701.30690279
 -701.30185686 -701.30617651 -701.29279641 -701.30353138 -701.2870543
 -701.31469733 -701.30770528 -701.31565982 -701.28657976 -701.29864709]
2025-06-23 08:59:26 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:59:26 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchiveAndLocalSearchPlus
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndLocalSearchPlus
# Description: Differential Evolution with adaptive archive, local search, and refined parameter control for multimodal optimization.
# Code:
class AdaptiveDEwithArchiveAndLocalSearchPlus:
    """
    Combines Differential Evolution (DE), an adaptive archive, local search, and refined parameter control for robust multimodal optimization.  Improves upon previous versions by incorporating a more sophisticated adaptive scaling mechanism and archive management.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * dim  # Adjust as needed
        self.archive_size = 200
        self.archive = []
        self.F_scale = 0.5
        self.F_scale_delta = 0.1
        self.CR = 0.9
        self.local_search_iterations = 5
        self.stagnation_count = 0 #Counter for stagnation
        self.stagnation_threshold = 10 * dim # Number of iterations without improvement before reset

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring = self._local_search(offspring, objective_function)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self._update_archive(offspring, offspring_fitness)
            self.population, fitness = self._select_next_generation(offspring, offspring_fitness)
            prev_best_fitness = self.best_fitness_overall
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness)
            self._adapt_F_scale(fitness)

            # Stagnation handling: Reset if no improvement after threshold
            if self.best_fitness_overall == prev_best_fitness:
                self.stagnation_count += 1
                if self.stagnation_count >= self.stagnation_threshold:
                    self._reset_population()
                    self.stagnation_count = 0
            else:
                self.stagnation_count = 0

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = np.zeros((self.population_size, self.dim))
        for i in range(self.population_size):
            pbest = self._select_pbest()
            a, b, c = self._select_different_indices(i)
            mutant = self.population[i] + self.F_scale * (pbest - self.population[i] + self.population[a] - self.population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(self.population[i], mutant)
        return offspring

    def _select_pbest(self):
        if self.archive:
            return random.choice(self.archive)[0]
        else:
            return self.population[np.argmin(objective_function(self.population))] # Needs objective_function in scope.


    def _select_different_indices(self, i):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == i or b == i or c == i:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _crossover(self, x, v):
        return np.where(np.random.rand(self.dim) < self.CR, v, x)

    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def _select_next_generation(self, offspring, offspring_fitness):
        combined_population = np.concatenate((self.population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness)) #fitness needs to be in scope.
        indices = np.argsort(combined_fitness)
        return combined_population[indices[:self.population_size]], combined_fitness[indices[:self.population_size]]

    def _adapt_F_scale(self, fitness):
        # More sophisticated adaptation: considers both improvement and diversity.
        improvement = np.min(fitness) < self.best_fitness_overall
        diversity = np.std(fitness) > 0.1 * (self.upper_bounds - self.lower_bounds).mean() # Check for diversity in fitness values

        if improvement or diversity:
            self.F_scale = min(1.0, self.F_scale + self.F_scale_delta)
        else:
            self.F_scale = max(0.0, self.F_scale - self.F_scale_delta)

    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]
        return self.best_solution_overall, self.best_fitness_overall

    def _local_search(self, offspring, objective_function):
        for i in range(len(offspring)):
            current_solution = offspring[i].copy()
            current_fitness = objective_function(current_solution.reshape(1,-1))[0]
            for _ in range(self.local_search_iterations):
                neighbor = current_solution + np.random.normal(0, 0.1, self.dim)
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
                if neighbor_fitness < current_fitness:
                    current_solution = neighbor
                    current_fitness = neighbor_fitness
            offspring[i] = current_solution
        return offspring

    def _reset_population(self):
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        

2025-06-23 08:59:26 INFO Unimodal AOCC mean: 0.1756
2025-06-23 08:59:26 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:59:26 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:59:26 INFO AOCC mean: 0.1756
2025-06-23 08:59:26 INFO Run function 2 complete. FEHistory len: 300, AOCC: 0.1757
2025-06-23 08:59:26 INFO Run function 2 complete. FEHistory len: 300, AOCC: 0.1756
2025-06-23 08:59:26 INFO FeHistory: [-701.29709151 -701.32000495 -701.30889136 -701.32263562 -701.27577275
 -701.28992928 -701.29298968 -701.35311564 -701.30657869 -701.31765824
 -701.28958248 -701.30766681 -701.3265632  -701.28720562 -701.27673828
 -701.30752217 -701.31875054 -701.29266939 -701.30093057 -701.30353132
 -701.32731954 -701.33396854 -701.29950701 -701.3371203  -701.30491993
 -701.29818038 -701.32183169 -701.30639968 -701.29770721 -701.32001987
 -701.30066843 -701.31661166 -701.30544597 -701.34340896 -701.30184615
 -701.30634748 -701.28380294 -701.33217793 -701.34908951 -701.30551692
 -701.28716834 -701.3013001  -701.31160196 -701.28302204 -701.30156019
 -701.32295473 -701.31637406 -701.34685393 -701.32168094 -701.296828
 -701.28600415 -701.28775106 -701.31555398 -701.30502157 -701.33759307
 -701.30708718 -701.30917917 -701.28009032 -701.34080936 -701.30873217
 -701.34656746 -701.34360908 -701.32334298 -701.29775268 -701.3468003
 -701.32076227 -701.27277422 -701.30928034 -701.33515552 -701.32705272
 -701.32967946 -701.29530559 -701.29695294 -701.28810198 -701.30896234
 -701.31507594 -701.31936458 -701.33484892 -701.30383415 -701.30596062
 -701.2856768  -701.34412087 -701.3286271  -701.30191149 -701.31057244
 -701.31533515 -701.29458789 -701.27230673 -701.32400672 -701.29531097
 -701.33430484 -701.34347456 -701.29657741 -701.29089074 -701.30558752
 -701.33754472 -701.30115004 -701.29441464 -701.35807839 -701.33043602
 -701.27298104 -701.30186477 -701.31321734 -701.31087741 -701.32782038
 -701.27187409 -701.32689813 -701.28923126 -701.31153986 -701.33254336
 -701.30369298 -701.29903997 -701.30129469 -701.29647398 -701.31887975
 -701.32844354 -701.35430153 -701.32134784 -701.33287012 -701.34997897
 -701.36158379 -701.27590295 -701.30854807 -701.28707025 -701.31896869
 -701.33047373 -701.35665699 -701.29856679 -701.30658995 -701.32554592
 -701.32205803 -701.30419648 -701.30050499 -701.31298883 -701.27658511
 -701.29858314 -701.33175819 -701.32868857 -701.35657172 -701.30229215
 -701.33190114 -701.29237084 -701.32695068 -701.32380314 -701.30688011
 -701.38316648 -701.29891693 -701.27940556 -701.31014369 -701.31842167
 -701.30927332 -701.30922201 -701.30765809 -701.29747051 -701.33322732
 -701.28855494 -701.29144828 -701.32299247 -701.30998958 -701.31141917
 -701.33944899 -701.29874514 -701.33431964 -701.30295625 -701.33161242
 -701.30824982 -701.29801628 -701.32109034 -701.28905996 -701.29442274
 -701.31135753 -701.31712289 -701.30280335 -701.34080865 -701.33946052
 -701.30260221 -701.32320967 -701.32211717 -701.30885926 -701.31704648
 -701.31042    -701.302117   -701.31520383 -701.33683287 -701.29614677
 -701.29210363 -701.32202831 -701.31875239 -701.27730113 -701.31906659
 -701.27944457 -701.32513512 -701.35354953 -701.32508452 -701.29997076
 -701.30198534 -701.32390676 -701.30175454 -701.31732377 -701.27946953
 -701.28409449 -701.32715436 -701.28882928 -701.330446   -701.27725802
 -701.27424173 -701.30485939 -701.30503327 -701.33468622 -701.38419282
 -701.30435457 -701.28847316 -701.30915231 -701.27819566 -701.30055003
 -701.29868069 -701.31109722 -701.31342868 -701.3152594  -701.2887084
 -701.3373188  -701.3349172  -701.33427449 -701.29738601 -701.32330757
 -701.30426698 -701.32018766 -701.32787418 -701.31060732 -701.3098636
 -701.29116131 -701.27776184 -701.30124772 -701.32143205 -701.29784891
 -701.31354086 -701.30433321 -701.33100483 -701.32030705 -701.29268053
 -701.29971194 -701.31085132 -701.29565623 -701.29142191 -701.30370852
 -701.29147147 -701.31824476 -701.30761483 -701.2945758  -701.29321406
 -701.33784483 -701.32832447 -701.32102605 -701.33217226 -701.29555668
 -701.33126031 -701.3193479  -701.29892889 -701.32357091 -701.3004632
 -701.31815011 -701.30370637 -701.32087752 -701.29934666 -701.28313386
 -701.32309255 -701.29145272 -701.32260699 -701.28176092 -701.32216452
 -701.29579539 -701.30246807 -701.31929104 -701.33057692 -701.31226423
 -701.27162685 -701.30980357 -701.30058446 -701.27815401 -701.31017479
 -701.31656962 -701.31846207 -701.30498865 -701.32239176 -701.31311427
 -701.29505784 -701.29868689 -701.3247406  -701.29931243 -701.28266009
 -701.29875117 -701.31905737 -701.32210444 -701.28336954 -701.30516673
 -701.31666595 -701.33080031 -701.28781051 -701.30883804 -701.31749815]
2025-06-23 08:59:26 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:59:26 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchiveAndLocalSearchPlus
# Name: AdaptiveDEwithArchiveAndLocalSearchPlus
# Description: Differential Evolution with adaptive scaling, archive, and local search enhanced for multimodal optimization.
import numpy as np
import random

class AdaptiveDEwithArchiveAndLocalSearchPlus:
    """
    Combines Differential Evolution (DE) with adaptive scaling, an archive for diversity, and local search to escape local optima in multimodal landscapes.  Improves upon previous versions by adding a more sophisticated archive management strategy and adaptive parameter control.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * self.dim
        self.archive_size = 200
        self.archive = []
        self.F_scale = 0.5
        self.CR = 0.9
        self.local_search_iterations = 10  # Number of iterations for local search
        self.archive_diversity_threshold = 0.1 # Threshold for archive diversity


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring = self.local_search(offspring, objective_function)  # Apply local search
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self.update_archive(offspring, offspring_fitness)
            self.population, fitness = self.selection(self.population, fitness, offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)
            self.adapt_parameters(fitness) # Adapt parameters based on performance

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        for i in range(self.population_size):
            pbest = self._select_pbest()
            a, b, c = self._select_different(i, self.population_size)
            mutant = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(population[i], mutant, self.CR)
        return offspring

    def _select_pbest(self):
        if self.archive:
            return random.choice(self.archive)[0]
        else:
            return self.population[np.argmin(self.population)] #Fallback to best in population

    def _crossover(self, x, v, CR):
        mask = np.random.rand(self.dim) < CR
        return np.where(mask, v, x)

    def _select_different(self, index, population_size):
        a, b, c = random.sample(range(population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(population_size), 3)
        return a, b, c

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                self._cull_archive() # More sophisticated culling
                self.archive.append((offspring[i], offspring_fitness[i]))

    def _cull_archive(self):
        #Cull based on fitness and diversity
        self.archive.sort(key=lambda item: item[1]) #Sort by fitness
        self.archive = self.archive[:int(self.archive_size * 0.8)] #Keep top 80%
        #Add diversity check and cull if needed
        if self._check_archive_diversity() < self.archive_diversity_threshold:
            self._remove_similar_solutions()



    def _check_archive_diversity(self):
        #Simple diversity check (can be replaced with more sophisticated methods)
        if len(self.archive) < 2:
            return 1.0 #Consider fully diverse if less than 2 solutions
        solutions = np.array([x[0] for x in self.archive])
        distances = np.linalg.norm(solutions[:, np.newaxis, :] - solutions[np.newaxis, :, :], axis=2)
        min_distance = np.min(distances[np.nonzero(distances)])
        return min_distance / (np.max(self.upper_bounds) - np.min(self.lower_bounds))


    def _remove_similar_solutions(self):
        #Remove solutions that are close to each other (simple approach)
        solutions = np.array([x[0] for x in self.archive])
        distances = np.linalg.norm(solutions[:, np.newaxis, :] - solutions[np.newaxis, :, :], axis=2)
        to_remove = []
        for i in range(len(self.archive)):
            for j in range(i+1, len(self.archive)):
                if distances[i,j] < 0.1*(np.max(self.upper_bounds) - np.min(self.lower_bounds)): # Adjust threshold as needed
                    if self.archive[i][1] > self.archive[j][1]:
                        to_remove.append(i)
                    else:
                        to_remove.append(j)
        
        to_remove = list(set(to_remove)) #Avoid duplicates
        to_remove.sort(reverse=True) #remove from the end to avoid index issues

        for index in to_remove:
            del self.archive[index]


    def selection(self, population, fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        return combined_population[indices[:self.population_size]], combined_fitness[indices[:self.population_size]]

    def _find_best(self, population, fitness):
        best_index = np.argmin(fitness)
        return population[best_index], fitness[best_index]

    def local_search(self, population, objective_function):
        for i in range(len(population)):
            current_solution = population[i].copy()
            current_fitness = objective_function(current_solution.reshape(1,-1))[0]
            for _ in range(self.local_search_iterations):
                neighbor = current_solution + np.random.normal(0, 0.1, self.dim) #Small perturbation
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
                if neighbor_fitness < current_fitness:
                    current_solution = neighbor
                    current_fitness = neighbor_fitness
            population[i] = current_solution
        return population
    
    def adapt_parameters(self, fitness):
        #Adapt F_scale based on improvement
        if np.min(fitness) < self.best_fitness_overall:
            self.F_scale = min(1.0, self.F_scale + 0.1) #Increase exploration
        else:
            self.F_scale = max(0.0, self.F_scale - 0.05) #Decrease exploration
        #Adapt CR based on diversity (example - could be improved)
        if self._check_archive_diversity() < 0.05:
            self.CR = max(0.1, self.CR - 0.1) #Reduce CR to intensify exploitation around promising regions
        elif self._check_archive_diversity() > 0.2:
            self.CR = min(0.99, self.CR + 0.1) #Increase CR to enhance exploration



2025-06-23 08:59:26 INFO Unimodal AOCC mean: 0.1757
2025-06-23 08:59:26 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:59:26 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:59:26 INFO AOCC mean: 0.1757
2025-06-23 08:59:26 INFO FeHistory: [-701.29812711 -701.32159748 -701.27866031 -701.31327739 -701.31159649
 -701.28936358 -701.28787496 -701.32835026 -701.29535435 -701.32731555
 -701.29298951 -701.31844409 -701.32678508 -701.32282767 -701.2959991
 -701.31660105 -701.32663402 -701.31273774 -701.29409386 -701.3299916
 -701.32502116 -701.31489114 -701.3235914  -701.32961918 -701.30927233
 -701.33133262 -701.32052826 -701.33344649 -701.30762884 -701.29776653
 -701.29552489 -701.2735294  -701.28067151 -701.29878523 -701.31507595
 -701.27869422 -701.28410979 -701.29022052 -701.28766192 -701.2791675
 -701.3810792  -701.31294333 -701.29973876 -701.28874581 -701.31942266
 -701.28466093 -701.33060409 -701.30470038 -701.32929964 -701.3202373
 -701.31559451 -701.30699124 -701.31503727 -701.29109911 -701.31063162
 -701.30278367 -701.33000004 -701.27214752 -701.29899721 -701.29492664
 -701.29568427 -701.30977372 -701.33424636 -701.32509555 -701.31092766
 -701.32717036 -701.29962525 -701.27385808 -701.29863679 -701.31912209
 -701.31473972 -701.29095836 -701.3129711  -701.32051743 -701.32091205
 -701.32071414 -701.28128462 -701.31569506 -701.29525657 -701.31301933
 -701.31170582 -701.30945507 -701.32090353 -701.30684723 -701.2956898
 -701.28423694 -701.33662514 -701.31330709 -701.31290801 -701.3463649
 -701.34624727 -701.29229832 -701.30698807 -701.3062453  -701.3396123
 -701.33343224 -701.28970472 -701.31546702 -701.28966259 -701.31889589
 -701.31618017 -701.32860079 -701.28126699 -701.30352465 -701.31166253
 -701.29383241 -701.32173128 -701.34462559 -701.27777143 -701.33342604
 -701.30291517 -701.30407667 -701.32298223 -701.29759253 -701.31166508
 -701.30592722 -701.29869594 -701.30369404 -701.28467986 -701.32205116
 -701.31368048 -701.28422034 -701.28466696 -701.27280615 -701.3112972
 -701.30795511 -701.30141627 -701.29208101 -701.31272575 -701.31847569
 -701.3325271  -701.32894078 -701.31540654 -701.29729716 -701.32593681
 -701.27876595 -701.28251615 -701.29388066 -701.28784202 -701.30171102
 -701.27705254 -701.2858072  -701.31210208 -701.30084421 -701.34797288
 -701.30297044 -701.3326893  -701.30205092 -701.30737498 -701.28232011
 -701.32322873 -701.31027    -701.28907453 -701.29213079 -701.29022471
 -701.28293338 -701.31829598 -701.29155694 -701.29873508 -701.27730593
 -701.32599217 -701.33575634 -701.31840205 -701.30200141 -701.3077968
 -701.30779914 -701.28004695 -701.32375799 -701.2805241  -701.31044091
 -701.33211527 -701.29343525 -701.30548159 -701.30643644 -701.33350871
 -701.31793636 -701.33674257 -701.30282784 -701.290017   -701.33547011
 -701.30225063 -701.30824665 -701.34629878 -701.31495733 -701.30192683
 -701.30537192 -701.31798632 -701.30279428 -701.31932583 -701.28981978
 -701.30147185 -701.32483373 -701.31731208 -701.29393992 -701.33311456
 -701.32479856 -701.32893405 -701.36069686 -701.29748983 -701.32984883
 -701.27782663 -701.3488034  -701.28491855 -701.31340039 -701.31892693
 -701.31701951 -701.30653692 -701.28649963 -701.30656513 -701.30269599
 -701.30998701 -701.30861597 -701.35331692 -701.27817511 -701.31470914
 -701.32598772 -701.29218414 -701.30694245 -701.2972558  -701.31340999
 -701.31273859 -701.33061053 -701.29863175 -701.31316908 -701.32790848
 -701.29358213 -701.29606121 -701.34287668 -701.29284353 -701.28499012
 -701.27485221 -701.31211648 -701.28616329 -701.29449904 -701.29570351
 -701.30766458 -701.28962942 -701.28738534 -701.33747437 -701.34796352
 -701.31127935 -701.31853532 -701.34152098 -701.29980252 -701.33712489
 -701.28573697 -701.30793409 -701.30211608 -701.28605826 -701.32372124
 -701.27788753 -701.30510833 -701.30380462 -701.32534068 -701.3266695
 -701.31423416 -701.30533004 -701.32710591 -701.32998765 -701.29938677
 -701.28433208 -701.30044556 -701.31787629 -701.33097317 -701.31275387
 -701.29554786 -701.31342175 -701.28054613 -701.29042018 -701.33871179
 -701.28811782 -701.30252318 -701.35258487 -701.3306078  -701.29587553
 -701.3328641  -701.28841998 -701.30902112 -701.30589067 -701.318008
 -701.32174423 -701.31159448 -701.31743102 -701.33934614 -701.31636164
 -701.28421782 -701.2943409  -701.32031914 -701.32763773 -701.32347515
 -701.29999805 -701.30653681 -701.33626887 -701.31872961 -701.29623183
 -701.28413176 -701.28897596 -701.32317125 -701.30689657 -701.30338654]
2025-06-23 08:59:26 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:59:26 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithArchiveAndLocalSearch
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndLocalSearch
# Description: Differential Evolution with adaptive archive, local search, and parameter control for multimodal optimization.
# Code:
class AdaptiveDEwithArchiveAndLocalSearch:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * self.dim  #Heuristic population size
        self.archive_size = 200
        self.archive = []
        self.F_scale = 0.5
        self.CR = 0.9
        self.local_search_iterations = 10
        self.adaptation_factor = 0.1 #controls adaptation speed

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring = self.local_search(offspring, objective_function)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self.update_archive(offspring, offspring_fitness)
            self.population, fitness = self.selection(self.population, fitness, offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)
            self.adapt_parameters(fitness) # Adapt parameters after each generation

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive),
            'final_F_scale': self.F_scale,
            'final_CR': self.CR
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        for i in range(self.population_size):
            pbest = self._select_pbest()
            a, b, c = self._select_different(i, self.population_size)
            mutant = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(population[i], mutant, self.CR)
        return offspring

    def _select_pbest(self):
        if self.archive:
            return random.choice(self.archive)[0]
        else:
            return self.population[np.argmin(self.population)]

    def _crossover(self, x, v, CR):
        mask = np.random.rand(self.dim) < CR
        return np.where(mask, v, x)

    def _select_different(self, index, population_size):
        a, b, c = random.sample(range(population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(population_size), 3)
        return a, b, c

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1]:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def selection(self, population, fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        return combined_population[indices[:self.population_size]], combined_fitness[indices[:self.population_size]]

    def _find_best(self, population, fitness):
        best_index = np.argmin(fitness)
        return population[best_index], fitness[best_index]

    def local_search(self, population, objective_function):
        for i in range(len(population)):
            current_solution = population[i].copy()
            current_fitness = objective_function(current_solution.reshape(1,-1))[0]
            for _ in range(self.local_search_iterations):
                neighbor = current_solution + np.random.normal(0, 0.1, self.dim)
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
                if neighbor_fitness < current_fitness:
                    current_solution = neighbor
                    current_fitness = neighbor_fitness
            population[i] = current_solution
        return population

    def adapt_parameters(self, fitness):
        #Adapt F_scale and CR based on fitness improvement.  Simpler adaptation for clarity.
        avg_fitness_improvement = np.mean(fitness) - self.best_fitness_overall
        if avg_fitness_improvement > 0 : #if some improvement
            self.F_scale *= (1 + self.adaptation_factor)
            self.CR *= (1 + self.adaptation_factor)
            self.F_scale = min(self.F_scale, 1.0) #bound to be reasonable
            self.CR = min(self.CR, 1.0)
        else:
            self.F_scale *= (1 - self.adaptation_factor)
            self.CR *= (1 - self.adaptation_factor)
            self.F_scale = max(self.F_scale, 0.1) #bound to be reasonable
            self.CR = max(self.CR, 0.1)

2025-06-23 08:59:26 INFO Unimodal AOCC mean: 0.1756
2025-06-23 08:59:26 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:59:26 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:59:26 INFO AOCC mean: 0.1756
2025-06-23 08:59:26 INFO Run function 2 complete. FEHistory len: 300, AOCC: 0.1757
2025-06-23 08:59:26 INFO FeHistory: [-701.31538111 -701.29413308 -701.31098741 -701.32735693 -701.30560947
 -701.31408435 -701.29685762 -701.29646843 -701.30683962 -701.29815925
 -701.33150539 -701.33658624 -701.3169523  -701.33010182 -701.28500782
 -701.34378916 -701.30195422 -701.29043597 -701.33275523 -701.2724107
 -701.29536812 -701.32795282 -701.29178201 -701.33280679 -701.3300863
 -701.34270951 -701.29173607 -701.30312849 -701.31799793 -701.28533895
 -701.28975614 -701.31129889 -701.32574721 -701.34805402 -701.29164666
 -701.29820399 -701.32016447 -701.3153289  -701.2899083  -701.29252432
 -701.30567393 -701.31698868 -701.3047193  -701.29137459 -701.29769605
 -701.30604807 -701.27036053 -701.32576021 -701.29312865 -701.30014702
 -701.31117844 -701.29637795 -701.2924869  -701.3095533  -701.29357342
 -701.31065088 -701.32035852 -701.34890606 -701.32382489 -701.27937494
 -701.33540826 -701.3165914  -701.31168001 -701.30102794 -701.33401212
 -701.28391135 -701.28776542 -701.31863533 -701.30260854 -701.29787364
 -701.29801854 -701.3180724  -701.31131072 -701.28796255 -701.29796964
 -701.30224041 -701.33878996 -701.29850811 -701.30933725 -701.29959569
 -701.33509142 -701.29879054 -701.31401676 -701.3002232  -701.30417048
 -701.38133103 -701.32138939 -701.31025824 -701.30957767 -701.3143846
 -701.28751113 -701.28651378 -701.3145166  -701.30747091 -701.30549049
 -701.30780498 -701.30746747 -701.27186548 -701.31882597 -701.31504336
 -701.3133866  -701.31510762 -701.27592478 -701.28855209 -701.3270509
 -701.31151548 -701.31166836 -701.32140399 -701.29629033 -701.29882384
 -701.32400899 -701.30832648 -701.30095314 -701.28903219 -701.31562333
 -701.2949338  -701.34097838 -701.34274614 -701.29076281 -701.27445838
 -701.33237735 -701.31730095 -701.28032324 -701.30309528 -701.306612
 -701.29549494 -701.29920147 -701.28098498 -701.32153714 -701.32273352
 -701.29809088 -701.29412029 -701.31347124 -701.32429467 -701.30942488
 -701.27687124 -701.32841443 -701.29189911 -701.32207179 -701.31857353
 -701.29628741 -701.33272688 -701.30996483 -701.2970495  -701.33116341
 -701.29677179 -701.31581373 -701.29307561 -701.3321585  -701.34542733
 -701.29997728 -701.32013179 -701.29355772 -701.30096676 -701.27893488
 -701.32496457 -701.31206074 -701.32503498 -701.32035786 -701.32508698
 -701.3221588  -701.27870454 -701.32316997 -701.28851542 -701.30611071
 -701.31557602 -701.29646642 -701.30083571 -701.30511113 -701.30848921
 -701.31028473 -701.31733716 -701.30603376 -701.3079879  -701.30482788
 -701.31098945 -701.32223539 -701.29935831 -701.31906799 -701.32635438
 -701.26413647 -701.30915704 -701.3163336  -701.31009901 -701.28413369
 -701.29671694 -701.30320781 -701.28127747 -701.31371422 -701.31148463
 -701.31545852 -701.29617478 -701.33938891 -701.3189226  -701.28676614
 -701.30919216 -701.31658673 -701.30674988 -701.31949765 -701.3337855
 -701.3249154  -701.30369124 -701.31314291 -701.3059222  -701.35316167
 -701.31093426 -701.3338526  -701.29451788 -701.27816836 -701.30632117
 -701.31558808 -701.2938282  -701.32332779 -701.28895611 -701.30422962
 -701.30605664 -701.34331142 -701.32827584 -701.30259948 -701.25869347
 -701.30942018 -701.30028708 -701.29286318 -701.30265275 -701.33471403
 -701.30750349 -701.29037115 -701.29076668 -701.33194279 -701.31804971
 -701.3142593  -701.30676282 -701.31014154 -701.29078832 -701.29849907
 -701.25689804 -701.31649641 -701.29476959 -701.35019842 -701.29920621
 -701.32197991 -701.36620684 -701.32988506 -701.28895968 -701.31159172
 -701.2862855  -701.34777595 -701.29830708 -701.28991574 -701.30577567
 -701.31674273 -701.29119673 -701.32287561 -701.31336958 -701.31610857
 -701.35837433 -701.3027137  -701.28491698 -701.30723245 -701.31588632
 -701.28744615 -701.32328863 -701.30303851 -701.28656034 -701.35132533
 -701.31426165 -701.28046754 -701.2826256  -701.31070448 -701.30702709
 -701.3147436  -701.31162701 -701.33951154 -701.30744192 -701.28833941
 -701.33930035 -701.29366687 -701.3202908  -701.31445827 -701.3102573
 -701.29336395 -701.31526248 -701.31672972 -701.31182837 -701.29766924
 -701.29585033 -701.31078746 -701.30967741 -701.30877039 -701.30138806
 -701.30046926 -701.31128742 -701.31506991 -701.29902406 -701.31488686
 -701.35088311 -701.32213125 -701.29535629 -701.29797454 -701.32918578]
2025-06-23 08:59:26 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:59:26 INFO Good algorithm:
Algorithm Name: AdaptiveDEwithClusteringAndLocalSearch
import numpy as np
import random

# Name: AdaptiveDEwithClusteringAndLocalSearch
# Description: Differential Evolution with adaptive scaling, clustering, and local search for multimodal optimization.
# Code:
class AdaptiveDEwithClusteringAndLocalSearch:
    """
    Combines Differential Evolution, adaptive scaling, clustering for diversity, and local search to escape local optima in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 10 * dim
        self.archive_size = 200
        self.archive = []
        self.F_scale = 0.5
        self.F_scale_delta = 0.1
        self.CR = 0.9
        self.local_search_iterations = 5  # Number of local search steps
        self.clustering_threshold = 0.2 #parameter for determining clusters


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring = self._local_search(offspring, objective_function)  # Apply local search
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self._update_archive(offspring, offspring_fitness)
            self.population, fitness = self._select_next_generation(offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness)
            self._adapt_F_scale(fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive),
            'final_F_scale': self.F_scale
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = np.zeros((self.population_size, self.dim))
        for i in range(self.population_size):
            pbest = self._select_pbest()
            a, b, c = self._select_different_indices(i)
            mutant = self.population[i] + self.F_scale * (pbest - self.population[i] + self.population[a] - self.population[b])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(self.population[i], mutant)
        return offspring

    def _select_pbest(self):
        if self.archive:
            return random.choice(self.archive)[0]  # Randomly select from archive for diversity
        else:
            return self.population[np.argmin(objective_function(self.population))]

    def _select_different_indices(self, i):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == i or b == i or c == i:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _crossover(self, x, v):
        return np.where(np.random.rand(self.dim) < self.CR, v, x)

    def _update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Smart replacement with clustering
                distances = np.array([np.linalg.norm(offspring[i] - x) for x, _ in self.archive])
                closest_index = np.argmin(distances)
                if offspring_fitness[i] < self.archive[closest_index][1] and distances[closest_index] > self.clustering_threshold:
                    self.archive[closest_index] = (offspring[i], offspring_fitness[i])


    def _select_next_generation(self, offspring, offspring_fitness):
        combined_population = np.concatenate((self.population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        indices = np.argsort(combined_fitness)
        return combined_population[indices[:self.population_size]], combined_fitness[indices[:self.population_size]]

    def _adapt_F_scale(self, fitness):
        if np.min(fitness) < self.best_fitness_overall:
            self.F_scale = min(1.0, self.F_scale + self.F_scale_delta)
        else:
            self.F_scale = max(0.0, self.F_scale - self.F_scale_delta)

    def _update_best(self, population, fitness):
        best_index = np.argmin(fitness)
        if fitness[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness[best_index]
            self.best_solution_overall = population[best_index]
        return self.best_solution_overall, self.best_fitness_overall

    def _local_search(self, offspring, objective_function):
        for i in range(len(offspring)):
            current_solution = offspring[i].copy()
            current_fitness = objective_function(current_solution.reshape(1,-1))[0]
            for _ in range(self.local_search_iterations):
                neighbor = current_solution + np.random.normal(0, 0.1, self.dim) #Small perturbation
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
                if neighbor_fitness < current_fitness:
                    current_solution = neighbor
                    current_fitness = neighbor_fitness
            offspring[i] = current_solution
        return offspring

2025-06-23 08:59:26 INFO Unimodal AOCC mean: 0.1757
2025-06-23 08:59:26 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:59:26 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:59:26 INFO AOCC mean: 0.1757
2025-06-23 08:59:44 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:59:44 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 08:59:45 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1771
2025-06-23 08:59:45 INFO FeHistory: [-701.3056389  -701.28371613 -701.27420778 ... -701.3704885  -701.40471343
 -701.44023607]
2025-06-23 08:59:45 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:59:45 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A differential evolution algorithm with adaptive mutation and clustering for multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 100
        self.F = 0.8  # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.cluster_threshold = 0.5 # Adjust based on problem scale and dimensionality
        self.clusters = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self._initialize_population()
        fitness = objective_function(population)
        self.eval_count += self.population_size
        
        self.best_solution_overall = population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            self._clustering(population, fitness) #Clustering step
            offspring = self._generate_offspring(population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            combined_population = np.vstack((population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))

            idx = np.argsort(combined_fitness)
            population = combined_population[idx[:self.population_size]]
            fitness = combined_fitness[idx[:self.population_size]]

            best_index = np.argmin(fitness)
            if fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[best_index]
                self.best_solution_overall = population[best_index]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'number_of_clusters': len(self.clusters)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))

    def _generate_offspring(self, population, fitness):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_mutants(i, population)  #select three random different individuals
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_mutants(self, i, population):
        indices = list(range(self.population_size))
        indices.remove(i)
        a, b, c = random.sample(indices, 3)
        return population[a], population[b], population[c]

    def _crossover(self, x, v):
        u = np.copy(x)
        for j in range(self.dim):
            if random.random() < self.CR:
                u[j] = v[j]
        return u
    
    def _clustering(self, population, fitness):
        # Simple distance-based clustering
        self.clusters = []
        assigned = [False] * len(population)
        for i in range(len(population)):
            if assigned[i]: continue
            cluster = [i]
            assigned[i] = True
            for j in range(i + 1, len(population)):
                if assigned[j]: continue
                dist = np.linalg.norm(population[i] - population[j])
                if dist < self.cluster_threshold:
                    cluster.append(j)
                    assigned[j] = True
            self.clusters.append(cluster)

2025-06-23 08:59:45 INFO Unimodal AOCC mean: 0.1771
2025-06-23 08:59:45 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:59:45 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:59:45 INFO AOCC mean: 0.1771
2025-06-23 08:59:47 INFO Run function 2 complete. FEHistory len: 10000, AOCC: 0.1825
2025-06-23 08:59:47 INFO FeHistory: [-701.31605451 -701.30571524 -701.31403923 ... -701.76481862 -701.76279892
 -701.76476334]
2025-06-23 08:59:47 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 08:59:47 INFO Good algorithm:
Algorithm Name: AdaptiveDifferentialEvolutionWithClustering
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A differential evolution algorithm incorporating adaptive mutation, clustering for diversity, and local search to efficiently explore multimodal landscapes.
# Code:
class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.cluster_threshold = 0.5 # Distance threshold for clustering


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self._initialize_population()
        fitness = objective_function(population)
        self.eval_count += self.population_size
        
        self.best_solution_overall = population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            combined_population = np.vstack((population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))

            idx = np.argsort(combined_fitness)
            population = combined_population[idx[:self.population_size]]
            fitness = combined_fitness[idx[:self.population_size]]

            # Update best solution
            best_index = np.argmin(fitness)
            if fitness[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness[best_index]
                self.best_solution_overall = population[best_index]

            # Adaptive Parameter Tuning (Simple example -  adjust as needed)
            self.F = max(0.1, self.F * (1 - self.eval_count / self.budget))

            #Clustering and Local Search (optional, add as needed)
            clusters = self._cluster_population(population, self.cluster_threshold)
            for cluster in clusters:
                if len(cluster) > 1:  # Perform local search only if there's more than one individual
                    best_in_cluster = self._local_search(cluster, objective_function)
                    
                    if best_in_cluster[1] < self.best_fitness_overall:
                        self.best_fitness_overall = best_in_cluster[1]
                        self.best_solution_overall = best_in_cluster[0]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))

    def _generate_offspring(self, population, fitness):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            # Select three distinct random individuals
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            #Crossover
            crosspoints = np.random.rand(self.dim) < self.CR
            offspring[i] = np.where(crosspoints, mutant, population[i])

        return offspring

    def _cluster_population(self, population, threshold):
        # Simple clustering using Euclidean distance (replace with a more sophisticated method if needed)
        clusters = []
        for i in range(len(population)):
            assigned = False
            for j in range(len(clusters)):
                if np.linalg.norm(population[i] - np.mean(clusters[j], axis=0)) < threshold:
                    clusters[j].append(population[i])
                    assigned = True
                    break
            if not assigned:
                clusters.append([population[i]])
        return clusters


    def _local_search(self, cluster, objective_function):
        best_solution = cluster[0]
        best_fitness = objective_function(np.array([best_solution]))[0]
        
        for i in range(len(cluster)):
            for j in range(10): # Run a few iterations
                neighbor = cluster[i] + np.random.normal(0,0.1,self.dim)
                neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                fitness = objective_function(np.array([neighbor]))[0]
                if fitness < best_fitness:
                    best_fitness = fitness
                    best_solution = neighbor
                    
        return best_solution, best_fitness



2025-06-23 08:59:47 INFO Unimodal AOCC mean: 0.1825
2025-06-23 08:59:47 INFO Multimodal (single component) AOCC mean: nan
2025-06-23 08:59:47 INFO Multimodal (multiple components) AOCC mean: nan
2025-06-23 08:59:47 INFO AOCC mean: 0.1825
