2025-06-24 15:54:27 INFO Initializing first population
2025-06-24 15:54:27 INFO Initializing population from 4 seed files...
2025-06-24 16:02:30 INFO Started evolutionary loop, best so far: 0.2796149162371689
2025-06-24 16:02:30 INFO Population length is: 5
2025-06-24 16:02:30 INFO --- Performing Long-Term Reflection at Generation 1 ---
2025-06-24 16:02:30 INFO Reflection Prompt: ### Problem Description
**(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.

### List heuristics
Below is a list of design heuristics ranked from best to worst based on their AOCC score on group function 3 of GNBG benchmark, where higher is better. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score on Multimodal instances with multiple components: 2.7961e-01# Name: AdaptiveGaussianArchiveEA
# Description: Seed from AdaptiveGaussianArchiveEA
# Code:
```python
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 2 (Overall AOCC Score on Multimodal instances with multiple components: 2.3446e-01# Name: AdaptivePopulationDE
# Description: Seed from AdaptivePopulationDE
# Code:
```python
import numpy as np
import random
# f18 aocc 0.8
# f20 aocc 0.5
# not so good again, get stuck in local optima
class AdaptivePopulationDE: 
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim
        self.min_population_size = 5 * self.dim
        self.max_population_size = 20 * self.dim
        self.population_adaptation_rate = 0.1

        self.F = 0.5  # Mutation factor
        self.Cr = 0.7 # Crossover rate

        self.stagnation_counter = 0
        self.stagnation_threshold = 5000

        self.archive = []
        self.archive_size = 100

        self.population = None
        self.fitness = None

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.stagnation_counter = 0

        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = objective_function(self.population)
        self.eval_count += self.population_size

        best_index = np.argmin(self.fitness)
        self.best_solution_overall = self.population[best_index]
        self.best_fitness_overall = self.fitness[best_index]

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(objective_function)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.update_archive(offspring, offspring_fitness)

            for i in range(self.population_size):
                if offspring_fitness[i] < self.fitness[i]:
                    self.population[i] = offspring[i]
                    self.fitness[i] = offspring_fitness[i]

            best_index = np.argmin(self.fitness)
            if self.fitness[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = self.fitness[best_index]
                self.stagnation_counter = 0
            else:
                self.stagnation_counter += len(offspring)

            self.adjust_population_size(objective_function)

            if self.stagnation_counter > self.stagnation_threshold:
                self.restart_population(objective_function)
                self.stagnation_counter = 0

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'population_size': self.population_size
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, objective_function):
        offspring = np.zeros((self.population_size, self.dim))

        for i in range(self.population_size):
            indices = list(range(self.population_size))
            indices.remove(i)
            if len(indices) < 2:
                continue  # Skip if not enough individuals

            a, b = random.sample(indices, 2)

            if self.archive and random.random() < 0.5:
                pbest = self.archive[random.randint(0, len(self.archive) - 1)][0]
            else:
                pbest = self.population[np.argmin(self.fitness)]

            mutant = self.population[i] + self.F * (pbest - self.population[i] + self.population[a] - self.population[b])

            for j in range(self.dim):
                if random.random() > self.Cr:
                    mutant[j] = self.population[i][j]

            offspring[i] = np.clip(mutant, self.lower_bounds, self.upper_bounds)

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])

    def adjust_population_size(self, objective_function):
        if random.random() < self.population_adaptation_rate:
            if self.stagnation_counter > self.stagnation_threshold / 2:
                new_size = min(int(self.population_size * 1.1), self.max_population_size)
            else:
                new_size = max(int(self.population_size * 0.9), self.min_population_size)

            new_size = int(new_size)
            if new_size > self.population_size:
                additional = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(new_size - self.population_size, self.dim))
                additional_fitness = objective_function(additional)
                self.population = np.vstack((self.population, additional))
                self.fitness = np.concatenate((self.fitness, additional_fitness))
                self.eval_count += len(additional)
            elif new_size < self.population_size:
                best_indices = np.argsort(self.fitness)[:new_size]
                self.population = self.population[best_indices]
                self.fitness = self.fitness[best_indices]

            self.population_size = new_size

    def restart_population(self, objective_function):
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness = objective_function(self.population)
        self.eval_count += self.population_size
        best_index = np.argmin(self.fitness)
        if self.fitness[best_index] < self.best_fitness_overall:
            self.best_solution_overall = self.population[best_index]
            self.best_fitness_overall = self.fitness[best_index]
```

### Rank 3 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveGaussianSamplingEA
# Description: Seed from AdaptiveGaussianSamplingEA
# Code:
```python
import numpy as np

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Standard Deviation for Gaussian Sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Adaptive Gaussian Sampling
            parents = self.tournament_selection(fitness_values, k=5)  # Tournament Selection
            offspring = self.gaussian_mutation(parents, self.sigma)

            # Bounds handling
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update population and best solution
            self.population = np.concatenate((self.population, offspring))
            fitness_values = np.concatenate((fitness_values, offspring_fitness))

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            # Adaptive Sigma
            self.sigma *= 0.99  # Gradually reduce sigma for finer search later.

            # Elitism
            sorted_pop = self.population[np.argsort(fitness_values)]
            self.population = sorted_pop[:self.population_size]
            fitness_values = fitness_values[np.argsort(fitness_values)][:self.population_size]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2  # Select half the population as parents
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

```

### Rank 4 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: EnhancedArchiveGuidedDE
# Description: Seed from EnhancedArchiveGuidedDE
# Code:
```python
import numpy as np
import random

class EnhancedArchiveGuidedDE: #aocc 0.15
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size_factor: float = 8.82865217019506, archive_size: int = 165.22481375900153, initial_F_scale: float = 0.3544373580018585):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = int(population_size_factor * self.dim)  # common heuristic
        self.archive_size = archive_size
        self.archive = []
        self.population = None
        self.F_scale = initial_F_scale  # initial scaling factor

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8,
                 F_scale_variation: float = 0.3, archive_update_threshold: float = 0.8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness, F_scale_variation)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness, archive_update_threshold)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            # Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness, F_scale_variation):
        offspring = np.zeros((self.population_size, self.dim))
        # Adaptive scaling factor
        self.F_scale = 0.5 + F_scale_variation * np.random.rand()  # scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)  # Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness, archive_update_threshold):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                # Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * archive_update_threshold:
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
```

### Rank 5 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: ArchipelagoDifferentialEvolution
# Description: Employs a multi-population DE with migration to explore diverse basins, using stochastic ranking for constraint handling and restart mechanisms.
# Code:
```python
import numpy as np
import random

# Name: ArchipelagoDifferentialEvolution
# Description: Employs a multi-population DE with migration to explore diverse basins, using stochastic ranking for constraint handling and restart mechanisms.

class ArchipelagoDifferentialEvolution:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], 
                 pop_size: int = 50, num_islands: int = 5, migration_interval: int = 500, migration_size: int = 5,
                 F: float = 0.7, Cr: float = 0.9):
        """
        Initializes the Archipelago Differential Evolution algorithm.

        Args:
            budget: Max function evaluations.
            dim: Problem dimensionality.
            lower_bounds: List of lower bounds for each dimension.
            upper_bounds: List of upper bounds for each dimension.
            pop_size: Population size for each island.
            num_islands: Number of islands in the archipelago.
            migration_interval: Number of evaluations between migrations.
            migration_size: Number of individuals to migrate.
            F: Differential evolution scaling factor.
            Cr: Differential evolution crossover rate.
        """
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.pop_size = pop_size
        self.num_islands = num_islands
        self.migration_interval = migration_interval
        self.migration_size = migration_size
        self.F = F
        self.Cr = Cr

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        # Initialize populations for each island
        self.populations = []
        self.fitnesses = []
        self.best_solutions = []
        self.best_fitnesses = []
        for _ in range(self.num_islands):
            population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.pop_size, self.dim))
            self.populations.append(population)
            self.fitnesses.append(np.full(self.pop_size, float('inf')))  # Initialize with large values
            self.best_solutions.append(None)
            self.best_fitnesses.append(float('inf'))

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """
        Optimizes the given objective function using Archipelago Differential Evolution.

        Args:
            objective_function: A callable that accepts a 2D NumPy array (N, dim) and returns a 1D NumPy array of N fitness values.
            acceptance_threshold: Threshold for early stopping (not actively used in this version).

        Returns:
            A tuple: (best_solution_1D_numpy_array, best_fitness_scalar, optimization_info_dict).
        """
        self.eval_count = 0  # Reset for this run

        # Main optimization loop
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                self.evolve_island(i, objective_function)

            if self.eval_count % self.migration_interval == 0:
                self.migrate()

            #Update overall best
            for i in range(self.num_islands):
                 if self.best_fitnesses[i] < self.best_fitness_overall:
                      self.best_fitness_overall = self.best_fitnesses[i]
                      self.best_solution_overall = self.best_solutions[i].copy()

            # Restart mechanism - triggered when local best unchanged for a while - implement for later evolutions
            #if some_restart_condition:
            #    self.restart_island(island_index)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def evolve_island(self, island_index: int, objective_function: callable):
        """
        Evolves a single island for one generation using Differential Evolution.

        Args:
            island_index: The index of the island to evolve.
            objective_function: The objective function to optimize.
        """
        population = self.populations[island_index]
        fitnesses = self.fitnesses[island_index]

        # Evaluate initial population
        unevaluated_indices = np.where(fitnesses == float('inf'))[0]
        if len(unevaluated_indices) > 0:
            fitnesses[unevaluated_indices] = objective_function(population[unevaluated_indices])
            self.eval_count += len(unevaluated_indices)

        for j in range(self.pop_size):
            # Mutation
            idxs = [idx for idx in range(self.pop_size) if idx != j]
            a, b, c = random.sample(idxs, 3)
            mutant = population[a] + self.F * (population[b] - population[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Crossover
            trial = population[j].copy()
            for k in range(self.dim):
                if random.random() < self.Cr:
                    trial[k] = mutant[k]

            # Selection
            trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
            trial_fitness = objective_function(np.array([trial]))[0]  # Evaluate single solution
            self.eval_count += 1
            if trial_fitness < fitnesses[j]:
                population[j] = trial
                fitnesses[j] = trial_fitness

                # Update best solution on the island
                if trial_fitness < self.best_fitnesses[island_index]:
                    self.best_fitnesses[island_index] = trial_fitness
                    self.best_solutions[island_index] = trial.copy()

        self.populations[island_index] = population
        self.fitnesses[island_index] = fitnesses

    def migrate(self):
        """
        Migrates individuals between islands.
        """
        #print("Migrating individuals...") # Debug Statement
        for _ in range(self.migration_size):
            # Select a random island to send migrants from
            sender_island = random.randint(0, self.num_islands - 1)
            
            # Select a random island to receive migrants, ensuring it's different from the sender
            receiver_island = sender_island
            while receiver_island == sender_island:
                receiver_island = random.randint(0, self.num_islands - 1)
            
            # Select a random individual to migrate from the sender island
            migrant_index = random.randint(0, self.pop_size - 1)
            migrant = self.populations[sender_island][migrant_index].copy()  # Copy to avoid modifying the original

            # Select a random individual to replace on the receiver island
            replacement_index = random.randint(0, self.pop_size - 1)

            # Perform the migration
            self.populations[receiver_island][replacement_index] = migrant
            
            #Evaluate the new migrant
            trial_fitness = objective_function(np.array([migrant]))[0]  # Evaluate single solution
            self.eval_count += 1
            self.fitnesses[receiver_island][replacement_index] = trial_fitness

            if trial_fitness < self.best_fitnesses[receiver_island]:
                    self.best_fitnesses[receiver_island] = trial_fitness
                    self.best_solutions[receiver_island] = migrant.copy()

    def restart_island(self, island_index: int):
         """Restarts a single island with new random solutions."""
         self.populations[island_index] = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.pop_size, self.dim))
         self.fitnesses[island_index] = np.full(self.pop_size, float('inf'))
         self.best_solutions[island_index] = None
         self.best_fitnesses[island_index] = float('inf')

```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 16:02:35 INFO Full response text: **Analysis:**
Comparing (AdaptiveGaussianArchiveEA) vs (ArchipelagoDifferentialEvolution), we see AdaptiveGaussianArchiveEA uses a single population with an archive to maintain diversity, focusing on adaptive mutation and recombination, while ArchipelagoDifferentialEvolution uses multiple isolated populations with periodic migration. Comparing (AdaptivePopulationDE) vs (EnhancedArchiveGuidedDE), both use archives, but AdaptivePopulationDE dynamically adjusts population size based on stagnation, which EnhancedArchiveGuidedDE does not. Comparing (AdaptiveGaussianArchiveEA) vs (AdaptivePopulationDE), the AdaptiveGaussianArchiveEA uses more direct control of the standard deviation of sampling with `self.sigma_decay` while the adaptive population DE adjusts population size and relies on DE operators like F and Cr. Comparing (AdaptiveGaussianSamplingEA) vs (EnhancedArchiveGuidedDE), AdaptiveGaussianSamplingEA uses a simplified Gaussian mutation and reduces the mutation over time. EnhancedArchiveGuidedDE instead attempts to generate a new population guided by the archive. Comparing (EnhancedArchiveGuidedDE) vs (ArchipelagoDifferentialEvolution), EnhancedArchiveGuidedDE features only one population but uses an archive to hopefully discover new regions. While the Archipelago DE relies more on the population existing on separate "islands" to generate diversity. Comparing (AdaptiveGaussianSamplingEA) vs (ArchipelagoDifferentialEvolution), it's noted that AdaptiveGaussianSamplingEA and ArchipelagoDifferentialEvolution represents a single population based methods verses multipopulation (island) based methods respectively. The better algorithm contains the archive structure. Overall: The more complex algorithms employ sophisticated archiving strategies, while simpler methods focus on mutation rate decay, adaptive population size, or utilize multiple independent populations. The key seems to be how an algorithm balances exploitation using memory and exploration through isolated populations with adaptive changes to algorithms parameters and its search space.

**Experience:**
Aggressive global exploration in deceptive landscapes benefits from combining archive-based memory for exploiting promising regions with mechanisms to enhance exploration, such as adjusting mutation rates or using multiple populations with migration, which gives exploration and diversity. Sophisticated algorithms can discover multiple optima.

2025-06-24 16:02:37 INFO Full response text: Okay, I understand. To design better heuristics, we need a self-reflection strategy that avoids pitfalls and focuses on actionable insights. Here's a redefined self-reflection strategy geared towards improving heuristic design:

*   **Keywords:** Adaptive memory, diversified search, landscape analysis, exploitation/exploration balance, multi-strategy approach, meta-heuristics.
*   **Advice:** Focus on understanding *why* specific archived solutions are promising. Experiment with dynamic strategy selection based on landscape features and performance metrics.
*   **Avoid:** Simply acknowledging multiple optima without understanding the characteristics that led to their discovery or how to exploit similar areas, overly-aggressive exploration that forgets learning.
*   **Explanation:** Move beyond just noting *what* works. Analyze the reasons behind successful strategies to extract transferable insights for adapting heuristics during the search process and make each exploration count.

2025-06-24 16:02:37 INFO Generating offspring via Crossover...
2025-06-24 16:06:03 INFO Crossover Prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


### Better code
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Worse code
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Analyze & experience
- Comparing (AdaptiveGaussianArchiveEA) vs (ArchipelagoDifferentialEvolution), we see AdaptiveGaussianArchiveEA uses a single population with an archive to maintain diversity, focusing on adaptive mutation and recombination, while ArchipelagoDifferentialEvolution uses multiple isolated populations with periodic migration. Comparing (AdaptivePopulationDE) vs (EnhancedArchiveGuidedDE), both use archives, but AdaptivePopulationDE dynamically adjusts population size based on stagnation, which EnhancedArchiveGuidedDE does not. Comparing (AdaptiveGaussianArchiveEA) vs (AdaptivePopulationDE), the AdaptiveGaussianArchiveEA uses more direct control of the standard deviation of sampling with `self.sigma_decay` while the adaptive population DE adjusts population size and relies on DE operators like F and Cr. Comparing (AdaptiveGaussianSamplingEA) vs (EnhancedArchiveGuidedDE), AdaptiveGaussianSamplingEA uses a simplified Gaussian mutation and reduces the mutation over time. EnhancedArchiveGuidedDE instead attempts to generate a new population guided by the archive. Comparing (EnhancedArchiveGuidedDE) vs (ArchipelagoDifferentialEvolution), EnhancedArchiveGuidedDE features only one population but uses an archive to hopefully discover new regions. While the Archipelago DE relies more on the population existing on separate "islands" to generate diversity. Comparing (AdaptiveGaussianSamplingEA) vs (ArchipelagoDifferentialEvolution), it's noted that AdaptiveGaussianSamplingEA and ArchipelagoDifferentialEvolution represents a single population based methods verses multipopulation (island) based methods respectively. The better algorithm contains the archive structure. Overall: The more complex algorithms employ sophisticated archiving strategies, while simpler methods focus on mutation rate decay, adaptive population size, or utilize multiple independent populations. The key seems to be how an algorithm balances exploitation using memory and exploration through isolated populations with adaptive changes to algorithms parameters and its search space.
- Okay, I understand. To design better heuristics, we need a self-reflection strategy that avoids pitfalls and focuses on actionable insights. Here's a redefined self-reflection strategy geared towards improving heuristic design:

*   **Keywords:** Adaptive memory, diversified search, landscape analysis, exploitation/exploration balance, multi-strategy approach, meta-heuristics.
*   **Advice:** Focus on understanding *why* specific archived solutions are promising. Experiment with dynamic strategy selection based on landscape features and performance metrics.
*   **Avoid:** Simply acknowledging multiple optima without understanding the characteristics that led to their discovery or how to exploit similar areas, overly-aggressive exploration that forgets learning.
*   **Explanation:** Move beyond just noting *what* works. Analyze the reasons behind successful strategies to extract transferable insights for adapting heuristics during the search process and make each exploration count.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 16:06:03 INFO Mutation prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


Current heuristics:
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- Okay, I understand. To design better heuristics, we need a self-reflection strategy that avoids pitfalls and focuses on actionable insights. Here's a redefined self-reflection strategy geared towards improving heuristic design:

*   **Keywords:** Adaptive memory, diversified search, landscape analysis, exploitation/exploration balance, multi-strategy approach, meta-heuristics.
*   **Advice:** Focus on understanding *why* specific archived solutions are promising. Experiment with dynamic strategy selection based on landscape features and performance metrics.
*   **Avoid:** Simply acknowledging multiple optima without understanding the characteristics that led to their discovery or how to exploit similar areas, overly-aggressive exploration that forgets learning.
*   **Explanation:** Move beyond just noting *what* works. Analyze the reasons behind successful strategies to extract transferable insights for adapting heuristics during the search process and make each exploration count.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-24 16:06:17 INFO Perform Harmony Search...
2025-06-24 16:06:26 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 16:10:59 INFO Run function 16 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:10:59 INFO FeHistory: [126844.81641642 156784.8602338  197122.70369762 ...  -4399.9
  -4399.9         -4399.9       ]
2025-06-24 16:10:59 INFO Expected Optimum FE: -5000
2025-06-24 16:10:59 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 16:15:31 INFO Run function 18 complete. FEHistory len: 150000, AOCC: 0.4474
2025-06-24 16:15:31 INFO FeHistory: [ 81262.20350664 196131.14623009 261059.16312071 ...  -5000.
  -5000.          -5000.        ]
2025-06-24 16:15:31 INFO Expected Optimum FE: -5000
2025-06-24 16:15:31 INFO Good algorithm:
Algorithm Name: EnhancedAdaptiveGaussianArchiveEA
import numpy as np

class EnhancedAdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 64.24053513658582,
                 archive_size: int = 135.01553112141966,
                 initial_sigma_ratio: float = 0.20398426566729055,
                 sigma_decay: float = 0.9966234482661166,
                 tournament_size: int = 3.868535383310345,
                 recombination_sigma_factor: float = 0.5277839310333214,
                 archive_similarity_threshold: float = 1.9665194150438098e-06):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.sigma = initial_sigma_ratio * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = float(sigma_decay)
        self.archive = []
        self.tournament_size = int(tournament_size)
        self.recombination_sigma_factor = float(recombination_sigma_factor)
        self.archive_similarity_threshold = float(archive_similarity_threshold)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:

        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-24 16:15:31 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 16:19:56 INFO Run function 19 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:19:56 INFO FeHistory: [213895.13654349 196760.97839183 193715.23739011 ...  -4317.9
  -4317.9         -4317.9       ]
2025-06-24 16:19:56 INFO Expected Optimum FE: -5000
2025-06-24 16:19:56 INFO Unimodal AOCC mean: nan
2025-06-24 16:19:56 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 16:19:56 INFO Multimodal (multiple components) AOCC mean: 0.1491
2025-06-24 16:19:56 INFO AOCC mean: 0.1491
2025-06-24 16:19:56 INFO Weighed AOCC mean: nan
2025-06-24 16:19:56 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 16:22:19 INFO Run function 16 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:22:19 INFO FeHistory: [142251.32523217 149821.2895418  157648.7807316  ...   5679.61364216
   5679.61364216   5679.61364216]
2025-06-24 16:22:19 INFO Expected Optimum FE: -5000
2025-06-24 16:22:19 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 16:24:41 INFO Run function 18 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:24:41 INFO FeHistory: [139841.28242803 203635.63628972 244171.80354614 ...   9434.09322399
   9434.09322399   9434.09322399]
2025-06-24 16:24:41 INFO Expected Optimum FE: -5000
2025-06-24 16:24:41 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 16:26:55 INFO Run function 19 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:26:55 INFO FeHistory: [198411.23247159 128947.46013527 172896.0319585  ...  11027.04348293
  11027.04348293  11027.04348293]
2025-06-24 16:26:55 INFO Expected Optimum FE: -5000
2025-06-24 16:26:55 INFO Unimodal AOCC mean: nan
2025-06-24 16:26:55 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 16:26:55 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 16:26:55 INFO AOCC mean: 0.0000
2025-06-24 16:26:55 INFO Weighed AOCC mean: nan
2025-06-24 16:26:56 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 16:26:56 ERROR Can not run the algorithm
2025-06-24 16:26:56 INFO Run function 16 complete. FEHistory len: 183, AOCC: 0.0000
2025-06-24 16:26:56 INFO FeHistory: [144235.45361711 175112.65425176 217469.75854593 176350.25829961
 171192.25232421 184800.12397897 181479.04408237 213563.72660248
 280367.92399841 152120.52398705 202831.99429982 218546.79738133
 209915.48783899 172283.27477983 230112.75893761 228630.56902938
 178192.75791015 157519.6100988  161748.30298119 171500.81698556
 189709.05634219 205428.19098734 166431.27628667 210155.32365708
 174174.10894378 192262.24195248 216832.11770683 172938.69231186
 161885.9077156  139944.98973517 190295.83559556 228384.1692253
 155640.56609199 236635.4036501  183313.00835607 214979.44028002
 183477.6537088  140707.37787463 207216.31186062 177970.1298632
 168625.19229203 159086.22062215 190124.41816602 197826.15054962
 192212.64629036 169659.88775942 153636.688251   154670.84262337
 230968.08589093 230630.35268052 156681.39714832 194921.17021635
 178769.32738348 234199.91580607 173657.63258229 184489.91337819
 165067.77266233 147108.69705868 213689.59727946 114218.56896905
 242834.54444768 155901.28173409 186704.77478833 223804.17105592
 269837.38650527 168910.20871141 214439.12974408 181041.44482085
 214960.25399915 187893.58573011 163905.91066212 214611.42892184
 164925.65771106 169167.24594018 213634.15746633 157775.04392866
 166476.89025027 193836.25844447 209265.24696732 192683.10657537
 139284.1662219  199950.24919247 224889.83805605 274574.07449679
 183109.68994993 220034.21208136 188852.66597609 180050.56124604
 191096.159944   164982.64684016 106965.96359347 190553.27734342
 223110.97105847 173797.71901168 232683.22451358 228524.5826899
 177453.36525357 163160.66911655 215646.61383924 195165.98027294
 182562.51654753 158958.95568686 197831.21176286 181812.86281934
 210698.75450457 143519.24285046 149675.53910298 198181.19227277
 185101.50888287 207177.41444995 244501.03700713 198218.1997137
 100217.54446391 216381.86732561 195148.79758548 206979.02046697
 185484.68104319 217975.72343684 190399.60933859 220248.44758866
 204722.14530479 105340.2767452  187578.37483972 155090.15414074
 163221.21171111 222059.61478514 267489.11672698 134511.2817672
 145144.06637801 236569.12578543 222636.68311799 218950.07049528
 218788.88585947 190483.1016412  189119.39749464 220526.27338876
 148803.50687199 176007.4899228  215800.49862823 152876.13661214
 181124.69753318 211621.87230606 291464.47254878 198562.53327014
 212837.58315444 194593.54050996 188409.27165109 222148.56120167
 207711.72331464 231175.33605287 141165.43728206 159552.98380293
 224711.56776065 125960.42155495 220503.03816544 194739.4990119
 215468.57108463 175111.16738919 216397.73746784 187688.98376224
 248169.69816433 184778.86706693 191531.25156276 158077.94876027
 177141.0014441  176036.25302414 153543.49897514 174087.13442367
 205838.44412006 211517.68238719 196199.48360938 174262.77080231
 190783.77647948 206173.88538903 176600.72693059 233261.58018624
 225407.17234927 184616.25241467 257531.74177637 220125.08378434
 239450.58690423 255084.41003303 224189.85118617]
2025-06-24 16:26:56 INFO Expected Optimum FE: -5000
2025-06-24 16:26:56 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 16:26:56 ERROR Can not run the algorithm
2025-06-24 16:26:57 INFO Run function 18 complete. FEHistory len: 183, AOCC: 0.0000
2025-06-24 16:26:57 INFO FeHistory: [115250.70912662 237793.80567415 143002.31558463 273374.86634198
 246472.38133347 297110.68148531 220847.13247059 243730.74783252
 185308.7316431  145820.15732456 182913.85897653 200203.11943483
 197753.81669625 191418.70193769 312789.81265888 217459.77228075
 185830.63261575 158910.4414539  165258.90559565 127705.04961853
 294838.07310763 169158.89479614 119742.30699742 192714.13312453
 185065.88408387 190048.18702943 252261.24258738 278837.76394603
 231901.586631   250072.59147689 199139.9749516  254011.49466844
 170510.41656947 150958.81348015 259237.17073663 224653.59206678
 112463.11090147 120660.9217646  276566.40176744 182480.30414364
 197875.85756641 223984.91352503 235482.55494945 255049.59904188
 174408.4372493  214126.97933237 116000.61443892 117693.3376446
 110916.04908663 196345.83801318 240329.68544777 234590.03832141
 266838.30896649 160807.97640412 203105.69444222 226058.65277546
 203138.25522443  99362.24972298 214977.46087098 233447.41058948
 337652.26102308 212097.77051331 204902.73091219 239650.07940343
 290586.88319078 222551.17529027 139390.36531887 195727.56875552
 154413.64286435 189024.30667076 213213.81291012 229095.57278836
 239349.56218451 148388.2421068  175377.29382889 152089.08278757
 197601.83324623 178075.98171302 278144.63787149 136280.61521049
 183927.88018376 205805.42512862 256829.1624742  215825.81852034
 175250.61792457 132090.3428387  230473.98409274 163831.02234241
 241958.02529854 157585.1982639  317547.08314753 185053.67457013
 196729.03334866 140219.05612007 150127.3789035  147455.42670845
 267956.54550352 222160.27718971 231244.08946051 163530.96125722
  79789.15514952 208955.87723668 210975.62055088 215661.10719985
 211457.18038692 163452.27521877 164102.46586946 140443.4602921
 229051.4466049  284158.2151997  156255.4644897  237270.91797525
 262856.65412105 189674.67097327 273361.34215666 180790.64162224
 255233.50347049 217639.70457427 192286.13091575 173717.74530685
 235602.34792607 179841.64500283 121359.74540087 179235.82385749
 197142.40703394 200030.83840779 226808.57476099 220196.80168018
 176943.29494915 184617.55810412 190956.68805134 209850.05780994
 207440.31674817 286706.15348622 151472.41666165 213628.18638579
 139763.08088396 307832.46717368 223070.49587808 211498.52751824
 276601.77062746 258901.3177399  191448.20008997 209609.34960433
 188288.33982038 230555.05546283 240542.50694572 228552.76835493
 105267.78759788 202923.96090541 146108.09611981 242249.15027002
 205308.87519694 215371.11014546 246717.7250333  155242.02985267
 211797.14903161 155723.9369852  177705.79016233 199222.14747375
 211988.13109813 181205.62723197 164426.00063764 172107.76654397
 240955.93143089 277054.53969816 201252.55563239 234330.7303504
 353491.39642796 184757.67007417 170058.34254534 294159.72039891
 194473.06073819 188782.89303816 255387.28446023 196302.74777105
 220669.28123201 188207.1942926  230713.7309932  169588.04137356
 179346.70612585 229735.48170024 110658.46161087]
2025-06-24 16:26:57 INFO Expected Optimum FE: -5000
2025-06-24 16:26:57 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 16:26:57 ERROR Can not run the algorithm
2025-06-24 16:26:57 INFO Run function 19 complete. FEHistory len: 183, AOCC: 0.0000
2025-06-24 16:26:57 INFO FeHistory: [127807.90152768 222161.01650672 174380.49167813 260681.82444321
 343689.39131126 224809.7849135  146793.63617889 273976.2980895
 210970.52509921 222937.55832181 247086.7919816  144388.23155238
 215051.32501082 312643.05793017 117631.04802932 219460.96380034
 196114.99646204 178737.89058187 266859.25392496 235755.08432381
 290225.24211157 260319.77904296 274115.42085989 211114.80406051
 250582.99265959 308657.25282163 214765.28475898 345639.15172393
 143445.39744279 103209.56677725 290207.27462396 296621.33279769
 236849.25693266 145183.58888835 241763.67103306 286400.68114416
 254637.6311171  196347.53205697 161924.7012458  236580.42566997
 226908.03483395 279245.11112657 167568.48093599 177554.53906642
 256330.68214563 284341.91686579 289252.25610316 123872.77741399
 245153.78992981 299892.08606645 244862.86706822 233169.60165514
 201652.8978816  261697.94809004 255325.8474289  263865.53135026
 149041.711778   331393.20238708 229804.35648057 328549.89343927
 341411.50303256 279933.20470483 144364.01312397 286057.83320518
 225017.88619703 209532.42145177 285977.56624536 188806.30245546
 276251.22679072 250392.38792949 234228.23035172 301797.25972972
 197769.36948626 171608.81825683 156524.56772996 295303.46376926
 131415.88237998 170421.51955958 289628.68549467 190091.29142608
 203940.96442083 327421.94759225 162220.74177465  77635.44247687
 235481.65113719 221066.24218161 143402.4243638  225375.05789534
 186366.2442879  194630.3710921  235775.60920396 212371.50456071
 201354.64581203 254282.97300832 200218.8444168  251530.95692661
 147717.07365731 301467.32239608 297479.04316005 231456.19995125
 111154.69506081 173939.64042276 196363.9301724  185298.75932001
 120677.86523967 207832.89835054 179542.49843767 301187.77067626
 217659.50380794 308993.45411294 116296.39585735 247700.29177662
 221471.29133932 175247.84522137 238347.10271819 306940.10959225
 217977.0936785  314049.61422597 274288.34718173 162314.15379928
 261823.33117955 177993.52539654 200989.05445747 234812.71582483
  85216.52372531 200655.1076851  271446.29839777 235129.2176796
 127000.73490427 232955.15859288 228016.36250121 234220.03072366
 187065.62440191 165368.92110906 134997.77735256 132408.42588345
 132642.86873345 351997.09426403 246439.80532468 220321.64738451
 261451.04070302 363565.98578824 142098.06612234 305087.71732408
 232581.39607693 291943.84862892 340838.8519044  169449.70982763
 262787.64836082 218801.81034552 139413.9946412  237630.30039259
 109992.33580911 170595.01777774 259803.96367362 233824.03944051
 287148.07012055 224734.14511011 277509.19245973 234362.8987627
 253027.34251836 149525.25009336 188950.2100326  191228.44882479
 133212.39286619 270536.33519794 225116.65999845 241100.48122433
 265365.47828054 279397.39199291 160850.32412401 231238.28601101
 184016.77932328 227814.1851611  205061.83687284 310430.40533869
 277388.25644684 190125.47004626 262509.39591287 155094.65590275
 181207.68894224 274067.78860543 166030.6114134 ]
2025-06-24 16:26:57 INFO Expected Optimum FE: -5000
2025-06-24 16:26:57 INFO Unimodal AOCC mean: nan
2025-06-24 16:26:57 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 16:26:57 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 16:26:57 INFO AOCC mean: 0.0000
2025-06-24 16:26:57 INFO Weighed AOCC mean: nan
2025-06-24 16:26:57 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 16:26:57 ERROR Can not run the algorithm
2025-06-24 16:26:58 INFO Run function 16 complete. FEHistory len: 192, AOCC: 0.0000
2025-06-24 16:26:58 INFO FeHistory: [151114.06241506 137608.87534595 150036.68283952 149842.18093622
 144662.65437224 133782.40134942 148388.49060587 122447.36558967
 131471.76336347 111000.44721015 134880.30214924 166357.71260329
 162805.87319216 153922.31386083 134669.26524827 103957.57798558
 136630.63785299 140894.24537782 133193.84614008 148140.05405488
 155427.51702501 130805.83615149 128402.78347709 139685.49879723
 112658.51559259 142863.5854769  154433.85705024 108008.21413661
 126821.7424577  151422.51496248 113759.23485401 129318.98590595
 113326.34514894 133146.63185046 107168.60339853 164207.59764119
 138963.15076557 151844.16042659 131865.73427052 105429.47117557
  95379.72337426 147575.64662264 139776.25798985 159964.0461212
 117354.72792338 132753.04594411 149918.41706697 167242.49271784
 112878.71447657 143577.66266062 129290.24943209 126040.68915887
 139480.57676429 103691.61539679 151971.58506691 146547.47760288
 175994.76579313 147107.45607323 109250.51971003 104827.69256625
 148849.56739045 125788.53853059 162911.70984892 164729.59388495
 113770.02198133 128841.2071652  141287.0215824  130181.98312964
 111947.06881423 155095.71394457 155958.42634508 140789.55152211
 145396.22052579 178825.63413355 103312.49282543 122646.05836841
 123367.78251061 125530.15129905 139972.19290011 119376.55136128
 133484.29273412 135256.18525156 158349.77154796 114255.80264611
 126021.84309078 128811.77170716 111065.85977068 144787.39656528
  72202.43516996 152578.36024789 105913.34648133 115933.91558144
 144962.14934996 159712.66025849 130202.76414451 100495.3000639
 124018.40464126 151564.81933304 156450.22278169 105221.0775977
 134434.9286755  121827.69027444 135264.01734918 110887.09087977
 130835.32479242 127652.7015535  144406.63648226 124817.47463305
 125025.15858451 106673.80370134 133893.55671461 142806.69054716
 159624.80365229 101747.10099378 133810.95478322 122412.0513851
 136241.81540129 173225.72659615 140599.71613766 152361.6435432
 165012.91371896 144857.87577665 147000.48655789 150245.79151932
 137035.13045124 119187.04153838 135262.33538007 153522.82498048
 156649.16994311 110185.83015525 141925.83249108 160708.60815605
 138108.37311546 150462.74757168 126223.15272652 166146.07891498
 134759.55669784 126730.31586148 143773.89752139 151939.8411053
 141826.459247   101836.18819921 160656.56610204 155655.73414245
 128253.0395628  128076.30024239 114031.74065253 134688.33231552
 162051.49085688 130358.27517497 151500.57901561 130066.18708801
 126997.56382626 122326.35507529 124079.63659504 130876.93076346
 132419.79440537 138126.89951399 123444.22016534 142957.26795454
 108450.77408423 153321.24022036 116713.06471588 116385.8106922
 133243.97833385 110160.62772426 142654.08368479 134935.09204868
 142573.19432125 145222.06595737 142610.35682656 148614.42529175
 141856.34823532  85576.89394503 118156.72216147 175901.58568532
 128662.885812   131812.16565535 132898.20307112 148189.17072803
 142473.95161434 119997.64101355 131623.47950909 145592.14720285
 120063.56192216 144622.87198733 158088.7666236  140492.01750015
 161733.3781239  124795.30055319 146104.95183695 107094.70901068]
2025-06-24 16:26:58 INFO Expected Optimum FE: -5000
2025-06-24 16:26:58 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 16:26:58 ERROR Can not run the algorithm
2025-06-24 16:26:58 INFO Run function 18 complete. FEHistory len: 192, AOCC: 0.0000
2025-06-24 16:26:58 INFO FeHistory: [157294.70564246 121702.29401667 176928.22421362 131856.78447761
 124286.92017455 133403.44677514 128966.74685575 158481.22441714
 155261.35900994 174303.19424728 143773.73162264 136378.86053559
 137853.35980845 189962.4570463  202760.58756394 211703.19403316
 135645.19172174 250044.47874848 173401.24626305 102332.76679808
 198912.63258118 162571.55231692 134570.79876165 107802.97821719
 104841.7553132  270479.91669486  98691.19110945 176895.22390019
 150655.35842853 152752.49231466 194701.90438628 160778.26010206
  80817.62831777 107466.88864734 103980.71368992 142498.20356473
 136804.68213434 101315.54310515 156896.85335101 143716.24893713
 159118.28792117 141629.43394575 225734.10682585 123557.49183575
 137416.1937823  161185.52139244 180236.92601355 142533.78572523
 137448.92725938 156414.93133672  92036.74375453 117549.60634896
 162372.50929375 150218.07511919 114325.75282513 164898.70163061
 203931.19496742 118158.56995751 127188.69077979 143834.0564199
 124359.23368058 138803.98646177 185422.76706768 162406.83783324
 130564.03461049 141403.11407074 150221.97062706 162934.23987789
 153303.65279497 160438.05917602 171571.93759107 200023.61286367
 106094.38301473 101910.78219351 148918.66779342 149354.29261043
 127336.32380535 151290.51960805 172550.38644105 150430.86498214
 136328.9061636  229049.45587459 135782.85068248 182839.87952331
 138077.20134067 143719.1558055  149622.20658509 210682.77061798
 131330.14313245 154971.87341254 122326.43260677 170785.15236974
 170857.46777155 247381.67593913 142381.48156158 139014.9566393
 168711.88845674 162219.91623554 137993.01803723 142925.83763402
 150852.29777087 194760.06210474 178581.33488943 177338.82111392
 181577.65258436 119566.29585183 116971.90569108 126364.95119294
 131488.29092919 133527.37125542 153356.59559305 141010.28379649
 124212.68572421 212438.52077345 120189.59448776 166510.5200417
 119695.62407998 117155.09857883 120847.31415494 115221.6622121
 147213.61405241 141777.01509354 153887.67054422 156957.11575531
 147933.40506515 118754.14944286 140348.53930212 159522.71335628
 193134.20007242 142030.39849533 131727.74694296 108195.78231319
 131400.9125105  184140.34042595 122820.2207254  124146.16109635
 185480.45536644 124064.11187686 171450.95325874 153162.91220209
 107342.21191613  99672.30539453 166082.66936643 191460.8573917
 195302.13449814 165248.83747096 143896.33257995 180426.68590947
 217132.23336383 188708.64358871 142408.29458233 138616.1355964
 143718.90179818 171559.67560459 119517.09564569 200171.55124941
 138273.66551663 201467.19424392 134053.47181527 242814.65093727
 156793.39337761 124603.59171662 151900.56140986 115194.01671902
 138091.4273945  174842.58670561 140871.06109592 176736.99842268
 117011.19908463 124762.36906167 210146.47793487 132994.16120467
 137871.23770765 198295.33470497 143810.82892423 101579.00594825
 117969.93650411 129993.02453003 157547.38283475 146956.39297336
 143945.4510269  129010.61648505 169148.27081736 145698.92998404
 137139.91880625 203220.2254003  128515.13349199 134598.05457837
 122537.35452642 212729.66440722 127989.7280309  152675.17052291]
2025-06-24 16:26:58 INFO Expected Optimum FE: -5000
2025-06-24 16:26:58 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 16:26:58 ERROR Can not run the algorithm
2025-06-24 16:26:59 INFO Run function 19 complete. FEHistory len: 192, AOCC: 0.0000
2025-06-24 16:26:59 INFO FeHistory: [127566.11989298 182731.7450599  255009.72088871 237251.01537815
 151630.93496029 173966.68021688 141095.61844755 182685.75214902
 159438.59396105 219763.85944255 107732.37556758 301564.90983975
 276329.37981871 281891.03930337 200950.02636877 182629.61434802
 127133.72801961 210582.37528067 154648.54027528 218966.13170173
  75743.94008877 188854.45490351 172773.48505446 180250.7795824
 129969.47055771 242914.35287401 184970.65373813 195261.12622283
 248124.33870463 291464.91046551 185759.36896179 212899.24952629
 289717.33085243 291253.64310683 224448.04822237 142395.95271542
 189894.42402178 168185.24215083 261610.76516541 194330.71859826
 210422.95554659 145972.85422384 271898.31446861 169328.16760362
 212582.40668226 152731.27903219 196832.89816162 219874.42053708
 160568.92227132 178438.67224629 166410.06651188 126374.55268314
 120236.92546934 350858.70662839 204197.30874899 143732.28506884
 198093.74244471  95450.6518833  178438.66951673 238722.73163159
 201204.01713114 154063.73761181 192951.00435818 202861.15259886
 136248.04683532 100192.67282429 162835.16088947 154984.51777797
 187485.07208058 146823.09100488 213080.14124167 270478.40098004
 140713.50520078 129754.30704806 233804.19223677 161225.57905916
 208789.92515319 123965.59493003 136483.8045344  162985.08042686
 169624.02648817 167770.28151939 225703.78202176 229949.49866015
 172828.61134876 159647.81916555 290385.42085856 176095.99197224
 249197.02298569 203406.43808734 183000.77345922 206952.89155782
 182934.46677338 146304.42121038 200495.37485614 222367.813691
 112497.83132411 226779.73632159 146921.16491653 182550.52149141
 136435.23462864 182560.34834498 182126.11384621 202769.78314145
 236669.52804793 244439.75662923 144611.25094587 165029.63490485
 164479.72974143 120745.99764596 125648.13561283 201712.71915613
 136550.48255425 133235.54128345 168194.48236201 248557.9066542
 196541.55983418 234199.96750994 142958.52549586 215883.50797438
 229445.46339646 119471.74263286 214353.45060871 195655.21716913
 175705.8972048  228700.57299928 140216.7564388  194918.34038199
  77767.97407999 176143.27113207 192826.2945351  287503.71094663
 238610.71386033 173689.89441472 125925.59908342 157471.37803454
 131651.41619037 183132.70539763 196827.31796251 225754.82769751
 260430.98818399 278719.37620382 137761.82202282 271373.41924983
 168763.93182756 127408.95965577 197740.9658536  141970.33352015
 303236.29146124 191497.13873741 381438.26316918 111089.50471833
 176398.59057762 164052.73979478 202175.49690875 157085.62495905
 160370.72123428 182100.16557833 224847.57586691 144111.01401472
 280779.94948617 252561.56434164 175304.92841461 161467.56121618
 237794.30398741 230863.78891547 182677.0890079  150039.0561024
 198058.84626586 193472.76259422 253262.13272003 121333.20416134
 190772.91684328  83896.93806356 163703.99782548 272882.88017558
 130064.4332618  231258.66167526 263819.51376093 180695.52819349
 164353.21964534 158261.83439551 229624.55136127 284618.29983243
 154545.87638792 233106.88232996 270895.41845076 162910.88900139
 245872.82712107 154514.24979816 196882.00034858 245302.3624969 ]
2025-06-24 16:26:59 INFO Expected Optimum FE: -5000
2025-06-24 16:26:59 INFO Unimodal AOCC mean: nan
2025-06-24 16:26:59 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 16:26:59 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 16:26:59 INFO AOCC mean: 0.0000
2025-06-24 16:26:59 INFO Weighed AOCC mean: nan
2025-06-24 16:26:59 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 16:31:02 INFO Run function 16 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:31:02 INFO FeHistory: [151600.72241748 236635.6526172  137117.95582093 ...  -4419.9
  -4419.9         -4419.9       ]
2025-06-24 16:31:02 INFO Expected Optimum FE: -5000
2025-06-24 16:31:02 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 16:35:00 INFO Run function 18 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:35:00 INFO FeHistory: [142538.9507579  246693.73195523 287344.76679771 ...  -4399.9
  -4399.9         -4399.9       ]
2025-06-24 16:35:00 INFO Expected Optimum FE: -5000
2025-06-24 16:35:00 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 16:38:55 INFO Run function 19 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:38:55 INFO FeHistory: [141073.23922348 204194.00901896 261663.03047071 ...  -4470.4
  -4470.4         -4470.4       ]
2025-06-24 16:38:55 INFO Expected Optimum FE: -5000
2025-06-24 16:38:55 INFO Unimodal AOCC mean: nan
2025-06-24 16:38:55 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 16:38:55 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 16:38:55 INFO AOCC mean: 0.0000
2025-06-24 16:38:55 INFO Weighed AOCC mean: nan
2025-06-24 16:38:55 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 16:42:04 INFO Run function 16 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:42:04 INFO FeHistory: [138735.7402159  111766.35320468 107431.92045086 ...  -4317.9
  -4317.9         -4317.9       ]
2025-06-24 16:42:04 INFO Expected Optimum FE: -5000
2025-06-24 16:42:04 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 16:45:16 INFO Run function 18 complete. FEHistory len: 150000, AOCC: 0.5631
2025-06-24 16:45:16 INFO FeHistory: [150487.13427385 136495.03751475 172164.69870158 ...  -5000.
  -5000.          -5000.        ]
2025-06-24 16:45:16 INFO Expected Optimum FE: -5000
2025-06-24 16:45:16 INFO Good algorithm:
Algorithm Name: EnhancedAdaptiveGaussianArchiveEA
import numpy as np

class EnhancedAdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 52.23394751192363,
                 archive_size: int = 161.961433094447,
                 initial_sigma_ratio: float = 0.16924288871488447,
                 sigma_decay: float = 0.9965270061216496,
                 tournament_size: int = 3.868535383310345,
                 recombination_sigma_factor: float = 0.8948602484977851,
                 archive_similarity_threshold: float = 8.330235927179514e-06):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.sigma = initial_sigma_ratio * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = float(sigma_decay)
        self.archive = []
        self.tournament_size = int(tournament_size)
        self.recombination_sigma_factor = float(recombination_sigma_factor)
        self.archive_similarity_threshold = float(archive_similarity_threshold)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:

        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-24 16:45:16 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 16:48:14 INFO Run function 19 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:48:14 INFO FeHistory: [200313.35167079 202559.43362774 134731.32995586 ...  -4317.9
  -4317.9         -4317.9       ]
2025-06-24 16:48:14 INFO Expected Optimum FE: -5000
2025-06-24 16:48:14 INFO Unimodal AOCC mean: nan
2025-06-24 16:48:14 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 16:48:14 INFO Multimodal (multiple components) AOCC mean: 0.1877
2025-06-24 16:48:14 INFO AOCC mean: 0.1877
2025-06-24 16:48:14 INFO Weighed AOCC mean: nan
2025-06-24 16:48:14 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 16:51:32 INFO Run function 16 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:51:32 INFO FeHistory: [146598.15459495 235355.90369078 288572.54734522 ...  -4317.9
  -4317.9         -4317.9       ]
2025-06-24 16:51:32 INFO Expected Optimum FE: -5000
2025-06-24 16:51:32 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 16:54:52 INFO Run function 18 complete. FEHistory len: 150000, AOCC: 0.7683
2025-06-24 16:54:52 INFO FeHistory: [158267.05463542 241323.79877885 305625.05516866 ...  -5000.
  -5000.          -5000.        ]
2025-06-24 16:54:52 INFO Expected Optimum FE: -5000
2025-06-24 16:54:52 INFO Good algorithm:
Algorithm Name: EnhancedAdaptiveGaussianArchiveEA
import numpy as np

class EnhancedAdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 129.9584869237829,
                 archive_size: int = 244.6710181717774,
                 initial_sigma_ratio: float = 0.5535641483221045,
                 sigma_decay: float = 0.9815373859711615,
                 tournament_size: int = 2.7062806164137596,
                 recombination_sigma_factor: float = 0.9543618283675844,
                 archive_similarity_threshold: float = 6.6715473879999525e-06):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.sigma = initial_sigma_ratio * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = float(sigma_decay)
        self.archive = []
        self.tournament_size = int(tournament_size)
        self.recombination_sigma_factor = float(recombination_sigma_factor)
        self.archive_similarity_threshold = float(archive_similarity_threshold)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:

        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-24 16:54:52 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 16:58:04 INFO Run function 19 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 16:58:04 INFO FeHistory: [176562.65149487 446422.45467169 354313.97640875 ...  -4317.9
  -4317.9         -4317.9       ]
2025-06-24 16:58:04 INFO Expected Optimum FE: -5000
2025-06-24 16:58:04 INFO Unimodal AOCC mean: nan
2025-06-24 16:58:04 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 16:58:04 INFO Multimodal (multiple components) AOCC mean: 0.2561
2025-06-24 16:58:04 INFO AOCC mean: 0.2561
2025-06-24 16:58:04 INFO Weighed AOCC mean: nan
2025-06-24 16:58:04 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:03:35 INFO Run function 16 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 17:03:35 INFO FeHistory: [168540.64015654 225855.06775532 218690.0908083  ...  -4317.89989608
  -4317.89994243  -4317.89996204]
2025-06-24 17:03:35 INFO Expected Optimum FE: -5000
2025-06-24 17:03:35 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:09:03 INFO Run function 18 complete. FEHistory len: 150000, AOCC: 0.2082
2025-06-24 17:09:03 INFO FeHistory: [177858.73197324 245103.30236788 268144.07207523 ...  -4999.99990705
  -4999.99995203  -4999.99988754]
2025-06-24 17:09:03 INFO Expected Optimum FE: -5000
2025-06-24 17:09:03 INFO Good algorithm:
Algorithm Name: EnhancedAdaptiveGaussianArchiveEA
import numpy as np

class EnhancedAdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 85.46621043674978,
                 archive_size: int = 244.6710181717774,
                 initial_sigma_ratio: float = 0.7693780804149236,
                 sigma_decay: float = 0.9966234482661166,
                 tournament_size: int = 3.6495664359704945,
                 recombination_sigma_factor: float = 0.8863505502129698,
                 archive_similarity_threshold: float = 5.425591755656379e-06):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.sigma = initial_sigma_ratio * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = float(sigma_decay)
        self.archive = []
        self.tournament_size = int(tournament_size)
        self.recombination_sigma_factor = float(recombination_sigma_factor)
        self.archive_similarity_threshold = float(archive_similarity_threshold)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:

        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-24 17:09:03 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:14:34 INFO Run function 19 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 17:14:34 INFO FeHistory: [106051.91345487 325305.32976322 174889.77609621 ...  -4419.89987263
  -4419.8998816   -4419.89988748]
2025-06-24 17:14:34 INFO Expected Optimum FE: -5000
2025-06-24 17:14:34 INFO Unimodal AOCC mean: nan
2025-06-24 17:14:34 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:14:34 INFO Multimodal (multiple components) AOCC mean: 0.0694
2025-06-24 17:14:34 INFO AOCC mean: 0.0694
2025-06-24 17:14:34 INFO Weighed AOCC mean: nan
2025-06-24 17:14:34 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:15:52 INFO Run function 16 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 17:15:52 INFO FeHistory: [138040.40613452 167926.26759848 149003.08722366 ...  -4317.9
  -4317.9         -4317.9       ]
2025-06-24 17:15:52 INFO Expected Optimum FE: -5000
2025-06-24 17:15:52 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:17:08 INFO Run function 18 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 17:17:08 INFO FeHistory: [107263.6133468  156812.05949716 158895.94545782 ...  -4470.31098464
  -4470.31098464  -4470.31098464]
2025-06-24 17:17:08 INFO Expected Optimum FE: -5000
2025-06-24 17:17:08 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:18:24 INFO Run function 19 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 17:18:24 INFO FeHistory: [226937.23885821 126057.05806673 194489.90657555 ...  -3912.39251885
  -3912.39251885  -3912.39251885]
2025-06-24 17:18:24 INFO Expected Optimum FE: -5000
2025-06-24 17:18:24 INFO Unimodal AOCC mean: nan
2025-06-24 17:18:24 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:18:24 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:18:24 INFO AOCC mean: 0.0000
2025-06-24 17:18:24 INFO Weighed AOCC mean: nan
2025-06-24 17:18:24 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:23:29 INFO Run function 16 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 17:23:29 INFO FeHistory: [114621.64218984 136717.31261469  94985.99785281 ...  -4317.9
  -4317.9         -4317.9       ]
2025-06-24 17:23:29 INFO Expected Optimum FE: -5000
2025-06-24 17:23:29 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:28:30 INFO Run function 18 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 17:28:30 INFO FeHistory: [224162.74814424 173836.56383513 249617.44281724 ...  -4317.9
  -4317.9         -4317.9       ]
2025-06-24 17:28:30 INFO Expected Optimum FE: -5000
2025-06-24 17:28:30 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:33:36 INFO Run function 19 complete. FEHistory len: 150000, AOCC: 0.0000
2025-06-24 17:33:36 INFO FeHistory: [134987.67536453 189850.39660628 185648.27401895 ...  -4317.9
  -4317.9         -4317.9       ]
2025-06-24 17:33:36 INFO Expected Optimum FE: -5000
2025-06-24 17:33:36 INFO Unimodal AOCC mean: nan
2025-06-24 17:33:36 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:33:36 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:33:36 INFO AOCC mean: 0.0000
2025-06-24 17:33:36 INFO Weighed AOCC mean: nan
2025-06-24 17:33:36 INFO Generation 1, best so far: 0.2796149162371689
2025-06-24 17:33:36 INFO Population length is: 6
2025-06-24 17:33:36 INFO --- Performing Long-Term Reflection at Generation 2 ---
2025-06-24 17:33:36 INFO Reflection Prompt: ### Problem Description
**(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.

### List heuristics
Below is a list of design heuristics ranked from best to worst based on their AOCC score on group function 3 of GNBG benchmark, where higher is better. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score on Multimodal instances with multiple components: 2.5609e-01# Name: EnhancedAdaptiveGaussianArchiveEA
# Description: 
# Code:
```python
import numpy as np

class EnhancedAdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 129.9584869237829,
                 archive_size: int = 244.6710181717774,
                 initial_sigma_ratio: float = 0.5535641483221045,
                 sigma_decay: float = 0.9815373859711615,
                 tournament_size: int = 2.7062806164137596,
                 recombination_sigma_factor: float = 0.9543618283675844,
                 archive_similarity_threshold: float = 6.6715473879999525e-06):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.sigma = initial_sigma_ratio * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = float(sigma_decay)
        self.archive = []
        self.tournament_size = int(tournament_size)
        self.recombination_sigma_factor = float(recombination_sigma_factor)
        self.archive_similarity_threshold = float(archive_similarity_threshold)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:

        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 2 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveArchipelagoDE
# Description: Combines a multi-island DE with an adaptive archive, dynamically focusing search based on island performance and archive guidance.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveArchipelagoDE
# Description: Combines a multi-island DE with an adaptive archive, dynamically focusing search based on island performance and archive guidance.

class AdaptiveArchipelagoDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 pop_size: int = 50, num_islands: int = 5, migration_interval: int = 500, migration_size: int = 5,
                 archive_size: int = 100, F: float = 0.7, Cr: float = 0.9, archive_probability: float = 0.1):
        """
        Initializes the Adaptive Archipelago Differential Evolution algorithm.

        Args:
            budget: Max function evaluations.
            dim: Problem dimensionality.
            lower_bounds: List of lower bounds for each dimension.
            upper_bounds: List of upper bounds for each dimension.
            pop_size: Population size for each island.
            num_islands: Number of islands in the archipelago.
            migration_interval: Number of evaluations between migrations.
            migration_size: Number of individuals to migrate.
            archive_size: Size of the archive to store promising solutions.
            F: Differential evolution scaling factor.
            Cr: Differential evolution crossover rate.
            archive_probability: Probability of using the archive to guide mutation.
        """
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.pop_size = pop_size
        self.num_islands = num_islands
        self.migration_interval = migration_interval
        self.migration_size = migration_size
        self.archive_size = archive_size
        self.F = F
        self.Cr = Cr
        self.archive_probability = archive_probability

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.archive = [] # Initialize archive here
        self.island_age = np.zeros(self.num_islands)

        # Initialize populations for each island
        self.populations = []
        self.fitnesses = []
        self.best_solutions = []
        self.best_fitnesses = []
        for _ in range(self.num_islands):
            population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.pop_size, self.dim))
            self.populations.append(population)
            self.fitnesses.append(np.full(self.pop_size, float('inf')))  # Initialize with large values
            self.best_solutions.append(None)
            self.best_fitnesses.append(float('inf'))

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """
        Optimizes the given objective function using Adaptive Archipelago Differential Evolution.

        Args:
            objective_function: A callable that accepts a 2D NumPy array (N, dim) and returns a 1D NumPy array of N fitness values.
            acceptance_threshold: Threshold for early stopping (not actively used in this version).

        Returns:
            A tuple: (best_solution_1D_numpy_array, best_fitness_scalar, optimization_info_dict).
        """
        self.eval_count = 0  # Reset for this run
        self.archive = [] # Clear old archive before each op.

        # Main optimization loop
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                self.evolve_island(i, objective_function)

            if self.eval_count % self.migration_interval == 0:
                self.migrate()

            #Update overall best
            for i in range(self.num_islands):
                 if self.best_fitnesses[i] < self.best_fitness_overall:
                      self.best_fitness_overall = self.best_fitnesses[i]
                      self.best_solution_overall = self.best_solutions[i].copy()
                      self.island_age[:] = 0

            # Stagnation Restart and Reset - adaptively restarts stalled island(s)
            self.island_age += 1
            stagnant_islands = np.where(self.island_age >= (self.budget / (self.num_islands * 500)))[0] # restart if 'old'
            for island_index in stagnant_islands:
                 self.restart_island(island_index)
                 self.island_age[island_index] = 0 # Reset stagnation time

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def evolve_island(self, island_index: int, objective_function: callable):
        """
        Evolves a single island for one generation using Differential Evolution.

        Args:
            island_index: The index of the island to evolve.
            objective_function: The objective function to optimize.
        """
        population = self.populations[island_index]
        fitnesses = self.fitnesses[island_index]

        # Evaluate initial population
        unevaluated_indices = np.where(fitnesses == float('inf'))[0]
        if len(unevaluated_indices) > 0:
            fitnesses[unevaluated_indices] = objective_function(population[unevaluated_indices])
            self.eval_count += len(unevaluated_indices)
            self.update_archive(population[unevaluated_indices], fitnesses[unevaluated_indices], objective_function)

        for j in range(self.pop_size):
            # Mutation
            idxs = [idx for idx in range(self.pop_size) if idx != j]
            a, b, c = random.sample(idxs, 3)
            mutant = population[a] + self.F * (population[b] - population[c])
            
            # Archive Guidance
            if random.random() < self.archive_probability and len(self.archive) > 0:
                archive_idx = random.randint(0, len(self.archive) -1)
                archived_solution = self.archive[archive_idx][:-1] #Exclude fitness
                mutant = 0.5 * mutant + 0.5 * archived_solution # Hybridized sampling. Can fine tune this coefficient.

            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Crossover
            trial = population[j].copy()
            for k in range(self.dim):
                if random.random() < self.Cr:
                    trial[k] = mutant[k]

            # Selection
            trial = np.clip(trial, self.lower_bounds, self.upper_bounds)
            trial_fitness = objective_function(np.array([trial]))[0]  # Evaluate single solution
            self.eval_count += 1

            if trial_fitness < fitnesses[j]:
                population[j] = trial
                fitnesses[j] = trial_fitness

                # Update best solution on the island
                if trial_fitness < self.best_fitnesses[island_index]:
                    self.best_fitnesses[island_index] = trial_fitness
                    self.best_solutions[island_index] = trial.copy()
                
            self.update_archive(np.array([trial]), np.array([trial_fitness]), objective_function) #Add Trial Solution

        self.populations[island_index] = population
        self.fitnesses[island_index] = fitnesses

    def migrate(self):
        """
        Migrates individuals between islands.
        """
        for _ in range(self.migration_size):
            # Select a random island to send migrants from
            sender_island = random.randint(0, self.num_islands - 1)
            
            # Select a random island to receive migrants, ensuring it's different from the sender
            receiver_island = sender_island
            while receiver_island == sender_island:
                receiver_island = random.randint(0, self.num_islands - 1)
            
            # Select a random individual to migrate from the sender island
            migrant_index = random.randint(0, self.pop_size - 1)
            migrant = self.populations[sender_island][migrant_index].copy()  # Copy to avoid modifying the original

            # Select a random individual to replace on the receiver island
            replacement_index = random.randint(0, self.pop_size - 1)

            # Perform the migration
            self.populations[receiver_island][replacement_index] = migrant
            
            #Evaluate the new migrant
            trial_fitness = objective_function(np.array([migrant]))[0]  # Evaluate single solution
            self.eval_count += 1
            self.fitnesses[receiver_island][replacement_index] = trial_fitness

            if trial_fitness < self.best_fitnesses[receiver_island]:
                self.best_fitnesses[receiver_island] = trial_fitness
                self.best_solutions[receiver_island] = migrant.copy()

    def restart_island(self, island_index: int):
         """Restarts a single island with new random solutions."""
         self.populations[island_index] = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.pop_size, self.dim))
         self.fitnesses[island_index] = np.full(self.pop_size, float('inf'))
         self.best_solutions[island_index] = None
         self.best_fitnesses[island_index] = float('inf')
    
    def update_archive(self, population, fitness_values, objective_function):
        """Updates the archive with solutions from new population."""
        combined = np.column_stack((population, fitness_values))
        
        #Filter out solutions and fitness
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                self.archive.append(sol)
        
        # Keep only best archive size
        self.archive.sort(key=lambda x: x[-1])
        self.archive = self.archive[:self.archive_size]
```

### Rank 3 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: ArchipelagoAdaptiveGaussianArchive
# Description: Employs an island model with adaptive Gaussian mutation guided by an archive and inter-island migration.
# Code:
```python
import numpy as np
import random

# Name: ArchipelagoAdaptiveGaussianArchive
# Description: Employs an island model with adaptive Gaussian mutation guided by an archive and inter-island migration.

class ArchipelagoAdaptiveGaussianArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = 5
        self.population_size = 50
        self.archive_size = 100
        self.migration_interval = 500
        self.migration_amount = 5

        self.islands = []
        for _ in range(self.num_islands):
            self.islands.append({
                'population': None,
                'fitness': None,
                'archive': [],
                'sigma': 0.5 * (self.upper_bounds - self.lower_bounds),
                'sigma_decay': 0.98
            })

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initialize islands
        for island in self.islands:
            island['population'] = self._initialize_population(island['sigma'])
            island['fitness'] = objective_function(island['population'])
            self.eval_count += self.population_size
            island['archive'] = self._update_archive(island['population'], island['fitness'], island['archive'])

        generation = 0
        while self.eval_count < self.budget:
            generation += 1
            for island in self.islands:
                parents = self._tournament_selection(island['population'], island['fitness'])
                offspring = self._gaussian_recombination(parents, island['sigma'])
                offspring = self._adaptive_mutation(offspring, island['sigma'])
                offspring_fitness = objective_function(offspring)
                self.eval_count += len(offspring)

                island['population'], island['fitness'] = self._select_next_generation(
                    island['population'], island['fitness'], offspring, offspring_fitness
                )
                island['archive'] = self._update_archive(
                    np.vstack((island['population'], offspring)),
                    np.concatenate((island['fitness'], offspring_fitness)),
                    island['archive']
                )
                self._update_best(offspring, offspring_fitness)
                island['sigma'] *= island['sigma_decay']
            
            if generation % (self.migration_interval // self.num_islands) == 0: #Stagger Migration
                self._perform_migration()

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self, sigma):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents, sigma):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring, sigma):
        mutated = offspring + np.random.normal(0, sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values, archive):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return list(np.array(new_archive[:self.archive_size]))

    def _perform_migration(self):
        # Select migrants from each island's population
        migrants = []
        for island in self.islands:
            sorted_indices = np.argsort(island['fitness'])
            migrants.append(island['population'][sorted_indices[:self.migration_amount]])

        # Migrate individuals to other islands
        for i in range(self.num_islands):
            target_island_index = (i + 1) % self.num_islands
            self.islands[target_island_index]['population'][-self.migration_amount:] = migrants[i]
            #Re-evaluate the fitness
            self.islands[target_island_index]['fitness'] = objective_function(self.islands[target_island_index]['population'])
            self.eval_count += self.migration_amount


```

### Rank 4 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveArchipelagoDE
# Description: Combines archipelago-based exploration with an adaptive archive to aggressively seek diverse optima.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveArchipelagoDE
# Description: Combines archipelago-based exploration with an adaptive archive to aggressively seek diverse optima.
# Code:
class AdaptiveArchipelagoDE:
    """
    Combines archipelago-based exploration with an adaptive archive to aggressively seek diverse optima,
    dynamically adjusting island populations and archive management based on performance.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, population_size_factor: float = 8.82865217019506, archive_size_factor: float = 30,
                 migration_interval: int = 5000, migration_rate: float = 0.1, initial_F_scale: float = 0.7,
                 archive_update_threshold: float = 0.8):
        """
        Initializes the AdaptiveArchipelagoDE algorithm.

        Args:
            budget: Max function evaluations.
            dim: Problem dimensionality.
            lower_bounds: List of lower bounds for each dimension.
            upper_bounds: List of upper bounds for each dimension.
            num_islands: Number of isolated subpopulations (islands).
            population_size_factor: Factor to determine initial population size per island.
            archive_size_factor: Archive size.
            migration_interval: Number of evaluations between migrations.
            migration_rate: Fraction of population to migrate between islands.
            initial_F_scale: Initial scaling factor for differential evolution.
            archive_update_threshold: Threshold for updating archive entries.
        """
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = num_islands
        self.population_size = int(population_size_factor * self.dim)
        self.archive_size = int(archive_size_factor * self.dim)
        self.migration_interval = migration_interval
        self.migration_rate = migration_rate
        self.initial_F_scale = initial_F_scale  # initial scaling factor
        self.archive_update_threshold = archive_update_threshold
        self.islands = []
        self.archives = []  # Separate archive for each island

        for _ in range(self.num_islands):
            self.islands.append(np.random.uniform(self.lower_bounds, self.upper_bounds,
                                                  size=(self.population_size, self.dim)))
            self.archives.append([])  # Initialize empty archive for each island

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, F_scale_variation: float = 0.3) -> tuple:
        """
        Optimizes the given objective function using the AdaptiveArchipelagoDE algorithm.

        Args:
            objective_function: The function to minimize. Accepts a 2D NumPy array and returns a 1D NumPy array of fitness values.
            acceptance_threshold: The threshold for early stopping.

        Returns:
            A tuple containing the best solution, its fitness, and optimization information.
        """
        self.eval_count = 0
        fitness_values = []

        for island in self.islands:
            fitness = objective_function(island)
            self.eval_count += self.population_size
            fitness_values.append(fitness)

        # Initialize overall best
        for i in range(self.num_islands):
            island_best_index = np.argmin(fitness_values[i])
            island_best_fitness = fitness_values[i][island_best_index]

            if island_best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = island_best_fitness
                self.best_solution_overall = self.islands[i][island_best_index]
                best_island_index = i

        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                # Generate offspring for each island
                offspring = self.generate_offspring(self.islands[i], fitness_values[i], F_scale_variation, self.archives[i])
                offspring_fitness = objective_function(offspring)
                self.eval_count += len(offspring)

                # Update archive for each island
                self.update_archive(self.archives[i], offspring, offspring_fitness, self.archive_update_threshold)

                # Select best solutions for next generation for each island
                combined_population = np.concatenate((self.islands[i], offspring))
                combined_fitness = np.concatenate((fitness_values[i], offspring_fitness))
                indices = np.argsort(combined_fitness)
                self.islands[i] = combined_population[indices[:self.population_size]]
                fitness_values[i] = combined_fitness[indices[:self.population_size]]

                # Update best solution for current island
                island_best_index = np.argmin(fitness_values[i])
                island_best_fitness = fitness_values[i][island_best_index]

                if island_best_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = island_best_fitness
                    self.best_solution_overall = self.islands[i][island_best_index]
                    best_island_index = i
            # Perform migration
            if self.eval_count % self.migration_interval == 0:
                self.migrate_individuals()

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness, F_scale_variation, archive):
        """
        Generates offspring using differential evolution with archive guidance.

        Args:
            population: The current population.
            fitness: The fitness values of the current population.
            F_scale_variation: Variation for the scaling factor.
            archive: The archive of promising solutions.

        Returns:
            The generated offspring.
        """
        offspring = np.zeros((self.population_size, self.dim))
        F_scale = self.initial_F_scale + F_scale_variation * np.random.rand()  # scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available) or from current best

            if archive:
                pbest_index = np.random.choice(len(archive))
                pbest = archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]  #fallback to population

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)  # Boundary handling

        return offspring

    def update_archive(self, archive, offspring, offspring_fitness, archive_update_threshold):
         """
         Update the archive with new offspring solutions, keeping it limited to archive_size, while prioritizing diversity
         """
         for i in range(len(offspring)):
             if len(archive) < self.archive_size:
                 archive.append((offspring[i], offspring_fitness[i]))
             else:
                 # Prioritize diversity in archive - consider replacing worst
                 worst_index = np.argmax([f for _, f in archive])
                 if offspring_fitness[i] < archive[worst_index][1] or len(archive) < self.archive_size * archive_update_threshold:
                     archive[worst_index] = (offspring[i], offspring_fitness[i])
    def migrate_individuals(self):
      """Migrates individuals between islands to increase diversity."""
      for i in range(self.num_islands):
          # Select individuals to migrate from island i
          num_migrants = int(self.migration_rate * self.population_size)
          migrant_indices = np.random.choice(self.population_size, num_migrants, replace=False)
          migrants = self.islands[i][migrant_indices]

          # Choose a destination island (different from the origin)
          destination_island = (i + 1) % self.num_islands

          # Replace some individuals on the destination island with the migrants
          replace_indices = np.random.choice(self.population_size, num_migrants, replace=False)
          self.islands[destination_island][replace_indices] = migrants
```

### Rank 5 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveArchipelagoGA
# Description: Combines adaptive Gaussian mutation with a dynamically managed archive spread across virtual islands.
# Code:
```python
import numpy as np

# Name: AdaptiveArchipelagoGA
# Description: Combines adaptive Gaussian mutation with a dynamically managed archive spread across virtual islands.
# Code:
class AdaptiveArchipelagoGA:
    """
    Combines adaptive Gaussian mutation with a dynamically managed archive spread across virtual islands.
    Virtual islands exchange information based on performance, and mutation adapts based on region characteristics.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.num_islands = 5  # Number of virtual islands
        self.population_size = 40  # Population size per island
        self.archive_size = 50 # Archive size per island. Total memory = num_islands * archive_size
        self.migration_interval = 50  # Evaluation interval for migration
        self.migration_rate = 0.1 #Fraction of the worst solutions to replace in each island
        
        self.islands = []
        self.archives = []
        self.sigmas = []  #Per-island sigma
        self.sigma_decay = 0.98
        

        for _ in range(self.num_islands):
            self.islands.append(np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim)))
            self.archives.append([])
            self.sigmas.append(0.5 * (self.upper_bounds - self.lower_bounds)) #Initial sigma, same scale as bounds.

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        fitness_values = []
        for i in range(self.num_islands):
            fitness = objective_function(self.islands[i])
            fitness_values.append(fitness)
            self.eval_count += self.population_size
            self.archives[i] = self._update_archive(self.islands[i], fitness, self.archives[i])

        generation = 0
        while self.eval_count < self.budget:
            generation += 1
            for i in range(self.num_islands):
                parents = self._tournament_selection(self.islands[i], fitness_values[i])
                offspring = self._gaussian_recombination(parents, self.sigmas[i])
                offspring = self._adaptive_mutation(offspring, self.sigmas[i])
                offspring_fitness = objective_function(offspring)
                self.eval_count += len(offspring)
                
                self.islands[i], fitness_values[i] = self._select_next_generation(
                    self.islands[i], fitness_values[i], offspring, offspring_fitness
                )
                self.archives[i] = self._update_archive(
                    np.vstack((self.islands[i], offspring)),
                    np.concatenate((fitness_values[i], offspring_fitness)),
                    self.archives[i]
                )

                self._update_best(offspring, offspring_fitness)
                self.sigmas[i] *= self.sigma_decay  # Adapt sigma based on generations

            if generation % self.migration_interval == 0:
                self._migrate()

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents, sigma):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring, sigma):
        mutated = offspring + np.random.normal(0, sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values, archive):
      combined = np.column_stack((population, fitness_values))
      new_archive = []
      for sol in combined:
          already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in archive)
          if not already_present:
              new_archive.append(sol)

      # Add the new solutions to the existing archive and sort by fitness.
      updated_archive = np.vstack((archive, new_archive)) if archive else np.array(new_archive)
      if updated_archive.size > 0: #Prevent crash if empty after combine operation
          updated_archive = updated_archive[np.argsort(updated_archive[:, -1])]
      else:
          return np.array([])

      return updated_archive[:self.archive_size]

    def _migrate(self):
        # Select individuals for migration (worst in each island)
        migrants = []
        for i in range(self.num_islands):
            sorted_indices = np.argsort(self.islands[i])
            num_migrants = int(self.migration_rate * self.population_size)
            migrants.append(self.islands[i][sorted_indices[-num_migrants:]]) #Worst solutions

        # Replace a fraction of the population on each island with migrants.
        for i in range(self.num_islands):
            target_island = (i + 1) % self.num_islands  # Send to the next island
            sorted_indices = np.argsort(self.islands[target_island])
            num_migrants = int(self.migration_rate * self.population_size)
            self.islands[target_island][sorted_indices[:num_migrants]] = migrants[i]
```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 17:33:43 INFO Full response text: **Analysis:**

Comparing (Rank 1) vs (Rank 5), we see that Rank 1 focuses on adaptive Gaussian mutation within a single population, employing an archive for enhanced exploration and a decaying sigma for exploitation. Conversely, Rank 5 divides the population into multiple virtual islands, each with its own population, archive, and adaptive mutation. It utilizes periodic migration between islands to promote diversity. Both algorithms use an archive and decaying sigma value. The recombination schemes differ considerably. Rank 1 uses "midpoint" recombination, while Rank 5 just passes the original parent as a template for new offspring and then uses Gaussian mutation on the offspring.

Comparing (Rank 2) vs (Rank 4), both use an island model with migration and an archive. Rank 2's island model evolves using DE with the possibility of incorporating solutions in the archive to bias mutation. Adaptive island restarts are also enabled in rank 2. Rank 4 evolves island using DE with archive guidance, adaptive migration of individuals. The recombination schemes differ considerably.

Comparing (Rank 1) vs (Rank 2), Rank 1 uses single population Adaptive Gaussian Mutation and Archiving, whereas Rank 2 implements an Adaptive Archipelago (Multi-Island) DE algorithm, coupled with an archive that it incorporates directly into its mutation strategy.

Comparing (Rank 3) vs (Rank 4), Rank 3 evolves islands through Adaptive Guassian Mutation and then migration. It looks more like a traditional GA style algorithm. Rank 4 has adaptive archiving in the islands, uses DE for evolution, and adapts its migration based on performance. Its mutation scheme adaptively selects solutions from either the islands or the archive for use in the mutation scheme.

Comparing (Rank 4) vs (Rank 5), Rank 4 uses DE (Differential Evolution) with adaptive archiving (single population) which evolves based on sampling from population and archive, coupled with adpative migration using DE between islands.
Rank 5 is Adaptive Archipelogo GA, employing (multiple population) gaussian mutation, updating the archive, then performing migration.

Overall: The better algorithms aggressively maintain both archives AND have ways of injecting solutions in the archive into their sampling schemes to bias/drive exploration based on prior good performing solutions. These same schemes employ an island model coupled with migration to also maintain diversity and prevent premature convergence, by aggressive exploring different attraction basins during the optimization process.

**Experience:**
Island models with migration are good for multi-component fitness landscapes. Adaptive Gaussian mutation and DE are beneficial. Also critical are methods to inject good solutions in a local archive into the core sampling mechanism of an individual population. Island models increase exploration and exploitation.

2025-06-24 17:33:46 INFO Full response text: Okay, I will help you design better heuristics by first refining the idea of "current self-reflection" while avoiding the pitfalls of "ineffective self-reflection". Here's a breakdown to guide your strategy:

*   **Keywords:** Island models, migration, adaptive mutation (Gaussian, DE), local archives, exploration, exploitation, multi-component fitness landscapes.

*   **Advice:** Focus on *balancing* exploration and exploitation within island models. Carefully design migration strategies that inject high-quality solutions from local archives into other islands without prematurely converging the entire population. Investigate how adaptive mutation and DE parameters respond to the *specific characteristics* of each island's landscape.

*   **Avoid:** General statements about "aggressive global exploration" or "deceptive landscapes" without providing concrete mechanisms for dealing with deception. Avoid vague assertions about sophistication.

*   **Explanation:** The goal is to create a *targeted* approach. Instead of just increasing diversity, we want to use islands and migration to intelligently explore and exploit different parts of the fitness landscape in a coordinated way. Adaptive mutation and archives should reinforce this intelligent exploration-exploitation balance, by using archives to store optimal solutions and inject them in other populations when they are struggling. This is particularly effective when the populations have very different properties.

Let's move on and design some more better heuristics!!!

2025-06-24 17:33:46 INFO Generating offspring via Crossover...
2025-06-24 17:36:59 INFO Crossover Prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


### Better code
EnhancedAdaptiveGaussianArchiveEA
import numpy as np

class EnhancedAdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 129.9584869237829,
                 archive_size: int = 244.6710181717774,
                 initial_sigma_ratio: float = 0.5535641483221045,
                 sigma_decay: float = 0.9815373859711615,
                 tournament_size: int = 2.7062806164137596,
                 recombination_sigma_factor: float = 0.9543618283675844,
                 archive_similarity_threshold: float = 6.6715473879999525e-06):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.sigma = initial_sigma_ratio * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = float(sigma_decay)
        self.archive = []
        self.tournament_size = int(tournament_size)
        self.recombination_sigma_factor = float(recombination_sigma_factor)
        self.archive_similarity_threshold = float(archive_similarity_threshold)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:

        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Worse code
EnhancedAdaptiveGaussianArchiveEA
import numpy as np

class EnhancedAdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 129.9584869237829,
                 archive_size: int = 244.6710181717774,
                 initial_sigma_ratio: float = 0.5535641483221045,
                 sigma_decay: float = 0.9815373859711615,
                 tournament_size: int = 2.7062806164137596,
                 recombination_sigma_factor: float = 0.9543618283675844,
                 archive_similarity_threshold: float = 6.6715473879999525e-06):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.sigma = initial_sigma_ratio * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = float(sigma_decay)
        self.archive = []
        self.tournament_size = int(tournament_size)
        self.recombination_sigma_factor = float(recombination_sigma_factor)
        self.archive_similarity_threshold = float(archive_similarity_threshold)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:

        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Analyze & experience
- Comparing (Rank 1) vs (Rank 5), we see that Rank 1 focuses on adaptive Gaussian mutation within a single population, employing an archive for enhanced exploration and a decaying sigma for exploitation. Conversely, Rank 5 divides the population into multiple virtual islands, each with its own population, archive, and adaptive mutation. It utilizes periodic migration between islands to promote diversity. Both algorithms use an archive and decaying sigma value. The recombination schemes differ considerably. Rank 1 uses "midpoint" recombination, while Rank 5 just passes the original parent as a template for new offspring and then uses Gaussian mutation on the offspring.

Comparing (Rank 2) vs (Rank 4), both use an island model with migration and an archive. Rank 2's island model evolves using DE with the possibility of incorporating solutions in the archive to bias mutation. Adaptive island restarts are also enabled in rank 2. Rank 4 evolves island using DE with archive guidance, adaptive migration of individuals. The recombination schemes differ considerably.

Comparing (Rank 1) vs (Rank 2), Rank 1 uses single population Adaptive Gaussian Mutation and Archiving, whereas Rank 2 implements an Adaptive Archipelago (Multi-Island) DE algorithm, coupled with an archive that it incorporates directly into its mutation strategy.

Comparing (Rank 3) vs (Rank 4), Rank 3 evolves islands through Adaptive Guassian Mutation and then migration. It looks more like a traditional GA style algorithm. Rank 4 has adaptive archiving in the islands, uses DE for evolution, and adapts its migration based on performance. Its mutation scheme adaptively selects solutions from either the islands or the archive for use in the mutation scheme.

Comparing (Rank 4) vs (Rank 5), Rank 4 uses DE (Differential Evolution) with adaptive archiving (single population) which evolves based on sampling from population and archive, coupled with adpative migration using DE between islands.
Rank 5 is Adaptive Archipelogo GA, employing (multiple population) gaussian mutation, updating the archive, then performing migration.

Overall: The better algorithms aggressively maintain both archives AND have ways of injecting solutions in the archive into their sampling schemes to bias/drive exploration based on prior good performing solutions. These same schemes employ an island model coupled with migration to also maintain diversity and prevent premature convergence, by aggressive exploring different attraction basins during the optimization process.
- Okay, I will help you design better heuristics by first refining the idea of "current self-reflection" while avoiding the pitfalls of "ineffective self-reflection". Here's a breakdown to guide your strategy:

*   **Keywords:** Island models, migration, adaptive mutation (Gaussian, DE), local archives, exploration, exploitation, multi-component fitness landscapes.

*   **Advice:** Focus on *balancing* exploration and exploitation within island models. Carefully design migration strategies that inject high-quality solutions from local archives into other islands without prematurely converging the entire population. Investigate how adaptive mutation and DE parameters respond to the *specific characteristics* of each island's landscape.

*   **Avoid:** General statements about "aggressive global exploration" or "deceptive landscapes" without providing concrete mechanisms for dealing with deception. Avoid vague assertions about sophistication.

*   **Explanation:** The goal is to create a *targeted* approach. Instead of just increasing diversity, we want to use islands and migration to intelligently explore and exploit different parts of the fitness landscape in a coordinated way. Adaptive mutation and archives should reinforce this intelligent exploration-exploitation balance, by using archives to store optimal solutions and inject them in other populations when they are struggling. This is particularly effective when the populations have very different properties.

Let's move on and design some more better heuristics!!!


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 17:36:59 INFO Mutation prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


Current heuristics:
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- Okay, I will help you design better heuristics by first refining the idea of "current self-reflection" while avoiding the pitfalls of "ineffective self-reflection". Here's a breakdown to guide your strategy:

*   **Keywords:** Island models, migration, adaptive mutation (Gaussian, DE), local archives, exploration, exploitation, multi-component fitness landscapes.

*   **Advice:** Focus on *balancing* exploration and exploitation within island models. Carefully design migration strategies that inject high-quality solutions from local archives into other islands without prematurely converging the entire population. Investigate how adaptive mutation and DE parameters respond to the *specific characteristics* of each island's landscape.

*   **Avoid:** General statements about "aggressive global exploration" or "deceptive landscapes" without providing concrete mechanisms for dealing with deception. Avoid vague assertions about sophistication.

*   **Explanation:** The goal is to create a *targeted* approach. Instead of just increasing diversity, we want to use islands and migration to intelligently explore and exploit different parts of the fitness landscape in a coordinated way. Adaptive mutation and archives should reinforce this intelligent exploration-exploitation balance, by using archives to store optimal solutions and inject them in other populations when they are struggling. This is particularly effective when the populations have very different properties.

Let's move on and design some more better heuristics!!!


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-24 17:48:44 INFO Perform Harmony Search...
2025-06-24 17:48:55 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:48:55 ERROR Can not run the algorithm
2025-06-24 17:48:56 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:56 INFO FeHistory: [164111.63446614]
2025-06-24 17:48:56 INFO Expected Optimum FE: -5000
2025-06-24 17:48:56 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:48:56 ERROR Can not run the algorithm
2025-06-24 17:48:56 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:56 INFO FeHistory: [194148.25752763]
2025-06-24 17:48:56 INFO Expected Optimum FE: -5000
2025-06-24 17:48:56 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:48:56 ERROR Can not run the algorithm
2025-06-24 17:48:56 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:56 INFO FeHistory: [102801.24251509]
2025-06-24 17:48:56 INFO Expected Optimum FE: -5000
2025-06-24 17:48:56 INFO Unimodal AOCC mean: nan
2025-06-24 17:48:56 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:48:56 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:48:56 INFO AOCC mean: 0.0000
2025-06-24 17:48:56 INFO Weighed AOCC mean: nan
2025-06-24 17:48:56 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:48:56 ERROR Can not run the algorithm
2025-06-24 17:48:57 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:57 INFO FeHistory: [158445.94170665]
2025-06-24 17:48:57 INFO Expected Optimum FE: -5000
2025-06-24 17:48:57 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:48:57 ERROR Can not run the algorithm
2025-06-24 17:48:57 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:57 INFO FeHistory: [208842.37702845]
2025-06-24 17:48:57 INFO Expected Optimum FE: -5000
2025-06-24 17:48:57 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:48:57 ERROR Can not run the algorithm
2025-06-24 17:48:57 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:57 INFO FeHistory: [169484.41102777]
2025-06-24 17:48:57 INFO Expected Optimum FE: -5000
2025-06-24 17:48:57 INFO Unimodal AOCC mean: nan
2025-06-24 17:48:57 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:48:57 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:48:57 INFO AOCC mean: 0.0000
2025-06-24 17:48:57 INFO Weighed AOCC mean: nan
2025-06-24 17:48:57 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:48:57 ERROR Can not run the algorithm
2025-06-24 17:48:58 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:58 INFO FeHistory: [146513.65009673]
2025-06-24 17:48:58 INFO Expected Optimum FE: -5000
2025-06-24 17:48:58 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:48:58 ERROR Can not run the algorithm
2025-06-24 17:48:58 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:58 INFO FeHistory: [170179.24288364]
2025-06-24 17:48:58 INFO Expected Optimum FE: -5000
2025-06-24 17:48:58 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:48:58 ERROR Can not run the algorithm
2025-06-24 17:48:59 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:59 INFO FeHistory: [181625.03287038]
2025-06-24 17:48:59 INFO Expected Optimum FE: -5000
2025-06-24 17:48:59 INFO Unimodal AOCC mean: nan
2025-06-24 17:48:59 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:48:59 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:48:59 INFO AOCC mean: 0.0000
2025-06-24 17:48:59 INFO Weighed AOCC mean: nan
2025-06-24 17:48:59 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:48:59 ERROR Can not run the algorithm
2025-06-24 17:48:59 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:59 INFO FeHistory: [87676.42810403]
2025-06-24 17:48:59 INFO Expected Optimum FE: -5000
2025-06-24 17:48:59 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:48:59 ERROR Can not run the algorithm
2025-06-24 17:48:59 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:48:59 INFO FeHistory: [118328.59094866]
2025-06-24 17:48:59 INFO Expected Optimum FE: -5000
2025-06-24 17:48:59 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:48:59 ERROR Can not run the algorithm
2025-06-24 17:49:00 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:00 INFO FeHistory: [195622.35705395]
2025-06-24 17:49:00 INFO Expected Optimum FE: -5000
2025-06-24 17:49:00 INFO Unimodal AOCC mean: nan
2025-06-24 17:49:00 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:49:00 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:49:00 INFO AOCC mean: 0.0000
2025-06-24 17:49:00 INFO Weighed AOCC mean: nan
2025-06-24 17:49:00 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:49:00 ERROR Can not run the algorithm
2025-06-24 17:49:00 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:00 INFO FeHistory: [147349.65848901]
2025-06-24 17:49:00 INFO Expected Optimum FE: -5000
2025-06-24 17:49:00 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:49:00 ERROR Can not run the algorithm
2025-06-24 17:49:00 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:00 INFO FeHistory: [187416.88938207]
2025-06-24 17:49:00 INFO Expected Optimum FE: -5000
2025-06-24 17:49:00 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:49:00 ERROR Can not run the algorithm
2025-06-24 17:49:01 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:01 INFO FeHistory: [136729.51124706]
2025-06-24 17:49:01 INFO Expected Optimum FE: -5000
2025-06-24 17:49:01 INFO Unimodal AOCC mean: nan
2025-06-24 17:49:01 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:49:01 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:49:01 INFO AOCC mean: 0.0000
2025-06-24 17:49:01 INFO Weighed AOCC mean: nan
2025-06-24 17:49:01 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:49:01 ERROR Can not run the algorithm
2025-06-24 17:49:01 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:01 INFO FeHistory: [161165.12498447]
2025-06-24 17:49:01 INFO Expected Optimum FE: -5000
2025-06-24 17:49:01 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:49:01 ERROR Can not run the algorithm
2025-06-24 17:49:01 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:01 INFO FeHistory: [158471.94039049]
2025-06-24 17:49:01 INFO Expected Optimum FE: -5000
2025-06-24 17:49:01 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:49:01 ERROR Can not run the algorithm
2025-06-24 17:49:02 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:02 INFO FeHistory: [74725.03234569]
2025-06-24 17:49:02 INFO Expected Optimum FE: -5000
2025-06-24 17:49:02 INFO Unimodal AOCC mean: nan
2025-06-24 17:49:02 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:49:02 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:49:02 INFO AOCC mean: 0.0000
2025-06-24 17:49:02 INFO Weighed AOCC mean: nan
2025-06-24 17:49:02 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:49:02 ERROR Can not run the algorithm
2025-06-24 17:49:02 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:02 INFO FeHistory: [116342.71834326]
2025-06-24 17:49:02 INFO Expected Optimum FE: -5000
2025-06-24 17:49:02 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:49:02 ERROR Can not run the algorithm
2025-06-24 17:49:03 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:03 INFO FeHistory: [71155.05885612]
2025-06-24 17:49:03 INFO Expected Optimum FE: -5000
2025-06-24 17:49:03 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:49:03 ERROR Can not run the algorithm
2025-06-24 17:49:03 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:03 INFO FeHistory: [157645.80302152]
2025-06-24 17:49:03 INFO Expected Optimum FE: -5000
2025-06-24 17:49:03 INFO Unimodal AOCC mean: nan
2025-06-24 17:49:03 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:49:03 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:49:03 INFO AOCC mean: 0.0000
2025-06-24 17:49:03 INFO Weighed AOCC mean: nan
2025-06-24 17:49:03 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:49:03 ERROR Can not run the algorithm
2025-06-24 17:49:03 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:03 INFO FeHistory: [124722.76653122]
2025-06-24 17:49:03 INFO Expected Optimum FE: -5000
2025-06-24 17:49:03 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:49:03 ERROR Can not run the algorithm
2025-06-24 17:49:04 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:04 INFO FeHistory: [169993.0738539]
2025-06-24 17:49:04 INFO Expected Optimum FE: -5000
2025-06-24 17:49:04 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:49:04 ERROR Can not run the algorithm
2025-06-24 17:49:04 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:04 INFO FeHistory: [218803.04868993]
2025-06-24 17:49:04 INFO Expected Optimum FE: -5000
2025-06-24 17:49:04 INFO Unimodal AOCC mean: nan
2025-06-24 17:49:04 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:49:04 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:49:04 INFO AOCC mean: 0.0000
2025-06-24 17:49:04 INFO Weighed AOCC mean: nan
2025-06-24 17:49:04 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:49:04 ERROR Can not run the algorithm
2025-06-24 17:49:04 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:04 INFO FeHistory: [153584.78419683]
2025-06-24 17:49:04 INFO Expected Optimum FE: -5000
2025-06-24 17:49:04 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:49:04 ERROR Can not run the algorithm
2025-06-24 17:49:05 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:05 INFO FeHistory: [231441.81648796]
2025-06-24 17:49:05 INFO Expected Optimum FE: -5000
2025-06-24 17:49:05 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:49:05 ERROR Can not run the algorithm
2025-06-24 17:49:05 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:05 INFO FeHistory: [165913.4174451]
2025-06-24 17:49:05 INFO Expected Optimum FE: -5000
2025-06-24 17:49:05 INFO Unimodal AOCC mean: nan
2025-06-24 17:49:05 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:49:05 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:49:05 INFO AOCC mean: 0.0000
2025-06-24 17:49:05 INFO Weighed AOCC mean: nan
2025-06-24 17:49:05 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:49:05 ERROR Can not run the algorithm
2025-06-24 17:49:05 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:05 INFO FeHistory: [152238.01040806]
2025-06-24 17:49:05 INFO Expected Optimum FE: -5000
2025-06-24 17:49:05 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:49:05 ERROR Can not run the algorithm
2025-06-24 17:49:06 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:06 INFO FeHistory: [131468.30451299]
2025-06-24 17:49:06 INFO Expected Optimum FE: -5000
2025-06-24 17:49:06 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:49:06 ERROR Can not run the algorithm
2025-06-24 17:49:06 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:49:06 INFO FeHistory: [200031.35905859]
2025-06-24 17:49:06 INFO Expected Optimum FE: -5000
2025-06-24 17:49:06 INFO Unimodal AOCC mean: nan
2025-06-24 17:49:06 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:49:06 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:49:06 INFO AOCC mean: 0.0000
2025-06-24 17:49:06 INFO Weighed AOCC mean: nan
2025-06-24 17:49:06 INFO Generation 2, best so far: 0.2796149162371689
2025-06-24 17:49:06 INFO Population length is: 6
2025-06-24 17:49:06 INFO --- Performing Long-Term Reflection at Generation 3 ---
2025-06-24 17:49:06 INFO Reflection Prompt: ### Problem Description
**(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.

### List heuristics
Below is a list of design heuristics ranked from best to worst based on their AOCC score on group function 3 of GNBG benchmark, where higher is better. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandDifferentialEvolutionArchiveMigration
# Description: Island model with DE, archive-guided mutation, and adaptive migration.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandDifferentialEvolutionArchiveMigration
# Description: Island model with DE, archive-guided mutation, and adaptive migration.
# Code:
class AdaptiveIslandDifferentialEvolutionArchiveMigration:
    """
    Island model with DE, archive-guided mutation, and adaptive migration rates.
    """

    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = 5
        self.population_size = 50
        self.archive_size = 100
        self.migration_interval = 500
        self.migration_rate = 0.1  # Initial migration rate
        self.migration_decay = 0.995 #decay migration rate.
        self.islands = []
        self.archives = []

        for _ in range(self.num_islands):
            population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
            self.islands.append(population)
            self.archives.append([])  # Each island has its archive

        self.F = 0.7  # DE parameter
        self.CR = 0.9  # DE parameter

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        fitness_values = []
        for i in range(self.num_islands):
            fitness = objective_function(self.islands[i])
            self.eval_count += self.population_size
            fitness_values.append(fitness)


        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                self.islands[i], fitness_values[i] = self._evolve_island(
                    self.islands[i], fitness_values[i], objective_function, i
                )
                self.archives[i] = self._update_archive(
                    self.islands[i], fitness_values[i], self.archives[i]
                )
                self._update_best(self.islands[i], fitness_values[i])

            if self.eval_count % self.migration_interval == 0:
                self._migrate_individuals(fitness_values)
                self.migration_rate *= self.migration_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _evolve_island(self, population, fitness_values, objective_function, island_index):
        new_population = np.copy(population)

        for i in range(self.population_size):
            # DE mutation with archive guidance
            donor_vector = self._de_mutation(population, fitness_values, island_index, i)
            
            # Crossover
            trial_vector = self._crossover(population[i], donor_vector)

            # Clip to bounds
            trial_vector = np.clip(trial_vector, self.lower_bounds, self.upper_bounds)

            # Evaluate trial vector
            trial_fitness = objective_function(trial_vector.reshape(1, -1))[0]
            self.eval_count += 1

            # Selection
            if trial_fitness < fitness_values[i]:
                new_population[i] = trial_vector
                fitness_values[i] = trial_fitness

        return new_population, fitness_values

    def _de_mutation(self, population, fitness_values, island_index, target_index):
        archive = self.archives[island_index]
        
        if len(archive) > 2 and np.random.rand() < 0.5:
            # Sample from archive with 50% probability
            indices = np.random.choice(len(archive), 2, replace=False)
            x_r1 = archive[indices[0]][:-1] # Remove the fitness score from archive member
            x_r2 = archive[indices[1]][:-1]

            # DE mutation from archive
            donor_vector = population[target_index] + self.F * (x_r1 - x_r2)

        else:
            # Classic DE mutation
            indices = np.random.choice(self.population_size, 3, replace=False)
            while target_index in indices: # Ensure target index is not in random indices
                 indices = np.random.choice(self.population_size, 3, replace=False)
            x_r1 = population[indices[0]]
            x_r2 = population[indices[1]]
            x_r3 = population[indices[2]]
            donor_vector = x_r1 + self.F * (x_r2 - x_r3)
            
        return donor_vector

    def _crossover(self, target_vector, donor_vector):
        trial_vector = np.copy(target_vector)
        for j in range(self.dim):
            if np.random.rand() < self.CR:
                trial_vector[j] = donor_vector[j]
        return trial_vector

    def _update_archive(self, population, fitness_values, archive):
         combined = np.column_stack((population, fitness_values))
         new_archive = []
         for sol in combined:
              already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in archive)
              if not already_present:
                    new_archive.append(sol)
         new_archive.sort(key=lambda x: x[-1]) # sort by fitness
         updated_archive = archive + new_archive
         updated_archive.sort(key=lambda x: x[-1])
         return updated_archive[:self.archive_size] # Keep top archive_size

    def _migrate_individuals(self, fitness_values):
        # Migrate individuals based on migration rate.
        num_to_migrate = int(self.population_size * self.migration_rate) #decaying

        for i in range(self.num_islands):
            # Sort population of island i by fitness
            sorted_indices = np.argsort(fitness_values[i]) #From worse to best
            immigrants = self.islands[i][sorted_indices[:num_to_migrate]] #pick bottom solutions.
            # Determine destination island (different from the current island).
            dest_island = (i + 1) % self.num_islands

            #Replace top solution of destination island with randomly select from immigrants
            self.islands[dest_island][-num_to_migrate:] = immigrants #Immigrants are added to the top of the list.
            
            # Re-evaluate fitness of destination island.
            fitness_dest_island = objective_function(self.islands[dest_island])
            self.eval_count += num_to_migrate
            fitness_values[dest_island] = fitness_dest_island

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
```

### Rank 2 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: ArchipelagoDEArchiveInjection
# Description: Island model DE with adaptive archive injection to enhance exploration.
# Code:
```python
import numpy as np

# Name: ArchipelagoDEArchiveInjection
# Description: Island model DE with adaptive archive injection to enhance exploration.

class ArchipelagoDEArchiveInjection:
    """
    Island model Differential Evolution with archive-guided mutation to balance exploration and exploitation in multimodal problems.
    """

    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, population_size: int = 50, archive_size: int = 100,
                 migration_interval: int = 500, migration_rate: float = 0.1,
                 mutation_factor: float = 0.5, crossover_rate: float = 0.7,
                 archive_prob: float = 0.1):  # Probability of using archive in mutation

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = int(num_islands)
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.migration_interval = int(migration_interval)
        self.migration_rate = float(migration_rate)
        self.mutation_factor = float(mutation_factor)
        self.crossover_rate = float(crossover_rate)
        self.archive_prob = float(archive_prob) # Probability of sampling from archive
        
        self.populations = [np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
                            for _ in range(self.num_islands)]
        self.fitness_values = [np.full(self.population_size, float('inf')) for _ in range(self.num_islands)]
        self.archives = [[] for _ in range(self.num_islands)]

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initial evaluation
        for i in range(self.num_islands):
            self.fitness_values[i] = objective_function(self.populations[i])
            self.eval_count += self.population_size
            self._update_archive(i, self.populations[i], self.fitness_values[i])
            self._update_best(self.populations[i], self.fitness_values[i])

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                # Differential Evolution
                new_population = np.copy(self.populations[i]) # Preallocate mem
                for j in range(self.population_size):
                    # Mutation with archive injection
                    if np.random.rand() < self.archive_prob and len(self.archives[i]) > 0:
                        # Select a solution from the archive
                        archive_idx = np.random.randint(len(self.archives[i]))
                        archive_solution = self.archives[i][archive_idx][:-1] # Exclude fitness from archive entry

                        idxs = np.random.choice(self.population_size, 2, replace=False)
                        mutant = archive_solution + self.mutation_factor * (self.populations[i][idxs[0]] - self.populations[i][idxs[1]])

                    else:
                        # Classic DE mutation
                        idxs = np.random.choice(self.population_size, 3, replace=False)
                        mutant = self.populations[i][idxs[0]] + self.mutation_factor * (self.populations[i][idxs[1]] - self.populations[i][idxs[2]])

                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                    # Crossover
                    crossover_mask = np.random.rand(self.dim) < self.crossover_rate
                    trial_vector = np.where(crossover_mask, mutant, self.populations[i][j])

                    # Evaluation
                    trial_fitness = objective_function(trial_vector.reshape(1, -1))[0]
                    self.eval_count += 1
                    
                    # Selection
                    if trial_fitness < self.fitness_values[i][j]:
                        new_population[j] = trial_vector
                        self.fitness_values[i][j] = trial_fitness
                self.populations[i] = new_population # Post assignemnt, not inline, for clear code reading.
                self._update_archive(i, self.populations[i], self.fitness_values[i])
                self._update_best(self.populations[i], self.fitness_values[i])

            # Migration
            if generation % self.migration_interval == 0:
                self._migrate()

            generation += 1
        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _update_archive(self, island_index, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archives[island_index])
            if not already_present:
                self.archives[island_index].append(sol)

        self.archives[island_index].sort(key=lambda x: x[-1])  # Sort by fitness
        self.archives[island_index] = self.archives[island_index][:self.archive_size]

    def _migrate(self):
        # Select migrants from each island and distribute them
        migrants = []
        for i in range(self.num_islands):
            num_migrants = int(self.population_size * self.migration_rate)
            indices = np.argsort(self.fitness_values[i])[:num_migrants]
            migrants.append(self.populations[i][indices])

        # Distribute migrants to other islands
        for i in range(self.num_islands):
            receiving_island = (i + 1) % self.num_islands
            num_migrants = migrants[i].shape[0] # Shape is much faster
            replace_indices = np.random.choice(self.population_size, num_migrants, replace=False)

            self.populations[receiving_island][replace_indices] = migrants[i]
            # Re-evaluate the replaced individuals
            self.fitness_values[receiving_island][replace_indices] = objective_function(migrants[i])
            self.eval_count += num_migrants

            self._update_archive(receiving_island, self.populations[receiving_island], self.fitness_values[receiving_island])

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
```

### Rank 3 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: ArchipelagoDEWithAdaptiveArchiveAndMigration
# Description: Island-based DE algorithm with adaptive archive-guided mutation and performance-based migration.
# Code:
```python
import numpy as np
import random

# Name: ArchipelagoDEWithAdaptiveArchiveAndMigration
# Description: Island-based DE algorithm with adaptive archive-guided mutation and performance-based migration.
# Code:
class ArchipelagoDEWithAdaptiveArchiveAndMigration:
    """
    An island-based DE algorithm that combines adaptive archive-guided mutation with
    performance-based migration to explore and exploit multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, population_size_factor: float = 8.82865217019506,
                 archive_size_factor: float = 30, migration_interval: int = 5000,
                 migration_rate: float = 0.1, initial_F_scale: float = 0.7,
                 archive_update_threshold: float = 0.8,
                 F_scale_variation: float = 0.3, stagnation_threshold: int = 10000):
        """
        Initializes the ArchipelagoDEWithAdaptiveArchiveAndMigration algorithm.

        Args:
            budget: Max function evaluations.
            dim: Problem dimensionality.
            lower_bounds: List of lower bounds for each dimension.
            upper_bounds: List of upper bounds for each dimension.
            num_islands: Number of isolated subpopulations (islands).
            population_size_factor: Factor to determine initial population size per island.
            archive_size_factor: Archive size.
            migration_interval: Number of evaluations between migrations.
            migration_rate: Fraction of population to migrate between islands.
            initial_F_scale: Initial scaling factor for differential evolution.
            archive_update_threshold: Threshold for updating archive entries.
            F_scale_variation: Variation for the scaling factor F.
            stagnation_threshold: Number of evaluations without improvement before adaptive actions.
        """
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.best_fitness_history = [] # track performance for adaptive adjustments

        self.num_islands = num_islands
        self.population_size = int(population_size_factor * self.dim)
        self.archive_size = int(archive_size_factor * self.dim)
        self.migration_interval = migration_interval
        self.migration_rate = migration_rate
        self.initial_F_scale = initial_F_scale
        self.archive_update_threshold = archive_update_threshold
        self.F_scale_variation = F_scale_variation
        self.stagnation_threshold = stagnation_threshold

        self.islands = []
        self.archives = []
        self.island_best_fitness = [] # track best fitness on each island

        for _ in range(self.num_islands):
            self.islands.append(np.random.uniform(self.lower_bounds, self.upper_bounds,
                                                  size=(self.population_size, self.dim)))
            self.archives.append([])  # Initialize empty archive for each island
            self.island_best_fitness.append(float('inf')) # Initialize island best fitness


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """
        Optimizes the given objective function using the ArchipelagoDEWithAdaptiveArchiveAndMigration algorithm.

        Args:
            objective_function: The function to minimize.
            acceptance_threshold: The threshold for early stopping.

        Returns:
            A tuple containing the best solution, its fitness, and optimization information.
        """
        self.eval_count = 0
        self.best_fitness_overall = float('inf')
        self.best_solution_overall = None
        self.best_fitness_history = []
        fitness_values = []

        for island in self.islands:
            fitness = objective_function(island)
            self.eval_count += self.population_size
            fitness_values.append(fitness)

        # Initialize overall best and island best
        for i in range(self.num_islands):
            island_best_index = np.argmin(fitness_values[i])
            island_best_fitness = fitness_values[i][island_best_index]
            self.island_best_fitness[i] = island_best_fitness

            if island_best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = island_best_fitness
                self.best_solution_overall = self.islands[i][island_best_index]

        self.best_fitness_history.append(self.best_fitness_overall)

        stagnation_counter = 0 # Track steps without improvement


        while self.eval_count < self.budget:
            improved = False  # Flag if any island improved the overall best

            for i in range(self.num_islands):
                # Generate offspring for each island
                offspring = self.generate_offspring(self.islands[i], fitness_values[i], self.archives[i])
                offspring_fitness = objective_function(offspring)
                self.eval_count += len(offspring)

                # Update archive for each island
                self.update_archive(self.archives[i], offspring, offspring_fitness, self.archive_update_threshold)

                # Select best solutions for next generation for each island
                combined_population = np.concatenate((self.islands[i], offspring))
                combined_fitness = np.concatenate((fitness_values[i], offspring_fitness))
                indices = np.argsort(combined_fitness)
                self.islands[i] = combined_population[indices[:self.population_size]]
                fitness_values[i] = combined_fitness[indices[:self.population_size]]

                # Update best solution for current island
                island_best_index = np.argmin(fitness_values[i])
                island_best_fitness = fitness_values[i][island_best_index]

                if island_best_fitness < self.island_best_fitness[i]:
                    self.island_best_fitness[i] = island_best_fitness

                if island_best_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = island_best_fitness
                    self.best_solution_overall = self.islands[i][island_best_index]
                    improved = True  # Mark overall best improvement

            # Adaptive Migration (Performance-based): migrate only if not improving
            if not improved and self.eval_count % self.migration_interval == 0:
                self.migrate_individuals()

            # Adaptive F Scale: If not improving overall, increase the F Scale variation, otherwise return to a "default" level of variation
            if not improved:
                self.F_scale_variation = min(1.0, self.F_scale_variation + 0.05)
            else:
                self.F_scale_variation = max(0.0, self.F_scale_variation - 0.02)  # Optionally reduce when doing well
            self.best_fitness_history.append(self.best_fitness_overall)

            # Check for stagnation (no improvement in best fitness)
            if len(self.best_fitness_history) > self.stagnation_threshold:
                if self.best_fitness_history[-1] == self.best_fitness_history[-self.stagnation_threshold]:
                    stagnation_counter +=1

                    # Take action if genuinely stuck. Increase diversification and reshuffle a few islands
                    # Increase migration rate or introduce more randomness
                    if stagnation_counter>3:

                        #restart islands in worst population to improve exploration.
                        worst_island_index = np.argmax(self.island_best_fitness)

                        #re-initialize island based on best overall individual as prior.
                        center = self.best_solution_overall

                        self.islands[worst_island_index] = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
                        self.islands[worst_island_index] = np.clip(self.islands[worst_island_index], self.lower_bounds, self.upper_bounds)

                        fitness = objective_function(self.islands[worst_island_index])
                        self.eval_count += self.population_size

                        island_best_index = np.argmin(fitness)
                        island_best_fitness = fitness[island_best_index]
                        self.island_best_fitness[worst_island_index] = island_best_fitness

                        # Clear out the stagnated islands' archive
                        self.archives[worst_island_index]=[]
                        stagnation_counter = 0 #restart counter

                else:
                    stagnation_counter = 0

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness, archive):
        """
        Generates offspring using differential evolution with archive guidance.

        Args:
            population: The current population.
            fitness: The fitness values of the current population.
            archive: The archive of promising solutions.

        Returns:
            The generated offspring.
        """
        offspring = np.zeros((self.population_size, self.dim))
        F_scale = self.initial_F_scale + self.F_scale_variation * np.random.rand()  # scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available) or from current best

            if archive:
                pbest_index = np.random.choice(len(archive))
                pbest = archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]  # fallback to population

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)  # Boundary handling

        return offspring

    def update_archive(self, archive, offspring, offspring_fitness, archive_update_threshold):
         """
         Update the archive with new offspring solutions, keeping it limited to archive_size, while prioritizing diversity
         """
         for i in range(len(offspring)):
             if len(archive) < self.archive_size:
                 archive.append((offspring[i], offspring_fitness[i]))
             else:
                 # Prioritize diversity in archive - consider replacing worst
                 worst_index = np.argmax([f for _, f in archive])
                 if offspring_fitness[i] < archive[worst_index][1] or len(archive) < self.archive_size * archive_update_threshold:
                     archive[worst_index] = (offspring[i], offspring_fitness[i])

    def migrate_individuals(self):
        """Migrates individuals between islands to increase diversity."""
        for i in range(self.num_islands):
            # Select individuals to migrate from island i
            num_migrants = int(self.migration_rate * self.population_size)
            migrant_indices = np.random.choice(self.population_size, num_migrants, replace=False)
            migrants = self.islands[i][migrant_indices]

            # Choose a destination island (different from the origin)
            destination_island = (i + 1) % self.num_islands

            # Replace some individuals on the destination island with the migrants
            replace_indices = np.random.choice(self.population_size, num_migrants, replace=False)
            self.islands[destination_island][replace_indices] = migrants

    def _initialize_population(self):

        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        return population
```

### Rank 4 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: ArchipelagoAdaptiveDE
# Description: Island model DE with adaptive archive-guided mutation and migration for multimodal optimization.
# Code:
```python
import numpy as np

# Name: ArchipelagoAdaptiveDE
# Description: Island model DE with adaptive archive-guided mutation and migration for multimodal optimization.
# Code:
class ArchipelagoAdaptiveDE:
    """
    Island model Differential Evolution with adaptive archive-guided mutation and dynamic migration.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5,
                 population_size: int = 50,
                 archive_size: int = 50,
                 migration_interval: int = 500,  # evaluations between migrations
                 migration_rate: float = 0.1,
                 f_factor: float = 0.5,
                 cr: float = 0.7,
                 archive_prob: float = 0.2): # Probability of using archive solutions in mutation
        """
        Initializes the ArchipelagoAdaptiveDE algorithm.

        Args:
            budget (int): The maximum number of function evaluations.
            dim (int): The dimensionality of the problem.
            lower_bounds (list[float]): The lower bounds of the search space.
            upper_bounds (list[float]): The upper bounds of the search space.
            num_islands (int): The number of islands in the archipelago.
            population_size (int): The population size of each island.
            archive_size (int): The maximum size of the archive on each island.
            migration_interval (int): The number of evaluations between migrations.
            migration_rate (float): The fraction of individuals that migrate.
            f_factor (float): The DE F factor (mutation scaling factor).
            cr (float): The DE CR factor (crossover probability).
            archive_prob (float): Probability of using archive solution to guide mutation.
        """
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.num_islands = int(num_islands)
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.migration_interval = int(migration_interval)
        self.migration_rate = float(migration_rate)
        self.f_factor = float(f_factor)
        self.cr = float(cr)
        self.archive_prob = float(archive_prob)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        # Island specific data structures
        self.populations = []
        self.fitness_values = []
        self.archives = []

        for _ in range(self.num_islands):
            population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
            self.populations.append(population)
            self.fitness_values.append(np.full(self.population_size, float('inf')))  # Initialize fitness values
            self.archives.append([]) # Initialize empty archives
            
    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """
        Optimizes the given objective function using the island-based DE algorithm.

        Args:
            objective_function (callable): The objective function to minimize.
            acceptance_threshold (float): The acceptance threshold (unused).

        Returns:
            tuple: A tuple containing the best solution, best fitness, and optimization information.
        """
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = float('inf')

        # Initial evaluation
        for i in range(self.num_islands):
            self.fitness_values[i] = objective_function(self.populations[i])
            self.eval_count += self.population_size
            self._update_archive(i, self.populations[i], self.fitness_values[i])
            self._update_best(i)

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                # DE Mutation, Crossover, and Selection
                new_population = np.copy(self.populations[i]) # Placeholder

                for j in range(self.population_size):
                    # Mutation
                    if np.random.rand() < self.archive_prob and self.archives[i]: # check if the archive is not empty
                         # Select an archive member to bias mutation toward
                         archive_idx = np.random.randint(len(self.archives[i]))
                         archive_solution = self.archives[i][archive_idx][:-1]  # exclude fitness val from selection of archive
                         idxs = np.random.choice(self.population_size, 2, replace=False)
                         mutant = self.populations[i][j] + self.f_factor * (archive_solution - self.populations[i][j])
                    else:
                        idxs = np.random.choice(self.population_size, 3, replace=False)
                        mutant = self.populations[i][idxs[0]] + self.f_factor * (self.populations[i][idxs[1]] - self.populations[i][idxs[2]])

                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                    # Crossover
                    crossover_mask = np.random.rand(self.dim) < self.cr
                    trial_vector = np.where(crossover_mask, mutant, self.populations[i][j])

                    # Evaluation
                    trial_fitness = objective_function(trial_vector.reshape(1, -1))[0]
                    self.eval_count += 1

                    # Selection
                    if trial_fitness < self.fitness_values[i][j]:
                        new_population[j] = trial_vector
                        self.fitness_values[i][j] = trial_fitness
                self.populations[i] = new_population
                self._update_archive(i, self.populations[i], self.fitness_values[i])
                self._update_best(i)


            # Migration
            if (self.eval_count // self.population_size) % (self.migration_interval // self.population_size) == 0: # migration interval
                self._migrate()

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _update_archive(self, island_index, population, fitness_values):
        """Updates the archive with the given population and fitness values."""
        combined = np.column_stack((population, fitness_values))
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archives[island_index])
            if not already_present:
                self.archives[island_index].append(sol)
        #Sort the Archive
        self.archives[island_index].sort(key=lambda x: x[-1])
        self.archives[island_index] = self.archives[island_index][:self.archive_size]

    def _migrate(self):
         """Migrates individuals between islands."""

         #Select migrators from each island
         migrators = []
         for i in range(self.num_islands):
            num_to_migrate = int(self.population_size * self.migration_rate)
            sorted_indices = np.argsort(self.fitness_values[i])
            migrators.append(self.populations[i][sorted_indices[:num_to_migrate]].copy()) #Important to take copy!!

         #Distribute migrators to other islands
         for i in range(self.num_islands):
            receiving_island = (i + 1) % self.num_islands
            num_migrators = migrators[i].shape[0]
            replace_indices = np.random.choice(self.population_size, num_migrators, replace=False)

            # Replace the population of receiving island with the migrators
            self.populations[receiving_island][replace_indices] = migrators[i].copy()

            #Re-evaluate the fitness of the replaced individuals
            new_fitness_values = []
            for individual in self.populations[receiving_island][replace_indices]:
               new_fitness_values.append(objective_function(individual.reshape(1, -1))[0])
               self.eval_count +=1
            self.fitness_values[receiving_island][replace_indices] = np.array(new_fitness_values)

            self._update_archive(receiving_island, self.populations[receiving_island], self.fitness_values[receiving_island])
            self._update_best(receiving_island)


    def _update_best(self, island_index):
        """Updates the overall best solution if a better solution is found on the given island."""
        best_index = np.argmin(self.fitness_values[island_index])
        if self.fitness_values[island_index][best_index] < self.best_fitness_overall:
            self.best_fitness_overall = self.fitness_values[island_index][best_index]
            self.best_solution_overall = self.populations[island_index][best_index].copy()
```

### Rank 5 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdvancedAdaptiveGaussianArchiveEA
# Description: 
# Code:
```python
import numpy as np

class AdvancedAdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """

    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 152.12581271342893, archive_size: int = 109.13327633513484, initial_sigma_fraction: float = 0.7636376613820393,
                 sigma_decay: float = 0.9465688145247038, tournament_size: int = 7.851029882584946, recombination_sigma_fraction: float = 0.44816960348706236,
                 archive_tolerance: float = 4.272702233167118e-05):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_fraction * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size
        self.recombination_sigma_fraction = recombination_sigma_fraction
        self.archive_tolerance = archive_tolerance

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_fraction, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_fraction, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_tolerance) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 17:49:10 INFO Full response text: **Analysis:**
Comparing (best) vs (worst), we see that the better algorithm `AdaptiveIslandDifferentialEvolutionArchiveMigration` uses an island model with Differential Evolution (DE), adaptive migration rates, and an archive-guided mutation strategy; while the worst algorithm `AdvancedAdaptiveGaussianArchiveEA` employs a simple Gaussian mutation strategy and tournament selection. (second best) vs (second worst) the better one, called `ArchipelagoAdaptiveDE` also uses an island model with adaptive DE and archive-guided mutation which the worst one called `ArchipelagoDEWithAdaptiveArchiveAndMigration` used an island model with similar DE algorithms. Comparing (1st) vs (2nd), we see that `AdaptiveIslandDifferentialEvolutionArchiveMigration` uses decaying migration rate versus the simple migration of `ArchipelagoDEArchiveInjection`, and that the first performs archive guided mutation with lower level DE mechanisms. (3rd) vs (4th) are quite similar, in the context adaptive archive-guided. Comparing (second worst) vs (worst), we see that DE of better code is replaced by gaussian recombination of worse one and this leads to simpler operations. Overall: The island model with DE and archive guidance appears to be critical for good performance on this benchmark. Adaptive mechanisms, especially in migration rate and mutation strategies, also contribute to performance. The most successful algorithms use diversity-maintenance strategies such as decaying or performance triggered rates and archive to focus exploration on areas near known, promising solutions. Simple approaches tend to perform worse by becoming trapped by local minima too quickly.

**Experience:**
Multi-population methods offer effective diversity in deceptive landscapes. Archive-guided mutation and adaptive migration rates help balance exploration and exploitation. Restarting stagnated islands also ensures progress. Always add self adapting mechanisms when applicable to cover unknown aspects of benchmark.

2025-06-24 17:49:13 INFO Full response text: Okay, I understand. Let's craft a definition of "Effective Self-Reflection" focused on designing better heuristics, specifically avoiding the pitfalls of the provided "Ineffective Self-Reflection." We want a definition that translates into actionable strategies and insights.

**Current Effective Self-Reflection Definition:**

*   **Keywords:** Balancing exploitation and exploration, adaptive mechanisms, diversity maintenance, landscape characteristics, performance bottlenecks, problem-specific knowledge, objective measurements, hypothesis generation, performance analysis.

*   **Advice:** Instead of simply listing mechanisms (archive, migration, restart), focus on *why* they are (or aren't) effective. Analyze *where* the algorithm spends its time. Form hypotheses about performance based on landscape *characteristics* (deceptiveness, multi-modality, ruggedness, dimensionality). Gather objective measurements to test those hypotheses rigorously. Use problem specific knowledge to improve the heuristic.

*   **Avoid:** Generic statements about "exploration and exploitation," vague claims of "diversity," and mere recitation of algorithmic components. Avoid circular reasoning where the algorithm's performance is attributed to the features that were designed to achieve that performance.

*   **Explanation:** Effective self-reflection requires a hypothesis-driven approach. Don't just describe *what* the algorithm does; analyze *why* it performs as it does on specific landscape features. Then think on how to improve it. Use objective measurements (e.g., solution distance to optima, population entropy, wall clock time), landscape characteristics, and problem-specific knowledge as diagnostic tools.

2025-06-24 17:49:13 INFO Generating offspring via Crossover...
2025-06-24 17:50:13 INFO Crossover Prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


### Better code
ArchipelagoDEArchiveInjection
import numpy as np

# Name: ArchipelagoDEArchiveInjection
# Description: Island model DE with adaptive archive injection to enhance exploration.

class ArchipelagoDEArchiveInjection:
    """
    Island model Differential Evolution with archive-guided mutation to balance exploration and exploitation in multimodal problems.
    """

    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, population_size: int = 50, archive_size: int = 100,
                 migration_interval: int = 500, migration_rate: float = 0.1,
                 mutation_factor: float = 0.5, crossover_rate: float = 0.7,
                 archive_prob: float = 0.1):  # Probability of using archive in mutation

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = int(num_islands)
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.migration_interval = int(migration_interval)
        self.migration_rate = float(migration_rate)
        self.mutation_factor = float(mutation_factor)
        self.crossover_rate = float(crossover_rate)
        self.archive_prob = float(archive_prob) # Probability of sampling from archive
        
        self.populations = [np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
                            for _ in range(self.num_islands)]
        self.fitness_values = [np.full(self.population_size, float('inf')) for _ in range(self.num_islands)]
        self.archives = [[] for _ in range(self.num_islands)]

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initial evaluation
        for i in range(self.num_islands):
            self.fitness_values[i] = objective_function(self.populations[i])
            self.eval_count += self.population_size
            self._update_archive(i, self.populations[i], self.fitness_values[i])
            self._update_best(self.populations[i], self.fitness_values[i])

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                # Differential Evolution
                new_population = np.copy(self.populations[i]) # Preallocate mem
                for j in range(self.population_size):
                    # Mutation with archive injection
                    if np.random.rand() < self.archive_prob and len(self.archives[i]) > 0:
                        # Select a solution from the archive
                        archive_idx = np.random.randint(len(self.archives[i]))
                        archive_solution = self.archives[i][archive_idx][:-1] # Exclude fitness from archive entry

                        idxs = np.random.choice(self.population_size, 2, replace=False)
                        mutant = archive_solution + self.mutation_factor * (self.populations[i][idxs[0]] - self.populations[i][idxs[1]])

                    else:
                        # Classic DE mutation
                        idxs = np.random.choice(self.population_size, 3, replace=False)
                        mutant = self.populations[i][idxs[0]] + self.mutation_factor * (self.populations[i][idxs[1]] - self.populations[i][idxs[2]])

                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                    # Crossover
                    crossover_mask = np.random.rand(self.dim) < self.crossover_rate
                    trial_vector = np.where(crossover_mask, mutant, self.populations[i][j])

                    # Evaluation
                    trial_fitness = objective_function(trial_vector.reshape(1, -1))[0]
                    self.eval_count += 1
                    
                    # Selection
                    if trial_fitness < self.fitness_values[i][j]:
                        new_population[j] = trial_vector
                        self.fitness_values[i][j] = trial_fitness
                self.populations[i] = new_population # Post assignemnt, not inline, for clear code reading.
                self._update_archive(i, self.populations[i], self.fitness_values[i])
                self._update_best(self.populations[i], self.fitness_values[i])

            # Migration
            if generation % self.migration_interval == 0:
                self._migrate()

            generation += 1
        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _update_archive(self, island_index, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archives[island_index])
            if not already_present:
                self.archives[island_index].append(sol)

        self.archives[island_index].sort(key=lambda x: x[-1])  # Sort by fitness
        self.archives[island_index] = self.archives[island_index][:self.archive_size]

    def _migrate(self):
        # Select migrants from each island and distribute them
        migrants = []
        for i in range(self.num_islands):
            num_migrants = int(self.population_size * self.migration_rate)
            indices = np.argsort(self.fitness_values[i])[:num_migrants]
            migrants.append(self.populations[i][indices])

        # Distribute migrants to other islands
        for i in range(self.num_islands):
            receiving_island = (i + 1) % self.num_islands
            num_migrants = migrants[i].shape[0] # Shape is much faster
            replace_indices = np.random.choice(self.population_size, num_migrants, replace=False)

            self.populations[receiving_island][replace_indices] = migrants[i]
            # Re-evaluate the replaced individuals
            self.fitness_values[receiving_island][replace_indices] = objective_function(migrants[i])
            self.eval_count += num_migrants

            self._update_archive(receiving_island, self.populations[receiving_island], self.fitness_values[receiving_island])

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]

### Worse code
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Analyze & experience
- Comparing (best) vs (worst), we see that the better algorithm `AdaptiveIslandDifferentialEvolutionArchiveMigration` uses an island model with Differential Evolution (DE), adaptive migration rates, and an archive-guided mutation strategy; while the worst algorithm `AdvancedAdaptiveGaussianArchiveEA` employs a simple Gaussian mutation strategy and tournament selection. (second best) vs (second worst) the better one, called `ArchipelagoAdaptiveDE` also uses an island model with adaptive DE and archive-guided mutation which the worst one called `ArchipelagoDEWithAdaptiveArchiveAndMigration` used an island model with similar DE algorithms. Comparing (1st) vs (2nd), we see that `AdaptiveIslandDifferentialEvolutionArchiveMigration` uses decaying migration rate versus the simple migration of `ArchipelagoDEArchiveInjection`, and that the first performs archive guided mutation with lower level DE mechanisms. (3rd) vs (4th) are quite similar, in the context adaptive archive-guided. Comparing (second worst) vs (worst), we see that DE of better code is replaced by gaussian recombination of worse one and this leads to simpler operations. Overall: The island model with DE and archive guidance appears to be critical for good performance on this benchmark. Adaptive mechanisms, especially in migration rate and mutation strategies, also contribute to performance. The most successful algorithms use diversity-maintenance strategies such as decaying or performance triggered rates and archive to focus exploration on areas near known, promising solutions. Simple approaches tend to perform worse by becoming trapped by local minima too quickly.
- Okay, I understand. Let's craft a definition of "Effective Self-Reflection" focused on designing better heuristics, specifically avoiding the pitfalls of the provided "Ineffective Self-Reflection." We want a definition that translates into actionable strategies and insights.

**Current Effective Self-Reflection Definition:**

*   **Keywords:** Balancing exploitation and exploration, adaptive mechanisms, diversity maintenance, landscape characteristics, performance bottlenecks, problem-specific knowledge, objective measurements, hypothesis generation, performance analysis.

*   **Advice:** Instead of simply listing mechanisms (archive, migration, restart), focus on *why* they are (or aren't) effective. Analyze *where* the algorithm spends its time. Form hypotheses about performance based on landscape *characteristics* (deceptiveness, multi-modality, ruggedness, dimensionality). Gather objective measurements to test those hypotheses rigorously. Use problem specific knowledge to improve the heuristic.

*   **Avoid:** Generic statements about "exploration and exploitation," vague claims of "diversity," and mere recitation of algorithmic components. Avoid circular reasoning where the algorithm's performance is attributed to the features that were designed to achieve that performance.

*   **Explanation:** Effective self-reflection requires a hypothesis-driven approach. Don't just describe *what* the algorithm does; analyze *why* it performs as it does on specific landscape features. Then think on how to improve it. Use objective measurements (e.g., solution distance to optima, population entropy, wall clock time), landscape characteristics, and problem-specific knowledge as diagnostic tools.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 17:50:13 INFO Mutation prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


Current heuristics:
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- Okay, I understand. Let's craft a definition of "Effective Self-Reflection" focused on designing better heuristics, specifically avoiding the pitfalls of the provided "Ineffective Self-Reflection." We want a definition that translates into actionable strategies and insights.

**Current Effective Self-Reflection Definition:**

*   **Keywords:** Balancing exploitation and exploration, adaptive mechanisms, diversity maintenance, landscape characteristics, performance bottlenecks, problem-specific knowledge, objective measurements, hypothesis generation, performance analysis.

*   **Advice:** Instead of simply listing mechanisms (archive, migration, restart), focus on *why* they are (or aren't) effective. Analyze *where* the algorithm spends its time. Form hypotheses about performance based on landscape *characteristics* (deceptiveness, multi-modality, ruggedness, dimensionality). Gather objective measurements to test those hypotheses rigorously. Use problem specific knowledge to improve the heuristic.

*   **Avoid:** Generic statements about "exploration and exploitation," vague claims of "diversity," and mere recitation of algorithmic components. Avoid circular reasoning where the algorithm's performance is attributed to the features that were designed to achieve that performance.

*   **Explanation:** Effective self-reflection requires a hypothesis-driven approach. Don't just describe *what* the algorithm does; analyze *why* it performs as it does on specific landscape features. Then think on how to improve it. Use objective measurements (e.g., solution distance to optima, population entropy, wall clock time), landscape characteristics, and problem-specific knowledge as diagnostic tools.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-24 17:50:35 INFO Perform Harmony Search...
2025-06-24 17:50:45 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:45 ERROR Can not run the algorithm
2025-06-24 17:50:45 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:45 INFO FeHistory: [128874.05562287]
2025-06-24 17:50:45 INFO Expected Optimum FE: -5000
2025-06-24 17:50:45 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:45 ERROR Can not run the algorithm
2025-06-24 17:50:45 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:45 INFO FeHistory: [142912.78823205]
2025-06-24 17:50:45 INFO Expected Optimum FE: -5000
2025-06-24 17:50:45 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:45 ERROR Can not run the algorithm
2025-06-24 17:50:46 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:46 INFO FeHistory: [190645.83231735]
2025-06-24 17:50:46 INFO Expected Optimum FE: -5000
2025-06-24 17:50:46 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:46 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:46 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:46 INFO AOCC mean: 0.0000
2025-06-24 17:50:46 INFO Weighed AOCC mean: nan
2025-06-24 17:50:46 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:46 ERROR Can not run the algorithm
2025-06-24 17:50:46 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:46 INFO FeHistory: [141515.52320716]
2025-06-24 17:50:46 INFO Expected Optimum FE: -5000
2025-06-24 17:50:46 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:46 ERROR Can not run the algorithm
2025-06-24 17:50:46 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:46 INFO FeHistory: [170871.05951559]
2025-06-24 17:50:46 INFO Expected Optimum FE: -5000
2025-06-24 17:50:46 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:46 ERROR Can not run the algorithm
2025-06-24 17:50:47 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:47 INFO FeHistory: [186954.35653633]
2025-06-24 17:50:47 INFO Expected Optimum FE: -5000
2025-06-24 17:50:47 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:47 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:47 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:47 INFO AOCC mean: 0.0000
2025-06-24 17:50:47 INFO Weighed AOCC mean: nan
2025-06-24 17:50:47 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:47 ERROR Can not run the algorithm
2025-06-24 17:50:47 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:47 INFO FeHistory: [152457.44265113]
2025-06-24 17:50:47 INFO Expected Optimum FE: -5000
2025-06-24 17:50:47 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:47 ERROR Can not run the algorithm
2025-06-24 17:50:47 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:47 INFO FeHistory: [164418.95199886]
2025-06-24 17:50:47 INFO Expected Optimum FE: -5000
2025-06-24 17:50:47 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:47 ERROR Can not run the algorithm
2025-06-24 17:50:48 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:48 INFO FeHistory: [164514.65031354]
2025-06-24 17:50:48 INFO Expected Optimum FE: -5000
2025-06-24 17:50:48 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:48 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:48 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:48 INFO AOCC mean: 0.0000
2025-06-24 17:50:48 INFO Weighed AOCC mean: nan
2025-06-24 17:50:48 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:48 ERROR Can not run the algorithm
2025-06-24 17:50:48 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:48 INFO FeHistory: [96152.63133865]
2025-06-24 17:50:48 INFO Expected Optimum FE: -5000
2025-06-24 17:50:48 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:48 ERROR Can not run the algorithm
2025-06-24 17:50:48 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:48 INFO FeHistory: [200803.32436549]
2025-06-24 17:50:48 INFO Expected Optimum FE: -5000
2025-06-24 17:50:48 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:48 ERROR Can not run the algorithm
2025-06-24 17:50:49 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:49 INFO FeHistory: [203619.24640133]
2025-06-24 17:50:49 INFO Expected Optimum FE: -5000
2025-06-24 17:50:49 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:49 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:49 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:49 INFO AOCC mean: 0.0000
2025-06-24 17:50:49 INFO Weighed AOCC mean: nan
2025-06-24 17:50:49 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:49 ERROR Can not run the algorithm
2025-06-24 17:50:49 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:49 INFO FeHistory: [176480.08645377]
2025-06-24 17:50:49 INFO Expected Optimum FE: -5000
2025-06-24 17:50:49 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:49 ERROR Can not run the algorithm
2025-06-24 17:50:50 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:50 INFO FeHistory: [102035.93275939]
2025-06-24 17:50:50 INFO Expected Optimum FE: -5000
2025-06-24 17:50:50 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:50 ERROR Can not run the algorithm
2025-06-24 17:50:50 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:50 INFO FeHistory: [165149.43015263]
2025-06-24 17:50:50 INFO Expected Optimum FE: -5000
2025-06-24 17:50:50 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:50 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:50 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:50 INFO AOCC mean: 0.0000
2025-06-24 17:50:50 INFO Weighed AOCC mean: nan
2025-06-24 17:50:50 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:50 ERROR Can not run the algorithm
2025-06-24 17:50:50 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:50 INFO FeHistory: [154553.24756744]
2025-06-24 17:50:50 INFO Expected Optimum FE: -5000
2025-06-24 17:50:50 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:50 ERROR Can not run the algorithm
2025-06-24 17:50:51 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:51 INFO FeHistory: [167969.66758169]
2025-06-24 17:50:51 INFO Expected Optimum FE: -5000
2025-06-24 17:50:51 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:51 ERROR Can not run the algorithm
2025-06-24 17:50:51 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:51 INFO FeHistory: [191897.19193883]
2025-06-24 17:50:51 INFO Expected Optimum FE: -5000
2025-06-24 17:50:51 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:51 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:51 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:51 INFO AOCC mean: 0.0000
2025-06-24 17:50:51 INFO Weighed AOCC mean: nan
2025-06-24 17:50:51 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:51 ERROR Can not run the algorithm
2025-06-24 17:50:51 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:51 INFO FeHistory: [119323.29398227]
2025-06-24 17:50:51 INFO Expected Optimum FE: -5000
2025-06-24 17:50:51 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:51 ERROR Can not run the algorithm
2025-06-24 17:50:52 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:52 INFO FeHistory: [217749.59942415]
2025-06-24 17:50:52 INFO Expected Optimum FE: -5000
2025-06-24 17:50:52 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:52 ERROR Can not run the algorithm
2025-06-24 17:50:52 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:52 INFO FeHistory: [108723.90599943]
2025-06-24 17:50:52 INFO Expected Optimum FE: -5000
2025-06-24 17:50:52 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:52 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:52 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:52 INFO AOCC mean: 0.0000
2025-06-24 17:50:52 INFO Weighed AOCC mean: nan
2025-06-24 17:50:52 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:52 ERROR Can not run the algorithm
2025-06-24 17:50:52 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:52 INFO FeHistory: [127303.29381787]
2025-06-24 17:50:52 INFO Expected Optimum FE: -5000
2025-06-24 17:50:52 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:52 ERROR Can not run the algorithm
2025-06-24 17:50:53 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:53 INFO FeHistory: [141754.30449058]
2025-06-24 17:50:53 INFO Expected Optimum FE: -5000
2025-06-24 17:50:53 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:53 ERROR Can not run the algorithm
2025-06-24 17:50:53 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:53 INFO FeHistory: [168079.04986305]
2025-06-24 17:50:53 INFO Expected Optimum FE: -5000
2025-06-24 17:50:53 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:53 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:53 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:53 INFO AOCC mean: 0.0000
2025-06-24 17:50:53 INFO Weighed AOCC mean: nan
2025-06-24 17:50:53 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:53 ERROR Can not run the algorithm
2025-06-24 17:50:53 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:53 INFO FeHistory: [95929.21024414]
2025-06-24 17:50:53 INFO Expected Optimum FE: -5000
2025-06-24 17:50:53 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:53 ERROR Can not run the algorithm
2025-06-24 17:50:54 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:54 INFO FeHistory: [173736.95097635]
2025-06-24 17:50:54 INFO Expected Optimum FE: -5000
2025-06-24 17:50:54 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:54 ERROR Can not run the algorithm
2025-06-24 17:50:54 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:54 INFO FeHistory: [167007.84882267]
2025-06-24 17:50:54 INFO Expected Optimum FE: -5000
2025-06-24 17:50:54 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:54 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:54 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:54 INFO AOCC mean: 0.0000
2025-06-24 17:50:54 INFO Weighed AOCC mean: nan
2025-06-24 17:50:54 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:50:54 ERROR Can not run the algorithm
2025-06-24 17:50:55 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:55 INFO FeHistory: [120751.89856627]
2025-06-24 17:50:55 INFO Expected Optimum FE: -5000
2025-06-24 17:50:55 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:50:55 ERROR Can not run the algorithm
2025-06-24 17:50:55 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:55 INFO FeHistory: [140401.99094104]
2025-06-24 17:50:55 INFO Expected Optimum FE: -5000
2025-06-24 17:50:55 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:50:55 ERROR Can not run the algorithm
2025-06-24 17:50:55 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:50:55 INFO FeHistory: [178328.70101821]
2025-06-24 17:50:55 INFO Expected Optimum FE: -5000
2025-06-24 17:50:55 INFO Unimodal AOCC mean: nan
2025-06-24 17:50:55 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:50:55 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:50:55 INFO AOCC mean: 0.0000
2025-06-24 17:50:55 INFO Weighed AOCC mean: nan
2025-06-24 17:50:55 INFO Generation 3, best so far: 0.2796149162371689
2025-06-24 17:50:55 INFO Population length is: 6
2025-06-24 17:50:55 INFO --- Performing Long-Term Reflection at Generation 4 ---
2025-06-24 17:50:55 INFO Reflection Prompt: ### Problem Description
**(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.

### List heuristics
Below is a list of design heuristics ranked from best to worst based on their AOCC score on group function 3 of GNBG benchmark, where higher is better. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandDEArchiveInjectionAndDiversityControl
# Description: An island-based DE with archive-guided mutation, adaptive population sizing and migration for multimodal exploration.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandDEArchiveInjectionAndDiversityControl
# Description: An island-based DE with archive-guided mutation, adaptive population sizing and migration for multimodal exploration.

class AdaptiveIslandDEArchiveInjectionAndDiversityControl:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = 5
        self.island_population_size = [50] * self.num_islands  # Initial population sizes
        self.archive_size = 200
        self.migration_interval = 500 # Eval based trigger, before was generation based
        self.migration_rate = 0.1
        self.de_scaling_factor = 0.7
        self.de_crossover_rate = 0.9

        self.islands = []
        self.island_best_solutions = [None] * self.num_islands
        self.island_best_fitnesses = [float('inf')] * self.num_islands
        self.island_archives = [[] for _ in range(self.num_islands)]


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        self.islands = self._initialize_islands()
        island_fitnesses = [objective_function(island) for island in self.islands]
        self.eval_count += sum(self.island_population_size)

        for i in range(self.num_islands):
            self._update_island_best(i, self.islands[i], island_fitnesses[i])
            self.island_archives[i] = self._update_archive(self.island_archives[i], self.islands[i], island_fitnesses[i])

        generation = 0
        last_migration = 0

        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                #DE step
                offspring = self._differential_evolution(i)
                offspring_fitness = objective_function(offspring)
                self.eval_count += len(offspring)

                self.islands[i], island_fitnesses[i] = self._select_next_generation(i, offspring, offspring_fitness) # Simple replacement for now

                self._update_island_best(i, offspring, offspring_fitness)
                self.island_archives[i] = self._update_archive(self.island_archives[i], offspring, offspring_fitness)
                self._inject_archive_to_island(i)
            
            if self.eval_count - last_migration > self.migration_interval:
                self._migrate_individuals()
                last_migration = self.eval_count

            self._update_overall_best()
            generation += 1

            #Island size adaptivity (Simplified example: increase based on performance)
            for i in range(self.num_islands):
                if self.island_best_fitnesses[i] < self.best_fitness_overall:
                    self.island_population_size[i] = min(2 * self.island_population_size[i], 200)  # Cap size
                else:
                   self.island_population_size[i] = max(self.island_population_size[i] // 2, 20)    # Min pop size

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_islands(self):
        islands = []
        for i in range(self.num_islands):
            island = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.island_population_size[i], self.dim))
            islands.append(island)
        return islands

    def _differential_evolution(self, island_index):
        island = self.islands[island_index]
        pop_size = len(island)
        offspring = np.zeros_like(island)

        for i in range(pop_size):
            # Ensure unique indices
            indices = list(range(pop_size))
            indices.remove(i)
            random_indices = np.random.choice(indices, 3, replace=False)
            r1, r2, r3 = island[random_indices[0]], island[random_indices[1]], island[random_indices[2]]

            # Mutation using archive, scale between the DE candidate and best archive member.
            if len(self.island_archives[island_index]) > 0:
               archive_member = self.island_archives[island_index][np.random.randint(len(self.island_archives[island_index]))][:-1]
               mutant = island[i] + self.de_scaling_factor * (archive_member - island[i]) + self.de_scaling_factor * (r1 - r2)
            else:
              mutant = r1 + self.de_scaling_factor * (r2 - r3)

            # Crossover
            for j in range(self.dim):
                if np.random.rand() < self.de_crossover_rate:
                    offspring[i, j] = mutant[j]
                else:
                    offspring[i, j] = island[i, j]

        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, island_index, offspring, offspring_fitness):
        island = self.islands[island_index]
        fitness = self._evaluate_island(island_index, island)

        combined_population = np.vstack((island, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))

        sorted_indices = np.argsort(combined_fitness)[:len(island)]
        next_generation = combined_population[sorted_indices]
        next_generation_fitness = combined_fitness[sorted_indices]

        return next_generation, next_generation_fitness

    def _migrate_individuals(self):
        #Simplified: Migrates a percentage of the worst individuals to a random island
        for i in range(self.num_islands):
            num_migrate = int(self.island_population_size[i] * self.migration_rate)
            fitness = self._evaluate_island(i, self.islands[i]) # make sure it's fresh.
            
            worst_indices = np.argsort(fitness)[-num_migrate:] #Pick worst to move.
            migrants = self.islands[i][worst_indices]
            self.islands[i] = np.delete(self.islands[i], worst_indices, axis=0) # remove moved
            
            fitness = np.delete(fitness, worst_indices)
            
            target_island = np.random.choice([x for x in range(self.num_islands) if x!=i]) # pick recipient

            self.islands[target_island] = np.vstack((self.islands[target_island], migrants))
            self.island_population_size[i] = self.islands[i].shape[0] #Update
            self.island_population_size[target_island] = self.islands[target_island].shape[0]
            
            self.migration_rate *= 0.95 # decay migration, fewer swaps over time

    def _update_island_best(self, island_index, population, fitness_values):
        for i, fitness in enumerate(fitness_values):
            if fitness < self.island_best_fitnesses[island_index]:
                self.island_best_fitnesses[island_index] = fitness
                self.island_best_solutions[island_index] = population[i].copy()  # Important: store a copy

                if fitness < self.best_fitness_overall:
                    self.best_fitness_overall = fitness
                    self.best_solution_overall = population[i].copy()


    def _update_overall_best(self):
        for i in range(self.num_islands):
            if self.island_best_fitnesses[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.island_best_fitnesses[i]
                self.best_solution_overall = self.island_best_solutions[i].copy()

    def _update_archive(self, archive, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        if len(archive) > 0:
           for arch in archive:
                new_archive.append(arch)

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in new_archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])  # Sort by fitness
        return new_archive[:self.archive_size] # truncate

    def _evaluate_island(self, island_index, population):
         #Evaluates the fitness of a population of solutions on an island.
         return objective_function(population) # call the function for each.

    def _inject_archive_to_island(self, island_index):
        """Injects a solution from the archive back to the island population."""
        if len(self.island_archives[island_index]) > 0 and len(self.islands[island_index]) < 100:
            # Choose a random solution from the archive
            archive_solution = self.island_archives[island_index][np.random.randint(len(self.island_archives[island_index]))][:-1]

            # Replace a random member of the island with the archive solution
            replace_index = np.random.randint(len(self.islands[island_index]))
            self.islands[island_index][replace_index] = archive_solution.copy()
```

### Rank 2 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandDEArchiveMigrationEnhanced
# Description: Island model DE with adaptive migration, archive-guided mutation, and explicit diversity maintenance.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandDEArchiveMigrationEnhanced
# Description: Island model DE with adaptive migration, archive-guided mutation, and explicit diversity maintenance.

class AdaptiveIslandDEArchiveMigrationEnhanced:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = 5
        self.population_size = 50  # Individuals per island
        self.archive_size = 100
        self.crossover_rate = 0.7
        self.f_factor = 0.5  # Differential weight

        self.islands = []
        self.island_fitness = []
        self.archives = []

        self.migration_interval = 500 # Evaluations between migrations
        self.migration_rate = 0.1 # Fraction of individuals to migrate
        self.migration_decay = 0.995 # Slowly reduce migration

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_fitness_overall = float('inf')
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self._initialize_islands()
        
        # Initial Evaluation
        for i in range(self.num_islands):
            fitness = objective_function(self.islands[i])
            self.island_fitness[i] = fitness
            self.eval_count += self.population_size
            self._update_archive(i, self.islands[i], fitness)
            self._update_best(self.islands[i], fitness)


        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                # DE step
                new_population = self._de_step(i, objective_function)
                self.eval_count += self.population_size
                
                # Archive update
                fitness = objective_function(new_population)
                self.eval_count+= self.population_size
                self._update_archive(i, new_population, fitness)

                # Select next generation, including best from archive
                combined_population = np.vstack((self.islands[i], new_population, self._sample_archive(i))) #consider archive member to next Gen
                combined_fitness = np.concatenate((self.island_fitness[i],fitness, objective_function(self._sample_archive(i))))
                self.eval_count+= len(self._sample_archive(i))
                
                sorted_indices = np.argsort(combined_fitness)
                self.islands[i] = combined_population[sorted_indices[:self.population_size]]
                self.island_fitness[i] = combined_fitness[sorted_indices[:self.population_size]]
                self._update_best(self.islands[i], self.island_fitness[i])

            # Migration
            if self.eval_count % self.migration_interval == 0:
                 self._migrate()
                 #Re-evaluate after migration
                 for i in range(self.num_islands):
                    self.island_fitness[i] = objective_function(self.islands[i])
                    self.eval_count += self.population_size
                    self._update_archive(i, self.islands[i], self.island_fitness[i])
                    self._update_best(self.islands[i], self.island_fitness[i])
                 
                 self.migration_rate *= self.migration_decay #Reduce migration


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_islands(self):
        for _ in range(self.num_islands):
            island = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
            self.islands.append(island)
            self.island_fitness.append(np.zeros(self.population_size))  # Placeholder
            self.archives.append([])

    def _de_step(self, island_index, objective_function):
        population = self.islands[island_index]
        new_population = np.copy(population)
        for i in range(self.population_size):
            # Mutation
            indices = np.random.choice(self.population_size, 3, replace=False)
            x_r1, x_r2, x_r3 = population[indices]

            # Archive guided strategy
            if np.random.rand() < 0.3 and len(self.archives[island_index]) > 0:
               archive_member = self._sample_archive(island_index,1)[0]
               v_i = x_r1 + self.f_factor * (archive_member - x_r2) # Sampled from the archive
            else:
              v_i = x_r1 + self.f_factor * (x_r2 - x_r3)
            

            # Crossover
            j_rand = np.random.randint(self.dim)
            for j in range(self.dim):
                if np.random.rand() < self.crossover_rate or j == j_rand:
                    new_population[i, j] = v_i[j]

            new_population[i] = np.clip(new_population[i], self.lower_bounds, self.upper_bounds)

        fitness = objective_function(new_population)

        # Selection
        improved_population = np.copy(population)
        for i in range(self.population_size):
            if fitness[i] < self.island_fitness[island_index][i]:
                improved_population[i] = new_population[i]
                self.island_fitness[island_index][i] = fitness[i]
                
        return improved_population

    def _update_archive(self, island_index, population, fitness_values):
       for i in range(len(population)):
        solution = population[i]
        fitness = fitness_values[i]
        
        combined = np.column_stack((solution, np.array([fitness])))

       
        already_present = any(np.allclose(solution, arch[:-1], atol=1e-6) for arch in self.archives[island_index])
        if not already_present:

            self.archives[island_index].append(np.concatenate((solution, [fitness])))

            self.archives[island_index].sort(key=lambda x: x[-1])
            self.archives[island_index] = self.archives[island_index][:self.archive_size]


    def _sample_archive(self, island_index, sample_size = None):
        if sample_size is None:
          sample_size = self.population_size
        
        if not self.archives[island_index]:
            # If archive is empty, return random solution from the search space
            archive_sample = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(sample_size,self.dim))
            
        elif len(self.archives[island_index]) < sample_size :
            archive_solutions = np.array([arch[:-1] for arch in self.archives[island_index]])
            archive_sample = np.vstack([archive_solutions,np.random.uniform(self.lower_bounds, self.upper_bounds, size=(sample_size-len(self.archives[island_index]),self.dim))])
        else:
            indices = np.random.choice(len(self.archives[island_index]), sample_size, replace=False)
            archive_sample = np.array([self.archives[island_index][i][:-1] for i in indices])

        return archive_sample


    def _migrate(self):
      #Best solutions migrate with decaying rates

      for i in range(self.num_islands):
            #Number of individuals to migrate
            num_migrate = int(self.migration_rate * self.population_size) #Migration rate decays

            #Sort the island and get best
            sorted_indices = np.argsort(self.island_fitness[i])
            migrants = self.islands[i][sorted_indices[:num_migrate]]

            #Distribute migrants to other islands
            other_islands = [j for j in range(self.num_islands) if j != i]
            np.random.shuffle(other_islands)

            for j in range(num_migrate):
                target_island = other_islands[j % len(other_islands)]
                
                #Replace worst individuals
                worst_index = np.argmax(self.island_fitness[target_island])
                self.islands[target_island][worst_index] = migrants[j]
                

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
```

### Rank 3 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandArchiveDE
# Description: Island model DE with adaptive mutation, archive injection and migration.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandArchiveDE
# Description: Island model DE with adaptive mutation, archive injection and migration.

class AdaptiveIslandArchiveDE:
    """
    Implements an island model Differential Evolution algorithm with archive injection
    and adaptive migration rates for exploring multimodal landscapes effectively.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = 5
        self.population_size = 50
        self.archive_size = 100
        self.migration_interval = 500
        self.migration_rate = 0.1
        self.migration_decay = 0.995  #Decaying to focus on exploitation near end

        self.islands = []
        self.island_fitness = []
        self.island_best_fitness = []
        self.island_best_solution = []

        self.archive = []
        self._initialize_islands()

    def _initialize_islands(self):
        for _ in range(self.num_islands):
            population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
            self.islands.append(population)
            self.island_fitness.append(np.full(self.population_size, float('inf'))) # Initialize fitness
            self.island_best_fitness.append(float('inf'))
            self.island_best_solution.append(None)  # No best yet


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                self._evolve_island(i, objective_function)

            if generation % self.migration_interval == 0:
                self._migrate_individuals(objective_function)
                self.migration_rate *= self.migration_decay

            generation += 1


        #Find best solution overall
        for i in range(self.num_islands):
            if self.island_best_fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.island_best_fitness[i]
                self.best_solution_overall = self.island_best_solution[i]


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _evolve_island(self, island_index, objective_function):
        population = self.islands[island_index]
        fitness = self.island_fitness[island_index]

        # Differential Evolution
        for i in range(self.population_size):
            # Mutation
            indices = np.random.choice(self.population_size, 3, replace=False)
            x_r1, x_r2, x_r3 = population[indices]

            # Archive-guided mutation to promote exploration
            if len(self.archive) > 0 and np.random.rand() < 0.2: #inject archive solution
                arch_idx = np.random.randint(len(self.archive))
                x_r1 = self.archive[arch_idx][:-1]  # Use archive vector for x_r1

            mutation_factor = np.random.uniform(0.5, 1.0) # Adaptive F
            crossover_rate = np.random.uniform(0.0, 0.5)
            
            v_i = x_r1 + mutation_factor * (x_r2 - x_r3) # DE mutation

            # Crossover
            u_i = np.where(np.random.rand(self.dim) < crossover_rate, v_i, population[i])
            u_i = np.clip(u_i, self.lower_bounds, self.upper_bounds) # Bound repair

            # Evaluate offspring
            f_u_i = objective_function(u_i.reshape(1, -1))[0]
            self.eval_count += 1

            # Selection
            if f_u_i < fitness[i]:
                population[i] = u_i
                fitness[i] = f_u_i

                # Update island best
                if f_u_i < self.island_best_fitness[island_index]:
                    self.island_best_fitness[island_index] = f_u_i
                    self.island_best_solution[island_index] = u_i

        self.islands[island_index] = population
        self.island_fitness[island_index] = fitness

        # Update Archive
        self._update_archive(population, fitness)
        

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        # Combine old and new archive members
        self.archive.extend(new_archive)
        self.archive = sorted(self.archive, key=lambda x: x[-1])[:self.archive_size]
        self.archive = np.array(self.archive)


    def _migrate_individuals(self, objective_function):
        # Send some individuals between islands

        for _ in range(int(self.population_size * self.migration_rate)): # Migrate some individuals
            source_island = np.random.randint(self.num_islands)
            dest_island = np.random.randint(self.num_islands)
            if source_island == dest_island:
                 continue
            
            immigrant_index = np.random.randint(self.population_size)
            immigrant = self.islands[source_island][immigrant_index]

            #Replace a random individual on destination island
            replace_index = np.random.randint(self.population_size)
            self.islands[dest_island][replace_index] = immigrant

            #Re-evaluate the fitness of the migrated individual.
            migrant_fitness = objective_function(immigrant.reshape(1, -1))[0]
            self.eval_count +=1
            self.island_fitness[dest_island][replace_index] = migrant_fitness

            #Check for island and global update
            if migrant_fitness < self.island_best_fitness[dest_island]:
                self.island_best_fitness[dest_island] = migrant_fitness
                self.island_best_solution[dest_island] = immigrant

```

### Rank 4 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandDEWithArchiveAndDecayingMigration
# Description: Combines island model DE with archive injection and decaying migration for enhanced exploration.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandDEWithArchiveAndDecayingMigration
# Description: Combines island model DE with archive injection and decaying migration for enhanced exploration.

class AdaptiveIslandDEWithArchiveAndDecayingMigration:
    """
    Island model DE with archive-guided mutation and decaying migration, dynamically adjusting migration strength to focus on promising regions.
    """

    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, population_size: int = 50, archive_size: int = 100,
                 initial_migration_rate: float = 0.2, migration_decay_rate: float = 0.995,
                 migration_interval: int = 500, mutation_factor: float = 0.5, crossover_rate: float = 0.7,
                 archive_prob: float = 0.1):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = int(num_islands)
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.initial_migration_rate = float(initial_migration_rate)
        self.migration_decay_rate = float(migration_decay_rate)
        self.migration_interval = int(migration_interval)
        self.mutation_factor = float(mutation_factor)
        self.crossover_rate = float(crossover_rate)
        self.archive_prob = float(archive_prob)  # Probability of sampling from archive

        self.populations = [np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
                            for _ in range(self.num_islands)]
        self.fitness_values = [np.full(self.population_size, float('inf')) for _ in range(self.num_islands)]
        self.archives = [[] for _ in range(self.num_islands)]
        self.migration_rate = self.initial_migration_rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initial evaluation
        for i in range(self.num_islands):
            self.fitness_values[i] = objective_function(self.populations[i])
            self.eval_count += self.population_size
            self._update_archive(i, self.populations[i], self.fitness_values[i])
            self._update_best(self.populations[i], self.fitness_values[i])

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                # Differential Evolution
                new_population = np.copy(self.populations[i])  # Preallocate mem
                for j in range(self.population_size):
                    # Mutation with archive injection
                    if np.random.rand() < self.archive_prob and len(self.archives[i]) > 0:
                        # Select a solution from the archive
                        archive_idx = np.random.randint(len(self.archives[i]))
                        archive_solution = self.archives[i][archive_idx][:-1]  # Exclude fitness from archive entry

                        idxs = np.random.choice(self.population_size, 2, replace=False)
                        mutant = archive_solution + self.mutation_factor * (self.populations[i][idxs[0]] - self.populations[i][idxs[1]])

                    else:
                        # Classic DE mutation
                        idxs = np.random.choice(self.population_size, 3, replace=False)
                        mutant = self.populations[i][idxs[0]] + self.mutation_factor * (self.populations[i][idxs[1]] - self.populations[i][idxs[2]])

                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                    # Crossover
                    crossover_mask = np.random.rand(self.dim) < self.crossover_rate
                    trial_vector = np.where(crossover_mask, mutant, self.populations[i][j])

                    # Evaluation
                    trial_fitness = objective_function(trial_vector.reshape(1, -1))[0]
                    self.eval_count += 1

                    # Selection
                    if trial_fitness < self.fitness_values[i][j]:
                        new_population[j] = trial_vector
                        self.fitness_values[i][j] = trial_fitness
                self.populations[i] = new_population  # Post assignemnt, not inline, for clear code reading.
                self._update_archive(i, self.populations[i], self.fitness_values[i])
                self._update_best(self.populations[i], self.fitness_values[i])

            # Migration
            if generation % self.migration_interval == 0:
                self._migrate()
                self.migration_rate *= self.migration_decay_rate # Decay Migration Rate to exploit

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _update_archive(self, island_index, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archives[island_index])
            if not already_present:
                self.archives[island_index].append(sol)

        self.archives[island_index].sort(key=lambda x: x[-1])  # Sort by fitness
        self.archives[island_index] = self.archives[island_index][:self.archive_size]

    def _migrate(self):
        # Select migrants from each island and distribute them
        migrants = []
        for i in range(self.num_islands):
            num_migrants = int(self.population_size * self.migration_rate)
            indices = np.argsort(self.fitness_values[i])[:num_migrants]
            migrants.append(self.populations[i][indices])

        # Distribute migrants to other islands
        for i in range(self.num_islands):
            receiving_island = (i + 1) % self.num_islands
            num_migrants = migrants[i].shape[0]  # Shape is much faster
            replace_indices = np.random.choice(self.population_size, num_migrants, replace=False)

            self.populations[receiving_island][replace_indices] = migrants[i]
            # Re-evaluate the replaced individuals
            migrants_to_evaluate = migrants[i]
            self.fitness_values[receiving_island][replace_indices] = objective_function(migrants_to_evaluate)
            self.eval_count += num_migrants

            self._update_archive(receiving_island, self.populations[receiving_island], self.fitness_values[receiving_island])


    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
```

### Rank 5 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: EnhancedAdaptiveGaussianArchiveEA
# Description: 
# Code:
```python
import numpy as np

class EnhancedAdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 93.97054407957746,
                 archive_size: int = 153.72088659426225,
                 initial_sigma_fraction: float = 0.6903539621832574,
                 sigma_decay: float = 0.9741366055384242,
                 tournament_size: int = 4.9830262754376795,
                 recombination_sigma_fraction: float = 0.6519391978306551,
                 archive_similarity_threshold: float = 7.285180488871764e-05):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_fraction * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size
        self.recombination_sigma_fraction = recombination_sigma_fraction
        self.archive_similarity_threshold = archive_similarity_threshold

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_fraction, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_fraction, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 17:50:59 INFO Full response text: **Analysis:**
Comparing (best) vs (worst), we see that the best incorporates adaptive island sizes, archive-guided mutation, and diversity control while the worst does not adapt the population sizes to individual island success and uses a simpler Gaussian mutation strategy without dedicated diversity mechanisms. (second best) vs (second worst) ...; both use island models and archives but the second best incorporates adaptive migration and explicit diversity maintenance features, whereas the second worst uses a fixed decaying rate of migration. Comparing (1st) vs (2nd), we see the adaptive island size seems most important, (3rd) vs (4th) ...; The AdaptiveIslandArchiveDE lacks explicit diversity control present in AdaptiveIslandDEWithArchiveAndDecayingMigration, (second worst) vs (worst), we see AdaptiveIslandDEWithArchiveAndDecayingMigration uses a more straightforward archive update rule and fixed parameter configurations instead of adaptively adjusting sigma. Overall: Island models are beneficial for multimodal problems. Archive-guided mutation improves exploration by injecting diverse solutions. Adaptive mechanisms, like adaptive population sizing and migration, help tailor the search to individual island performances. Explicit diversity mechanisms is helpfull too.

**Experience:**
Focus on adaptive multi-population approaches, archive-guided mutation, and maintaining diversity within each island. Adaptive population sizes per island help allocate resources effectively. Adaptive mutation and explicit diversity mechamisms allow the algorithm to better navigate complex landscapes.

2025-06-24 17:51:04 INFO Full response text: Okay, here's a refined approach to self-reflection that aims to generate ideas for better heuristics, avoiding the pitfalls identified in the "Ineffective self-reflection" section.  It focuses on concrete aspects rather than generalizations. We're shooting for actionable insights. I named the class below as `StrategicSelfReflection`.

```python
class StrategicSelfReflection:
    """
    A class to guide self-reflection for designing improved heuristics,
    avoiding common pitfalls and focusing on actionable strategies.
    """

    def __init__(self):
        pass

    def reflect(self):
        """
        Generates guiding principles for improved heuristic design.
        """

        print("Strategic Self-Reflection for Heuristic Design:")
        print("-------------------------------------------")

        print("* **Keywords:** Landscape analysis, problem decomposition, memory mechanisms (archives), mutation adaptation, migration strategies, stagnation detection, resource allocation, diversity measures, fitness correlation.")

        print("* **Advice:** 1. Prioritize landscape analysis to understand problem structure (e.g., ruggedness, deceptiveness, separability) *before* designing heuristics. 2.  Decompose problems into sub-problems suitable for island models, considering communication costs. 3. Design archive update strategies based on solution quality *and* diversity contribution. 4. Develop a rigorous stagnation detection method based on lack of fitness progress and loss of diversity.")

        print("* **Avoid:** 1. Vague statements about 'exploration' and 'exploitation' without concrete mechanisms.  2. Applying techniques blindly (e.g., adding an archive just because it seems beneficial). 3. Focusing solely on algorithm parameters *before* understanding the problem's characteristics. 4. Assuming that multi-population approaches *automatically* guarantee diversity. Explicit diversity measuring should exist. 5. High migration of genetic material could kill diversity between islands. Do not add if not needed!")

        print("* **Explanation:** Effective heuristics require a deep understanding of the problem. Landscape analysis informs the choice of appropriate diversity mechanisms, memory structures, and adaptation strategies. Problem decomposition identifies suitable island configurations. Explicit stagnation detection and resource allocation mechanisms prevent wasted computation. Avoid generic claims about exploration/exploitation; instead, focus on how specific mechanisms achieve these goals *in the context of the problem landscape*.")


# Example usage:
reflector = StrategicSelfReflection()
reflector.reflect()
```

Key improvements and explanations:

*   **Keywords:** A more focused list helps ground the thinking process. Keywords direct attention to relevant areas.
*   **Advice:** Provides specific and actionable guidance, moving beyond general statements. These points give more specific instructions instead of broad generalization that apply to pretty much everything, as the previous version was doing. For example, landscape analysis is suggested at the very beginning, before touching the algorithm parts.
*   **Avoid:** Lists specific traps to avoid when designing or modifying heuristics. These counter 'cargo cult' approaches. We avoid vague ideas that are generally applicable and push towards specificity
*   **Explanation:** Emphasizes the importance of problem understanding *before* algorithm design. It highlights the need for contextual awareness in the application of different mechanisms.

2025-06-24 17:51:04 INFO Generating offspring via Crossover...
2025-06-24 17:52:16 INFO Crossover Prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


### Better code
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Worse code
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Analyze & experience
- Comparing (best) vs (worst), we see that the best incorporates adaptive island sizes, archive-guided mutation, and diversity control while the worst does not adapt the population sizes to individual island success and uses a simpler Gaussian mutation strategy without dedicated diversity mechanisms. (second best) vs (second worst) ...; both use island models and archives but the second best incorporates adaptive migration and explicit diversity maintenance features, whereas the second worst uses a fixed decaying rate of migration. Comparing (1st) vs (2nd), we see the adaptive island size seems most important, (3rd) vs (4th) ...; The AdaptiveIslandArchiveDE lacks explicit diversity control present in AdaptiveIslandDEWithArchiveAndDecayingMigration, (second worst) vs (worst), we see AdaptiveIslandDEWithArchiveAndDecayingMigration uses a more straightforward archive update rule and fixed parameter configurations instead of adaptively adjusting sigma. Overall: Island models are beneficial for multimodal problems. Archive-guided mutation improves exploration by injecting diverse solutions. Adaptive mechanisms, like adaptive population sizing and migration, help tailor the search to individual island performances. Explicit diversity mechanisms is helpfull too.
- Okay, here's a refined approach to self-reflection that aims to generate ideas for better heuristics, avoiding the pitfalls identified in the "Ineffective self-reflection" section.  It focuses on concrete aspects rather than generalizations. We're shooting for actionable insights. I named the class below as `StrategicSelfReflection`.

```python
class StrategicSelfReflection:
    """
    A class to guide self-reflection for designing improved heuristics,
    avoiding common pitfalls and focusing on actionable strategies.
    """

    def __init__(self):
        pass

    def reflect(self):
        """
        Generates guiding principles for improved heuristic design.
        """

        print("Strategic Self-Reflection for Heuristic Design:")
        print("-------------------------------------------")

        print("* **Keywords:** Landscape analysis, problem decomposition, memory mechanisms (archives), mutation adaptation, migration strategies, stagnation detection, resource allocation, diversity measures, fitness correlation.")

        print("* **Advice:** 1. Prioritize landscape analysis to understand problem structure (e.g., ruggedness, deceptiveness, separability) *before* designing heuristics. 2.  Decompose problems into sub-problems suitable for island models, considering communication costs. 3. Design archive update strategies based on solution quality *and* diversity contribution. 4. Develop a rigorous stagnation detection method based on lack of fitness progress and loss of diversity.")

        print("* **Avoid:** 1. Vague statements about 'exploration' and 'exploitation' without concrete mechanisms.  2. Applying techniques blindly (e.g., adding an archive just because it seems beneficial). 3. Focusing solely on algorithm parameters *before* understanding the problem's characteristics. 4. Assuming that multi-population approaches *automatically* guarantee diversity. Explicit diversity measuring should exist. 5. High migration of genetic material could kill diversity between islands. Do not add if not needed!")

        print("* **Explanation:** Effective heuristics require a deep understanding of the problem. Landscape analysis informs the choice of appropriate diversity mechanisms, memory structures, and adaptation strategies. Problem decomposition identifies suitable island configurations. Explicit stagnation detection and resource allocation mechanisms prevent wasted computation. Avoid generic claims about exploration/exploitation; instead, focus on how specific mechanisms achieve these goals *in the context of the problem landscape*.")


# Example usage:
reflector = StrategicSelfReflection()
reflector.reflect()
```

Key improvements and explanations:

*   **Keywords:** A more focused list helps ground the thinking process. Keywords direct attention to relevant areas.
*   **Advice:** Provides specific and actionable guidance, moving beyond general statements. These points give more specific instructions instead of broad generalization that apply to pretty much everything, as the previous version was doing. For example, landscape analysis is suggested at the very beginning, before touching the algorithm parts.
*   **Avoid:** Lists specific traps to avoid when designing or modifying heuristics. These counter 'cargo cult' approaches. We avoid vague ideas that are generally applicable and push towards specificity
*   **Explanation:** Emphasizes the importance of problem understanding *before* algorithm design. It highlights the need for contextual awareness in the application of different mechanisms.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 17:52:16 INFO Mutation prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


Current heuristics:
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- Okay, here's a refined approach to self-reflection that aims to generate ideas for better heuristics, avoiding the pitfalls identified in the "Ineffective self-reflection" section.  It focuses on concrete aspects rather than generalizations. We're shooting for actionable insights. I named the class below as `StrategicSelfReflection`.

```python
class StrategicSelfReflection:
    """
    A class to guide self-reflection for designing improved heuristics,
    avoiding common pitfalls and focusing on actionable strategies.
    """

    def __init__(self):
        pass

    def reflect(self):
        """
        Generates guiding principles for improved heuristic design.
        """

        print("Strategic Self-Reflection for Heuristic Design:")
        print("-------------------------------------------")

        print("* **Keywords:** Landscape analysis, problem decomposition, memory mechanisms (archives), mutation adaptation, migration strategies, stagnation detection, resource allocation, diversity measures, fitness correlation.")

        print("* **Advice:** 1. Prioritize landscape analysis to understand problem structure (e.g., ruggedness, deceptiveness, separability) *before* designing heuristics. 2.  Decompose problems into sub-problems suitable for island models, considering communication costs. 3. Design archive update strategies based on solution quality *and* diversity contribution. 4. Develop a rigorous stagnation detection method based on lack of fitness progress and loss of diversity.")

        print("* **Avoid:** 1. Vague statements about 'exploration' and 'exploitation' without concrete mechanisms.  2. Applying techniques blindly (e.g., adding an archive just because it seems beneficial). 3. Focusing solely on algorithm parameters *before* understanding the problem's characteristics. 4. Assuming that multi-population approaches *automatically* guarantee diversity. Explicit diversity measuring should exist. 5. High migration of genetic material could kill diversity between islands. Do not add if not needed!")

        print("* **Explanation:** Effective heuristics require a deep understanding of the problem. Landscape analysis informs the choice of appropriate diversity mechanisms, memory structures, and adaptation strategies. Problem decomposition identifies suitable island configurations. Explicit stagnation detection and resource allocation mechanisms prevent wasted computation. Avoid generic claims about exploration/exploitation; instead, focus on how specific mechanisms achieve these goals *in the context of the problem landscape*.")


# Example usage:
reflector = StrategicSelfReflection()
reflector.reflect()
```

Key improvements and explanations:

*   **Keywords:** A more focused list helps ground the thinking process. Keywords direct attention to relevant areas.
*   **Advice:** Provides specific and actionable guidance, moving beyond general statements. These points give more specific instructions instead of broad generalization that apply to pretty much everything, as the previous version was doing. For example, landscape analysis is suggested at the very beginning, before touching the algorithm parts.
*   **Avoid:** Lists specific traps to avoid when designing or modifying heuristics. These counter 'cargo cult' approaches. We avoid vague ideas that are generally applicable and push towards specificity
*   **Explanation:** Emphasizes the importance of problem understanding *before* algorithm design. It highlights the need for contextual awareness in the application of different mechanisms.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-24 17:52:39 INFO Perform Harmony Search...
2025-06-24 17:52:48 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:48 ERROR Can not run the algorithm
2025-06-24 17:52:48 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:48 INFO FeHistory: [82599.11289468]
2025-06-24 17:52:48 INFO Expected Optimum FE: -5000
2025-06-24 17:52:49 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:49 ERROR Can not run the algorithm
2025-06-24 17:52:49 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:49 INFO FeHistory: [135135.67264774]
2025-06-24 17:52:49 INFO Expected Optimum FE: -5000
2025-06-24 17:52:49 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:49 ERROR Can not run the algorithm
2025-06-24 17:52:49 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:49 INFO FeHistory: [146811.55577656]
2025-06-24 17:52:49 INFO Expected Optimum FE: -5000
2025-06-24 17:52:49 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:49 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:49 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:49 INFO AOCC mean: 0.0000
2025-06-24 17:52:49 INFO Weighed AOCC mean: nan
2025-06-24 17:52:49 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:49 ERROR Can not run the algorithm
2025-06-24 17:52:50 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:50 INFO FeHistory: [98736.04066887]
2025-06-24 17:52:50 INFO Expected Optimum FE: -5000
2025-06-24 17:52:50 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:50 ERROR Can not run the algorithm
2025-06-24 17:52:50 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:50 INFO FeHistory: [170062.75428719]
2025-06-24 17:52:50 INFO Expected Optimum FE: -5000
2025-06-24 17:52:50 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:50 ERROR Can not run the algorithm
2025-06-24 17:52:50 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:50 INFO FeHistory: [141926.46046971]
2025-06-24 17:52:50 INFO Expected Optimum FE: -5000
2025-06-24 17:52:50 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:50 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:50 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:50 INFO AOCC mean: 0.0000
2025-06-24 17:52:50 INFO Weighed AOCC mean: nan
2025-06-24 17:52:50 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:50 ERROR Can not run the algorithm
2025-06-24 17:52:51 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:51 INFO FeHistory: [109932.88012432]
2025-06-24 17:52:51 INFO Expected Optimum FE: -5000
2025-06-24 17:52:51 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:51 ERROR Can not run the algorithm
2025-06-24 17:52:51 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:51 INFO FeHistory: [127322.7843299]
2025-06-24 17:52:51 INFO Expected Optimum FE: -5000
2025-06-24 17:52:51 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:51 ERROR Can not run the algorithm
2025-06-24 17:52:51 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:51 INFO FeHistory: [165403.99986394]
2025-06-24 17:52:51 INFO Expected Optimum FE: -5000
2025-06-24 17:52:51 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:51 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:51 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:51 INFO AOCC mean: 0.0000
2025-06-24 17:52:51 INFO Weighed AOCC mean: nan
2025-06-24 17:52:51 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:51 ERROR Can not run the algorithm
2025-06-24 17:52:52 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:52 INFO FeHistory: [168259.6944871]
2025-06-24 17:52:52 INFO Expected Optimum FE: -5000
2025-06-24 17:52:52 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:52 ERROR Can not run the algorithm
2025-06-24 17:52:52 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:52 INFO FeHistory: [114111.40255794]
2025-06-24 17:52:52 INFO Expected Optimum FE: -5000
2025-06-24 17:52:52 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:52 ERROR Can not run the algorithm
2025-06-24 17:52:52 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:52 INFO FeHistory: [134505.64113501]
2025-06-24 17:52:52 INFO Expected Optimum FE: -5000
2025-06-24 17:52:52 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:52 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:52 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:52 INFO AOCC mean: 0.0000
2025-06-24 17:52:52 INFO Weighed AOCC mean: nan
2025-06-24 17:52:52 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:52 ERROR Can not run the algorithm
2025-06-24 17:52:53 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:53 INFO FeHistory: [82095.77110207]
2025-06-24 17:52:53 INFO Expected Optimum FE: -5000
2025-06-24 17:52:53 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:53 ERROR Can not run the algorithm
2025-06-24 17:52:53 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:53 INFO FeHistory: [160040.63378135]
2025-06-24 17:52:53 INFO Expected Optimum FE: -5000
2025-06-24 17:52:53 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:53 ERROR Can not run the algorithm
2025-06-24 17:52:53 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:53 INFO FeHistory: [200716.29147621]
2025-06-24 17:52:53 INFO Expected Optimum FE: -5000
2025-06-24 17:52:53 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:53 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:53 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:53 INFO AOCC mean: 0.0000
2025-06-24 17:52:53 INFO Weighed AOCC mean: nan
2025-06-24 17:52:53 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:53 ERROR Can not run the algorithm
2025-06-24 17:52:54 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:54 INFO FeHistory: [83946.90187524]
2025-06-24 17:52:54 INFO Expected Optimum FE: -5000
2025-06-24 17:52:54 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:54 ERROR Can not run the algorithm
2025-06-24 17:52:54 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:54 INFO FeHistory: [180173.99035926]
2025-06-24 17:52:54 INFO Expected Optimum FE: -5000
2025-06-24 17:52:54 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:54 ERROR Can not run the algorithm
2025-06-24 17:52:55 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:55 INFO FeHistory: [181712.65699911]
2025-06-24 17:52:55 INFO Expected Optimum FE: -5000
2025-06-24 17:52:55 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:55 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:55 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:55 INFO AOCC mean: 0.0000
2025-06-24 17:52:55 INFO Weighed AOCC mean: nan
2025-06-24 17:52:55 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:55 ERROR Can not run the algorithm
2025-06-24 17:52:55 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:55 INFO FeHistory: [99781.91121109]
2025-06-24 17:52:55 INFO Expected Optimum FE: -5000
2025-06-24 17:52:55 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:55 ERROR Can not run the algorithm
2025-06-24 17:52:55 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:55 INFO FeHistory: [197300.21967945]
2025-06-24 17:52:55 INFO Expected Optimum FE: -5000
2025-06-24 17:52:55 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:55 ERROR Can not run the algorithm
2025-06-24 17:52:56 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:56 INFO FeHistory: [102318.82474299]
2025-06-24 17:52:56 INFO Expected Optimum FE: -5000
2025-06-24 17:52:56 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:56 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:56 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:56 INFO AOCC mean: 0.0000
2025-06-24 17:52:56 INFO Weighed AOCC mean: nan
2025-06-24 17:52:56 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:56 ERROR Can not run the algorithm
2025-06-24 17:52:56 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:56 INFO FeHistory: [154315.98478059]
2025-06-24 17:52:56 INFO Expected Optimum FE: -5000
2025-06-24 17:52:56 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:56 ERROR Can not run the algorithm
2025-06-24 17:52:56 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:56 INFO FeHistory: [127635.1936912]
2025-06-24 17:52:56 INFO Expected Optimum FE: -5000
2025-06-24 17:52:56 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:56 ERROR Can not run the algorithm
2025-06-24 17:52:57 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:57 INFO FeHistory: [180761.56496462]
2025-06-24 17:52:57 INFO Expected Optimum FE: -5000
2025-06-24 17:52:57 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:57 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:57 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:57 INFO AOCC mean: 0.0000
2025-06-24 17:52:57 INFO Weighed AOCC mean: nan
2025-06-24 17:52:57 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:57 ERROR Can not run the algorithm
2025-06-24 17:52:57 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:57 INFO FeHistory: [127859.34551248]
2025-06-24 17:52:57 INFO Expected Optimum FE: -5000
2025-06-24 17:52:57 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:57 ERROR Can not run the algorithm
2025-06-24 17:52:57 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:57 INFO FeHistory: [101608.09275025]
2025-06-24 17:52:57 INFO Expected Optimum FE: -5000
2025-06-24 17:52:57 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:57 ERROR Can not run the algorithm
2025-06-24 17:52:58 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:58 INFO FeHistory: [183314.33476634]
2025-06-24 17:52:58 INFO Expected Optimum FE: -5000
2025-06-24 17:52:58 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:58 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:58 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:58 INFO AOCC mean: 0.0000
2025-06-24 17:52:58 INFO Weighed AOCC mean: nan
2025-06-24 17:52:58 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:52:58 ERROR Can not run the algorithm
2025-06-24 17:52:58 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:58 INFO FeHistory: [137950.86566081]
2025-06-24 17:52:58 INFO Expected Optimum FE: -5000
2025-06-24 17:52:58 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:52:58 ERROR Can not run the algorithm
2025-06-24 17:52:58 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:58 INFO FeHistory: [148024.00905471]
2025-06-24 17:52:58 INFO Expected Optimum FE: -5000
2025-06-24 17:52:58 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:52:58 ERROR Can not run the algorithm
2025-06-24 17:52:59 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:52:59 INFO FeHistory: [159814.71491293]
2025-06-24 17:52:59 INFO Expected Optimum FE: -5000
2025-06-24 17:52:59 INFO Unimodal AOCC mean: nan
2025-06-24 17:52:59 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:52:59 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:52:59 INFO AOCC mean: 0.0000
2025-06-24 17:52:59 INFO Weighed AOCC mean: nan
2025-06-24 17:52:59 INFO Generation 4, best so far: 0.2796149162371689
2025-06-24 17:52:59 INFO Population length is: 6
2025-06-24 17:52:59 INFO --- Performing Long-Term Reflection at Generation 5 ---
2025-06-24 17:52:59 INFO Reflection Prompt: ### Problem Description
**(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.

### List heuristics
Below is a list of design heuristics ranked from best to worst based on their AOCC score on group function 3 of GNBG benchmark, where higher is better. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandDEWithGaussianLocalSearch
# Description: Island-based DE with adaptive migration and Gaussian local search to refine promising solutions.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandDEWithGaussianLocalSearch
# Description: Island-based DE with adaptive migration and Gaussian local search to refine promising solutions.

class AdaptiveIslandDEWithGaussianLocalSearch:
    """
    Island model DE with adaptive migration, archive injection, decaying migration,
    and Gaussian local search within islands to exploit promising solutions.
    """

    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, population_size: int = 50, archive_size: int = 100,
                 initial_migration_rate: float = 0.2, migration_decay_rate: float = 0.995,
                 migration_interval: int = 500, mutation_factor: float = 0.5, crossover_rate: float = 0.7,
                 archive_prob: float = 0.1, local_search_prob: float = 0.1, local_search_sigma_scale: float = 0.01):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = int(num_islands)
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.initial_migration_rate = float(initial_migration_rate)
        self.migration_decay_rate = float(migration_decay_rate)
        self.migration_interval = int(migration_interval)
        self.mutation_factor = float(mutation_factor)
        self.crossover_rate = float(crossover_rate)
        self.archive_prob = float(archive_prob)  # Probability of sampling from archive
        self.local_search_prob = float(local_search_prob)
        self.local_search_sigma_scale = float(local_search_sigma_scale)

        self.populations = [np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
                            for _ in range(self.num_islands)]
        self.fitness_values = [np.full(self.population_size, float('inf')) for _ in range(self.num_islands)]
        self.archives = [[] for _ in range(self.num_islands)]
        self.migration_rate = self.initial_migration_rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initial evaluation
        for i in range(self.num_islands):
            self.fitness_values[i] = objective_function(self.populations[i])
            self.eval_count += self.population_size
            self._update_archive(i, self.populations[i], self.fitness_values[i])
            self._update_best(self.populations[i], self.fitness_values[i])

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                # Differential Evolution
                new_population = np.copy(self.populations[i])
                for j in range(self.population_size):
                    # Mutation with archive injection
                    if np.random.rand() < self.archive_prob and len(self.archives[i]) > 0:
                        # Select a solution from the archive
                        archive_idx = np.random.randint(len(self.archives[i]))
                        archive_solution = self.archives[i][archive_idx][:-1]  # Exclude fitness from archive entry

                        idxs = np.random.choice(self.population_size, 2, replace=False)
                        mutant = archive_solution + self.mutation_factor * (self.populations[i][idxs[0]] - self.populations[i][idxs[1]])

                    else:
                        # Classic DE mutation
                        idxs = np.random.choice(self.population_size, 3, replace=False)
                        mutant = self.populations[i][idxs[0]] + self.mutation_factor * (self.populations[i][idxs[1]] - self.populations[i][idxs[2]])

                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                    # Crossover
                    crossover_mask = np.random.rand(self.dim) < self.crossover_rate
                    trial_vector = np.where(crossover_mask, mutant, self.populations[i][j])

                    # Local search
                    if np.random.rand() < self.local_search_prob:
                        trial_vector = self._gaussian_local_search(trial_vector, objective_function)

                    # Evaluation
                    trial_fitness = objective_function(trial_vector.reshape(1, -1))[0]
                    self.eval_count += 1

                    # Selection
                    if trial_fitness < self.fitness_values[i][j]:
                        new_population[j] = trial_vector
                        self.fitness_values[i][j] = trial_fitness

                self.populations[i] = new_population
                self._update_archive(i, self.populations[i], self.fitness_values[i])
                self._update_best(self.populations[i], self.fitness_values[i])

            # Migration
            if generation % self.migration_interval == 0:
                self._migrate()
                self.migration_rate *= self.migration_decay_rate # Decay Migration Rate

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _gaussian_local_search(self, solution, objective_function):
      """Perform Gaussian local search around a solution."""
      sigma = self.local_search_sigma_scale * (self.upper_bounds - self.lower_bounds)
      new_solution = solution + np.random.normal(0, sigma, self.dim)
      new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
      new_fitness = objective_function(new_solution.reshape(1, -1))[0]
      self.eval_count += 1

      original_fitness = objective_function(solution.reshape(1, -1))[0]
      self.eval_count += 1

      if new_fitness < original_fitness:
          return new_solution
      else:
          return solution

    def _update_archive(self, island_index, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archives[island_index])
            if not already_present:
                self.archives[island_index].append(sol)

        self.archives[island_index].sort(key=lambda x: x[-1])  # Sort by fitness
        self.archives[island_index] = self.archives[island_index][:self.archive_size]

    def _migrate(self):
        # Select migrants from each island and distribute them
        migrants = []
        for i in range(self.num_islands):
            num_migrants = int(self.population_size * self.migration_rate)
            indices = np.argsort(self.fitness_values[i])[:num_migrants]
            migrants.append(self.populations[i][indices])

        # Distribute migrants to other islands
        for i in range(self.num_islands):
            receiving_island = (i + 1) % self.num_islands
            num_migrants = migrants[i].shape[0]
            replace_indices = np.random.choice(self.population_size, num_migrants, replace=False)

            self.populations[receiving_island][replace_indices] = migrants[i]
            # Re-evaluate the replaced individuals
            migrants_to_evaluate = migrants[i]
            self.fitness_values[receiving_island][replace_indices] = objective_function(migrants_to_evaluate)
            self.eval_count += num_migrants

            self._update_archive(receiving_island, self.populations[receiving_island], self.fitness_values[receiving_island])

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]
```

### Rank 2 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandDEWithArchiveDiversityAndPopulationAdaptation
# Description: Combines island model DE with archive-guided mutation, dynamic population sizing, and diversity maintenance.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandDEWithArchiveDiversityAndPopulationAdaptation
# Description: Combines island model DE with archive-guided mutation, dynamic population sizing, and diversity maintenance.

class AdaptiveIslandDEWithArchiveDiversityAndPopulationAdaptation:
    """
    Island model DE with archive-guided mutation, dynamic population sizing, and diversity maintenance
    for robust global exploration in multi-modal landscapes.
    """

    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, initial_population_size: int = 50, archive_size: int = 100,
                 initial_migration_rate: float = 0.2, migration_decay_rate: float = 0.995,
                 migration_interval: int = 500, mutation_factor: float = 0.5, crossover_rate: float = 0.7,
                 archive_prob: float = 0.1, diversity_threshold: float = 0.01, population_adaptation_rate: float = 0.05):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = int(num_islands)
        self.initial_population_size = int(initial_population_size)
        self.population_sizes = [int(initial_population_size) for _ in range(self.num_islands)]  # Dynamic population sizes
        self.archive_size = int(archive_size)
        self.initial_migration_rate = float(initial_migration_rate)
        self.migration_decay_rate = float(migration_decay_rate)
        self.migration_interval = int(migration_interval)
        self.mutation_factor = float(mutation_factor)
        self.crossover_rate = float(crossover_rate)
        self.archive_prob = float(archive_prob)  # Probability of sampling from archive
        self.diversity_threshold = float(diversity_threshold) # Threshold for triggering population increase.
        self.population_adaptation_rate = float(population_adaptation_rate)

        self.populations = [np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_sizes[i], self.dim))
                            for i in range(self.num_islands)]
        self.fitness_values = [np.full(self.population_sizes[i], float('inf')) for i in range(self.num_islands)]
        self.archives = [[] for _ in range(self.num_islands)]
        self.migration_rate = self.initial_migration_rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initial evaluation
        for i in range(self.num_islands):
            self.fitness_values[i] = objective_function(self.populations[i])
            self.eval_count += self.population_sizes[i]
            self._update_archive(i, self.populations[i], self.fitness_values[i])
            self._update_best(self.populations[i], self.fitness_values[i])

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                # Differential Evolution
                new_population = np.copy(self.populations[i])
                for j in range(self.population_sizes[i]):
                    # Mutation with archive injection
                    if np.random.rand() < self.archive_prob and len(self.archives[i]) > 0:
                        # Select a solution from the archive
                        archive_idx = np.random.randint(len(self.archives[i]))
                        archive_solution = self.archives[i][archive_idx][:-1]  # Exclude fitness from archive entry

                        idxs = np.random.choice(self.population_sizes[i], 2, replace=False)
                        mutant = archive_solution + self.mutation_factor * (self.populations[i][idxs[0]] - self.populations[i][idxs[1]])

                    else:
                        # Classic DE mutation
                        idxs = np.random.choice(self.population_sizes[i], 3, replace=False)
                        mutant = self.populations[i][idxs[0]] + self.mutation_factor * (self.populations[i][idxs[1]] - self.populations[i][idxs[2]])

                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                    # Crossover
                    crossover_mask = np.random.rand(self.dim) < self.crossover_rate
                    trial_vector = np.where(crossover_mask, mutant, self.populations[i][j])

                    # Evaluation
                    trial_fitness = objective_function(trial_vector.reshape(1, -1))[0]
                    self.eval_count += 1

                    # Selection
                    if trial_fitness < self.fitness_values[i][j]:
                        new_population[j] = trial_vector
                        self.fitness_values[i][j] = trial_fitness

                self.populations[i] = new_population
                self._update_archive(i, self.populations[i], self.fitness_values[i])
                self._update_best(self.populations[i], self.fitness_values[i])

            # Migration
            if generation % self.migration_interval == 0:
                self._migrate()
                self.migration_rate *= self.migration_decay_rate  # Decay Migration Rate to exploit

            # Population adaptation
            self._adapt_population_sizes()

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _update_archive(self, island_index, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archives[island_index])
            if not already_present:
                self.archives[island_index].append(sol)

        self.archives[island_index].sort(key=lambda x: x[-1])  # Sort by fitness
        self.archives[island_index] = self.archives[island_index][:self.archive_size]

    def _migrate(self):
        # Select migrants from each island and distribute them
        migrants = []
        for i in range(self.num_islands):
            num_migrants = int(self.population_sizes[i] * self.migration_rate)
            indices = np.argsort(self.fitness_values[i])[:num_migrants]
            migrants.append(self.populations[i][indices])

        # Distribute migrants to other islands
        for i in range(self.num_islands):
            receiving_island = (i + 1) % self.num_islands
            num_migrants = migrants[i].shape[0]  # Use .shape for faster access
            replace_indices = np.random.choice(self.population_sizes[receiving_island], num_migrants, replace=False)

            self.populations[receiving_island][replace_indices] = migrants[i]
            # Re-evaluate the replaced individuals
            migrants_to_evaluate = migrants[i]
            self.fitness_values[receiving_island][replace_indices] = objective_function(migrants_to_evaluate)
            self.eval_count += num_migrants

            self._update_archive(receiving_island, self.populations[receiving_island], self.fitness_values[receiving_island])

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]

    def _calculate_diversity(self, population):
        """Calculates the average distance between individuals in the population."""
        if len(population) <= 1:
            return 1.0  # Maximum diversity if only one individual

        distances = []
        for i in range(len(population)):
            for j in range(i + 1, len(population)):
                distances.append(np.linalg.norm(population[i] - population[j]))
        return np.mean(distances)

    def _adapt_population_sizes(self):
        """Dynamically adjust population sizes based on island performance and diversity."""
        total_fitness = sum([np.mean(self.fitness_values[i]) for i in range(self.num_islands)])
        if total_fitness == 0:  # Avoid division by zero in case all islands have fitness 0
            island_fitness_proportions = [1 / self.num_islands] * self.num_islands  # Equal distribution
        else:
            island_fitness_proportions = [np.mean(self.fitness_values[i]) / total_fitness for i in range(self.num_islands)]
        
        for i in range(self.num_islands):
            diversity = self._calculate_diversity(self.populations[i])

            # Increase population if diversity is low and island is promising
            if diversity < self.diversity_threshold and island_fitness_proportions[i] < np.mean(island_fitness_proportions): # Promote higher exploitation in well doing islands
                self.population_sizes[i] = min(int(self.population_sizes[i] * (1 + self.population_adaptation_rate)), self.initial_population_size * 3)  # Cap population size
                new_individuals = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_sizes[i] - len(self.populations[i]), self.dim))
                self.populations[i] = np.vstack((self.populations[i], new_individuals))

                new_fitness_values = objective_function(new_individuals)
                self.fitness_values[i] = np.concatenate((self.fitness_values[i],new_fitness_values))
                self.eval_count += len(new_individuals)

                self._update_archive(i,self.populations[i], self.fitness_values[i])
                #self._update_best(self.populations[i], self.fitness_values[i]) Update in main for loop rather here to reuse already written logic

            # Reduce population if the island is stagnating and isn't contributing.
            elif island_fitness_proportions[i] > np.mean(island_fitness_proportions) and diversity > self.diversity_threshold: # and island_fitness_proportions[i] > np.mean(island_fitness_proportions):# Added a fitness consideration
                self.population_sizes[i] = max(int(self.population_sizes[i] * (1 - self.population_adaptation_rate)), self.initial_population_size // 5) # Reduce and respect minimal pop size to continue DE
                indices_to_keep = np.argsort(self.fitness_values[i])[:self.population_sizes[i]]  #Keep the best only
                self.populations[i] = self.populations[i][indices_to_keep]
                self.fitness_values[i] = self.fitness_values[i][indices_to_keep]
```

### Rank 3 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandDEWithAdaptiveP
# Description: Island model DE with adaptive population sizes, archive, and dynamic migration.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandDEWithAdaptiveP
# Description: Island model DE with adaptive population sizes, archive, and dynamic migration.

class AdaptiveIslandDEWithAdaptiveP:
    """
    Island model DE that adapts population sizes based on island performance, maintains an archive for exploration,
    and uses a decaying migration rate to balance exploration and exploitation in complex landscapes.
    """

    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = 5
        self.base_population_size = 50
        self.island_population_sizes = [self.base_population_size] * self.num_islands
        self.archive_size = 100
        self.migration_interval = 500
        self.initial_migration_rate = 0.2
        self.migration_rate = self.initial_migration_rate
        self.migration_decay = 0.995
        self.stagnation_threshold = 1000  # Generations without improvement to trigger re-initialization
        self.stagnation_counter = [0] * self.num_islands  # Counter for stagnation on each island
        self.population_size_adaptation_rate = 0.05

        self.islands = []
        self.island_fitness = []
        self.island_best_fitness = []
        self.island_best_solution = []

        self.archive = []
        self._initialize_islands()

    def _initialize_islands(self):
        for _ in range(self.num_islands):
            population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.base_population_size, self.dim))
            self.islands.append(population)
            self.island_fitness.append(np.full(self.base_population_size, float('inf')))
            self.island_best_fitness.append(float('inf'))
            self.island_best_solution.append(None)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                self._evolve_island(i, objective_function)

            if generation % self.migration_interval == 0:
                self._migrate_individuals(objective_function)
                self.migration_rate *= self.migration_decay

            self._adapt_population_sizes(objective_function)

            generation += 1

        # Find best solution overall
        for i in range(self.num_islands):
            if self.island_best_fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.island_best_fitness[i]
                self.best_solution_overall = self.island_best_solution[i]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _evolve_island(self, island_index, objective_function):
        population = self.islands[island_index]
        fitness = self.island_fitness[island_index]
        population_size = len(population) #Dynamic size.

        # Differential Evolution
        for i in range(population_size):
            # Mutation
            indices = np.random.choice(population_size, 3, replace=False)
            x_r1, x_r2, x_r3 = population[indices]

            # Archive-guided mutation
            if len(self.archive) > 0 and np.random.rand() < 0.2: # Archive injection
                arch_idx = np.random.randint(len(self.archive))
                x_r1 = self.archive[arch_idx][:-1] # Use archive vector

            mutation_factor = np.random.uniform(0.5, 1.0) # Adaptive F
            crossover_rate = np.random.uniform(0.0, 0.5)

            v_i = x_r1 + mutation_factor * (x_r2 - x_r3) # DE mutation

            # Crossover
            u_i = np.where(np.random.rand(self.dim) < crossover_rate, v_i, population[i])
            u_i = np.clip(u_i, self.lower_bounds, self.upper_bounds) # Bound repair

            # Evaluate offspring
            f_u_i = objective_function(u_i.reshape(1, -1))[0]
            self.eval_count += 1

            # Selection
            if f_u_i < fitness[i]:
                population[i] = u_i
                fitness[i] = f_u_i
                self.stagnation_counter[island_index] = 0 # reset the stagnation

                # Update island best
                if f_u_i < self.island_best_fitness[island_index]:
                    self.island_best_fitness[island_index] = f_u_i
                    self.island_best_solution[island_index] = u_i
            else:
                self.stagnation_counter[island_index]+=1
                if self.stagnation_counter[island_index] > self.stagnation_threshold:
                    # Re-initialize the island population
                    population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(population_size, self.dim))
                    fitness = np.full(population_size, float('inf'))  # Reset fitness values
                    self.stagnation_counter[island_index] = 0
                    # Evaluate and update fitness values
                    fitness = objective_function(population)
                    self.eval_count += population_size
                

        self.islands[island_index] = population
        self.island_fitness[island_index] = fitness
        #self._update_island(island_index, population, fitness, objective_function)


        # Update Archive
        self._update_archive(population, fitness)

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        # Combine old and new archive members
        self.archive.extend(new_archive)
        self.archive = sorted(self.archive, key=lambda x: x[-1])[:self.archive_size]
        self.archive = np.array(self.archive)

    def _migrate_individuals(self, objective_function):

        for _ in range(int(self.base_population_size * self.migration_rate)): # Migrate some individuals
            source_island = np.random.randint(self.num_islands)
            dest_island = np.random.randint(self.num_islands)
            if source_island == dest_island:
                 continue

            source_population_size = len(self.islands[source_island])
            immigrant_index = np.random.randint(source_population_size)
            immigrant = self.islands[source_island][immigrant_index]

            #Replace a random individual on destination island
            dest_population_size = len(self.islands[dest_island])
            replace_index = np.random.randint(dest_population_size)
            self.islands[dest_island][replace_index] = immigrant

            #Re-evaluate the fitness of the migrated individual.
            migrant_fitness = objective_function(immigrant.reshape(1, -1))[0]
            self.eval_count +=1
            self.island_fitness[dest_island][replace_index] = migrant_fitness

            #Check for island and global update
            if migrant_fitness < self.island_best_fitness[dest_island]:
                self.island_best_fitness[dest_island] = migrant_fitness
                self.island_best_solution[dest_island] = immigrant

    def _adapt_population_sizes(self, objective_function):
        """Adapts population sizes based on island performance."""
        island_fitness_ranks = np.argsort(self.island_best_fitness) #lower better

        # Increase population size for better-performing islands, ensure each island has at least dim + 1 individuals
        for i in range(self.num_islands // 2):  # Increase population size of top 50% performing islands
            island_index = island_fitness_ranks[i]
            increase = int(self.island_population_sizes[island_index] * self.population_size_adaptation_rate) + 1
            new_size = max(self.dim + 1, self.island_population_sizes[island_index] + increase)  # Ensure at least dim + 1
            
            if new_size > self.budget // self.num_islands:
              new_size = self.budget // self.num_islands
             
            self.island_population_sizes[island_index] = new_size
            self._resize_island(island_index, objective_function)

        # Decrease population size for poor-performing islands
        for i in range(self.num_islands // 2, self.num_islands): # Last 50%
            island_index = island_fitness_ranks[i]
            decrease = int(self.island_population_sizes[island_index] * self.population_size_adaptation_rate) + 1
            new_size = max(self.dim + 1, self.island_population_sizes[island_index] - decrease)  # Ensure at least dim + 1
            
            self.island_population_sizes[island_index] = new_size
            self._resize_island(island_index, objective_function)

    def _resize_island(self, island_index, objective_function):
        """Resizes an island's population, re-evaluating fitness where needed."""

        current_population = self.islands[island_index]
        current_fitness = self.island_fitness[island_index]
        current_size = len(current_population)
        new_size = self.island_population_sizes[island_index]
        
        if new_size == current_size:
            return  # No resizing needed
        

        if new_size > current_size: #Enlarge population
           new_individuals = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(new_size - current_size, self.dim))
           new_fitness = objective_function(new_individuals)
           self.eval_count += len(new_individuals)

           self.islands[island_index] = np.vstack((current_population, new_individuals))
           self.island_fitness[island_index] = np.concatenate((current_fitness, new_fitness))
        else: # reduce the size. Sort by fitness
           sorted_indices = np.argsort(current_fitness)[:new_size]  # Trim worse individuals
           self.islands[island_index] = current_population[sorted_indices]
           self.island_fitness[island_index] = current_fitness[sorted_indices]

        # Update island best if needed
        best_index = np.argmin(self.island_fitness[island_index])
        self.island_best_fitness[island_index] = self.island_fitness[island_index][best_index]
        self.island_best_solution[island_index] = self.islands[island_index][best_index]
```

### Rank 4 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: IslandArchiveAdaptiveGaussianEA
# Description: Employs an island model with adaptive population sizing, archive-guided mutation, and a stagnation detection mechanism to explore multimodal landscapes aggressively.
# Code:
```python
import numpy as np

# Name: IslandArchiveAdaptiveGaussianEA
# Description: Employs an island model with adaptive population sizing, archive-guided mutation, and a stagnation detection mechanism to explore multimodal landscapes aggressively.

class IslandArchiveAdaptiveGaussianEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """
        Initializes the IslandArchiveAdaptiveGaussianEA algorithm.
        """
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = 5
        self.island_sizes = np.full(self.num_islands, int(100 / self.num_islands))
        self.archive_size = 200
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.98
        self.archives = [[] for _ in range(self.num_islands)]
        self.migration_probability = 0.1
        self.stagnation_threshold = 1000  # Evaluations without improvement
        self.stagnation_counts = np.zeros(self.num_islands)
        self.diversity_threshold = 0.01 # required normalized diversity for restart
        self.archive_elitism_rate = 0.2

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """
        Optimizes the given objective function using an island model.
        """
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        islands = self._initialize_islands()
        fitness_values = self._evaluate_islands(islands, objective_function)

        for i in range(self.num_islands):
            self.archives[i] = self._update_archive(islands[i], fitness_values[i], self.archives[i])

        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                parents = self._tournament_selection(islands[i], fitness_values[i])
                offspring = self._gaussian_recombination(parents, i)
                offspring = self._archive_guided_mutation(offspring, i) #Archive guided
                offspring_fitness = objective_function(offspring)
                self.eval_count += len(offspring)
                islands[i], fitness_values[i] = self._select_next_generation(
                    islands[i], fitness_values[i], offspring, offspring_fitness
                )
                self.archives[i] = self._update_archive(np.vstack((islands[i], offspring)), np.concatenate((fitness_values[i], offspring_fitness)), self.archives[i])
                self._update_best(offspring, offspring_fitness)
                self.sigma *= self.sigma_decay
                self.stagnation_counts[i] += len(offspring)

            self._migrate_individuals(islands, fitness_values)
            self._adjust_island_sizes(fitness_values)  # Adaptive island sizing
            self._handle_stagnation(islands, fitness_values, objective_function)  #Checks stagnation
            
        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_islands(self):
        """Initializes the populations for each island."""
        islands = []
        for size in self.island_sizes:
            island = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(size, self.dim))
            islands.append(island)
        return islands

    def _evaluate_islands(self, islands, objective_function):
        """Evaluates the populations of each island."""
        fitness_values = []
        for island in islands:
            fitness = objective_function(island)
            self.eval_count += len(island)
            fitness_values.append(fitness)
        return fitness_values

    def _tournament_selection(self, population, fitness_values):
        """Selects parents using tournament selection."""
        tournament_size = 5
        num_parents = len(population) // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents, island_index):
         """Performs Gaussian recombination on selected parents."""
         offspring = []
         for i in range(0, len(parents), 2):
            if i + 1 >= len(parents):
                parent1 = parents[i]
                parent2 = parents[i] #In case only one parent avaliable, clone it.
            else:
                parent1 = parents[i]
                parent2 = parents[i + 1]
                
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
         return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _archive_guided_mutation(self, offspring, island_index):
        """Mutates offspring, guided by solutions in the archive."""
        if len(self.archives[island_index]) == 0:
            mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
            return np.clip(mutated, self.lower_bounds, self.upper_bounds)

        archive_sol = self.archives[island_index][np.random.randint(len(self.archives[island_index]))][:-1]  # Exclude fitness
        mutation_direction = archive_sol - offspring
        mutated = offspring + np.random.normal(mutation_direction, self.sigma, size=offspring.shape)

        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        """Selects the next generation using elitism."""
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:len(population)]]  # Keep island size constant
        next_fit = combined_fit[sorted_indices[:len(population)]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        """Updates the overall best solution."""
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values, archive):
        """Updates the archive with new solutions, maintaining diversity."""
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])

        updated_archive = np.vstack((archive, new_archive)) if len(archive) > 0 and len(new_archive)> 0 else np.array(archive) if len(new_archive) == 0 else np.array(new_archive)
        
        # Archive Elitism: Always keep the best X% from the PREVIOUS archive:
        archive = archive[np.argsort(archive[:, -1])][:int(self.archive_elitism_rate*self.archive_size)] if len(archive) > 0 else []  # Keep best elitism solutions
        
        #Add new ones if len(archive) < self.archive_size

        #Add best (up to) archive_size - len(archive), according to updated_archive.
        how_many = self.archive_size - len(archive) if len(archive) <= self.archive_size else 0

        if(len(updated_archive) > 0 and how_many > 0):
          updated_archive.sort(axis = 0, key=lambda x: x[-1])#fitness
          best_new = updated_archive[:how_many]
          archive = np.vstack((archive, best_new)) if len(archive) > 0 else np.array(best_new)

        if(len(archive) > 0):
          archive = archive[np.argsort(archive[:, -1])]  #Best elements at the top
          return archive[:self.archive_size] if len(archive) >= self.archive_size else archive
        else:
          return np.array([])

    def _migrate_individuals(self, islands, fitness_values):
        """Migrates individuals between islands with a certain probability."""
        for i in range(self.num_islands):
            if np.random.rand() < self.migration_probability:
                # Select a random island to migrate to
                j = np.random.randint(self.num_islands)
                while j == i:
                    j = np.random.randint(self.num_islands)

                # Select a random individual to migrate
                migrant_index = np.random.randint(len(islands[i]))
                migrant = islands[i][migrant_index].copy()
                migrant_fitness = fitness_values[i][migrant_index].copy()

                # Replace a random individual on the target island
                replace_index = np.random.randint(len(islands[j]))
                islands[j][replace_index] = migrant
                fitness_values[j][replace_index] = migrant_fitness

    def _adjust_island_sizes(self, fitness_values):
         """Adaptively adjust island sizes based on average fitness."""
         avg_fitnesses = [np.mean(fitness) for fitness in fitness_values]
         total_fitness = sum(avg_fitnesses)

         if total_fitness == 0:
            # Prevent division by zero
            probabilities = np.ones(self.num_islands) / self.num_islands
         else:
            probabilities = [fitness / total_fitness for fitness in avg_fitnesses]

         new_sizes = np.round(probabilities * 100).astype(int) #Recalculate size from fitness

         # Ensure total population size remains the same
         diff = np.sum(new_sizes) - 100
         if diff > 0: #reduce size until reach budget size
            for _ in range(diff): #reduce until diff = 0
               index_to_reduce = np.argmax(new_sizes) #island with bigger population number
               new_sizes[index_to_reduce] -= 1 #removing element
         elif diff < 0: #add size until reach budget size
            for _ in range(abs(diff)):
               index_to_increase = np.argmin(new_sizes) #island with smaller population number
               new_sizes[index_to_increase] += 1  #adding element

         self.island_sizes = new_sizes #update island sizes

         # Resize Islands
         for i in range(self.num_islands):
            current_size = len(islands[i])
            new_size = self.island_sizes[i]

            if current_size < new_size:
               # Add random solutions
               num_to_add = new_size - current_size
               new_solutions = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(num_to_add, self.dim))
               islands[i] = np.vstack((islands[i], new_solutions))
               new_fitness = objective_function(new_solutions) #Calculate fitness to those solutions
               self.eval_count += len(new_solutions)
               fitness_values[i] = np.concatenate((fitness_values[i], new_fitness))
            elif current_size > new_size:
               # Remove worst solutions
               num_to_remove = current_size - new_size
               worst_indices = np.argsort(fitness_values[i])[-num_to_remove:]
               mask = np.ones(current_size, dtype=bool)
               mask[worst_indices] = False
               islands[i] = islands[i][mask]
               fitness_values[i] = fitness_values[i][mask]

    def _handle_stagnation(self, islands, fitness_values, objective_function):
         """Checks if an island is stagnated and, if so, restarts it."""
         for i in range(self.num_islands):
            if self.stagnation_counts[i] > self.stagnation_threshold: # if an islan is stangnated (no changes in fitness)
                
               #Diversity check
               normalized_dist = self._calculate_population_diversity(islands[i]) #population diversity (normalized by dimension size)
               if normalized_dist < self.diversity_threshold:
                  print(f"re-init island {i}")
                  # Re-initialize the island
                  islands[i] = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.island_sizes[i], self.dim))
                  fitness_values[i] = objective_function(islands[i])
                  self.eval_count += len(islands[i])
                  self.archives[i] = [] #Reset archive
                  self.stagnation_counts[i] = 0 #reset counter

    def _calculate_population_diversity(self, population):
       """Euclidean distance of each individual from the population centroid"""
       centroid = np.mean(population, axis=0) #Find the "center" of the population
       distances = np.linalg.norm(population - centroid, axis=1) # L2 norm
       average_distance = np.mean(distances) #Average distance each element has from population centroid

       #To measure diversity, we "normalize" the Euclidean distance
       #Larger search spaces (dimensions) will have larger diversity numbers
       normalized_distance = average_distance / (np.sqrt(self.dim)) #Euclidean Distance Normalization

       return normalized_distance #normalize the diversity

```

### Rank 5 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveGaussianArchiveEA
# Description: 
# Code:
```python
import numpy as np

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 81.05823042615071, archive_size: int = 204.4744784358444, sigma_initial_factor: float = 0.30079161908612717,
                 sigma_decay: float = 0.9335308937814801, tournament_size: int = 7.463923399222831, recombination_sigma_factor: float = 0.2273446819921407,
                 archive_similarity_threshold: float = 9.362065512732052e-06):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_initial_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size
        self.recombination_sigma_factor = recombination_sigma_factor
        self.archive_similarity_threshold = archive_similarity_threshold

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 17:53:05 INFO Full response text: **Analysis:**

Comparing (Rank 1) vs (Rank 5), we see `AdaptiveIslandDEWithGaussianLocalSearch` focuses on island-based DE, adaptive migration, and Gaussian local search for refinement; while `AdaptiveGaussianArchiveEA` uses adaptive Gaussian sampling with an archive and tournament selection for efficiency. The Rank 1's local search component provides exploitative benefits, while the Rank 5's simplified mutation maintains diversity. Also the rank 1 algorithm introduce migration and isalnd based optimisation.

Comparing (Rank 2) vs (Rank 4), we see `AdaptiveIslandDEWithArchiveDiversityAndPopulationAdaptation` combines island DE with archive-guided mutation, dynamic population sizing, and diversity maintenance, while `IslandArchiveAdaptiveGaussianEA` employs an island model with adaptive population sizing, archive-guided mutation, and stagnation detection. Rank 2 explicitly manages diversity and population size adaptively, promoting exploration through dynamic adjustments whereas the population size adaptation relies on average fitness, it reduces population if no changes occur in fitness.
(Rank 2's diversity is also checked within its adaptation step). And both rely on archive guide mutation and island population sizes.

Comparing (Rank 1) vs (Rank 2), we see Rank 1 uses a decaying migration rate combined with local search, while Rank 2 includes dynamic population sizing based on diversity and archive-guided mutation (using stagnation/normal population distribution ratio, etc). The diversity calculations and population adaptation contribute to Rank 2's stronger exploratory behaviour.

Comparing (Rank 3) vs (Rank 4), we see Rank 3 uses adaptive population sizes, an archive, and dynamic migration while Rank 4 employs an island model with adaptive population sizing, archive-guided mutation, and stagnation detection.. Both are island-based. While the Rank 3 stagnates due to too simple logic to reset the island. And doesn't contain population diversity check for that specific island while the Rank 4 it does have.

Comparing (Rank 4) vs (Rank 5), we see the integration of island model within Rank 4 leads to better exploitative and explorative advantages. The dynamic adaptation of archive, stagnation check is not found in Rank 5. Rank 5 relies less on restarts within it's logic.


Overall:

Effective strategies for deceptive landscapes involve multi-population approaches that emphasize both diversity and exploitation. Island models combined with archive-guided mutation are prevalent, adapting migration and population sizes dynamically. Stagnation detection and re-initialization help prevent premature convergence.

**Experience:**

Island-based algorithms with adaptive components and archives can effectively tackle multi-modal problems. Dynamic population sizing, guided mutation (archives), and migration strategies that responds to diversity (stagnation detection and or diversity thresholds.)  Also simple stochastic reset increases exploration.

2025-06-24 17:53:08 INFO Full response text: Okay, I understand the task. We need to refine the concept of "Current self-reflection" to focus on building better heuristics for multimodal optimization, specifically *avoiding* the pitfalls and vague statements present in the "Ineffective self-reflection" examples.

Here's a breakdown of a refined approach, designed to steer us towards concrete heuristic design improvements:

*   **Keywords:** *Diversity maintenance, Adaptive operators, Island model granularity, Convergence monitoring*. The "Ineffective self-reflection" mentions these concepts but doesn't explain how to make them specific or manageable. We will provide clear guidelines on this.

*   **Advice:** Focus on: (1) *Quantifying* diversity *effectively*. Implement a diversity metric that isn't easily fooled and ties directly to optimization goals (e.g., distance to known optima or coverage of the search space). (2) Tune operator adaptation *parameters on the fly* based on observable trends. This requires a well-defined, *local* adaptation scheme, not a global one. Also, tune based on observable diversity metrics, like mean feature diversity between individuals. (3) *Granularity control* of island models. Automatically adjust the number and size of islands according to population diversity and island convergence rates. Adjust also migration probabilities/speeds.

*   **Avoid:** *Oversimplification.* Steer clear of vague statements like "increase exploration and exploitation." *Blindly adding "self-adapting" mechanisms.*  The adaptation needs a clear purpose and performance feedback loop to be validated, based on *observable features of the run*.  *Using global archive.* In high-dimensional multimodal spaces, a single archive quickly becomes bloated and ineffective. Island-specific archives or more focused sampling strategies work best.

*   **Explanation:** The primary goal is to design heuristics that are *data-driven and observable*. Instead of vaguely claiming diversity maintenance is helpful, the approach needs to have a specific diversity metric and mechanism for adapting based on that metric. Heuristics should provide ways to modulate the granularity of the island model based on convergence rates, thereby focusing computation. The effectiveness of each heuristic MUST be measurable on-the-fly so a new one can replace it with ease.

2025-06-24 17:53:08 INFO Generating offspring via Crossover...
2025-06-24 17:54:16 INFO Crossover Prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


### Better code
AdaptiveGaussianArchiveEA
import numpy as np

class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 81.05823042615071, archive_size: int = 204.4744784358444, sigma_initial_factor: float = 0.30079161908612717,
                 sigma_decay: float = 0.9335308937814801, tournament_size: int = 7.463923399222831, recombination_sigma_factor: float = 0.2273446819921407,
                 archive_similarity_threshold: float = 9.362065512732052e-06):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = sigma_initial_factor * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []
        self.tournament_size = tournament_size
        self.recombination_sigma_factor = recombination_sigma_factor
        self.archive_similarity_threshold = archive_similarity_threshold

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_factor, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Worse code
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

### Analyze & experience
- Comparing (Rank 1) vs (Rank 5), we see `AdaptiveIslandDEWithGaussianLocalSearch` focuses on island-based DE, adaptive migration, and Gaussian local search for refinement; while `AdaptiveGaussianArchiveEA` uses adaptive Gaussian sampling with an archive and tournament selection for efficiency. The Rank 1's local search component provides exploitative benefits, while the Rank 5's simplified mutation maintains diversity. Also the rank 1 algorithm introduce migration and isalnd based optimisation.

Comparing (Rank 2) vs (Rank 4), we see `AdaptiveIslandDEWithArchiveDiversityAndPopulationAdaptation` combines island DE with archive-guided mutation, dynamic population sizing, and diversity maintenance, while `IslandArchiveAdaptiveGaussianEA` employs an island model with adaptive population sizing, archive-guided mutation, and stagnation detection. Rank 2 explicitly manages diversity and population size adaptively, promoting exploration through dynamic adjustments whereas the population size adaptation relies on average fitness, it reduces population if no changes occur in fitness.
(Rank 2's diversity is also checked within its adaptation step). And both rely on archive guide mutation and island population sizes.

Comparing (Rank 1) vs (Rank 2), we see Rank 1 uses a decaying migration rate combined with local search, while Rank 2 includes dynamic population sizing based on diversity and archive-guided mutation (using stagnation/normal population distribution ratio, etc). The diversity calculations and population adaptation contribute to Rank 2's stronger exploratory behaviour.

Comparing (Rank 3) vs (Rank 4), we see Rank 3 uses adaptive population sizes, an archive, and dynamic migration while Rank 4 employs an island model with adaptive population sizing, archive-guided mutation, and stagnation detection.. Both are island-based. While the Rank 3 stagnates due to too simple logic to reset the island. And doesn't contain population diversity check for that specific island while the Rank 4 it does have.

Comparing (Rank 4) vs (Rank 5), we see the integration of island model within Rank 4 leads to better exploitative and explorative advantages. The dynamic adaptation of archive, stagnation check is not found in Rank 5. Rank 5 relies less on restarts within it's logic.


Overall:

Effective strategies for deceptive landscapes involve multi-population approaches that emphasize both diversity and exploitation. Island models combined with archive-guided mutation are prevalent, adapting migration and population sizes dynamically. Stagnation detection and re-initialization help prevent premature convergence.
- Okay, I understand the task. We need to refine the concept of "Current self-reflection" to focus on building better heuristics for multimodal optimization, specifically *avoiding* the pitfalls and vague statements present in the "Ineffective self-reflection" examples.

Here's a breakdown of a refined approach, designed to steer us towards concrete heuristic design improvements:

*   **Keywords:** *Diversity maintenance, Adaptive operators, Island model granularity, Convergence monitoring*. The "Ineffective self-reflection" mentions these concepts but doesn't explain how to make them specific or manageable. We will provide clear guidelines on this.

*   **Advice:** Focus on: (1) *Quantifying* diversity *effectively*. Implement a diversity metric that isn't easily fooled and ties directly to optimization goals (e.g., distance to known optima or coverage of the search space). (2) Tune operator adaptation *parameters on the fly* based on observable trends. This requires a well-defined, *local* adaptation scheme, not a global one. Also, tune based on observable diversity metrics, like mean feature diversity between individuals. (3) *Granularity control* of island models. Automatically adjust the number and size of islands according to population diversity and island convergence rates. Adjust also migration probabilities/speeds.

*   **Avoid:** *Oversimplification.* Steer clear of vague statements like "increase exploration and exploitation." *Blindly adding "self-adapting" mechanisms.*  The adaptation needs a clear purpose and performance feedback loop to be validated, based on *observable features of the run*.  *Using global archive.* In high-dimensional multimodal spaces, a single archive quickly becomes bloated and ineffective. Island-specific archives or more focused sampling strategies work best.

*   **Explanation:** The primary goal is to design heuristics that are *data-driven and observable*. Instead of vaguely claiming diversity maintenance is helpful, the approach needs to have a specific diversity metric and mechanism for adapting based on that metric. Heuristics should provide ways to modulate the granularity of the island model based on convergence rates, thereby focusing computation. The effectiveness of each heuristic MUST be measurable on-the-fly so a new one can replace it with ease.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 17:54:16 INFO Mutation prompt: **(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.


Current heuristics:
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- Okay, I understand the task. We need to refine the concept of "Current self-reflection" to focus on building better heuristics for multimodal optimization, specifically *avoiding* the pitfalls and vague statements present in the "Ineffective self-reflection" examples.

Here's a breakdown of a refined approach, designed to steer us towards concrete heuristic design improvements:

*   **Keywords:** *Diversity maintenance, Adaptive operators, Island model granularity, Convergence monitoring*. The "Ineffective self-reflection" mentions these concepts but doesn't explain how to make them specific or manageable. We will provide clear guidelines on this.

*   **Advice:** Focus on: (1) *Quantifying* diversity *effectively*. Implement a diversity metric that isn't easily fooled and ties directly to optimization goals (e.g., distance to known optima or coverage of the search space). (2) Tune operator adaptation *parameters on the fly* based on observable trends. This requires a well-defined, *local* adaptation scheme, not a global one. Also, tune based on observable diversity metrics, like mean feature diversity between individuals. (3) *Granularity control* of island models. Automatically adjust the number and size of islands according to population diversity and island convergence rates. Adjust also migration probabilities/speeds.

*   **Avoid:** *Oversimplification.* Steer clear of vague statements like "increase exploration and exploitation." *Blindly adding "self-adapting" mechanisms.*  The adaptation needs a clear purpose and performance feedback loop to be validated, based on *observable features of the run*.  *Using global archive.* In high-dimensional multimodal spaces, a single archive quickly becomes bloated and ineffective. Island-specific archives or more focused sampling strategies work best.

*   **Explanation:** The primary goal is to design heuristics that are *data-driven and observable*. Instead of vaguely claiming diversity maintenance is helpful, the approach needs to have a specific diversity metric and mechanism for adapting based on that metric. Heuristics should provide ways to modulate the granularity of the island model based on convergence rates, thereby focusing computation. The effectiveness of each heuristic MUST be measurable on-the-fly so a new one can replace it with ease.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-24 17:56:29 INFO Perform Harmony Search...
2025-06-24 17:56:39 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:39 ERROR Can not run the algorithm
2025-06-24 17:56:39 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:39 INFO FeHistory: [130499.38077241]
2025-06-24 17:56:39 INFO Expected Optimum FE: -5000
2025-06-24 17:56:39 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:39 ERROR Can not run the algorithm
2025-06-24 17:56:39 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:39 INFO FeHistory: [140011.24096144]
2025-06-24 17:56:39 INFO Expected Optimum FE: -5000
2025-06-24 17:56:39 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:39 ERROR Can not run the algorithm
2025-06-24 17:56:40 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:40 INFO FeHistory: [209507.18121441]
2025-06-24 17:56:40 INFO Expected Optimum FE: -5000
2025-06-24 17:56:40 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:40 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:40 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:40 INFO AOCC mean: 0.0000
2025-06-24 17:56:40 INFO Weighed AOCC mean: nan
2025-06-24 17:56:40 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:40 ERROR Can not run the algorithm
2025-06-24 17:56:40 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:40 INFO FeHistory: [93088.40056763]
2025-06-24 17:56:40 INFO Expected Optimum FE: -5000
2025-06-24 17:56:40 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:40 ERROR Can not run the algorithm
2025-06-24 17:56:40 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:40 INFO FeHistory: [150298.9182996]
2025-06-24 17:56:40 INFO Expected Optimum FE: -5000
2025-06-24 17:56:40 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:40 ERROR Can not run the algorithm
2025-06-24 17:56:41 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:41 INFO FeHistory: [241554.021275]
2025-06-24 17:56:41 INFO Expected Optimum FE: -5000
2025-06-24 17:56:41 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:41 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:41 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:41 INFO AOCC mean: 0.0000
2025-06-24 17:56:41 INFO Weighed AOCC mean: nan
2025-06-24 17:56:41 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:41 ERROR Can not run the algorithm
2025-06-24 17:56:41 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:41 INFO FeHistory: [161178.63339981]
2025-06-24 17:56:41 INFO Expected Optimum FE: -5000
2025-06-24 17:56:41 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:41 ERROR Can not run the algorithm
2025-06-24 17:56:42 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:42 INFO FeHistory: [181635.84283805]
2025-06-24 17:56:42 INFO Expected Optimum FE: -5000
2025-06-24 17:56:42 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:42 ERROR Can not run the algorithm
2025-06-24 17:56:42 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:42 INFO FeHistory: [114006.66272333]
2025-06-24 17:56:42 INFO Expected Optimum FE: -5000
2025-06-24 17:56:42 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:42 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:42 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:42 INFO AOCC mean: 0.0000
2025-06-24 17:56:42 INFO Weighed AOCC mean: nan
2025-06-24 17:56:42 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:42 ERROR Can not run the algorithm
2025-06-24 17:56:42 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:42 INFO FeHistory: [109874.37815233]
2025-06-24 17:56:42 INFO Expected Optimum FE: -5000
2025-06-24 17:56:42 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:42 ERROR Can not run the algorithm
2025-06-24 17:56:43 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:43 INFO FeHistory: [209155.02856939]
2025-06-24 17:56:43 INFO Expected Optimum FE: -5000
2025-06-24 17:56:43 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:43 ERROR Can not run the algorithm
2025-06-24 17:56:43 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:43 INFO FeHistory: [146885.29195148]
2025-06-24 17:56:43 INFO Expected Optimum FE: -5000
2025-06-24 17:56:43 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:43 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:43 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:43 INFO AOCC mean: 0.0000
2025-06-24 17:56:43 INFO Weighed AOCC mean: nan
2025-06-24 17:56:43 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:43 ERROR Can not run the algorithm
2025-06-24 17:56:43 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:43 INFO FeHistory: [108772.96326487]
2025-06-24 17:56:43 INFO Expected Optimum FE: -5000
2025-06-24 17:56:43 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:43 ERROR Can not run the algorithm
2025-06-24 17:56:44 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:44 INFO FeHistory: [137978.7872429]
2025-06-24 17:56:44 INFO Expected Optimum FE: -5000
2025-06-24 17:56:44 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:44 ERROR Can not run the algorithm
2025-06-24 17:56:44 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:44 INFO FeHistory: [241108.18209329]
2025-06-24 17:56:44 INFO Expected Optimum FE: -5000
2025-06-24 17:56:44 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:44 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:44 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:44 INFO AOCC mean: 0.0000
2025-06-24 17:56:44 INFO Weighed AOCC mean: nan
2025-06-24 17:56:44 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:44 ERROR Can not run the algorithm
2025-06-24 17:56:44 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:44 INFO FeHistory: [89397.08667011]
2025-06-24 17:56:44 INFO Expected Optimum FE: -5000
2025-06-24 17:56:44 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:44 ERROR Can not run the algorithm
2025-06-24 17:56:45 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:45 INFO FeHistory: [156526.85442492]
2025-06-24 17:56:45 INFO Expected Optimum FE: -5000
2025-06-24 17:56:45 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:45 ERROR Can not run the algorithm
2025-06-24 17:56:45 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:45 INFO FeHistory: [203472.23348534]
2025-06-24 17:56:45 INFO Expected Optimum FE: -5000
2025-06-24 17:56:45 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:45 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:45 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:45 INFO AOCC mean: 0.0000
2025-06-24 17:56:45 INFO Weighed AOCC mean: nan
2025-06-24 17:56:45 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:45 ERROR Can not run the algorithm
2025-06-24 17:56:45 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:45 INFO FeHistory: [125996.03236166]
2025-06-24 17:56:45 INFO Expected Optimum FE: -5000
2025-06-24 17:56:45 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:46 ERROR Can not run the algorithm
2025-06-24 17:56:46 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:46 INFO FeHistory: [126374.71061227]
2025-06-24 17:56:46 INFO Expected Optimum FE: -5000
2025-06-24 17:56:46 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:46 ERROR Can not run the algorithm
2025-06-24 17:56:46 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:46 INFO FeHistory: [160962.85202767]
2025-06-24 17:56:46 INFO Expected Optimum FE: -5000
2025-06-24 17:56:46 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:46 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:46 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:46 INFO AOCC mean: 0.0000
2025-06-24 17:56:46 INFO Weighed AOCC mean: nan
2025-06-24 17:56:46 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:46 ERROR Can not run the algorithm
2025-06-24 17:56:47 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:47 INFO FeHistory: [134124.58554807]
2025-06-24 17:56:47 INFO Expected Optimum FE: -5000
2025-06-24 17:56:47 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:47 ERROR Can not run the algorithm
2025-06-24 17:56:47 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:47 INFO FeHistory: [139006.01320378]
2025-06-24 17:56:47 INFO Expected Optimum FE: -5000
2025-06-24 17:56:47 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:47 ERROR Can not run the algorithm
2025-06-24 17:56:47 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:47 INFO FeHistory: [173606.4145064]
2025-06-24 17:56:47 INFO Expected Optimum FE: -5000
2025-06-24 17:56:47 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:47 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:47 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:47 INFO AOCC mean: 0.0000
2025-06-24 17:56:47 INFO Weighed AOCC mean: nan
2025-06-24 17:56:47 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:47 ERROR Can not run the algorithm
2025-06-24 17:56:48 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:48 INFO FeHistory: [146058.2377883]
2025-06-24 17:56:48 INFO Expected Optimum FE: -5000
2025-06-24 17:56:48 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:48 ERROR Can not run the algorithm
2025-06-24 17:56:48 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:48 INFO FeHistory: [147986.3583696]
2025-06-24 17:56:48 INFO Expected Optimum FE: -5000
2025-06-24 17:56:48 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:48 ERROR Can not run the algorithm
2025-06-24 17:56:48 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:48 INFO FeHistory: [148094.82062315]
2025-06-24 17:56:48 INFO Expected Optimum FE: -5000
2025-06-24 17:56:48 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:48 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:48 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:48 INFO AOCC mean: 0.0000
2025-06-24 17:56:48 INFO Weighed AOCC mean: nan
2025-06-24 17:56:48 INFO --- GNBG Problem Parameters for f16 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0 0 0 0 0 0 0 0 0 0]
----------------------------------------
2025-06-24 17:56:48 ERROR Can not run the algorithm
2025-06-24 17:56:49 INFO Run function 16 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:49 INFO FeHistory: [133785.05604167]
2025-06-24 17:56:49 INFO Expected Optimum FE: -5000
2025-06-24 17:56:49 INFO --- GNBG Problem Parameters for f18 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.22159228 0.42314776 0.4901829  0.25862884 0.37043014 0.37440768
 0.26098797 0.491006   0.27569772 0.45404864]
----------------------------------------
2025-06-24 17:56:49 ERROR Can not run the algorithm
2025-06-24 17:56:49 INFO Run function 18 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:49 INFO FeHistory: [157502.18780961]
2025-06-24 17:56:49 INFO Expected Optimum FE: -5000
2025-06-24 17:56:49 INFO --- GNBG Problem Parameters for f19 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -5000.000000
  Lambda (Curvature): [1 1 1 1 1]
  Mu (Asymmetry/Depth): [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]
----------------------------------------
2025-06-24 17:56:49 ERROR Can not run the algorithm
2025-06-24 17:56:49 INFO Run function 19 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-24 17:56:49 INFO FeHistory: [84537.35890238]
2025-06-24 17:56:49 INFO Expected Optimum FE: -5000
2025-06-24 17:56:49 INFO Unimodal AOCC mean: nan
2025-06-24 17:56:49 INFO Multimodal (single component) AOCC mean: nan
2025-06-24 17:56:49 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-24 17:56:49 INFO AOCC mean: 0.0000
2025-06-24 17:56:49 INFO Weighed AOCC mean: nan
2025-06-24 17:56:49 INFO Generation 5, best so far: 0.2796149162371689
2025-06-24 17:56:49 INFO Population length is: 6
2025-06-24 17:56:49 INFO --- Performing Long-Term Reflection at Generation 6 ---
2025-06-24 17:56:49 INFO Reflection Prompt: ### Problem Description
**(CR: Capacity and Role)**
Act as an expert in designing algorithms for deceptive, multi-component landscapes. Your specialty is creating strategies that use multi-population or island models to perform aggressive global exploration.

**(I: Insight)**
Your objective is to design a novel optimization algorithm for the GNBG benchmark. The most difficult functions (f16-f24) have multiple, separate basins of attraction where the global optimum can be hidden. An algorithm must be able to discover and explore these competing regions to succeed.

**(S: Statement & P: Personality)**
Provide a complete, novel Python class that implements a multi-population or island-based search strategy to solve this challenge. The class must have an `__init__(...)` and `optimize(...)` method as specified in the main task description.

### List heuristics
Below is a list of design heuristics ranked from best to worst based on their AOCC score on group function 3 of GNBG benchmark, where higher is better. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandGaussianWithDynamicGranularity
# Description: Island model with adaptive Gaussian mutation, dynamic island granularity, and archive-guided restarts for multimodal exploration.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandGaussianWithDynamicGranularity
# Description: Island model with adaptive Gaussian mutation, dynamic island granularity, and archive-guided restarts for multimodal exploration.

class AdaptiveIslandGaussianWithDynamicGranularity:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = 4  # Initial number of islands
        self.island_population_size = 50
        self.migration_rate = 0.1
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)  # Initial sigma for Gaussian mutation
        self.sigma_decay = 0.98
        self.islands = []
        self.island_best_fitness = []
        self.island_best_solution = []
        self.archive_size = 100
        self.archive = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0  # Reset for this run

        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        self._initialize_islands(objective_function)

        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                self._evolve_island(i, objective_function)
            self._migrate_individuals(objective_function)
            self._adapt_island_granularity(objective_function)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_islands(self, objective_function):
        self.islands = []
        self.island_best_fitness = []
        self.island_best_solution = []
        for _ in range(self.num_islands):
            population = np.random.uniform(self.lower_bounds, self.upper_bounds,
                                          size=(self.island_population_size, self.dim))
            self.islands.append(population)
            fitness_values = objective_function(population)
            self.eval_count += self.island_population_size
            best_index = np.argmin(fitness_values)
            self.island_best_fitness.append(fitness_values[best_index])
            self.island_best_solution.append(population[best_index])
            self._update_archive(population, fitness_values) #update archive to each islands
        self.island_best_fitness = np.array(self.island_best_fitness) #type casting to array since adapt_island_granularity relies on it.
    def _evolve_island(self, island_index, objective_function):
        population = self.islands[island_index]
        fitness_values = objective_function(population)
        self.eval_count += self.island_population_size

        parents = self._tournament_selection(population, fitness_values)
        offspring = self._gaussian_recombination(parents)
        offspring = self._adaptive_mutation(offspring)

        offspring_fitness = objective_function(offspring)
        self.eval_count += len(offspring)

        population, fitness_values = self._select_next_generation(
            population, fitness_values, offspring, offspring_fitness
        )
        self.islands[island_index] = population #update island population

        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.island_best_fitness[island_index]:
            self.island_best_fitness[island_index] = fitness_values[best_index]
            self.island_best_solution[island_index] = population[best_index]

            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness_values[best_index]
                self.best_solution_overall = population[best_index]
        self._update_archive(population, fitness_values) #update archive to each islands

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.island_population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.island_population_size]]
        next_fit = combined_fit[sorted_indices[:self.island_population_size]]
        return next_gen, next_fit
    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        self.archive = np.array(new_archive[:self.archive_size])
    def _migrate_individuals(self, objective_function):
        # For each island, send a copy of its best individual to a randomly chosen different island.
        for i in range(self.num_islands):
            destination_island = i
            while destination_island == i:
                destination_island = np.random.randint(0, self.num_islands)

            immigrant = np.copy(self.island_best_solution[i])
            
            # Replace a randomly chosen member of the destination island with the immigrant
            replace_index = np.random.randint(0, self.island_population_size)
            self.islands[destination_island][replace_index] = immigrant

            # Evaluate fitness of immigrant on destination island
            immigrant_fitness = objective_function(immigrant.reshape(1, -1))[0]
            self.eval_count += 1

            # Update island's best if necessary
            if immigrant_fitness < self.island_best_fitness[destination_island]:
                self.island_best_fitness[destination_island] = immigrant_fitness
                self.island_best_solution[destination_island] = immigrant

            self._update_archive(self.islands[destination_island], objective_function(self.islands[destination_island]))  #Recompute and add it to achive.
            #Consider to re-rank the solutions after migration, more costly in computation but might helps by reranking current individuals, current complexity is already pretty high...
    def _adapt_island_granularity(self, objective_function):
        # If all islands have converged to similar fitness values, reduce the number of islands and increase population size.
        # If islands have very different fitness values, increase the number of islands and reduce population size
        fitness_std = np.std(self.island_best_fitness)

        if fitness_std < 1e-2 and self.num_islands > 1:  # Islands are similar; merge
            self.num_islands = max(1, self.num_islands // 2)  # Reduce by half, but ensure at least 1
            self.island_population_size = min(200, self.island_population_size * 2)  #Increase pop
            self._initialize_islands(objective_function) # Re-initialize

        elif fitness_std > 1 and self.num_islands < 8:  # Islands are diverse; split
            self.num_islands = min(8, self.num_islands * 2)  # Double, no more than 8 islands
            self.island_population_size = max(20, self.island_population_size // 2)  # Reduce pop
            self._initialize_islands(objective_function)

```

### Rank 2 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveIslandDEWithAdaptiveArchiveAndGranularity
# Description: Island-based DE with adaptive archive injection, population sizing, and migration granularity.
# Code:
```python
import numpy as np

# Name: AdaptiveIslandDEWithAdaptiveArchiveAndGranularity
# Description: Island-based DE with adaptive archive injection, population sizing, and migration granularity.

class AdaptiveIslandDEWithAdaptiveArchiveAndGranularity:
    """
    Island model DE with adaptive migration probabilities and archive injection,
    and dynamic island sizes based on population diversity and convergence.
    """

    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, initial_population_size: int = 50, archive_size: int = 100,
                 initial_migration_rate: float = 0.2, migration_decay_rate: float = 0.995,
                 migration_interval: int = 500, mutation_factor: float = 0.5, crossover_rate: float = 0.7,
                 archive_prob: float = 0.1, local_search_prob: float = 0.1, local_search_sigma_scale: float = 0.01,
                 diversity_threshold: float = 0.01):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = int(num_islands)
        self.initial_population_size = int(initial_population_size)
        self.archive_size = int(archive_size)
        self.initial_migration_rate = float(initial_migration_rate)
        self.migration_decay_rate = float(migration_decay_rate)
        self.migration_interval = int(migration_interval)
        self.mutation_factor = float(mutation_factor)
        self.crossover_rate = float(crossover_rate)
        self.archive_prob = float(archive_prob)
        self.local_search_prob = float(local_search_prob)
        self.local_search_sigma_scale = float(local_search_sigma_scale)
        self.diversity_threshold = float(diversity_threshold)

        self.populations = [np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.initial_population_size, self.dim))
                            for _ in range(self.num_islands)]
        self.fitness_values = [np.full(self.initial_population_size, float('inf')) for _ in range(self.num_islands)]
        self.archives = [[] for _ in range(self.num_islands)]
        self.migration_rate = self.initial_migration_rate
        self.population_sizes = [self.initial_population_size] * self.num_islands
        self.stagnation_counter = [0] * self.num_islands # Track island stagnation.

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        # Initial evaluation
        for i in range(self.num_islands):
            self.fitness_values[i] = objective_function(self.populations[i])
            self.eval_count += self.population_sizes[i] #self.population_size
            self._update_archive(i, self.populations[i], self.fitness_values[i])
            self._update_best(self.populations[i], self.fitness_values[i])

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                # Differential Evolution
                new_population = np.copy(self.populations[i])
                for j in range(self.population_sizes[i]): # self.population_size:
                    # Mutation with archive injection
                    if np.random.rand() < self.archive_prob and len(self.archives[i]) > 0:
                        # Select a solution from the archive
                        archive_idx = np.random.randint(len(self.archives[i]))
                        archive_solution = self.archives[i][archive_idx][:-1]  # Exclude fitness from archive entry

                        idxs = np.random.choice(self.population_sizes[i], 2, replace=False) # self.population_size
                        mutant = archive_solution + self.mutation_factor * (self.populations[i][idxs[0]] - self.populations[i][idxs[1]])

                    else:
                        # Classic DE mutation
                        idxs = np.random.choice(self.population_sizes[i], 3, replace=False) # self.population_size
                        mutant = self.populations[i][idxs[0]] + self.mutation_factor * (self.populations[i][idxs[1]] - self.populations[i][idxs[2]])

                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                    # Crossover
                    crossover_mask = np.random.rand(self.dim) < self.crossover_rate
                    trial_vector = np.where(crossover_mask, mutant, self.populations[i][j])

                    # Local search
                    if np.random.rand() < self.local_search_prob:
                        trial_vector = self._gaussian_local_search(trial_vector, objective_function)

                    # Evaluation
                    trial_fitness = objective_function(trial_vector.reshape(1, -1))[0]
                    self.eval_count += 1

                    # Selection
                    if trial_fitness < self.fitness_values[i][j]:
                        new_population[j] = trial_vector
                        self.fitness_values[i][j] = trial_fitness

                self.populations[i] = new_population
                self._update_archive(i, self.populations[i], self.fitness_values[i])
                self._update_best(self.populations[i], self.fitness_values[i])

            # Migration
            if generation % self.migration_interval == 0:
                self._migrate()
                self.migration_rate *= self.migration_decay_rate # Decay Migration Rate

            # Adapt population size based on diversity (and stagnation)
            self._adapt_population_size(objective_function)


            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _gaussian_local_search(self, solution, objective_function):
        """Perform Gaussian local search around a solution."""
        sigma = self.local_search_sigma_scale * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + np.random.normal(0, sigma, self.dim)
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        new_fitness = objective_function(new_solution.reshape(1, -1))[0]
        self.eval_count += 1

        original_fitness = objective_function(solution.reshape(1, -1))[0]
        self.eval_count += 1

        if new_fitness < original_fitness:
            return new_solution
        else:
            return solution

    def _update_archive(self, island_index, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archives[island_index])
            if not already_present:
                self.archives[island_index].append(sol)

        self.archives[island_index].sort(key=lambda x: x[-1])  # Sort by fitness
        self.archives[island_index] = self.archives[island_index][:self.archive_size]

    def _migrate(self):
        # Select migrants from each island and distribute them
        migrants = []
        for i in range(self.num_islands):
            num_migrants = int(self.population_sizes[i] * self.migration_rate) # self.population_size
            indices = np.argsort(self.fitness_values[i])[:num_migrants]
            migrants.append(self.populations[i][indices])

        # Distribute migrants to other islands
        for i in range(self.num_islands):
            receiving_island = (i + 1) % self.num_islands
            num_migrants = migrants[i].shape[0]
            replace_indices = np.random.choice(self.population_sizes[receiving_island], num_migrants, replace=False)  # self.population_size

            self.populations[receiving_island][replace_indices] = migrants[i]
            # Re-evaluate the replaced individuals
            migrants_to_evaluate = migrants[i]
            self.fitness_values[receiving_island][replace_indices] = objective_function(migrants_to_evaluate)
            self.eval_count += num_migrants

            self._update_archive(receiving_island, self.populations[receiving_island], self.fitness_values[receiving_island])

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]

    def _calculate_diversity(self, population):
        """Calculates average euclidean distance between individuals."""
        if len(population) <= 1:
            return 1.0  # Max diversity if only one individual
        distances = []
        for i in range(len(population)):
            for j in range(i + 1, len(population)):
                distances.append(np.linalg.norm(population[i] - population[j]))
        return np.mean(distances)

    def _adapt_population_size(self, objective_function):
        """Adapt population size of each island based on diversity."""
        for i in range(self.num_islands):
            diversity = self._calculate_diversity(self.populations[i])
            if diversity < self.diversity_threshold:
                self.stagnation_counter[i] += 1
                # Reduce population size if diversity is low and island is stagnating
                if self.stagnation_counter[i] > 2:

                    new_size = max(10, int(self.population_sizes[i] * 0.8)) #Reduce
                    self.population_sizes[i] = new_size

                    # Resample the population, keeping the best individuals.
                    best_indices = np.argsort(self.fitness_values[i])[:new_size]
                    new_population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(new_size, self.dim))
                    new_population[:len(best_indices)] = self.populations[i][best_indices]

                    self.populations[i] = new_population
                    self.fitness_values[i] = np.full(new_size, float('inf')) #objective_function(self.populations[i]) # re eval
                    self.fitness_values[i][:len(best_indices)] = self.fitness_values[i][best_indices]#np.full(new_size, float('inf'))

                    self.fitness_values[i] = objective_function(self.populations[i]) # re eval
                    self.eval_count += self.population_sizes[i] # population size instead of new size since re-evaluating.

                    self._update_archive(i, self.populations[i], self.fitness_values[i])
                    self.stagnation_counter[i] = 0  # Reset stagnation counter

            else:
                self.stagnation_counter[i] = 0
                if self.population_sizes[i] < self.initial_population_size : # grow slowly?
                    new_size = min(self.initial_population_size, int(self.population_sizes[i]*1.2))
                    if new_size > self.population_sizes[i]: # increase ONLY by 1.
                       self.population_sizes[i] = new_size
                       new_population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(new_size, self.dim))
                       # best_indices is kept so information carried over to next round.

                       best_indices = np.argsort(self.fitness_values[i])[:self.population_sizes[i]] # OLD Population size instead of new pop
                       new_population[:len(best_indices)] = self.populations[i][best_indices]


                       self.populations[i] = new_population
                       self.fitness_values[i] = np.full(new_size, float('inf')) #objective_function(self.populations[i])
                       self.fitness_values[i][:len(best_indices)] =  self.fitness_values[i][best_indices]

                       self.fitness_values[i] = objective_function(self.populations[i])
                       self.eval_count += self.population_sizes[i]

                       self._update_archive(i, self.populations[i], self.fitness_values[i])

```

### Rank 3 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: IslandAdaptiveArchiveDE
# Description: Island model with adaptive mutation, archive-guided exploration, and dynamic population sizing based on diversity.
# Code:
```python
import numpy as np

# Name: IslandAdaptiveArchiveDE
# Description: Island model with adaptive mutation, archive-guided exploration, and dynamic population sizing based on diversity.
# Code:
class IslandAdaptiveArchiveDE:
    """
    Island model with adaptive mutation, archive-guided exploration, and dynamic population sizing based on diversity.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, population_size: int = 50, archive_size: int = 100,
                 mutation_factor: float = 0.5, crossover_rate: float = 0.7,
                 migration_interval: int = 50, migration_probability: float = 0.1,
                 archive_similarity_threshold: float = 1e-6, stagnation_tolerance: int = 50,
                 diversity_threshold: float = 0.1):

        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.num_islands = int(num_islands)
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.mutation_factor = mutation_factor
        self.crossover_rate = crossover_rate
        self.migration_interval = int(migration_interval)
        self.migration_probability = migration_probability
        self.archive_similarity_threshold = archive_similarity_threshold
        self.stagnation_tolerance = int(stagnation_tolerance)
        self.diversity_threshold = diversity_threshold

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.islands = []
        self.archives = [[] for _ in range(self.num_islands)]
        self.island_best_fitness = [float('inf')] * self.num_islands
        self.island_best_solution = [None] * self.num_islands
        self.island_stagnation_counters = [0] * self.num_islands
        self.island_population_sizes = [self.population_size] * self.num_islands

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0

        self.islands = self._initialize_islands()
        fitness_values = np.array([objective_function(island) for island in self.islands])
        self.eval_count += self.num_islands * self.population_size

        for i in range(self.num_islands):
            self._update_island_best(i, self.islands[i], fitness_values[i])
            self.archives[i] = self._update_archive(self.archives[i], self.islands[i], fitness_values[i])

        generation = 0
        while self.eval_count < self.budget:
            for i in range(self.num_islands):
                if self.eval_count >= self.budget:
                    break

                # Adaptive Population Size Adjustment
                self._adapt_population_size(i)
                
                # DE mutation and crossover
                offspring = self._differential_evolution(i)
                offspring_fitness = objective_function(offspring)
                self.eval_count += len(offspring)

                # Select survivors
                self._select_next_generation(i, offspring, offspring_fitness)

                # Update best solution and archive
                self._update_island_best(i, self.islands[i], self.island_fitness[i])
                self.archives[i] = self._update_archive(self.archives[i], self.islands[i], self.island_fitness[i])
                

                #Stagnation Check
                if abs(self.island_best_fitness[i] - fitness_values[i].min()) < acceptance_threshold :
                    self.island_stagnation_counters[i] += 1
                else:
                    self.island_stagnation_counters[i] = 0

                if self.island_stagnation_counters[i] > self.stagnation_tolerance:
                   self._reinitialize_island(i)  # re-initialize stagnated islands

            # Perform migration
            if generation % self.migration_interval == 0:
                self._migrate_individuals()

            generation += 1
            self._update_overall_best()

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_islands(self):
        islands = []
        for _ in range(self.num_islands):
            island = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
            islands.append(island)
        return islands

    def _differential_evolution(self, island_index):
        island = self.islands[island_index]
        population_size = self.island_population_sizes[island_index]
        offspring = np.zeros((population_size, self.dim))

        for i in range(population_size):
            # Selection of random indices for DE
            idxs = [idx for idx in range(population_size) if idx != i]
            a, b, c = np.random.choice(idxs, 3, replace=False)

            # Mutation and crossover
            mutant = island[a] + self.mutation_factor * (island[b] - island[c])
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            for j in range(self.dim):
                if np.random.rand() <= self.crossover_rate:
                    offspring[i, j] = mutant[j]
                else:
                    offspring[i, j] = island[i, j]

        return offspring

    def _select_next_generation(self, island_index, offspring, offspring_fitness):
        island = self.islands[island_index]
        population_size = self.island_population_sizes[island_index]
        island_fitness = self.island_fitness[island_index]
        
        combined_population = np.vstack((island, offspring))
        combined_fitness = np.concatenate((island_fitness, offspring_fitness))
        
        # Tournament selection or truncation selection
        sorted_indices = np.argsort(combined_fitness)[:population_size]
        
        self.islands[island_index] = combined_population[sorted_indices]
        self.island_fitness[island_index] = combined_fitness[sorted_indices]



    def _update_archive(self, archive, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])

        archive = np.array(new_archive[:self.archive_size])
        return archive


    def _migrate_individuals(self):
        for i in range(self.num_islands):
            if np.random.rand() < self.migration_probability:
                # Select a random island to migrate to
                dest_island = np.random.choice([idx for idx in range(self.num_islands) if idx != i])

                # Select a random individual from the current island
                migrant_index = np.random.randint(0, self.island_population_sizes[i])
                migrant = self.islands[i][migrant_index]
                migrant_fitness = self.island_fitness[i][migrant_index]
                
                # Replace a random individual in the destination island
                replace_index = np.random.randint(0, self.island_population_sizes[dest_island])
                self.islands[dest_island][replace_index] = migrant
                self.island_fitness[dest_island][replace_index] = migrant_fitness

    def _update_overall_best(self):
        for i in range(self.num_islands):
            if self.island_best_fitness[i] < self.best_fitness_overall:
                self.best_fitness_overall = self.island_best_fitness[i]
                self.best_solution_overall = self.island_best_solution[i]

    def _update_island_best(self, island_index, population, fitness_values):
         best_index = np.argmin(fitness_values)
         if fitness_values[best_index] < self.island_best_fitness[island_index]:
              self.island_best_fitness[island_index] = fitness_values[best_index]
              self.island_best_solution[island_index] = population[best_index].copy()

    def _adapt_population_size(self, island_index):
         island = self.islands[island_index]
         population_size = self.island_population_sizes[island_index]
         if population_size > 10:
              distances = np.linalg.norm(island - np.mean(island, axis=0), axis=1)
              diversity = np.mean(distances)

              if diversity < self.diversity_threshold:
                self.island_population_sizes[island_index] = max(10, int(population_size * 0.9)) #reduce population if diversity too low
                self.islands[island_index] = self.islands[island_index][:self.island_population_sizes[island_index]]
                self.island_fitness[island_index] = self.island_fitness[island_index][:self.island_population_sizes[island_index]]
         elif population_size < self.population_size:
              self.island_population_sizes[island_index] = min(self.population_size, population_size + int(population_size * 0.1))
              self.islands[island_index] = np.vstack((self.islands[island_index], np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.island_population_sizes[island_index] - population_size, self.dim))))
              self.island_fitness[island_index] = np.concatenate((self.island_fitness[island_index], np.full(self.island_population_sizes[island_index] - population_size, float('inf')))) # Assign worse fitness until re-evaluated
         self.island_fitness[island_index] = np.array([np.inf] * self.island_population_sizes[island_index])

    def _reinitialize_island(self, island_index):
        self.islands[island_index] = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.island_population_sizes[island_index], self.dim))
        self.island_fitness[island_index] = np.array([np.inf] * self.island_population_sizes[island_index])
        self.island_stagnation_counters[island_index] = 0
        self.island_best_fitness[island_index] = float('inf')
        self.archives[island_index] = []
        self.island_best_solution[island_index] = None

```

### Rank 4 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: IslandArchiveGaussianMutationEA
# Description: Island model with adaptive mutation, archive-guided reinitialization, and dynamic island management.
# Code:
```python
import numpy as np

# Name: IslandArchiveGaussianMutationEA
# Description: Island model with adaptive mutation, archive-guided reinitialization, and dynamic island management.
# Code:
class IslandArchiveGaussianMutationEA:
    """
    Implements an island model with adaptive Gaussian mutation, 
    archive-guided reinitialization, and dynamic island management
    to explore multimodal landscapes effectively.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 num_islands: int = 5, population_size: int = 50, archive_size: int = 100,
                 migration_interval: int = 500, migration_rate: float = 0.1,
                 mutation_rate_initial: float = 0.1, mutation_rate_decay: float = 0.99,
                 island_reset_threshold: int = 5000, archive_similarity_threshold: float = 1e-6):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.num_islands = int(num_islands)
        self.population_size = int(population_size)
        self.archive_size = int(archive_size)
        self.migration_interval = int(migration_interval)
        self.migration_rate = migration_rate
        self.mutation_rate = mutation_rate_initial
        self.mutation_rate_decay = mutation_rate_decay
        self.island_reset_threshold = island_reset_threshold
        self.archive_similarity_threshold = archive_similarity_threshold

        self.islands = []
        self.island_archives = [[] for _ in range(self.num_islands)]
        self.island_last_reset = [0] * self.num_islands

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0  # Reset for this run
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim) if self.dim > 0 else np.array([])
        self.best_fitness_overall = float('inf')
        
        self.islands = self._initialize_islands(objective_function)
        island_fitnesses = [objective_function(island) for island in self.islands]
        self.eval_count += self.num_islands * self.population_size

        for i in range(self.num_islands):
            self.island_archives[i] = self._update_archive(self.islands[i], island_fitnesses[i], self.island_archives[i])
        
        generation = 0
        while self.eval_count < self.budget:
            generation += 1
            for i in range(self.num_islands):
                # Genetic operations on each island
                parents = self._tournament_selection(self.islands[i], island_fitnesses[i])
                offspring = self._gaussian_recombination(parents, self.mutation_rate)
                offspring = self._adaptive_mutation(offspring, self.mutation_rate)
                offspring_fitness = objective_function(offspring)
                self.eval_count += len(offspring)

                self.islands[i], island_fitnesses[i] = self._select_next_generation(self.islands[i], island_fitnesses[i], offspring, offspring_fitness)

                # Archive update
                self.island_archives[i] = self._update_archive(
                    np.vstack((self.islands[i], offspring)),
                    np.concatenate((island_fitnesses[i], offspring_fitness)),
                    self.island_archives[i]
                )

                # Update best solution
                self._update_best(offspring, offspring_fitness)

                # Reset island if stagnating
                if (self.eval_count - self.island_last_reset[i]) > self.island_reset_threshold:
                    self.islands[i] = self._reinitialize_island(i)
                    island_fitnesses[i] = objective_function(self.islands[i])
                    self.eval_count += self.population_size
                    self.island_archives[i] = self._update_archive(self.islands[i], island_fitnesses[i], self.island_archives[i])
                    self.island_last_reset[i] = self.eval_count
                    self.mutation_rate = 0.1 #LLM: reset to initial mutation after reset/stagnation.

            # Migration
            if generation % (self.migration_interval // 10) == 0: # LLM: Migrate more frequently
                self._migrate_individuals(objective_function) #LLM: Function is changed to allow for better migration


            # Adaptive mutation rate decay
            self.mutation_rate *= self.mutation_rate_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_islands(self, objective_function):
        islands = []
        for _ in range(self.num_islands):
            center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
            island = np.random.normal(center, 0.2 * (self.upper_bounds - self.lower_bounds), size=(self.population_size, self.dim)) #Initial sigma is decreased to force initial diversity
            island = np.clip(island, self.lower_bounds, self.upper_bounds)
            islands.append(island)
        return islands

    def _reinitialize_island(self, island_index):
        """Reinitializes an island using a combination of random and archive-guided sampling."""
        new_island = np.zeros((self.population_size, self.dim))
        archive = self.island_archives[island_index]

        for i in range(self.population_size):
            if np.random.rand() < 0.5 and len(archive) > 0:  # Use archive half the time
                # Sample from archive
                idx = np.random.randint(0, len(archive))
                new_island[i] = archive[idx][:-1] # Exclude fitness from the end
            else:
                # Random reinitialization within bounds, decrease init_sigma since last one didn't work
                new_island[i] = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        return new_island


    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents, mutation_rate):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, mutation_rate * (self.upper_bounds - self.lower_bounds), self.dim) #LLM: Decaying rate is used.
            child2 = midpoint + np.random.normal(0, mutation_rate * (self.upper_bounds - self.lower_bounds), self.dim) #LLM: Decaying rate is used.
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring, mutation_rate):
        mutated = offspring + np.random.normal(0, mutation_rate * (self.upper_bounds - self.lower_bounds), size=offspring.shape) #LLM: Decaying rate is used.
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values, archive): #LM: Fixed archive update to save in a numpy-array.
         combined = np.column_stack((population, fitness_values))
         new_archive = []
         for sol in combined:
             already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in archive)
             if not already_present:
                 new_archive.append(sol)
         new_archive.sort(key=lambda x: x[-1]) # Sorted
         return np.array(new_archive[:self.archive_size])
        
    def _migrate_individuals(self, objective_function):
        """Migrates individuals between islands using a ring topology."""
        for i in range(self.num_islands):
            # Select migrants from current island
            island_fitnesses = objective_function(self.islands[i])
            num_migrants = int(self.population_size * self.migration_rate)
            migrant_indices = np.argsort(island_fitnesses)[:num_migrants]  # Best individuals migrate
            migrants = self.islands[i][migrant_indices]

            # Determine destination island (ring topology)
            destination_island = (i + 1) % self.num_islands

            # Replace worst individuals in destination island with migrants
            destination_island_fitnesses = objective_function(self.islands[destination_island])
            worst_indices = np.argsort(destination_island_fitnesses)[-num_migrants:]
            self.islands[destination_island][worst_indices] = migrants
```

### Rank 5 (Overall AOCC Score on Multimodal instances with multiple components: 0.0000e+00# Name: AdaptiveGaussianArchiveEAEnhanced
# Description: 
# Code:
```python
import numpy as np
class AdaptiveGaussianArchiveEAEnhanced:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 176.54189789612138, archive_size: int = 418.469053205273, initial_sigma_ratio: float = 0.7261308659808348,
                 sigma_decay: float = 0.9136856898964653, tournament_size: int = 2.6615521745573627, recombination_sigma_ratio: float = 0.7590694529683059,
                 archive_similarity_threshold: float = 3.2567673686796746e-05):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size  # Increased archive size for better diversity
        self.sigma = initial_sigma_ratio * (self.upper_bounds - self.lower_bounds) # Increased initial sigma
        self.sigma_decay = sigma_decay # Slightly faster decay
        self.archive = []
        self.tournament_size = tournament_size
        self.recombination_sigma_ratio = recombination_sigma_ratio
        self.archive_similarity_threshold = archive_similarity_threshold

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), self.tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_ratio, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma * self.recombination_sigma_ratio, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=self.archive_similarity_threshold) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-24 17:56:55 INFO Full response text: **Analysis:**

Comparing (Rank 1) vs (Rank 5), we see that Rank 1 uses an island model with dynamic granularity, adapting the number of islands based on fitness diversity, along with Gaussian mutation and an archive. Rank 5, on the other hand, uses a standard EA with adaptive Gaussian mutation and an archive, but lacks the island structure. The key difference is the dynamic island granularity, facilitating better exploration of multiple basins of attraction.

Comparing (Rank 2) vs (Rank 4), Rank 2 uses an island model with DE operators, adaptive archive injection, and granularity control based on diversity and stagnation. Rank 4 also has an island model with adaptive Gaussian mutation and archive-guided restarts. Rank 2 uses DE operators, while Rank 4 relies on gaussian mutation and recombination which might be faster for exploration. Rank 2 adapt population sizes based on both diversity and stagnation, while Rank 4 adaptive rate and island reset.

Comparing (Rank 1) vs (Rank 2), we see Rank 1 adaptive granularity is related to std of islands while Rank 2 adaptive population sizes are related to solution diversity. 

Comparing (Rank 3) vs (Rank 4), Rank 3 stagnation check is related to best fitness from best solutions from island.

Comparing (Rank 4) vs (Rank 5), we observe the incorporation of an island model significantly enhances the algorithm's ability to explore multiple basins compared to a single population approach. Rank 4 has island model, but has simple gaussian exploration.

Comparing (Rank 2) vs (Rank 3), Rank 2 uses archive from island population vs Rank 3 update/uses archive from outside, more general search space.

Comparing (Rank 3) vs (Rank5) , we see that Rank 3 using island model while Rank 5 using single population approach again, Rank 3 uses diversity Threshold to re-initial solution.

Comparing (second worst) vs (worst), we see the addition of island models improve performancer for group function 3 of GNBG benchmark.

Overall: Island models with dynamic adaptation of population sizes and migration strategies appear crucial for good performance. Adaptive mutation rates and archive usage also contribute significantly. Stagnation checks and island re-initialization enhance exploration of local optima. Finally, DE operators, or Gaussian recombination, helps.

**Experience:**

Multi-population approaches enhanced with an archive strategy, dynamic parameter control (population size, mutation), and adaptive island reset mechanisms perform best in challenging multimodal landscapes. Considering both population diversity and stagnation are good mechanisms for adaptation.

2025-06-24 17:56:59 INFO Full response text: Okay, let's refine self-reflection to design better heuristics, focusing on effectiveness and avoiding the pitfalls of ineffective approaches.

Here's a structured self-reflection, designed to move *beyond* the generic observations and towards actionable insights for designing superior heuristics.

*   **Keywords:** *Adaptive Balance, Diversity Maintenance, Informed Restart, Resource Allocation, Granularity of Adaptation.* (Focus is on how these are achieved intelligently.)

*   **Advice:** Instead of just stating facts, ask deep questions. How *specifically* does an archive guide mutation? What type of stagnation needs *what* type of reset? At what *granularity* should adaptation happen: solution level, population level, landscape level, or a hybrid approach? Do *existing* population measures robustly capture 'diversity', and if not, what better measures may there be? How do multiple adaptation mechanisms *interact*, and does this *interaction* amplify or counteract individual adaptation.

*   **Avoid:** Simply listing algorithm components (population size control, mutation). Avoid focusing on vague concepts of "exploration" and "exploitation." These concepts need to be directly tied to specific mechanisms and landscape features. Don't just claim diversity is 'good' without defining what diversity *metric* you mean, what a *good range* of value might be, or explain how that type of diversity assists in traversing the search space in question. Avoid high level adaptation that does not focus on specific niches, or issues, it is important to identify the problem clearly before adapting to it.

*   **Explanation:** True adaptive heuristic design requires more than just having features like "migration" or "adaptive mutation." The real key lies in *how* these components adapt based on nuanced landscape assessments. Instead of stating that X helps, you should be asking "Under *what specific conditions* should X be triggered, and *to what degree* should X be applied?".

2025-06-24 17:56:59 INFO Generating offspring via Crossover...
