2025-06-20 17:54:07 INFO Initializing first population
2025-06-20 17:54:07 INFO Initializing population from 4 seed files...
2025-06-20 17:54:07 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 17:54:13 INFO Run function 4 complete. FEHistory len: 70000, AOCC: 0.3308
2025-06-20 17:54:13 INFO FeHistory: [ 6.15929283e+05  8.83998970e+05  9.24565928e+05 ... -3.82620521e+02
 -3.82620521e+02 -3.82620521e+02]
2025-06-20 17:54:13 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 17:54:13 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 17:54:13 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:54:19 INFO Run function 7 complete. FEHistory len: 70000, AOCC: 0.3767
2025-06-20 17:54:19 INFO FeHistory: [179034.73040282 234440.55397203 276902.07776482 ...   -912.8573739
   -912.8573739    -912.85737391]
2025-06-20 17:54:19 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 17:54:19 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 17:54:19 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:54:25 INFO Run function 8 complete. FEHistory len: 70000, AOCC: 0.3754
2025-06-20 17:54:25 INFO FeHistory: [168724.82100173 279973.72229223 311963.32964909 ...   -656.78899791
   -656.78899793   -656.7889979 ]
2025-06-20 17:54:25 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 17:54:25 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 17:54:25 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 17:54:43 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0523
2025-06-20 17:54:43 INFO FeHistory: [173.33724227 177.62243677 210.44299762 ... -79.24127362 -79.24127417
 -79.24127381]
2025-06-20 17:54:43 INFO Expected Optimum FE: -100
2025-06-20 17:54:43 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
2025-06-20 17:54:43 INFO Multimodal (single component) AOCC mean: 0.3761
2025-06-20 17:54:43 INFO AOCC mean: 0.2838
2025-06-20 17:54:43 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 17:54:52 INFO Run function 4 complete. FEHistory len: 70000, AOCC: 0.1366
2025-06-20 17:54:52 INFO FeHistory: [ 5.81007253e+05  5.78416212e+05  5.81007253e+05 ... -2.83544900e+02
 -3.82022819e+02  3.10117163e+03]
2025-06-20 17:54:52 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 17:54:52 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-20 17:54:52 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:55:02 INFO Run function 7 complete. FEHistory len: 70000, AOCC: 0.2729
2025-06-20 17:55:02 INFO FeHistory: [145924.25145353 145960.40783532 145924.25145353 ...   -211.38477479
   -912.83621515   -885.36045292]
2025-06-20 17:55:02 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 17:55:02 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-20 17:55:02 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:55:11 INFO Run function 8 complete. FEHistory len: 70000, AOCC: 0.2860
2025-06-20 17:55:11 INFO FeHistory: [156553.01079532 158597.39721767 156553.01079532 ...    526.10615264
   -656.77139377   -603.51810816]
2025-06-20 17:55:11 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 17:55:11 INFO Good algorithm:
Algorithm Name: AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








2025-06-20 17:55:11 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 17:55:45 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 17:55:45 INFO FeHistory: [182.77268698 188.73930585 182.77268698 ... 147.83445408 136.63298248
 142.3702083 ]
2025-06-20 17:55:45 INFO Expected Optimum FE: -100
2025-06-20 17:55:45 INFO Multimodal (single component) AOCC mean: 0.2795
2025-06-20 17:55:45 INFO AOCC mean: 0.1739
2025-06-20 17:55:45 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 17:55:51 INFO Run function 4 complete. FEHistory len: 70000, AOCC: 0.1216
2025-06-20 17:55:51 INFO FeHistory: [ 8.15058838e+05  9.43771056e+05  1.00084900e+06 ... -3.82616260e+02
 -3.82611962e+02 -3.82612504e+02]
2025-06-20 17:55:51 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 17:55:51 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 17:55:51 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:55:56 INFO Run function 7 complete. FEHistory len: 70000, AOCC: 0.0891
2025-06-20 17:55:56 INFO FeHistory: [171188.39707336 189504.50277847 199409.72677068 ...   -912.79528782
   -912.68876929   -912.70979668]
2025-06-20 17:55:56 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 17:55:56 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 17:55:56 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:56:01 INFO Run function 8 complete. FEHistory len: 70000, AOCC: 0.1403
2025-06-20 17:56:01 INFO FeHistory: [215314.31962639 243157.24137983 216666.09207974 ...   -656.76700616
   -656.77521944   -656.77767675]
2025-06-20 17:56:01 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 17:56:01 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 17:56:01 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 17:56:18 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0444
2025-06-20 17:56:18 INFO FeHistory: [159.93432367 182.94600691 184.08691714 ... -83.35842468 -82.7405576
 -85.46007476]
2025-06-20 17:56:18 INFO Expected Optimum FE: -100
2025-06-20 17:56:18 INFO Good algorithm:
Algorithm Name: EnhancedArchiveGuidedDE
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
2025-06-20 17:56:18 INFO Multimodal (single component) AOCC mean: 0.1147
2025-06-20 17:56:18 INFO AOCC mean: 0.0989
2025-06-20 17:56:18 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 17:56:25 INFO Run function 4 complete. FEHistory len: 70000, AOCC: 0.3163
2025-06-20 17:56:25 INFO FeHistory: [ 9.72037725e+05  8.43803496e+05  8.99833163e+05 ... -3.82620470e+02
 -3.82620470e+02 -3.82620469e+02]
2025-06-20 17:56:25 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 17:56:25 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 17:56:25 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:56:31 INFO Run function 7 complete. FEHistory len: 70000, AOCC: 0.3689
2025-06-20 17:56:31 INFO FeHistory: [125014.28469207 221597.4017861  167751.22905081 ...   -912.8573578
   -912.85736072   -912.85736353]
2025-06-20 17:56:31 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 17:56:31 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 17:56:31 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:56:38 INFO Run function 8 complete. FEHistory len: 70000, AOCC: 0.3703
2025-06-20 17:56:38 INFO FeHistory: [212038.13648596 147918.05276907 198012.087564   ...   -656.78898442
   -656.78898942   -656.78898606]
2025-06-20 17:56:38 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 17:56:38 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 17:56:38 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 17:56:57 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0212
2025-06-20 17:56:57 INFO FeHistory: [169.0953593  176.22163409 183.50660891 ... -52.08165669 -51.90278452
 -52.08405329]
2025-06-20 17:56:57 INFO Expected Optimum FE: -100
2025-06-20 17:56:57 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 17:56:57 INFO Multimodal (single component) AOCC mean: 0.3696
2025-06-20 17:56:57 INFO AOCC mean: 0.2692
2025-06-20 17:56:57 INFO Started evolutionary loop, best so far: 0.2837987433248586
2025-06-20 17:56:57 INFO --- Performing Long-Term Reflection at Generation 1 ---
2025-06-20 17:57:03 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization, focusing on a localized exploration around a randomly chosen center point, leading to a more efficient initial population in high-dimensional spaces.  In contrast, EnhancedArchiveGuidedDE uses uniform random initialization, which can be inefficient for high-dimensional problems. AdaptiveGaussianSamplingEA also incorporates adaptive mutation and recombination strategies, leading to better exploitation of promising regions, while EnhancedArchiveGuidedDE relies on a relatively simple DE strategy.

(second best) AdaptiveGaussianMutationDE vs (second worst) AdaptiveMultimodalOptimizerImproved: AdaptiveGaussianMutationDE utilizes differential evolution with an adaptive Gaussian mutation, combining the global search capabilities of DE with localized refinement through Gaussian perturbations.  AdaptiveMultimodalOptimizerImproved employs a local search with simulated annealing and a tabu list, which can be less efficient in high-dimensional spaces due to the focus on local optima.  AdaptiveGaussianMutationDE's adaptive mutation scale provides more control over the exploration-exploitation balance.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianMutationDE, we see both use adaptive Gaussian mechanisms but differ in their core optimization strategies. AdaptiveGaussianSamplingEA uses a more direct approach with Gaussian recombination and mutation for population updates, creating a more controlled search.  AdaptiveGaussianMutationDE employs differential evolution, introducing more diversity but potentially less control over the search trajectory.

(3rd) AdaptiveMultimodalOptimizerImproved vs (4th) EnhancedArchiveGuidedDE: AdaptiveMultimodalOptimizerImproved attempts to escape local optima using a simulated annealing approach, whereas EnhancedArchiveGuidedDE uses an archive to guide the search, aiming for diversity in solutions. Both suffer from limitations in high-dimensional spaces: the local search of AdaptiveMultimodalOptimizerImproved might be trapped in local optima, and EnhancedArchiveGuidedDE's archive management may struggle with maintaining relevant diversity in high dimensions.

Comparing (second worst) AdaptiveMultimodalOptimizerImproved vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveMultimodalOptimizerImproved attempts to mitigate local optima issues via simulated annealing and a tabu list, whereas EnhancedArchiveGuidedDE offers a less sophisticated approach with uniform random initialization and a potentially inefficient archive management strategy.


Overall: The top-performing algorithms leverage adaptive Gaussian sampling or mutation for better initialization and population updates, showcasing the benefits of controlled exploration in high-dimensional spaces. The lower-ranked algorithms either lack effective mechanisms for escaping local optima (AdaptiveMultimodalOptimizerImproved) or suffer from inefficient exploration strategies in high dimensions (EnhancedArchiveGuidedDE).


**Experience:**

Developing effective initialization strategies for high-dimensional problems requires a careful balance between exploration and exploitation.  Adaptive Gaussian approaches, coupled with efficient population update mechanisms, are crucial for navigating complex search spaces efficiently.  Leveraging elements from successful methods such as DE or simulated annealing needs further refinement for optimal performance.

2025-06-20 17:57:03 INFO Generating offspring via Crossover...
2025-06-20 17:57:11 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 17:57:20 INFO Run function 4 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 17:57:20 INFO FeHistory: [1024573.69556172  459936.37900604  888755.91319327 ...   67251.22871133
  101738.85562228   56244.76611237]
2025-06-20 17:57:20 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 17:57:20 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:57:29 INFO Run function 7 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 17:57:29 INFO FeHistory: [162971.73728041 186923.06722716 211219.9765265  ...  16522.06626856
  25028.94270575  39552.12971847]
2025-06-20 17:57:29 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 17:57:29 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:57:38 INFO Run function 8 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 17:57:38 INFO FeHistory: [237144.17003142 263592.85168637 268485.65999012 ...  54080.60499919
  40379.90610982  39716.33733873]
2025-06-20 17:57:38 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 17:57:38 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 17:57:59 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 17:57:59 INFO FeHistory: [214.12988462 166.48700478 163.92701428 ...  83.98187656  71.94424076
 101.46480178]
2025-06-20 17:57:59 INFO Expected Optimum FE: -100
2025-06-20 17:57:59 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 17:57:59 INFO AOCC mean: 0.0000
2025-06-20 17:58:08 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 17:58:15 INFO Run function 4 complete. FEHistory len: 70000, AOCC: 0.3513
2025-06-20 17:58:15 INFO FeHistory: [ 1.65632271e+06  1.55797693e+06  1.77754191e+06 ... -3.82620505e+02
 -3.82620503e+02 -3.82620508e+02]
2025-06-20 17:58:15 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 17:58:15 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithSampling
import numpy as np
import random

# Name: AdaptiveGaussianDEwithSampling
# Description: Combines adaptive Gaussian mutation DE with localized sampling for efficient multimodal optimization.
# Code:
class AdaptiveGaussianDEwithSampling:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianDEwithSampling."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.sampling_radius = 20.0  # Initial sampling radius
        self.sampling_radius_decay = 0.95

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Localized Sampling
                if np.random.rand() < 0.1: # 10% chance of localized sampling
                    center = self.population[np.argmin(self.fitness_values)] #Best individual
                    mutant = center + np.random.normal(0, self.sampling_radius, self.dim)
                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            self.sampling_radius *= self.sampling_radius_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return center + np.random.normal(0, self.sampling_radius, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 17:58:15 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:58:22 INFO Run function 7 complete. FEHistory len: 70000, AOCC: 0.4295
2025-06-20 17:58:22 INFO FeHistory: [165403.77440725 235281.01590633 204280.45795097 ...   -912.85737046
   -912.85737001   -912.85737101]
2025-06-20 17:58:22 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 17:58:22 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithSampling
import numpy as np
import random

# Name: AdaptiveGaussianDEwithSampling
# Description: Combines adaptive Gaussian mutation DE with localized sampling for efficient multimodal optimization.
# Code:
class AdaptiveGaussianDEwithSampling:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianDEwithSampling."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.sampling_radius = 20.0  # Initial sampling radius
        self.sampling_radius_decay = 0.95

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Localized Sampling
                if np.random.rand() < 0.1: # 10% chance of localized sampling
                    center = self.population[np.argmin(self.fitness_values)] #Best individual
                    mutant = center + np.random.normal(0, self.sampling_radius, self.dim)
                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            self.sampling_radius *= self.sampling_radius_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return center + np.random.normal(0, self.sampling_radius, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 17:58:22 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:58:29 INFO Run function 8 complete. FEHistory len: 70000, AOCC: 0.4369
2025-06-20 17:58:29 INFO FeHistory: [227264.44987615 174699.35843848 119783.10311638 ...   -656.78899126
   -656.78899231   -656.78899713]
2025-06-20 17:58:29 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 17:58:29 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithSampling
import numpy as np
import random

# Name: AdaptiveGaussianDEwithSampling
# Description: Combines adaptive Gaussian mutation DE with localized sampling for efficient multimodal optimization.
# Code:
class AdaptiveGaussianDEwithSampling:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianDEwithSampling."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.sampling_radius = 20.0  # Initial sampling radius
        self.sampling_radius_decay = 0.95

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Localized Sampling
                if np.random.rand() < 0.1: # 10% chance of localized sampling
                    center = self.population[np.argmin(self.fitness_values)] #Best individual
                    mutant = center + np.random.normal(0, self.sampling_radius, self.dim)
                    mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay
            self.sampling_radius *= self.sampling_radius_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        return center + np.random.normal(0, self.sampling_radius, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

2025-06-20 17:58:29 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 17:58:48 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0069
2025-06-20 17:58:48 INFO FeHistory: [204.472197   207.71749607 196.07951444 ... -19.54035288 -19.5399631
 -19.54029313]
2025-06-20 17:58:48 INFO Expected Optimum FE: -100
2025-06-20 17:58:48 INFO Multimodal (single component) AOCC mean: 0.4332
2025-06-20 17:58:48 INFO AOCC mean: 0.3062
2025-06-20 17:58:48 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

### Worse code
AdaptiveGaussianMutationDE
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

### Analyze & experience
- Comparing (best) AdaptiveGaussianSamplingEA vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianSamplingEA uses adaptive Gaussian sampling for initialization, focusing on a localized exploration around a randomly chosen center point, leading to a more efficient initial population in high-dimensional spaces.  In contrast, EnhancedArchiveGuidedDE uses uniform random initialization, which can be inefficient for high-dimensional problems. AdaptiveGaussianSamplingEA also incorporates adaptive mutation and recombination strategies, leading to better exploitation of promising regions, while EnhancedArchiveGuidedDE relies on a relatively simple DE strategy.

(second best) AdaptiveGaussianMutationDE vs (second worst) AdaptiveMultimodalOptimizerImproved: AdaptiveGaussianMutationDE utilizes differential evolution with an adaptive Gaussian mutation, combining the global search capabilities of DE with localized refinement through Gaussian perturbations.  AdaptiveMultimodalOptimizerImproved employs a local search with simulated annealing and a tabu list, which can be less efficient in high-dimensional spaces due to the focus on local optima.  AdaptiveGaussianMutationDE's adaptive mutation scale provides more control over the exploration-exploitation balance.

Comparing (1st) AdaptiveGaussianSamplingEA vs (2nd) AdaptiveGaussianMutationDE, we see both use adaptive Gaussian mechanisms but differ in their core optimization strategies. AdaptiveGaussianSamplingEA uses a more direct approach with Gaussian recombination and mutation for population updates, creating a more controlled search.  AdaptiveGaussianMutationDE employs differential evolution, introducing more diversity but potentially less control over the search trajectory.

(3rd) AdaptiveMultimodalOptimizerImproved vs (4th) EnhancedArchiveGuidedDE: AdaptiveMultimodalOptimizerImproved attempts to escape local optima using a simulated annealing approach, whereas EnhancedArchiveGuidedDE uses an archive to guide the search, aiming for diversity in solutions. Both suffer from limitations in high-dimensional spaces: the local search of AdaptiveMultimodalOptimizerImproved might be trapped in local optima, and EnhancedArchiveGuidedDE's archive management may struggle with maintaining relevant diversity in high dimensions.

Comparing (second worst) AdaptiveMultimodalOptimizerImproved vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveMultimodalOptimizerImproved attempts to mitigate local optima issues via simulated annealing and a tabu list, whereas EnhancedArchiveGuidedDE offers a less sophisticated approach with uniform random initialization and a potentially inefficient archive management strategy.


Overall: The top-performing algorithms leverage adaptive Gaussian sampling or mutation for better initialization and population updates, showcasing the benefits of controlled exploration in high-dimensional spaces. The lower-ranked algorithms either lack effective mechanisms for escaping local optima (AdaptiveMultimodalOptimizerImproved) or suffer from inefficient exploration strategies in high dimensions (EnhancedArchiveGuidedDE).
- Developing effective initialization strategies for high-dimensional problems requires a careful balance between exploration and exploitation.  Adaptive Gaussian approaches, coupled with efficient population update mechanisms, are crucial for navigating complex search spaces efficiently.  Leveraging elements from successful methods such as DE or simulated annealing needs further refinement for optimal performance.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 17:58:48 INFO Generation 2, best so far: 0.3061604966767426
2025-06-20 17:58:48 INFO --- Performing Long-Term Reflection at Generation 2 ---
2025-06-20 17:58:52 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianDEwithSampling vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithSampling uses adaptive Gaussian mutation and localized sampling, leading to more efficient exploration of the search space and better handling of multimodal functions.  EnhancedArchiveGuidedDE, on the other hand, relies on a simpler DE strategy and an archive that may not effectively guide the search in high dimensions.  (second best) AdaptiveGaussianSamplingEA vs (second worst) AdaptiveMultimodalOptimizerImproved shows that AdaptiveGaussianSamplingEA leverages adaptive Gaussian sampling and tournament selection for effective population management. AdaptiveMultimodalOptimizerImproved employs a more simplistic local search with simulated annealing, making it less robust in high-dimensional spaces. Comparing (1st) vs (2nd), we see that AdaptiveGaussianDEwithSampling incorporates both adaptive Gaussian mutation and localized sampling, providing a more nuanced exploration strategy compared to AdaptiveGaussianSamplingEA's reliance on Gaussian perturbation. (3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveMultimodalOptimizerImproved shows that AdaptiveGaussianMutationDE uses a more sophisticated DE strategy with adaptive Gaussian perturbation, offering better exploitation compared to AdaptiveMultimodalOptimizerImproved's simpler local search. Comparing (second worst) AdaptiveMultimodalOptimizerImproved vs (worst) EnhancedArchiveGuidedDE, we observe that AdaptiveMultimodalOptimizerImproved's local search, even with its limitations, offers more targeted exploration than EnhancedArchiveGuidedDE's archive-based approach in this high-dimensional setting. Overall: The top-performing algorithms utilize adaptive Gaussian sampling and mutation strategies combined with effective population management techniques like tournament selection or localized sampling to navigate high-dimensional search spaces efficiently.  Simpler strategies like random uniform initialization and less sophisticated local searches are significantly less effective.


**Experience:**

Designing effective population initialization for high-dimensional problems requires incorporating adaptive mechanisms that adjust to the search space's characteristics and handle multimodality.  Combining diverse exploration and exploitation strategies is crucial for success.

2025-06-20 17:58:52 INFO Generating offspring via Crossover...
2025-06-20 17:59:01 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 17:59:08 INFO Run function 4 complete. FEHistory len: 70000, AOCC: 0.3684
2025-06-20 17:59:08 INFO FeHistory: [ 6.06925622e+05  9.12869725e+05  7.27742574e+05 ... -3.82620502e+02
 -3.82620510e+02 -3.82620501e+02]
2025-06-20 17:59:08 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 17:59:08 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianDEwithTournamentSelection
import numpy as np
import random

# Name: AdaptiveGaussianDEwithTournamentSelection
# Description: Combines adaptive Gaussian mutation DE with tournament selection for efficient multimodal optimization.
# Code:
class AdaptiveGaussianDEwithTournamentSelection:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        """Initializes AdaptiveGaussianDEwithTournamentSelection."""
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99
        self.tournament_size = 5


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        """Runs the optimization."""
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])
                mutant += np.random.normal(0, self.mutation_scale / 2, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1

                #Tournament Selection
                tournament_indices = np.random.choice(self.population_size, self.tournament_size, replace=False)
                tournament_fitness = self.fitness_values[tournament_indices]
                winner_index = tournament_indices[np.argmin(tournament_fitness)]

                if trial_fitness < self.fitness_values[winner_index]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[winner_index])
                    new_fitness_values.append(self.fitness_values[winner_index])

                best_solution, best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        #Adaptive Gaussian Sampling Initialization from a central point
        center = np.mean(np.array([self.lower_bounds,self.upper_bounds]),axis=0)
        return center + np.random.normal(0, (self.upper_bounds - self.lower_bounds)/4, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
2025-06-20 17:59:08 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:59:16 INFO Run function 7 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 17:59:16 INFO FeHistory: [266640.87460119 192416.2417118  233227.88292004 ...   2206.39944871
   2206.39941749   2206.39945687]
2025-06-20 17:59:16 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 17:59:16 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:59:23 INFO Run function 8 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 17:59:23 INFO FeHistory: [117461.96409175 134516.1357866  194377.77134447 ...   5055.44645939
   5055.4467957    5055.44653785]
2025-06-20 17:59:23 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 17:59:23 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 17:59:43 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 17:59:43 INFO FeHistory: [173.07509892 154.47811926 188.29264045 ... 105.53198068  86.10855214
 214.42447495]
2025-06-20 17:59:43 INFO Expected Optimum FE: -100
2025-06-20 17:59:43 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 17:59:43 INFO AOCC mean: 0.0921
2025-06-20 17:59:54 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 17:59:54 ERROR Can not run the algorithm
2025-06-20 17:59:54 INFO Run function 4 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 17:59:54 INFO FeHistory: []
2025-06-20 17:59:54 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 17:59:54 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:59:54 ERROR Can not run the algorithm
2025-06-20 17:59:54 INFO Run function 7 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 17:59:54 INFO FeHistory: []
2025-06-20 17:59:54 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 17:59:54 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 17:59:54 ERROR Can not run the algorithm
2025-06-20 17:59:54 INFO Run function 8 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 17:59:54 INFO FeHistory: []
2025-06-20 17:59:54 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 17:59:54 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 17:59:54 ERROR Can not run the algorithm
2025-06-20 17:59:54 INFO Run function 24 complete. FEHistory len: 0, AOCC: 0.0000
2025-06-20 17:59:54 INFO FeHistory: []
2025-06-20 17:59:54 INFO Expected Optimum FE: -100
2025-06-20 17:59:54 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 17:59:54 INFO AOCC mean: 0.0000
2025-06-20 17:59:54 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Worse code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Analyze & experience
- Comparing (best) AdaptiveGaussianDEwithSampling vs (worst) EnhancedArchiveGuidedDE, we see that AdaptiveGaussianDEwithSampling uses adaptive Gaussian mutation and localized sampling, leading to more efficient exploration of the search space and better handling of multimodal functions.  EnhancedArchiveGuidedDE, on the other hand, relies on a simpler DE strategy and an archive that may not effectively guide the search in high dimensions.  (second best) AdaptiveGaussianSamplingEA vs (second worst) AdaptiveMultimodalOptimizerImproved shows that AdaptiveGaussianSamplingEA leverages adaptive Gaussian sampling and tournament selection for effective population management. AdaptiveMultimodalOptimizerImproved employs a more simplistic local search with simulated annealing, making it less robust in high-dimensional spaces. Comparing (1st) vs (2nd), we see that AdaptiveGaussianDEwithSampling incorporates both adaptive Gaussian mutation and localized sampling, providing a more nuanced exploration strategy compared to AdaptiveGaussianSamplingEA's reliance on Gaussian perturbation. (3rd) AdaptiveGaussianMutationDE vs (4th) AdaptiveMultimodalOptimizerImproved shows that AdaptiveGaussianMutationDE uses a more sophisticated DE strategy with adaptive Gaussian perturbation, offering better exploitation compared to AdaptiveMultimodalOptimizerImproved's simpler local search. Comparing (second worst) AdaptiveMultimodalOptimizerImproved vs (worst) EnhancedArchiveGuidedDE, we observe that AdaptiveMultimodalOptimizerImproved's local search, even with its limitations, offers more targeted exploration than EnhancedArchiveGuidedDE's archive-based approach in this high-dimensional setting. Overall: The top-performing algorithms utilize adaptive Gaussian sampling and mutation strategies combined with effective population management techniques like tournament selection or localized sampling to navigate high-dimensional search spaces efficiently.  Simpler strategies like random uniform initialization and less sophisticated local searches are significantly less effective.
- Designing effective population initialization for high-dimensional problems requires incorporating adaptive mechanisms that adjust to the search space's characteristics and handle multimodality.  Combining diverse exploration and exploitation strategies is crucial for success.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 17:59:54 INFO Generation 3, best so far: 0.3061604966767426
2025-06-20 17:59:54 INFO --- Performing Long-Term Reflection at Generation 3 ---
2025-06-20 17:59:58 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianDEwithSampling` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best uses adaptive Gaussian mutation and localized sampling for efficient multimodal optimization, while the worst uses an archive to guide the search but lacks the adaptive mechanisms for handling complex landscapes.  (second best) `AdaptiveGaussianSamplingEA` vs (second worst) `AdaptiveMultimodalOptimizerImproved` shows that the former uses adaptive Gaussian sampling and tournament selection, improving exploration and exploitation, while the latter relies on local search and perturbation, which can get stuck in local optima. Comparing (1st) `AdaptiveGaussianDEwithSampling` vs (2nd) `AdaptiveGaussianSamplingEA`, we see that the top performer combines DE with sampling, giving it a more robust search strategy compared to the EA's simple recombination and mutation.  (3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveMultimodalOptimizerImproved` demonstrates that adding adaptive Gaussian perturbation to DE improves exploration compared to the multimodal optimizer's more basic local search approach. Comparing (second worst) `AdaptiveMultimodalOptimizerImproved` vs (worst) `EnhancedArchiveGuidedDE`, we observe that the archive-based approach suffers from a lack of adaptive mechanisms to efficiently explore the high-dimensional space, whereas the multimodal optimizer, despite its weaknesses, has an adaptive perturbation mechanism. Overall: Adaptive Gaussian mutation and sampling methods, combined with mechanisms to control exploration and exploitation (like adaptive scaling factors and localized search), consistently outperform simpler strategies.  The use of archives needs more sophisticated management to effectively guide search in high-dimensional spaces.


**Experience:**

Adaptive strategies for mutation strength and sampling radius significantly enhance performance in high-dimensional spaces. Combining different evolutionary operators (e.g., DE and Gaussian mutation) yields more robust and effective search strategies.  Careful management of archives is crucial for their effectiveness; otherwise they might hinder exploration.

2025-06-20 17:59:58 INFO Generating offspring via Crossover...
2025-06-20 18:00:07 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:00:07 ERROR Can not run the algorithm
2025-06-20 18:00:07 INFO Run function 4 complete. FEHistory len: 109, AOCC: 0.0000
2025-06-20 18:00:07 INFO FeHistory: [ 963630.6300784  1193902.24799464  883045.34301876  989153.64392906
  706541.10968653 1031561.61018199  849313.55931601  777114.00533655
  860574.00767276  970541.30426002  890565.38768013 1164151.2352437
 1209356.65333651 1189383.64739329  605401.11898664 1159767.62229838
  997538.57483903  929000.65687012  836296.55091564  776498.23922732
  907832.29556024  996399.72176152  943972.78422602  600351.71819865
  758276.0909152  1059170.58468655  534254.06003455 1010118.83178803
  607020.31330179  836434.16196729 1137390.12425461  586269.18497463
  995156.0135875   645994.79104912  994288.6113507  1343654.34008588
  656414.41064985  955092.17778382 1158400.84965531 1333586.2377992
 1059312.02182852 1341327.36188556  602955.44838368  754100.65308033
 1180974.11775944  754256.99316503 1076522.12439133  938840.4133998
  646431.428004    798357.39245238 1271558.57191314 1009705.35926877
  947799.46980627  917154.50428896 1292823.56312351 1006838.00262135
  881739.49000924  975106.78457537  863834.28447428  672027.9393372
  881473.43669655 1059321.45273509  792538.95986741  729994.10561304
 1190411.96979262  653769.39503485 1021491.90661443  697934.95257878
  991871.18665529  651213.14150367  586873.78593093 1323010.03356576
  686610.41471121  557934.2359856  1482164.29400297  881959.23442816
 1023455.2361966   759677.03430483  885111.41231145 1069900.30084703
  551067.848323   1033987.23765364  868087.0636869   663542.44370202
  890754.0554892   678414.56207863  490032.30470263  894301.07444651
  986476.66397801 1333518.54218933  926808.43808267 1004786.91570606
  548805.20721675  885504.67664669  738856.61450755 1269398.32564567
  947097.10441992  741722.94672274 1041505.60929044  826424.59320179
 1411951.79909881  729897.24251709  960605.5411141  1002900.21282858
 1025774.60150384  792909.1166114  1291614.61765286  913024.33501001
  811248.55718548]
2025-06-20 18:00:07 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:00:07 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:07 ERROR Can not run the algorithm
2025-06-20 18:00:07 INFO Run function 7 complete. FEHistory len: 105, AOCC: 0.0000
2025-06-20 18:00:07 INFO FeHistory: [245602.98405404 221210.10318625 298806.67291685 199237.18809957
 202038.73107689  93373.12695433 252074.25865522 329313.8664137
 274428.36653654 294568.35060236 165416.57997571 233227.46170206
 301877.12011886  94415.63721912 190244.2018372  266351.06610273
 248581.16164876 216993.12488493 252955.43235913 233776.60893197
 255285.58775884 177338.57608058 132100.15505479 190408.29338343
 180837.71664883 357469.42282845 124498.85738432 271187.00567281
 158486.81821368 190100.33344667 145434.94482757 170979.93522171
 184096.45554254 203412.62683525 172631.83801146 231088.54947853
 181245.81082235 169264.44370261 213887.0318778  133300.85483395
 172229.24307822 140817.00606508 203820.86182025 233450.47448903
 200582.02979288 215167.69398597 147754.04645384 224956.10077719
 233300.6301419  298636.40270475 205440.48226034 143701.760366
 302339.72361573 135203.20945208 302620.91375734 215000.80999735
 161273.13292463 177521.64225191 182174.49138715 196964.68225984
 213108.98152334 122646.54774057 215836.64958161 201727.51868471
 252889.58389442 241650.61529599 244268.0566582  198213.17685276
 316434.63026912 187280.64999342  93677.80889045 255641.87192923
 228375.8380754   95624.81375023 303576.36895663 141118.46087605
 226935.61287795 182361.13062831 254588.44996645 171786.6364206
 142462.68811503 109225.36665843 184677.00828061 154757.50453126
 178930.18244642 233350.81534355 118135.12280258 200725.24069693
  98807.55080584 241728.63904468 197639.83798535 205339.36857845
 312747.40443379 148717.31928954 144414.89825601 175051.54625681
 143700.59841693 165927.91867251 291329.83227864 210034.60590262
 293624.22047406 162452.495317   287830.09068561 228673.38131551
 226442.52773845]
2025-06-20 18:00:07 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:00:07 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:07 ERROR Can not run the algorithm
2025-06-20 18:00:07 INFO Run function 8 complete. FEHistory len: 103, AOCC: 0.0000
2025-06-20 18:00:07 INFO FeHistory: [202033.90487315 149187.77223986 130097.50362848 121686.20449049
 187233.93815208 250727.82245518 161082.19856348 190511.67223142
 170064.34157773 245828.20636824 171133.54223562 175937.4197715
 172671.7353708  130146.59814244 106633.31253915 170248.73111016
 186307.89735815 246544.14108955 152141.73582152 162149.47577832
 155965.76489103 201748.12889623 169097.89520909 146610.31117927
 170471.28418729 171545.10398141 130143.67475901 254778.94538321
 181872.24167871 327954.04193807 266301.14660212 221440.95479113
 183556.69019481 122503.54820495 137759.88937    209912.97135807
 116992.0657175  232522.82481072 260081.25730082 171067.45394291
 210336.93006073 233011.08173612 122346.62367572 127524.68034079
 195935.65060046 180026.06383818 153191.19630218 107777.30702883
 262315.48015673 257524.97300737 168472.57793201 245200.17215938
 332501.53198849 130742.08729223 233399.32320004 190733.34044064
 214072.90054128 145900.39777365 202541.39841181 202523.97091571
 228425.48706427 177701.34137873 335763.47647561 252825.27130427
 132258.37703988 250913.88072372 190596.53474708 212547.05716379
 161212.93343326 229092.69071965 147551.13459438 265768.32914423
 155056.65376582 115112.04250744 216927.60259832 253815.02379014
 155337.99199198 258161.37253125 176897.8591863  261505.28229053
 134372.33012891 194964.24521484 207312.19922612 150940.82363452
 192080.90151925 161500.97946348 170798.82637817 245008.19006463
 144081.39289809 183248.90864892 198681.00860045 296270.75765891
 207061.69588717 180649.73472842 198433.51552991 158073.97892273
 133163.26930546 232158.79343705 164647.40957061 169134.11721212
 235985.3309537  118335.31003774 126807.54834497]
2025-06-20 18:00:07 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:00:07 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:00:07 ERROR Can not run the algorithm
2025-06-20 18:00:07 INFO Run function 24 complete. FEHistory len: 107, AOCC: 0.0000
2025-06-20 18:00:07 INFO FeHistory: [160.54388693 168.46628018 145.15770318 182.13883693 187.81939599
 213.4381008  212.52380811 173.24663418 201.57107754 206.43049004
 191.08947942 214.90782371 197.74824666 189.76593092 166.92728102
 193.95603504 194.09736818 179.76772202 208.09995828 210.09828078
 176.7846041  193.42616227 197.24365331 196.42933284 179.67727654
 186.56513302 199.98650953 168.94971958 170.32251175 178.37068629
 195.31312077 196.5081051  205.85730607 204.20758397 191.59865013
 184.03297934 204.40098856 176.79068938 203.59313894 204.02365405
 165.57574898 214.38523994 197.92991602 179.35752208 205.49642587
 165.64863772 170.20672825 147.57195153 182.21531055 156.29928877
 204.002934   168.62647323 202.39311237 174.98471808 196.72849188
 167.11068505 168.89233626 176.54777448 187.54409998 180.75688164
 170.08112681 179.19272353 196.58324306 185.66207596 185.71857967
 169.7214778  185.03265652 157.66345433 181.44126522 166.24878031
 173.14908575 176.16604292 187.59081887 162.30159247 222.69089151
 172.1578544  194.80061948 194.56208567 184.94858028 220.14411275
 189.89278388 202.42145805 209.75224263 178.04917933 173.60284099
 177.3128798  139.58949426 169.31491767 187.90749859 182.7235236
 161.25558787 154.47556426 195.45753147 197.10088806 180.42475002
 208.20912257 180.02247861 191.60401219 203.42218404 192.08121599
 199.18473516 189.95573093 177.28453385 188.58564882 182.79138357
 203.70830321 224.04368359]
2025-06-20 18:00:07 INFO Expected Optimum FE: -100
2025-06-20 18:00:07 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:00:07 INFO AOCC mean: 0.0000
2025-06-20 18:00:16 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:00:16 ERROR Can not run the algorithm
2025-06-20 18:00:16 INFO Run function 4 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:00:16 INFO FeHistory: [ 959705.26050194 1118025.10070744 1085563.17664158 1157785.09168065
 1068289.97396719 1132346.12681022 1133472.40084608 1002039.45986092
 1106545.13740652 1034613.82554236 1589443.85048698 1208464.63892285
  901906.38294158 1175786.23546813 1055648.53316863 1274636.17549976
 1210655.04806795 1093400.61306258 1280714.05025562 1292571.65165693
 1213147.67590319 1320601.47392796 1140298.2592176   999586.50612745
 1190597.75651155 1258391.72919558 1053831.2740204  1155238.68374904
  981817.93223795 1390577.68443525 1108388.71742021 1020755.00319863
 1058452.11439348 1041706.89295413  939523.91991308 1012508.24113984
 1081928.60762214 1067181.43335264 1334476.54776844  813584.11408368
 1161252.46343013 1230600.1875561  1223744.11758628 1621578.80423639
  997389.11955054 1146189.47619967 1382375.00120331  951221.68713285
 1591384.93542458  815859.9621863  1049370.59774433  912958.90215938
 1322985.59842874 1269117.45826311 1347836.14756618 1375856.1148196
  983744.45922035 1189605.1880275  1269559.03641222 1354446.09602889
 1342249.97622585 1211842.86729329 1096130.93376051  868251.0466755
 1276872.38813088 1293069.83839615 1154454.32823275 1181489.55070163
 1449639.18796178 1156864.70109043 1278924.05494412 1087319.16384406
 1008240.23701779 1186602.72607445 1195028.69231022 1412619.86340597
 1433222.2696112  1132897.34660885 1543395.33197615 1457880.49362765
 1188679.13600534  787682.08581044 1036044.07290289 1414499.73830483
 1247321.55510907 1631809.03476053 1186523.82457453 1222733.09672563
  910712.42806719 1272221.87224063 1246289.26314471 1213545.30885856
 1010132.83207928 1018992.66694502 1336913.31967047 1372414.35599248
 1440898.10844682 1271262.48621063 1276841.40184179  718345.30962454]
2025-06-20 18:00:16 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:00:16 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:16 ERROR Can not run the algorithm
2025-06-20 18:00:16 INFO Run function 7 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-20 18:00:16 INFO FeHistory: [236388.27712477 194745.49823654 300222.21902355 182378.8228967
 363586.01430339 205822.0722968  235494.34804942 274537.73376246
 260567.14217025 221057.52071557 308356.96756973 192020.46449535
 231419.06379732 252840.97692625 251868.74147502 252815.08395011
 228903.613736   277821.58201421 255641.18091147 281205.96043368
 170680.0985875  211430.79044514 181335.67576748 284823.63258539
 265050.59961647 179317.06464469 246988.46894527 212384.91279143
 267229.80290113 206665.01437955 184605.24350729 133232.14157787
 320746.81532287 208032.82922227 234530.04175366 191236.67093639
 224009.60328802 214302.6536903  271240.51865445 291785.13624979
 187515.54372547 222717.40398805 290259.46310885 169237.13371989
 295462.8089579  216457.53240959 261897.30631902 233091.02070578
 213559.92918726 299741.48651436 175143.16040977 312821.95322899
 196016.13693676 201800.60986851 232259.22779942 174249.67304149
 206465.93062203 197156.64135024 321598.78035982 135318.97881174
 211916.68439773 224950.06219943 235174.23746882 222922.53698876
 260923.84434495 302480.85519136 209268.3876597  355376.23524806
 310098.54056584 238316.2160792  183176.80768984 248668.55330524
 212036.84468821 180251.44748188 286146.09398308 266980.03275098
 249345.6407113  176042.79876884 203501.15789    259183.00391551
 250090.68705869 372406.19459472 185696.33953792 245038.69033916
 294496.61030573 192038.54157218 275216.29028283 182324.11257546
 233012.27880651 181151.72744961 268317.24193733 342455.73948268
 236793.83388228 280798.42846385 259581.89638184 310776.43133655
 197255.49659221 210003.04163807 208312.52568641 249740.80025417
 296121.03075568 184629.99981278 281425.79845801 176834.33725017
 279935.96689373 347242.2547081  306980.31082459 140033.85727504
 432917.63295164 214390.75455086 346634.64343545 275807.6320197
 261091.18145196 306355.19447625 281558.00986747 281326.52174166
 197010.2755483  234499.29012236 313311.83612071 336271.71477501
 260323.38316521 264238.2732934  153304.91801975 272633.31749918
 285107.04386329 180803.97281321 339652.17843721 237177.44756905
 304887.25541881 352332.44194367 418113.40342041 228852.98185472
 385789.15353648 225963.38153319 283559.93500215 250124.84874574
 210808.88810258 256844.72993267 334414.86192118 202836.14773846
 262736.40314021 255079.81256745 288363.78930941 227912.11975362
 288028.75594743 238621.21839791 329640.92133755 278545.4441395
 425064.18259142 380596.336857   250146.68526905 200859.76595168
 245975.76562173 252197.57592599 241932.00631575 246858.95765567
 285940.22820051 218808.78342207 202689.37105565 348732.18656136
 300181.2212003  287175.58272276 358485.25704931 219943.33228818
 320158.61186584 316630.47333795 322094.7120983  344599.27633196
 214017.81765389 246947.36078131 351831.59719501 366816.39437556
 245835.32896766 287943.17165658 216878.55782126 239921.84711503
 383256.27511791 280777.62568396 357862.28770069 171166.33101336
 333477.88142802 291129.34844753 242104.28298683 227403.30759224
 347087.71846952 326528.21479414 254435.7544063  427817.18349565
 245324.89445352 319099.10677481 280018.51523763 231386.62706682
 236776.804764   318007.67939987 336843.53078152 255206.33848901
 205203.80780541 213351.64875867 300508.36469707 235490.9324519 ]
2025-06-20 18:00:16 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:00:16 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:16 ERROR Can not run the algorithm
2025-06-20 18:00:16 INFO Run function 8 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:00:16 INFO FeHistory: [226351.37958869 161238.36013544 208796.32676281 161170.17319763
 298841.12483211 236393.4387033  146012.59595456 123572.07731984
 168485.68155439 299823.43740649 190652.44932872 260376.85527247
 130203.23364191 211267.84015778 254918.11005545 224593.98775791
 236451.95809328 173108.98677042 231356.34744947 227277.72094241
 180349.29044027 110963.2241753  201677.35550072 241364.3085485
 160905.37359263 148203.17128081 206560.85336661 204913.58051406
 154707.69645021 155989.07986959 203548.09596889 229653.66477886
 229190.62754954 191154.22093771 219701.53052286 268647.62524495
 173162.6607978  235788.34517475 195562.05062396 252388.7746573
 253047.91335921 206181.22994605 221827.36960978 196109.33825635
 138021.89367349 204187.55325319 231814.68049721 157715.85567797
 210646.82807457 187537.48197544 186622.14564415 187575.00234957
 199415.22949946 256026.60808102 231997.27706837 209446.81875831
 226611.17539327 280114.10226834 188565.84748572 190161.95821452
 197190.10106735 215430.14297463 185808.83357041 166232.35279173
 188170.73172093 228057.15810164 253028.72539531 207797.21683229
 190632.16271087 213644.81830403 238722.24723916 169560.69295034
 171362.9816033  248764.05232185 272009.21654511 188261.77522253
 214691.30054113 264131.82761674 214215.34179695 190338.06005185
 192707.4230682  183787.58861132 197003.40194672 170345.76580683
 185215.35136318 155614.01326655 204698.53296888 179083.28868313
 179871.86551425 199070.01464033 252078.9713063  210610.81175689
 209829.94732075 257718.98400281 173565.99060746 149501.01212435
 249203.54337076 183575.27194179 219930.29617972 222450.34433857]
2025-06-20 18:00:16 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:00:16 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:00:16 ERROR Can not run the algorithm
2025-06-20 18:00:17 INFO Run function 24 complete. FEHistory len: 100, AOCC: 0.0000
2025-06-20 18:00:17 INFO FeHistory: [205.17988406 194.06248573 222.52452461 180.57024139 201.19593797
 193.63110534 207.37802345 234.48010433 201.39823388 199.80660547
 190.31114836 201.1551067  234.07220113 169.3807859  214.74683507
 218.19998322 202.51133584 157.84879602 206.2947559  211.28160124
 201.64622555 209.44751113 203.73880152 207.99919943 229.445263
 185.61828798 226.72798039 200.50820425 192.36204063 203.56814651
 206.78104998 158.17676556 212.23765704 187.87293583 230.78858276
 169.40093736 194.41688704 151.0611596  221.09100274 183.89035977
 151.3533229  191.62128374 175.79235198 165.05822501 181.9048054
 130.32016912 206.71811827 185.17564459 188.58842058 184.84091357
 184.86724264 172.87691535 132.21890334 224.41642085 223.40158767
 183.97277084 182.56568627 188.91239534 202.70118882 217.94051554
 186.84951562 206.46543545 181.14014726 167.99075337 182.30519555
 193.38098433 212.58715308 187.74846945 202.37247597 199.52066833
 207.51209441 199.08393972 239.10447757 188.58679211 222.94991719
 209.53281238 200.272111   191.75559833 188.51798475 185.67505246
 181.34144791 216.62546743 172.9302574  204.98527581 196.11635346
 167.91900582 192.09116839 254.13087515 205.77782324 192.68093388
 169.58009054 231.02473903 200.41853821 210.0966719  179.7982694
 214.40311129 195.76367282 208.30829053 198.41200858 178.42382503]
2025-06-20 18:00:17 INFO Expected Optimum FE: -100
2025-06-20 18:00:17 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:00:17 INFO AOCC mean: 0.0000
2025-06-20 18:00:17 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Worse code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Analyze & experience
- Comparing (best) `AdaptiveGaussianDEwithSampling` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best uses adaptive Gaussian mutation and localized sampling for efficient multimodal optimization, while the worst uses an archive to guide the search but lacks the adaptive mechanisms for handling complex landscapes.  (second best) `AdaptiveGaussianSamplingEA` vs (second worst) `AdaptiveMultimodalOptimizerImproved` shows that the former uses adaptive Gaussian sampling and tournament selection, improving exploration and exploitation, while the latter relies on local search and perturbation, which can get stuck in local optima. Comparing (1st) `AdaptiveGaussianDEwithSampling` vs (2nd) `AdaptiveGaussianSamplingEA`, we see that the top performer combines DE with sampling, giving it a more robust search strategy compared to the EA's simple recombination and mutation.  (3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveMultimodalOptimizerImproved` demonstrates that adding adaptive Gaussian perturbation to DE improves exploration compared to the multimodal optimizer's more basic local search approach. Comparing (second worst) `AdaptiveMultimodalOptimizerImproved` vs (worst) `EnhancedArchiveGuidedDE`, we observe that the archive-based approach suffers from a lack of adaptive mechanisms to efficiently explore the high-dimensional space, whereas the multimodal optimizer, despite its weaknesses, has an adaptive perturbation mechanism. Overall: Adaptive Gaussian mutation and sampling methods, combined with mechanisms to control exploration and exploitation (like adaptive scaling factors and localized search), consistently outperform simpler strategies.  The use of archives needs more sophisticated management to effectively guide search in high-dimensional spaces.
- Adaptive strategies for mutation strength and sampling radius significantly enhance performance in high-dimensional spaces. Combining different evolutionary operators (e.g., DE and Gaussian mutation) yields more robust and effective search strategies.  Careful management of archives is crucial for their effectiveness; otherwise they might hinder exploration.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:00:17 INFO Generation 4, best so far: 0.3061604966767426
2025-06-20 18:00:17 INFO --- Performing Long-Term Reflection at Generation 4 ---
2025-06-20 18:00:21 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianDEwithSampling` vs (worst) `EnhancedArchiveGuidedDE`, we see that adaptive Gaussian mutation and localized sampling in the best performer lead to significantly better exploration and exploitation of the search space compared to the simpler DE approach with an archive in the worst performer. The adaptive mechanisms allow the algorithm to adjust its search strategy based on the current state of the optimization process, which is crucial for high-dimensional problems.  `EnhancedArchiveGuidedDE` lacks such adaptive components. (second best) `AdaptiveGaussianSamplingEA` vs (second worst) `AdaptiveMultimodalOptimizerImproved`: The former utilizes adaptive Gaussian sampling and a more robust selection mechanism (tournament selection) leading to better convergence than the latter's simpler local search approach with simulated annealing. `AdaptiveGaussianSamplingEA` incorporates adaptive mutation and recombination operators, making it more efficient. `AdaptiveMultimodalOptimizerImproved` struggles with the high dimensionality, showing slower and less efficient exploration. Comparing (1st) `AdaptiveGaussianDEwithSampling` vs (2nd) `AdaptiveGaussianSamplingEA`, we see that while both use adaptive Gaussian approaches, the DE framework in the best-performing algorithm introduces a more efficient way to manage the balance between exploration and exploitation. `AdaptiveGaussianSamplingEA` relies more heavily on Gaussian perturbations, potentially leading to less effective exploration in complex landscapes. (3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveMultimodalOptimizerImproved`: The former uses Differential Evolution with adaptive Gaussian mutation, providing a more directed search compared to the latter's less structured local search and simulated annealing based approach.  The adaptive mutation in `AdaptiveGaussianMutationDE` helps fine-tune the search intensity throughout optimization. Comparing (second worst) `AdaptiveMultimodalOptimizerImproved` vs (worst) `EnhancedArchiveGuidedDE`, we see that `AdaptiveMultimodalOptimizerImproved` at least employs some local search, which is a valuable technique for local optimization.  `EnhancedArchiveGuidedDE`, on the other hand, relies only on a static population size and archive, missing the crucial adaptive elements that make the top performers successful. Overall: The top-performing algorithms effectively combine adaptive mechanisms, robust selection operators, and carefully designed mutation strategies to efficiently navigate the high-dimensional search space.  The lower-performing algorithms lack these crucial adaptive elements and show less effective exploration and exploitation.


**Experience:**

Adaptive mechanisms (mutation scale decay, sampling radius decay, sigma decay) are crucial for handling high-dimensional spaces with wide bounds.  Combining DE with Gaussian perturbations is highly effective.  Robust selection (tournament) surpasses simpler approaches.  A well-designed archive with diversity considerations improves performance.

2025-06-20 18:00:21 INFO Generating offspring via Crossover...
2025-06-20 18:00:31 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:00:31 ERROR Can not run the algorithm
2025-06-20 18:00:31 INFO Run function 4 complete. FEHistory len: 564, AOCC: 0.0000
2025-06-20 18:00:31 INFO FeHistory: [ 604065.22296714  748890.88026104  754410.45918784 1174835.43977432
  916542.8596288   508388.58259724 1216811.5433157   928540.46973745
  723241.42875538 1109587.00584258  686998.11225463  810985.40494612
  748951.99816649 1095687.45648329  803597.44313672  492945.16728454
  748086.97509551  688043.75907183 1129730.15975319 1299667.82547776
  767211.06929561  981898.36274905  456701.5111913   791273.90969976
  856928.86008991  954144.15185148  727203.35710961 1057557.38073854
  873636.52936356  645622.19880686  846906.06385126 1715036.37965644
  781537.89026955  629038.16575406  577821.58213513 1236155.29177615
  647863.21391613  763950.06663065 1058636.7937702   844627.96978503
  936718.97040018 1166066.89406323  497481.97744517  878478.32965825
  733384.38056252  824084.9884458   531684.40637716  523134.58231038
 1046930.05945645  508740.24484452 1085801.90040752 1188538.66055852
  907692.23075815 1162933.08767855  851179.02996789  938016.07369185
 1023741.31505661  656981.30161796  815214.00327473  764952.93618946
  757694.6419863   508062.60658651  735958.32279097  959480.41303659
  800313.74206028  817023.0335568   800922.76563779  621622.20419322
 1166440.23025528 1107744.89473906  909066.0182803   851015.58118756
 1597128.95535151  867651.87558508  312966.67813381 1408737.24826818
  738258.93434379  658153.16286681 1011006.90955457 1093771.60007527
 1063108.88552771  546250.55185299  740009.80903599  715534.31336406
  747848.27746939  741282.45440245  616588.213715   1313633.23248637
  842886.18004814  915534.48926499 1437493.3207177   497191.52894243
 1135017.70037508  699816.74247686 1290497.64334205  687030.96023557
  914052.87019125  808939.13280213 1370662.57306044 1162762.19345028
  638570.12123578  604065.22296714  898533.51024598  604065.22296714
  748890.88026104 1221922.17158101  604065.22296714  748890.88026104
  754410.45918784 1430195.59252852  604065.22296714  748890.88026104
  754410.45918784 1174835.43977432  897001.77002072  604065.22296714
  748890.88026104  754410.45918784 1174835.43977432  897001.77002072
 1296788.45654679  604065.22296714  748890.88026104  754410.45918784
 1174835.43977432  897001.77002072  508388.58259724 1120994.66872097
  604065.22296714  748890.88026104  754410.45918784 1174835.43977432
  897001.77002072  508388.58259724 1120994.66872097 1363888.56160906
  604065.22296714  748890.88026104  754410.45918784 1174835.43977432
  897001.77002072  508388.58259724 1120994.66872097  928540.46973745
 1008799.73855277  604065.22296714  748890.88026104  754410.45918784
 1174835.43977432  897001.77002072  508388.58259724 1120994.66872097
  928540.46973745  723241.42875538  785152.3017657   604065.22296714
  748890.88026104  754410.45918784 1174835.43977432  897001.77002072
  508388.58259724 1120994.66872097  928540.46973745  723241.42875538
  785152.3017657   449181.32676798  604065.22296714  748890.88026104
  754410.45918784 1174835.43977432  897001.77002072  508388.58259724
 1120994.66872097  928540.46973745  723241.42875538  785152.3017657
  449181.32676798  641909.00642838  604065.22296714  748890.88026104
  754410.45918784 1174835.43977432  897001.77002072  508388.58259724
 1120994.66872097  928540.46973745  723241.42875538  785152.3017657
  449181.32676798  641909.00642838  714576.11407143  604065.22296714
  748890.88026104  754410.45918784 1174835.43977432  897001.77002072
  508388.58259724 1120994.66872097  928540.46973745  723241.42875538
  785152.3017657   449181.32676798  641909.00642838  714576.11407143
  784256.97326673  604065.22296714  748890.88026104  754410.45918784
 1174835.43977432  897001.77002072  508388.58259724 1120994.66872097
  928540.46973745  723241.42875538  785152.3017657   449181.32676798
  641909.00642838  714576.11407143  784256.97326673  977269.22608441
  604065.22296714  748890.88026104  754410.45918784 1174835.43977432
  897001.77002072  508388.58259724 1120994.66872097  928540.46973745
  723241.42875538  785152.3017657   449181.32676798  641909.00642838
  714576.11407143  784256.97326673  803597.44313672  857984.20536421
  604065.22296714  748890.88026104  754410.45918784 1174835.43977432
  897001.77002072  508388.58259724 1120994.66872097  928540.46973745
  723241.42875538  785152.3017657   449181.32676798  641909.00642838
  714576.11407143  784256.97326673  803597.44313672  492945.16728454
  747794.96573987  604065.22296714  748890.88026104  754410.45918784
 1174835.43977432  897001.77002072  508388.58259724 1120994.66872097
  928540.46973745  723241.42875538  785152.3017657   449181.32676798
  641909.00642838  714576.11407143  784256.97326673  803597.44313672
  492945.16728454  747794.96573987  549335.40200548  604065.22296714
  748890.88026104  754410.45918784 1174835.43977432  897001.77002072
  508388.58259724 1120994.66872097  928540.46973745  723241.42875538
  785152.3017657   449181.32676798  641909.00642838  714576.11407143
  784256.97326673  803597.44313672  492945.16728454  747794.96573987
  549335.40200548 1228676.04785697  604065.22296714  748890.88026104
  754410.45918784 1174835.43977432  897001.77002072  508388.58259724
 1120994.66872097  928540.46973745  723241.42875538  785152.3017657
  449181.32676798  641909.00642838  714576.11407143  784256.97326673
  803597.44313672  492945.16728454  747794.96573987  549335.40200548
 1129730.15975319 1254718.26595514  604065.22296714  748890.88026104
  754410.45918784 1174835.43977432  897001.77002072  508388.58259724
 1120994.66872097  928540.46973745  723241.42875538  785152.3017657
  449181.32676798  641909.00642838  714576.11407143  784256.97326673
  803597.44313672  492945.16728454  747794.96573987  549335.40200548
 1129730.15975319 1254718.26595514  831992.0955755   604065.22296714
  748890.88026104  754410.45918784 1174835.43977432  897001.77002072
  508388.58259724 1120994.66872097  928540.46973745  723241.42875538
  785152.3017657   449181.32676798  641909.00642838  714576.11407143
  784256.97326673  803597.44313672  492945.16728454  747794.96573987
  549335.40200548 1129730.15975319 1254718.26595514  767211.06929561
  738763.11174105  604065.22296714  748890.88026104  754410.45918784
 1174835.43977432  897001.77002072  508388.58259724 1120994.66872097
  928540.46973745  723241.42875538  785152.3017657   449181.32676798
  641909.00642838  714576.11407143  784256.97326673  803597.44313672
  492945.16728454  747794.96573987  549335.40200548 1129730.15975319
 1254718.26595514  767211.06929561  738763.11174105  586572.73438776
  604065.22296714  748890.88026104  754410.45918784 1174835.43977432
  897001.77002072  508388.58259724 1120994.66872097  928540.46973745
  723241.42875538  785152.3017657   449181.32676798  641909.00642838
  714576.11407143  784256.97326673  803597.44313672  492945.16728454
  747794.96573987  549335.40200548 1129730.15975319 1254718.26595514
  767211.06929561  738763.11174105  456701.5111913   748419.85112405
  604065.22296714  748890.88026104  754410.45918784 1174835.43977432
  897001.77002072  508388.58259724 1120994.66872097  928540.46973745
  723241.42875538  785152.3017657   449181.32676798  641909.00642838
  714576.11407143  784256.97326673  803597.44313672  492945.16728454
  747794.96573987  549335.40200548 1129730.15975319 1254718.26595514
  767211.06929561  738763.11174105  456701.5111913   748419.85112405
  856183.7671773   604065.22296714  748890.88026104  754410.45918784
 1174835.43977432  897001.77002072  508388.58259724 1120994.66872097
  928540.46973745  723241.42875538  785152.3017657   449181.32676798
  641909.00642838  714576.11407143  784256.97326673  803597.44313672
  492945.16728454  747794.96573987  549335.40200548 1129730.15975319
 1254718.26595514  767211.06929561  738763.11174105  456701.5111913
  748419.85112405  856183.7671773   822134.13317071  604065.22296714
  748890.88026104  754410.45918784 1174835.43977432  897001.77002072
  508388.58259724 1120994.66872097  928540.46973745  723241.42875538
  785152.3017657   449181.32676798  641909.00642838  714576.11407143
  784256.97326673  803597.44313672  492945.16728454  747794.96573987
  549335.40200548 1129730.15975319 1254718.26595514  767211.06929561
  738763.11174105  456701.5111913   748419.85112405  856183.7671773
  822134.13317071  848902.79332161  604065.22296714  748890.88026104
  754410.45918784 1174835.43977432  897001.77002072  508388.58259724
 1120994.66872097  928540.46973745  723241.42875538  785152.3017657
  449181.32676798  641909.00642838  714576.11407143  784256.97326673
  803597.44313672  492945.16728454  747794.96573987  549335.40200548
 1129730.15975319 1254718.26595514  767211.06929561  738763.11174105
  456701.5111913   748419.85112405  856183.7671773   822134.13317071
  727203.35710961  909767.29439151  604065.22296714  748890.88026104
  754410.45918784 1174835.43977432  897001.77002072  508388.58259724
 1120994.66872097  928540.46973745  723241.42875538  785152.3017657
  449181.32676798  641909.00642838  714576.11407143  784256.97326673
  803597.44313672  492945.16728454  747794.96573987  549335.40200548
 1129730.15975319 1254718.26595514  767211.06929561  738763.11174105
  456701.5111913   748419.85112405  856183.7671773   822134.13317071
  727203.35710961  909767.29439151 1191299.00357827  604065.22296714
  748890.88026104  754410.45918784 1174835.43977432  897001.77002072
  508388.58259724 1120994.66872097  928540.46973745  723241.42875538
  785152.3017657   449181.32676798  641909.00642838  714576.11407143
  784256.97326673  803597.44313672  492945.16728454  747794.96573987
  549335.40200548 1129730.15975319 1254718.26595514  767211.06929561
  738763.11174105  456701.5111913   748419.85112405  856183.7671773
  822134.13317071  727203.35710961  909767.29439151  873636.52936356]
2025-06-20 18:00:31 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:00:31 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:31 ERROR Can not run the algorithm
2025-06-20 18:00:31 INFO Run function 7 complete. FEHistory len: 270, AOCC: 0.0000
2025-06-20 18:00:31 INFO FeHistory: [218395.69575617 220490.84482598 246486.2704729  161842.22590038
 193849.81021221 133203.31352309 123126.86131567 264070.69835849
 207403.25428368 147228.44516442 235815.12599851 215942.01269
 182029.95978973 225588.57693463 134539.22037348 155592.82140788
 241864.04495156  61451.39941859 303523.81154154 227317.08635377
 165980.14042332 228571.46952009 213582.45744062 192044.59434707
 156735.58444362 253134.47217726 272955.34476452 230624.17254528
 231132.2523779  173848.17333857 235090.07690786 281446.93364723
 200811.55477136 268965.20474824 164398.5245737  163327.62208882
 122951.60065147 234536.01851244 263152.17671024 171737.77606429
 163974.78112138 245251.07846148 207910.70803836 245648.02609459
 159447.08774317 164606.60173883 152464.18791798 198093.76319722
 277970.20605628 134708.8602615  253073.56926033 196004.86689224
 202770.88472812 224936.9340385  151863.12803153 257954.05385737
 121425.22043565 224379.88269956 290156.34260672 189638.60529106
 162272.5442001  121323.25345234 233115.62419797 197887.08696547
 123632.84278264 308901.19642756 225641.88071223 161929.37406067
 163359.32472207 180994.61598457 143765.01957192 337445.44176097
 203220.89310343 235495.04882213 190117.45244144 267120.83836365
 220424.12030639 270849.6707537  228880.13095197 216108.20544543
 151297.48770989 187958.51576977 183183.33446978 237267.75715872
 166443.34728791 161107.26037913 166470.08416081 174968.9602948
 225999.25219757 131121.14172176 281842.2279039  234704.1681344
 240659.61725895 170727.55723329 223533.09161044 267239.06998113
 244133.42567591 199486.97076589 194689.54577045 299349.8527792
 222529.0624454  218395.69575617 265086.52137816 218395.69575617
 220490.84482598 282349.15349818 218395.69575617 220490.84482598
 246486.2704729  210651.03707396 218395.69575617 220490.84482598
 246486.2704729  161842.22590038 197765.36078975 218395.69575617
 220490.84482598 246486.2704729  161842.22590038 193849.81021221
 135417.82776065 218395.69575617 220490.84482598 246486.2704729
 161842.22590038 193849.81021221 133203.31352309 174195.06586666
 218395.69575617 220490.84482598 246486.2704729  161842.22590038
 193849.81021221 133203.31352309 123126.86131567 236631.16679427
 218395.69575617 220490.84482598 246486.2704729  161842.22590038
 193849.81021221 133203.31352309 123126.86131567 236631.16679427
 237445.72273568 218395.69575617 220490.84482598 246486.2704729
 161842.22590038 193849.81021221 133203.31352309 123126.86131567
 236631.16679427 207403.25428368 187375.67460022 218395.69575617
 220490.84482598 246486.2704729  161842.22590038 193849.81021221
 133203.31352309 123126.86131567 236631.16679427 207403.25428368
 147228.44516442 338083.05077362 218395.69575617 220490.84482598
 246486.2704729  161842.22590038 193849.81021221 133203.31352309
 123126.86131567 236631.16679427 207403.25428368 147228.44516442
 235815.12599851 162748.30385408 218395.69575617 220490.84482598
 246486.2704729  161842.22590038 193849.81021221 133203.31352309
 123126.86131567 236631.16679427 207403.25428368 147228.44516442
 235815.12599851 162748.30385408 351596.51481385 218395.69575617
 220490.84482598 246486.2704729  161842.22590038 193849.81021221
 133203.31352309 123126.86131567 236631.16679427 207403.25428368
 147228.44516442 235815.12599851 162748.30385408 182029.95978973
 232113.1697564  218395.69575617 220490.84482598 246486.2704729
 161842.22590038 193849.81021221 133203.31352309 123126.86131567
 236631.16679427 207403.25428368 147228.44516442 235815.12599851
 162748.30385408 182029.95978973 225588.57693463 122021.07035356
 218395.69575617 220490.84482598 246486.2704729  161842.22590038
 193849.81021221 133203.31352309 123126.86131567 236631.16679427
 207403.25428368 147228.44516442 235815.12599851 162748.30385408
 182029.95978973 225588.57693463 122021.07035356 201646.19212894
 218395.69575617 220490.84482598 246486.2704729  161842.22590038
 193849.81021221 133203.31352309 123126.86131567 236631.16679427
 207403.25428368 147228.44516442 235815.12599851 162748.30385408
 182029.95978973 225588.57693463 122021.07035356 155592.82140788
 321912.80282045 218395.69575617 220490.84482598 246486.2704729
 161842.22590038 193849.81021221 133203.31352309 123126.86131567
 236631.16679427 207403.25428368 147228.44516442 235815.12599851
 162748.30385408 182029.95978973 225588.57693463 122021.07035356
 155592.82140788 241864.04495156]
2025-06-20 18:00:31 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:00:31 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:31 ERROR Can not run the algorithm
2025-06-20 18:00:31 INFO Run function 8 complete. FEHistory len: 120, AOCC: 0.0000
2025-06-20 18:00:31 INFO FeHistory: [161204.89664887 196504.76819698  96543.10250441 135766.42797582
 170477.07356839 280369.36746092 188115.16891965 200079.9405246
 129868.19736736 235984.04765608 175745.78142731 176386.71037467
 206469.43979118 147373.86817034 128053.38126752 255859.54832797
 228315.76929473 135558.05090204 245474.68069448 256733.37317943
 180158.95054179 171623.71964708 211536.13669212 233654.45065454
 228900.15245533 174878.86896055 164075.01776393 283124.04777266
 145060.87695166 232028.8795316  160074.82740013 235112.92435186
 144233.63977471 228876.97028565 175998.74143127 168210.82688008
 181889.04556631 222278.83322324 195985.23836507 254439.63071253
 236215.65678046 171415.24121836 243784.91901496 185149.45253101
 145811.59288675 236950.78275185 106621.63575155 199978.69777214
 152631.97151556 252856.10184808 261219.29782898 191265.31612986
 237694.09658692 189157.8242252  185500.371345   170890.21680623
 190373.6635032   85403.76199105 177742.36105375 164895.32049357
 159164.65892588 168902.98008351 189991.820271   206173.71565898
 195645.14167633 170145.23810813 141446.77858724 191934.17717352
 276439.22998258 223823.05369192 173525.00953648  99246.15959146
 181923.58364618 185227.40779069 160137.37508633 145099.67976458
 152137.61146916 220602.07779852 193099.94170225 113801.12014253
 145051.25006394 121935.46972188 189475.22004961 205060.95592483
 202040.25919775 164767.84379683 196241.96566967 187266.24751362
 116893.93832679 178503.43468485 115596.69550491 157569.47872094
 161380.55541187 207962.18426174 221292.86834313 195128.73067409
 173618.59297009  91231.79488616 134136.48197891 220778.5950983
 117952.0210598  117952.0210598  255062.10430694 117952.0210598
 196504.76819698 165767.61815593 117952.0210598  196504.76819698
  96543.10250441 144333.12480856 117952.0210598  196504.76819698
  96543.10250441 135766.42797582 293251.44276092 117952.0210598
 196504.76819698  96543.10250441 135766.42797582 170477.07356839]
2025-06-20 18:00:31 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:00:31 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:00:32 ERROR Can not run the algorithm
2025-06-20 18:00:32 INFO Run function 24 complete. FEHistory len: 477, AOCC: 0.0000
2025-06-20 18:00:32 INFO FeHistory: [179.92025817 202.52537931 172.9133106  172.59741218 202.76032557
 203.06859271 211.31459892 178.07000293 149.463515   195.00711764
 164.92649421 201.01816623 194.49159249 193.85429261 171.93082298
 187.22467061 188.17054123 159.51258367 192.08452612 209.87075226
 178.51340283 164.72111325 182.32360317 219.22039206 205.20498477
 181.17998513 228.03873509 152.4690023  209.61180292 158.49303739
 208.12086037 202.66125364 184.11408743 162.0416333  202.25646516
 209.67701895 186.35019247 181.55782366 203.17754454 179.36585433
 172.77459056 169.38640451 172.96919519 199.4336431  200.32980805
 182.85618842 189.52366841 176.34393546 199.79654974 191.84376221
 173.69851331 174.9705928  189.97128874 183.2414638  166.09927816
 196.69850001 167.97712775 156.18443029 155.52832209 196.29831213
 209.85648053 204.75370238 211.30206123 208.52157985 228.73091165
 207.15747495 179.38044427 179.0864151  191.91139502 171.44015833
 205.20114181 191.10380625 178.67297986 208.9240578  189.15314224
 188.06156474 165.30931042 142.45458857 192.48667972 182.74625428
 166.56463707 208.08382689 203.95195892 203.73400571 182.85530952
 219.39784782 167.49961873 179.14244626 172.7835698  193.27208547
 182.08992027 174.61783244 191.62661521 159.09986458 224.10130163
 201.21632643 146.20443305 157.18249621 159.35386782 200.66106375
 207.43234289 179.92025817 195.13755762 179.92025817 195.13755762
 144.81858371 179.92025817 195.13755762 144.81858371 180.47035456
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 221.18602074 179.92025817 195.13755762 144.81858371 172.59741218
 155.20142984 203.06859271 186.55682299 179.92025817 195.13755762
 144.81858371 172.59741218 155.20142984 203.06859271 186.55682299
 202.72696529 179.92025817 195.13755762 144.81858371 172.59741218
 155.20142984 203.06859271 186.55682299 178.07000293 190.21102158
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 203.06859271 186.55682299 178.07000293 149.463515   207.74234992
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 203.06859271 186.55682299 178.07000293 149.463515   195.00711764
 222.27977984 179.92025817 195.13755762 144.81858371 172.59741218
 155.20142984 203.06859271 186.55682299 178.07000293 149.463515
 195.00711764 164.92649421 178.97512908 179.92025817 195.13755762
 144.81858371 172.59741218 155.20142984 203.06859271 186.55682299
 178.07000293 149.463515   195.00711764 164.92649421 178.97512908
 177.36000624 179.92025817 195.13755762 144.81858371 172.59741218
 155.20142984 203.06859271 186.55682299 178.07000293 149.463515
 195.00711764 164.92649421 178.97512908 177.36000624 185.64563824
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 203.06859271 186.55682299 178.07000293 149.463515   195.00711764
 164.92649421 178.97512908 177.36000624 185.64563824 212.43271769
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 203.06859271 186.55682299 178.07000293 149.463515   195.00711764
 164.92649421 178.97512908 177.36000624 185.64563824 171.93082298
 187.23990055 179.92025817 195.13755762 144.81858371 172.59741218
 155.20142984 203.06859271 186.55682299 178.07000293 149.463515
 195.00711764 164.92649421 178.97512908 177.36000624 185.64563824
 171.93082298 187.22467061 164.16630647 179.92025817 195.13755762
 144.81858371 172.59741218 155.20142984 203.06859271 186.55682299
 178.07000293 149.463515   195.00711764 164.92649421 178.97512908
 177.36000624 185.64563824 171.93082298 187.22467061 164.16630647
 189.84088511 179.92025817 195.13755762 144.81858371 172.59741218
 155.20142984 203.06859271 186.55682299 178.07000293 149.463515
 195.00711764 164.92649421 178.97512908 177.36000624 185.64563824
 171.93082298 187.22467061 164.16630647 159.51258367 199.80352718
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 203.06859271 186.55682299 178.07000293 149.463515   195.00711764
 164.92649421 178.97512908 177.36000624 185.64563824 171.93082298
 187.22467061 164.16630647 159.51258367 192.08452612 229.30924137
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 203.06859271 186.55682299 178.07000293 149.463515   195.00711764
 164.92649421 178.97512908 177.36000624 185.64563824 171.93082298
 187.22467061 164.16630647 159.51258367 192.08452612 209.87075226
 184.97368362 179.92025817 195.13755762 144.81858371 172.59741218
 155.20142984 203.06859271 186.55682299 178.07000293 149.463515
 195.00711764 164.92649421 178.97512908 177.36000624 185.64563824
 171.93082298 187.22467061 164.16630647 159.51258367 192.08452612
 209.87075226 178.51340283 157.18405117 179.92025817 195.13755762
 144.81858371 172.59741218 155.20142984 203.06859271 186.55682299
 178.07000293 149.463515   195.00711764 164.92649421 178.97512908
 177.36000624 185.64563824 171.93082298 187.22467061 164.16630647
 159.51258367 192.08452612 209.87075226 178.51340283 157.18405117
 163.45027162 179.92025817 195.13755762 144.81858371 172.59741218
 155.20142984 203.06859271 186.55682299 178.07000293 149.463515
 195.00711764 164.92649421 178.97512908 177.36000624 185.64563824
 171.93082298 187.22467061 164.16630647 159.51258367 192.08452612
 209.87075226 178.51340283 157.18405117 163.45027162 177.88776584
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 203.06859271 186.55682299 178.07000293 149.463515   195.00711764
 164.92649421 178.97512908 177.36000624 185.64563824 171.93082298
 187.22467061 164.16630647 159.51258367 192.08452612 209.87075226
 178.51340283 157.18405117 163.45027162 177.88776584 203.65758596
 179.92025817 195.13755762 144.81858371 172.59741218 155.20142984
 203.06859271 186.55682299 178.07000293 149.463515   195.00711764
 164.92649421 178.97512908 177.36000624 185.64563824 171.93082298
 187.22467061 164.16630647 159.51258367 192.08452612 209.87075226
 178.51340283 157.18405117 163.45027162 177.88776584 203.65758596
 179.54853928 179.92025817 195.13755762 144.81858371 172.59741218
 155.20142984 203.06859271 186.55682299 178.07000293 149.463515
 195.00711764 164.92649421 178.97512908 177.36000624 185.64563824
 171.93082298 187.22467061 164.16630647 159.51258367 192.08452612
 209.87075226 178.51340283 157.18405117 163.45027162 177.88776584
 203.65758596 179.54853928]
2025-06-20 18:00:32 INFO Expected Optimum FE: -100
2025-06-20 18:00:32 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:00:32 INFO AOCC mean: 0.0000
2025-06-20 18:00:41 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:00:41 ERROR Can not run the algorithm
2025-06-20 18:00:41 INFO Run function 4 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 18:00:41 INFO FeHistory: [1068227.62474252  652681.44565317  605832.0657108   783047.24964014
  686430.45642009  664385.23293119  780343.09193419  369098.49205187
  535009.02523462  704877.78714382  689865.376172    700886.37702787
  395050.57887477  596685.34550857  568472.30230236  461967.12016941
  750224.14415001  586576.47956858  720625.51111068  696773.32405774
  653014.9119026   639906.15689622  709278.90269621  767806.03989141
  657846.25468171  639189.88080232  552710.91240454  827829.16287117
 1025817.85226485  665510.08191669  595186.22254013  642495.90751575
  640765.51149021  652160.99530925  660536.70920493  552700.13294999
  453190.65267476  579773.75368502  633577.35905045  759260.65585584
  584161.99682481  554414.31678864  722625.89335173  787037.75493765
  643835.52400212  789365.31548761  626851.59894355  612620.63236236
  744582.74933799  566235.69052748  580507.81905015  771001.24543586
  612621.11526895  646824.1449455   560981.45023246  564620.74423321
  639152.9690193   555364.26104713  527756.11202781  557556.03311179
  493605.76384206  573339.88003432  801141.11447681  569591.79295241
  561836.16686043  703322.10877876  736530.43772655  762386.09703607
  698817.94030387  810297.27119795  635626.9480087   464089.74455061
  892468.40628391  736867.21287893  639038.92925633  691359.68709085
  498794.54192283  638343.79136407  643999.03480822  704145.56073174
  775579.9860408   832543.88662075  596861.89551577  569583.85304117
  603087.70201409  621953.07713633  627432.68453775  555913.87025528
  666793.4680654   339749.61734419  748030.28978626  732037.01431011
  676002.52397915  474731.70661621  510385.7929057   551064.49683677
  743348.37533738  687130.21666889  580317.82200356  604876.705065
  698729.75740166]
2025-06-20 18:00:41 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:00:41 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:41 ERROR Can not run the algorithm
2025-06-20 18:00:41 INFO Run function 7 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 18:00:41 INFO FeHistory: [127477.86065634 290554.81319705 309959.14626437 256509.56614028
 277586.28464388 320999.8103702  287203.34644138 231927.20989612
 232265.22628502 312891.71503989 225549.26368968 301588.03390781
 302071.18797873 300788.55050744 314826.89327369 194742.49773607
 254703.8454616  301751.72898951 276451.70301035 302281.68293591
 270834.22206674 345653.56329874 289701.2761205  285747.85540608
 347546.81018287 330063.52147382 225062.33467672 315464.35317581
 343706.19717311 307639.05052688 291161.95044576 285757.25549324
 283657.6961687  340776.4360974  305176.6698536  367461.43438063
 297070.00275198 209006.42604296 238796.53232817 282646.4762381
 399639.72186326 301303.23226164 297027.13490709 226235.4424755
 286538.73906685 347315.93452268 178875.98964757 293840.03603511
 321057.72868748 261018.31426312 231669.27777882 267095.37322696
 245787.09451542 233634.080079   313802.11597101 327426.55476856
 287560.75060938 245159.91812795 294453.12458116 296176.03095546
 284447.56194177 293769.31478204 333908.68964759 279455.39767725
 299384.08933957 343916.36044316 343506.19453812 350075.3109702
 195116.15696942 319649.18256419 281818.43319927 292367.98095883
 289560.72147909 241570.91567176 227934.88926023 297898.08017711
 283022.35872428 202058.34729065 372658.50478716 281078.68685734
 246814.8287948  260866.61180904 273590.3885111  317930.13938224
 247708.41984038 237704.99143964 264368.65101987 236823.34794709
 347597.60108307 248279.54733593 283908.07219539 336816.22551552
 308355.5885798  233223.57118998 313245.33032712 241897.83360605
 316276.62386415 256194.82415058 255613.31311926 292556.83181735
 229417.55698287]
2025-06-20 18:00:41 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:00:41 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:41 ERROR Can not run the algorithm
2025-06-20 18:00:41 INFO Run function 8 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 18:00:41 INFO FeHistory: [277207.73342281 152873.67476693 124483.58124653 252384.34924425
 224591.09830855 216062.26104134 173318.4304235  277315.44809017
 194606.46217406 178573.95674606 193575.93152196 171669.72776494
 175973.37424365 216499.69983289 198473.20213549 138608.4841346
 130700.43339325 167387.17226929 169313.75152857 161104.26862609
 171773.21792944 232852.6452393  146134.82852359 202630.04964752
 253165.14484705 257176.65717518 273682.17736163 170612.46250914
 145584.98835022 224594.23454616 223456.62220314 267550.78299372
 149613.98964606 261601.8068558  194172.60776172 208900.9084269
 259518.60816314 134550.81653041 184595.63402602 144105.32772917
 226150.32756995 122103.7112312  227193.47352392 155146.18397129
 255863.79110386 110196.47012723 183763.14984465 203503.74642555
 226557.18930582 195222.57128031 149212.54051193 177765.59884971
 207836.17118769 182357.86717493 213470.930131   300240.7621078
 167868.93044751 186635.36477707 192629.61710403 202379.13930251
 198467.16894263 131400.17330923 158273.34602999 139187.65707218
 137103.62571347 226272.72530486 146101.15086182  95341.16671425
 189323.28516855 185345.77551926 210809.86863345 249255.06341153
 137185.97099267 221032.65878078 181018.13268004 243166.57502376
 188872.59864349 236707.08846227 200071.69644202 152247.81325077
 211830.89761049 251852.18976261 204868.61184264 246088.45646714
 172271.54817305 243945.59503966 135822.71351279 189243.86149726
 210691.49679199 182946.94181204 202392.85601058 179726.41670712
 164775.3278141  132146.93714967 129170.44679285 152392.99661913
 213232.40873286 189687.20657738 194353.30056892 190934.97358834
 262123.12251288]
2025-06-20 18:00:41 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:00:41 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:00:41 ERROR Can not run the algorithm
2025-06-20 18:00:41 INFO Run function 24 complete. FEHistory len: 101, AOCC: 0.0000
2025-06-20 18:00:41 INFO FeHistory: [183.99441484 154.4349563  194.45397744 198.13895983 163.23743983
 175.60754958 206.80384101 189.38040914 189.62782909 171.13149014
 181.36839868 154.23990528 206.5118151  212.42468965 189.61241221
 194.1299893  208.40776003 161.67398604 176.09931351 189.37187681
 183.07339116 199.10586221 207.56286208 168.03272277 193.64785022
 210.73148147 153.61465672 149.48305622 204.87820428 151.93638487
 172.00047569 176.93081567 189.78386681 133.76725071 147.33475251
 170.17226639 216.96675461 151.34976564 132.44151186 189.4930776
 199.81039563 187.48621783 174.56158925 206.39441046 208.94324554
 139.12367291 193.63068922 178.20705393 205.2706835  200.45134438
 191.14926986 173.6825041  201.93656827 193.38841846 187.54761698
 179.94332522 175.28328144 197.28956032 180.18868693 138.64999755
 176.12161931 162.10621317 194.98473431 171.63916693 179.45703881
 179.21659415 179.13641558 222.72886662 175.26578202 190.79774747
 199.60465224 171.35560995 194.47172907 154.12047774 200.31744532
 193.07174979 200.4306668  222.66279572 167.36030266 195.70446509
 177.29673753 180.18403631 205.73470321 130.47556848 186.71689023
 167.63524114 198.12261313 204.23115398 171.04250542 198.67136764
 191.60660937 203.32862983 208.40933853 155.86267018 216.40556382
 183.2272963  183.02050537 167.10295942 191.58121328 175.56384284
 198.93096976]
2025-06-20 18:00:41 INFO Expected Optimum FE: -100
2025-06-20 18:00:41 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:00:41 INFO AOCC mean: 0.0000
2025-06-20 18:00:41 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Worse code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Analyze & experience
- Comparing (best) `AdaptiveGaussianDEwithSampling` vs (worst) `EnhancedArchiveGuidedDE`, we see that adaptive Gaussian mutation and localized sampling in the best performer lead to significantly better exploration and exploitation of the search space compared to the simpler DE approach with an archive in the worst performer. The adaptive mechanisms allow the algorithm to adjust its search strategy based on the current state of the optimization process, which is crucial for high-dimensional problems.  `EnhancedArchiveGuidedDE` lacks such adaptive components. (second best) `AdaptiveGaussianSamplingEA` vs (second worst) `AdaptiveMultimodalOptimizerImproved`: The former utilizes adaptive Gaussian sampling and a more robust selection mechanism (tournament selection) leading to better convergence than the latter's simpler local search approach with simulated annealing. `AdaptiveGaussianSamplingEA` incorporates adaptive mutation and recombination operators, making it more efficient. `AdaptiveMultimodalOptimizerImproved` struggles with the high dimensionality, showing slower and less efficient exploration. Comparing (1st) `AdaptiveGaussianDEwithSampling` vs (2nd) `AdaptiveGaussianSamplingEA`, we see that while both use adaptive Gaussian approaches, the DE framework in the best-performing algorithm introduces a more efficient way to manage the balance between exploration and exploitation. `AdaptiveGaussianSamplingEA` relies more heavily on Gaussian perturbations, potentially leading to less effective exploration in complex landscapes. (3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveMultimodalOptimizerImproved`: The former uses Differential Evolution with adaptive Gaussian mutation, providing a more directed search compared to the latter's less structured local search and simulated annealing based approach.  The adaptive mutation in `AdaptiveGaussianMutationDE` helps fine-tune the search intensity throughout optimization. Comparing (second worst) `AdaptiveMultimodalOptimizerImproved` vs (worst) `EnhancedArchiveGuidedDE`, we see that `AdaptiveMultimodalOptimizerImproved` at least employs some local search, which is a valuable technique for local optimization.  `EnhancedArchiveGuidedDE`, on the other hand, relies only on a static population size and archive, missing the crucial adaptive elements that make the top performers successful. Overall: The top-performing algorithms effectively combine adaptive mechanisms, robust selection operators, and carefully designed mutation strategies to efficiently navigate the high-dimensional search space.  The lower-performing algorithms lack these crucial adaptive elements and show less effective exploration and exploitation.
- Adaptive mechanisms (mutation scale decay, sampling radius decay, sigma decay) are crucial for handling high-dimensional spaces with wide bounds.  Combining DE with Gaussian perturbations is highly effective.  Robust selection (tournament) surpasses simpler approaches.  A well-designed archive with diversity considerations improves performance.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:00:41 INFO Generation 5, best so far: 0.3061604966767426
2025-06-20 18:00:41 INFO --- Performing Long-Term Reflection at Generation 5 ---
2025-06-20 18:00:46 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianDEwithSampling` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best utilizes adaptive Gaussian mutation and localized sampling for efficient multimodal optimization, resulting in a significantly higher AOCC score.  In contrast, the worst relies on a simpler DE strategy with an archive, showing less exploration and exploitation balance. (second best) `AdaptiveGaussianSamplingEA` vs (second worst) `AdaptiveMultimodalOptimizerImproved`: The second-best employs adaptive Gaussian sampling and tournament selection, leading to better convergence. The second worst uses a local search with simulated annealing, which suffers from potential premature convergence in high dimensions. Comparing (1st) vs (2nd), we see that the top algorithm incorporates a more sophisticated mutation strategy (combining Gaussian with DE) and localized sampling, which appears crucial for high-dimensional multimodal optimization. (3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveMultimodalOptimizerImproved`: The third-ranked algorithm improves on basic DE by adding adaptive Gaussian mutation, enhancing exploration. The fourth is a local search method, which in high dimensions struggles compared to population-based methods. Comparing (second worst) `AdaptiveMultimodalOptimizerImproved` vs (worst) `EnhancedArchiveGuidedDE`, we see the local search struggles against population-based approaches.  `EnhancedArchiveGuidedDE` lacks the adaptive mechanisms of the higher-ranked algorithms and simpler DE scheme with an archive. Overall: Adaptive Gaussian sampling and mutation, combined with techniques to improve exploration and exploitation (like localized sampling and DE's difference vectors), are key to superior performance on the GNBG benchmark in high dimensions.  Simple local search and basic DE variants perform poorly.


**Experience:**

Adaptive mechanisms for mutation strength and sampling radius are vital for high-dimensional problems.  Combining population-based methods with elements of local search can potentially yield even better results.  Careful consideration of exploration-exploitation balance and the choice of selection and recombination strategies are critical.

2025-06-20 18:00:46 INFO Generating offspring via Crossover...
2025-06-20 18:00:56 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:00:56 ERROR Can not run the algorithm
2025-06-20 18:00:56 INFO Run function 4 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-20 18:00:56 INFO FeHistory: [ 948635.80830572 1109538.30375642  792628.26985289  899450.59797979
  833004.62337546  917482.21397703  752489.05118562  703258.48818545
 1176611.36472712 1230735.17035813 1684635.70093695 1020583.96588895
  833356.21832282  814866.03758668  782121.6745755  1042737.05707172
 1282326.68062227  537336.42460808 1072455.77568762  808290.61853554
  756275.64965648  741630.74932893  710338.38745371  663790.33118128
  900899.60994798 1070888.62698438 1110418.83586159 1108347.90183199
 1126190.69851304  655595.57737226  572363.73895082  665458.85102438
  854762.79765076  560326.8789127  1151235.23660756  831894.13541059
 1229604.70724513 1225853.81155615  997489.61339034  768439.12208614
 1310124.16548616  732782.51793544  999923.83347962  725370.76265203
  670936.55110495 1066477.31630188  977474.22054557  473995.81492554
  573497.39418437  961958.49310777 1253205.76098505  955165.56646426
  898347.36010642 1094857.84317194 1047899.07564911  998211.09796769
 1064768.61358102  968539.91843613  700700.45935613  676937.97132176
  894400.16540588  848832.58770912  826813.59370777 1072651.70918802
  938344.88287206 1039685.69675482  588891.0969148   871391.57591061
  382704.99112476 1073888.95563084  761698.73834318  727512.29259362
  671694.20182887  711299.61765881  751854.0270639  1135778.94302068
 1179475.36940485 1015170.17312123 1556459.09349236  558414.41277194
  966028.74073825  704697.49699895  799105.80137113  896534.76821846
 1055086.38221927 1029828.80043299  754102.56037933  995344.9005764
  777906.0598777   644670.19603777  920490.00072889  704880.21455623
  697670.87558462  631447.78490247  963222.07906173 1308260.54407341
  667356.77218449  564267.5202207   677449.61342661 1081624.08796138
  836333.30482528  835784.48649892 1285065.73548501  941274.51950399
 1186156.64869951 1055858.10868043 1241767.66934237  664364.42989997
 1157907.80604584  760933.99563409 1550115.83553166 1437943.17800061
 1199147.3659253  1127751.43855567 1449731.25427392  710084.06591527
 1312533.1169192   686978.34054548  914351.04658891  845816.06862695
  673588.96809701  815587.99930614  989471.70368328  679730.43522625
 1157373.02808758  740244.88966453 1384143.69230899  943056.76596647
 1259977.79795501  685060.05670815  640652.481465    587278.73127297
  507009.75805295  931115.88183022 1489685.4375616  1154714.80554211
 1451525.14503324 1289514.68217095 1191478.26597365  849430.33457937
 1312526.35319849  689686.61924885 1081071.49252568  695444.49354281
  651490.63352895 1083816.32561996  734984.13958895  811015.94973395
  601578.21771886  997934.07889561  971192.97139078 1408671.73070008
  745273.99102101 1103254.98032434 1188391.54745849 1138554.44999111
  982531.32832823  780104.99581423  979617.98379525  845015.36782413
  952568.89911938  932815.39791876 1070458.73591281 1026823.71777046
 1029476.63915309  779783.05840794  704079.47566337 1059081.05581022
  771444.70445098  879987.67325496 1139069.02065272  581846.97956653
 1090828.4770165   783505.38522192  830030.77184336 1113930.8094682
  810046.22030659 1197362.71348465 1341329.50775733 1243191.31466025
 1064133.81217433 1054966.47318065 1008832.18736775  999581.41799824
  949801.45130033  989158.61640936 1168097.15337805 1034081.79813123
  728995.27809728  847554.43524234  647080.98000679  689702.84095341
 1081001.43117169  739852.58671185 1126507.59859673 1877617.04284322
  766685.82555953  416976.26509061  972706.46695566 1224740.06077831]
2025-06-20 18:00:56 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:00:56 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:56 ERROR Can not run the algorithm
2025-06-20 18:00:56 INFO Run function 7 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-20 18:00:56 INFO FeHistory: [198353.66846189 235289.20540156 162440.61888986 157816.43291507
 208453.82114098 279746.14277962 142796.83785015 202884.1685277
 233528.99235471 164334.25574469 223645.49900595  81712.70558674
 209561.48759279 244060.04792145 323640.49110896 147346.97819402
 174852.10730575 275703.01473565 232437.65454706 262414.99083899
 343605.45418376 219824.58752301 212672.45088363 158931.08878246
 182721.47808007 203904.75353148 259097.23918128 205365.69407079
 286874.86108927 165666.50727085 149762.33543058 290822.65405043
 186238.26677598 185070.85750424 234722.84895529 116057.65788736
 215340.94613073 250326.6901307  151911.83990271 162641.14574552
 123620.17968064 145508.42570596 329978.4698731  158645.79993992
 126877.03348432 123425.20558941 176305.4967354  181134.43066403
 205841.64612016 283532.54129617 155495.35459378 213348.60761293
 129359.19079089 195906.90006882 227852.99196844 144032.2826535
 155185.46713624 184623.94443501 220728.09497599 217397.40458724
 282946.54306301 162465.15277737 116818.16848352 182075.4091408
 170274.73994326 235320.92069536 132169.05157372 119641.21586334
 237786.84522933 117983.82212929 203800.55788526 173866.52699435
 191894.68478058 230711.51575074 130747.70610744 271141.22472362
 179758.58074707 264383.3543322  237700.47798351 152718.72856277
 149644.26531911 194321.82102977 221505.99071079 145039.94405583
 239824.01901575 193616.84357328 124729.85391381 197285.87423801
 305841.11546382 200123.20193263 242490.49747282 201387.45158138
 164917.98951088 242467.28714646 210872.79651281 215729.61572583
 211686.5104589  218428.28566702 317635.74102259 163561.42782655
 287384.85778052 316854.13886948 248155.94113493 178458.44747113
 208446.30654721 203804.79925629 137181.0742236  182335.06107362
 198096.45240871 251769.87817427 183456.15469322 184176.62720675
 306809.39235581 231765.0295666  357813.70749973 212247.85733755
 214664.09944837 143911.08457161 215124.7264575  208417.7202422
 287313.41001398 176946.69107821 325355.03153243 298740.65393783
 193307.18997011 165834.7393999  257924.23072366 153609.47933663
 238216.96212117 209524.70606025 169295.89140614 247645.38645394
 217709.49807354 198697.00539624 265503.52902465 171117.68729412
 242815.18206684 208950.95282564 123312.80416709 261548.51244605
 134678.95350083 241625.58474661 246578.0771974  194669.59481231
 194254.19060109 251471.44564921 220046.90456379 111094.47815203
 223727.40634549 286514.71141203 184616.67262595 225594.13042147
 166287.60936848 220212.45296803 221218.46855602 150227.05519618
 116614.23353883 236825.24717914 285301.74579406 270738.64901505
 329144.83046198 241387.80331946 110857.58114133 235566.89219271
 290156.01201835 258984.6943353  307773.53450755 252758.25835104
 302525.67277401 224148.25175047 249693.91783278 294656.05108488
 407866.86374892 248436.81735712 157550.04131519 292713.08125024
 266097.06776671 413552.01690675 320118.71391718 224627.68975667
 254699.88852714 254887.42573537 273958.64846977 288374.66802313
 317166.1066257  198506.45573231 188647.09780324 220352.21702869
 260621.72225261 243192.73260073 292773.50777054 216967.3294661
 227614.87524694 224690.47769633 345010.49147523 211132.16650801
 179518.22964164 233748.76114455 257569.95251323 307111.65101174]
2025-06-20 18:00:56 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:00:56 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:00:56 ERROR Can not run the algorithm
2025-06-20 18:00:56 INFO Run function 8 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-20 18:00:56 INFO FeHistory: [209712.6367836  186212.87019716 196045.16829336 177193.01802476
 196903.45175621 201498.61963464 132242.47602014 148185.65852288
 168920.80463071 109242.21975744 251812.05478398 126155.81085193
 250593.61248428 181124.00032898 213346.96023462 186062.02125077
  82351.40051718 233375.54708522 115887.05179828 161529.17209489
 250422.04251026 280991.9161636  115348.3356432  170726.43310186
 167721.40495423 152780.31279247 197580.67692995 218125.28172652
 189316.4421496  232255.23026002 194365.70575682 128632.1579433
 192428.99465591 204313.94262088 191687.26067173 287382.59307693
  93071.83548815 199934.54356003 196851.90216964 180790.6330225
 176037.12643031 155425.26619986 201386.65836985 188877.3048962
 173201.25550041 253061.45643582 131191.70777896 158390.53851856
 215154.64074637 260978.85716745 193020.01768917 234401.30042979
 190722.84548781 270040.50186481 121809.87878831 213598.87270587
 209406.47173306 220591.84758963 259556.00536311 175255.85120403
 235280.53794514 260279.26845028 268960.92773579 213469.88547556
 196438.79051538 286025.16944351 171179.06331582 132250.0869525
 137210.86195108 141737.29002098 290915.26347103 224084.04315004
 145297.04058828 282682.62348628 186967.06306435 254261.37075863
  72791.05550073 215984.21272329 185005.14336037 250858.44668177
 204259.75209852 167029.63769741 164525.51352734 264847.64760175
 217970.6654282  309523.01026243 134144.02184185 191493.78687285
 212813.49858855 208126.68053052 121055.64178735 148568.90205375
 180958.66229645 186623.33025921 290187.02342332 108075.09616625
 231852.59865161 211694.40415274 187083.64520473 161008.12649972
 280816.95663681 207126.66688911 196730.26743154 252623.66460451
 173033.93721503 192077.73418422 143007.9442525  201172.19895479
 172624.48892406 141879.52941891 212803.94061578 181435.5119995
 337561.78339997 165133.15051976 183135.08859101 244460.43582328
 129909.01143458 335988.55562503  97914.87022051 172373.60079319
 236102.47639186 152242.15885433 142284.8352641  225470.38225418
 253363.49604947 170393.22982547 281491.21538083 114775.88623505
 317309.5149238  262965.11831212 249736.01311914 132772.12191169
 294403.63490946 236547.53276273 162435.23440978 277098.29812484
  97034.15661798 297128.08680946 232537.85517076 175614.55247176
 299717.15979074 202237.43488194 254253.75162255 160691.60095728
 213175.37994221 195871.84033473 194953.09966035 167555.76123086
 229353.86151731 269684.44948653 124186.81770105 195826.28559889
 305435.52343234 302986.07301093 170380.39650605 169603.85983009
 274967.99671113 175771.04818843 273572.1225893  318262.86686459
 279851.13528691 367910.66817281 260995.61483342 118109.48270957
 175760.10849158 224330.25838843 130651.67432225 151530.41753537
 164328.63171694 169201.97539019 275705.71244837 186119.32054141
 191660.69874289 277618.73409597 306372.74436066 173441.11661789
 252345.54845797 252654.59773292 191636.34449021 153051.57279593
 248831.2822004  190842.80836784 219295.36949709 139077.25730966
 169709.79346489 298308.03344197 129514.04656074 127109.21432715
 210983.64157209 411129.94981168 162516.37326281 186790.21320017
 215909.74513849 279985.79716196 244105.94165005 180193.9868819
 284411.91754912 235997.06565425 208784.57919287 217849.10374811]
2025-06-20 18:00:56 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:00:56 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:00:56 ERROR Can not run the algorithm
2025-06-20 18:00:57 INFO Run function 24 complete. FEHistory len: 200, AOCC: 0.0000
2025-06-20 18:00:57 INFO FeHistory: [178.24231828 186.56924485 168.63932833 186.26530045 196.86761484
 171.27788052 217.75479307 196.18229287 179.27509257 191.68234797
 191.50231044 197.06368826 187.06980911 168.94288584 198.18528232
 164.57932834 186.48267434 210.50261397 188.54288993 151.96997894
 204.90895287 224.02124469 201.63928287 203.24453247 189.31590632
 194.47914834 210.42893549 184.95210757 145.53588319 207.35848633
 180.75866542 167.43487346 158.10581278 150.14740809 229.55860886
 220.68561789 204.41442041 185.23353521 186.08395138 196.95203491
 193.00706454 182.89642649 194.77275738 204.51884236 181.0727904
 173.75414615 158.78213679 212.08918257 146.09663394 199.72153196
 194.33402209 202.86586842 214.6226685  187.85410265 200.99761441
 188.67596113 228.82270123 186.34517663 164.32426637 160.77169256
 185.58837897 174.97521685 178.48410856 183.48961475 181.30176991
 161.76315769 148.16106179 195.60311548 188.00255633 154.3090375
 187.4059506  156.4077445  142.42134154 214.12661136 186.92347011
 190.39819126 133.51137502 220.15759865 178.24130635 210.0741003
 159.27316568 219.55203844 207.87010093 190.50981874 185.46467746
 225.18469253 179.94365296 151.32137642 198.69061837 167.25076591
 156.14965971 202.12040553 171.64467752 208.6931303  202.82038317
 197.6428664  174.78793965 220.10703201 187.25004574 197.97417841
 208.05010143 244.61821807 188.03534983 175.78560176 185.99414881
 159.75503743 203.46851321 180.94596095 168.16639906 213.84854131
 207.58251859 222.36231706 185.83929544 192.19993358 213.91608252
 208.51046256 230.78371495 186.25883458 172.24380909 211.27120814
 219.37441821 197.28471623 203.52255187 176.52894034 188.6732211
 187.17265192 184.89244089 178.34157179 185.83630107 184.87531551
 194.34043495 198.04772331 157.47352203 193.89627549 211.7811991
 213.81302588 200.34349979 190.71644176 205.12204893 219.74485096
 212.0926743  191.66331517 218.3285098  212.92605823 192.74517458
 196.91710588 158.83901185 202.59230612 218.48715181 192.59630239
 204.37259205 184.24327835 221.55752243 222.24146735 225.48439059
 156.75714376 228.03011241 183.30935556 218.79818655 212.16194064
 215.47228658 178.31748829 184.98914924 203.6388801  191.72321988
 223.48382374 184.46477292 180.99629805 215.97403845 183.49165864
 195.55068102 171.49897813 206.76710782 219.98316102 181.51922446
 190.77648983 168.96711088 213.9600851  185.26710903 205.45919779
 223.41122647 219.66186768 186.28601981 201.38607709 186.05046541
 184.25650761 181.49377833 168.36910741 212.7510638  163.55049248
 193.64599178 210.89192891 193.15717739 181.92095607 190.64036147
 212.64471605 230.21886774 209.64788591 225.0891228  205.93493291]
2025-06-20 18:00:57 INFO Expected Optimum FE: -100
2025-06-20 18:00:57 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:00:57 INFO AOCC mean: 0.0000
2025-06-20 18:01:05 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-20 18:01:10 INFO Run function 4 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 18:01:10 INFO FeHistory: [758406.450647   961191.37018    698866.92336895 ...  46918.14999608
  63108.56821002  17784.10995977]
2025-06-20 18:01:10 INFO Expected Optimum FE: -382.6205211774271
2025-06-20 18:01:10 INFO --- GNBG Problem Parameters for f7 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -912.857374
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:01:16 INFO Run function 7 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 18:01:16 INFO FeHistory: [147773.03992932 243991.83813244 265290.23780715 ...  23727.38007392
  17683.3633367   17115.14496716]
2025-06-20 18:01:16 INFO Expected Optimum FE: -912.8573739743372
2025-06-20 18:01:16 INFO --- GNBG Problem Parameters for f8 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -656.788998
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0.2 0.2]
----------------------------------------
2025-06-20 18:01:22 INFO Run function 8 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 18:01:22 INFO FeHistory: [215395.7004697  187809.6092587  167617.62533631 ...  27065.22555118
  19143.73043321  21446.67512684]
2025-06-20 18:01:22 INFO Expected Optimum FE: -656.7889979935655
2025-06-20 18:01:22 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-20 18:01:39 INFO Run function 24 complete. FEHistory len: 70000, AOCC: 0.0000
2025-06-20 18:01:39 INFO FeHistory: [174.84821366 177.09603246 156.92330937 ...  59.19239     53.97580605
  64.25392168]
2025-06-20 18:01:39 INFO Expected Optimum FE: -100
2025-06-20 18:01:39 INFO Multimodal (single component) AOCC mean: 0.0000
2025-06-20 18:01:39 INFO AOCC mean: 0.0000
2025-06-20 18:01:39 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population initialization function in Python.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark function. The key challenge is creating a good 
starting population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]). A simple uniform random
initialization is often ineffective.




### Better code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Worse code
AdaptiveGaussianSamplingEA
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

### Analyze & experience
- Comparing (best) `AdaptiveGaussianDEwithSampling` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best utilizes adaptive Gaussian mutation and localized sampling for efficient multimodal optimization, resulting in a significantly higher AOCC score.  In contrast, the worst relies on a simpler DE strategy with an archive, showing less exploration and exploitation balance. (second best) `AdaptiveGaussianSamplingEA` vs (second worst) `AdaptiveMultimodalOptimizerImproved`: The second-best employs adaptive Gaussian sampling and tournament selection, leading to better convergence. The second worst uses a local search with simulated annealing, which suffers from potential premature convergence in high dimensions. Comparing (1st) vs (2nd), we see that the top algorithm incorporates a more sophisticated mutation strategy (combining Gaussian with DE) and localized sampling, which appears crucial for high-dimensional multimodal optimization. (3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveMultimodalOptimizerImproved`: The third-ranked algorithm improves on basic DE by adding adaptive Gaussian mutation, enhancing exploration. The fourth is a local search method, which in high dimensions struggles compared to population-based methods. Comparing (second worst) `AdaptiveMultimodalOptimizerImproved` vs (worst) `EnhancedArchiveGuidedDE`, we see the local search struggles against population-based approaches.  `EnhancedArchiveGuidedDE` lacks the adaptive mechanisms of the higher-ranked algorithms and simpler DE scheme with an archive. Overall: Adaptive Gaussian sampling and mutation, combined with techniques to improve exploration and exploitation (like localized sampling and DE's difference vectors), are key to superior performance on the GNBG benchmark in high dimensions.  Simple local search and basic DE variants perform poorly.
- Adaptive mechanisms for mutation strength and sampling radius are vital for high-dimensional problems.  Combining population-based methods with elements of local search can potentially yield even better results.  Careful consideration of exploration-exploitation balance and the choice of selection and recombination strategies are critical.

Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-20 18:01:39 INFO Generation 6, best so far: 0.3061604966767426
2025-06-20 18:01:39 INFO --- Performing Long-Term Reflection at Generation 6 ---
2025-06-20 18:01:44 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveGaussianDEwithSampling` vs (worst) `EnhancedArchiveGuidedDE`, we see that the best uses adaptive Gaussian mutation and localized sampling, focusing on exploration and exploitation around promising regions.  The worst uses an archive to maintain diversity but lacks the adaptive mechanisms for efficient search in high-dimensional spaces. (second best) `AdaptiveGaussianSamplingEA` vs (second worst) `AdaptiveMultimodalOptimizerImproved`, the former leverages adaptive Gaussian sampling and tournament selection for efficient exploration and exploitation, while the latter employs local search and perturbation but lacks the refined sampling strategy. Comparing (1st) vs (2nd), we see that `AdaptiveGaussianDEwithSampling` incorporates Differential Evolution (DE) for population diversification, resulting in better exploration than the simpler Gaussian recombination in `AdaptiveGaussianSamplingEA`. (3rd) `AdaptiveGaussianMutationDE` vs (4th) `AdaptiveMultimodalOptimizerImproved`,  `AdaptiveGaussianMutationDE` uses adaptive Gaussian perturbation within a DE framework offering a balance of exploration and exploitation, while `AdaptiveMultimodalOptimizerImproved` relies heavily on local search and perturbation, making it less effective in high dimensions. Comparing (second worst) vs (worst), we see that `AdaptiveMultimodalOptimizerImproved` incorporates a tabu list and simulated annealing to escape local optima, performing better than `EnhancedArchiveGuidedDE` which only uses a simple archive. Overall: The top-performing algorithms emphasize adaptive mechanisms, combining exploration and exploitation strategies effectively.  Adaptive sampling and incorporating DE prove highly beneficial in navigating the high-dimensional GNBG landscape. The worst-performing algorithms lack the sophistication and adaptivity of the best ones.

**Experience:**

Adaptive mechanisms, especially in sampling and mutation strength, are crucial for high-dimensional optimization. Combining exploration (global search) and exploitation (local search) strategies is key.  Differential Evolution is a powerful technique for population diversification.  The effective balance of these factors determines the success of a population initialization strategy.

2025-06-20 18:01:44 INFO Generating offspring via Crossover...
2025-06-20 18:01:55 INFO --- GNBG Problem Parameters for f4 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -382.620521
  Lambda (Curvature): [1]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
