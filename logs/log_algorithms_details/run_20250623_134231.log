2025-06-23 13:42:32 INFO Initializing first population
2025-06-23 13:42:32 INFO Initializing population from 8 seed files...
2025-06-23 14:24:32 INFO Started evolutionary loop, best so far: 0.13788948561334638
2025-06-23 14:24:32 INFO Population length is: 20
2025-06-23 14:24:32 INFO --- Performing Long-Term Reflection at Generation 1 ---
2025-06-23 14:24:32 INFO Reflection Prompt: ### Problem Description
Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    
### List heuristics
Below is a list of design heuristics ranked from best to worst based on their average AOCC score, where higher is better, across a subset of the GNBG benchmark. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score: 1.3789e-01 |             AOCC Score on Unimodal instances: 2.3410e-01 |             AOCC Score on Multimodal instances with a single component: 1.2516e-01 |             AOCC Score on Multimodal instances with multiple components: 5.4407e-02)
# Name: AdaptiveGaussianArchiveEA
# Description: Seed from AdaptiveGaussianArchiveEA
# Code:
```python
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 2 (Overall AOCC Score: 1.3390e-01 |             AOCC Score on Unimodal instances: 2.0756e-01 |             AOCC Score on Multimodal instances with a single component: 1.2224e-01 |             AOCC Score on Multimodal instances with multiple components: 7.1897e-02)
# Name: AdaptiveGaussianSamplingEAwithArchive
# Description: Seed from AdaptiveGaussianSamplingEAwithArchive
# Code:
```python
import numpy as np
import random
class AdaptiveGaussianSamplingEAwithArchive:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []

        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])

        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)

        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []

        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)

        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 3 (Overall AOCC Score: 1.2434e-01 |             AOCC Score on Unimodal instances: 2.0753e-01 |             AOCC Score on Multimodal instances with a single component: 1.2025e-01 |             AOCC Score on Multimodal instances with multiple components: 4.5244e-02)
# Name: AdaptiveGaussianSamplingEA
# Description: Seed from AdaptiveGaussianSamplingEA
# Code:
```python
import numpy as np
class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Gaussian width
        self.sigma_decay = 0.99 #Decay rate of sigma


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            # Selection (tournament selection)
            parents = self._tournament_selection(population, fitness_values)

            # Recombination (Gaussian perturbation)
            offspring = self._gaussian_recombination(parents)

            # Mutation (adjust sigma adaptively)
            offspring = self._adaptive_mutation(offspring)

            #Evaluation
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            
            #Selection for next generation
            population, fitness_values = self._select_next_generation(population, fitness_values, offspring, offspring_fitness)

            # Update best solution
            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        #Adaptive Gaussian Sampling
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        population = np.clip(population, self.lower_bounds, self.upper_bounds)
        return population

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size //2 #Binary Recombination

        selected_parents = []
        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])
        return np.array(selected_parents)


    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i+1]
            child1 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = (parent1 + parent2) / 2 + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)


    def _adaptive_mutation(self, offspring):
        #Simple Mutation, sigma is already updated
        offspring += np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        sorted_indices = np.argsort(combined_fit)
        
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit
    

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]
```

### Rank 4 (Overall AOCC Score: 1.2391e-01 |             AOCC Score on Unimodal instances: 2.5102e-01 |             AOCC Score on Multimodal instances with a single component: 1.2070e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDifferentialEvolutionWithEnhancedInitialization
# Description: Seed from AdaptiveDifferentialEvolutionWithEnhancedInitialization
# Code:
```python
import numpy as np
from scipy.optimize import minimize

class AdaptiveDifferentialEvolutionWithEnhancedInitialization:
    """
    Combines Differential Evolution with enhanced initialization near known optima and local search for multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], known_optimum=None):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)
        self.known_optimum = known_optimum  # Allow for None if no known optimum

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.local_search_freq = 5 # Perform local search every 5 generations

    def initialize_population(self, num_samples):
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(num_samples, self.dim))
        
        if self.known_optimum is not None:
            num_near_optimum = int(0.3 * num_samples) # 30% near the optimum
            noise_scale = 20 # Adjust noise scale as needed. Experiment with this!
            noise = np.random.normal(scale=noise_scale, size=(num_near_optimum, self.dim))
            population[:num_near_optimum, :] = self.known_optimum + noise
            population[:num_near_optimum, :] = np.clip(population[:num_near_optimum, :], self.lower_bounds, self.upper_bounds)

        return population

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = self.initialize_population(self.population_size)
        fitness = objective_function(population)
        self.eval_count += len(fitness)
        best_solution = population[np.argmin(fitness)]
        best_fitness = np.min(fitness)
        self.best_solution_overall = best_solution
        self.best_fitness_overall = best_fitness

        generation = 0
        while self.eval_count < self.budget:
            # Differential Evolution
            new_population = np.zeros_like(population)
            for i in range(self.population_size):
                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if trial_fitness[0] < fitness[i]:
                    new_population[i] = trial
                    fitness[i] = trial_fitness[0]
                else:
                    new_population[i] = population[i]

            population = new_population
            best_solution = population[np.argmin(fitness)]
            best_fitness = np.min(fitness)

            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            # Local Search
            if generation % self.local_search_freq == 0:
                result = minimize(objective_function, best_solution, method='L-BFGS-B', bounds=list(zip(self.lower_bounds, self.upper_bounds)))
                if result.fun < best_fitness:
                    best_fitness = result.fun
                    best_solution = result.x
                    self.eval_count += result.nfev

                    if best_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = best_fitness
                        self.best_solution_overall = best_solution

            generation += 1

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info









```

### Rank 5 (Overall AOCC Score: 1.1761e-01 |             AOCC Score on Unimodal instances: 2.0601e-01 |             AOCC Score on Multimodal instances with a single component: 1.2088e-01 |             AOCC Score on Multimodal instances with multiple components: 2.5945e-02)
# Name: AdaptiveGaussianSamplingEA
# Description: Seed from AdaptiveGaussianSamplingEA
# Code:
```python
import numpy as np

class AdaptiveGaussianSamplingEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.2 * (self.upper_bounds - self.lower_bounds)  # Initial Standard Deviation for Gaussian Sampling

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            # Adaptive Gaussian Sampling
            parents = self.tournament_selection(fitness_values, k=5)  # Tournament Selection
            offspring = self.gaussian_mutation(parents, self.sigma)

            # Bounds handling
            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update population and best solution
            self.population = np.concatenate((self.population, offspring))
            fitness_values = np.concatenate((fitness_values, offspring_fitness))

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_solution_overall = self.population[best_index]
                self.best_fitness_overall = fitness_values[best_index]

            # Adaptive Sigma
            self.sigma *= 0.99  # Gradually reduce sigma for finer search later.

            # Elitism
            sorted_pop = self.population[np.argsort(fitness_values)]
            self.population = sorted_pop[:self.population_size]
            fitness_values = fitness_values[np.argsort(fitness_values)][:self.population_size]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def tournament_selection(self, fitnesses, k):
        num_parents = len(fitnesses) // 2  # Select half the population as parents
        parents = np.zeros((num_parents, self.dim))
        for i in range(num_parents):
            tournament = np.random.choice(len(fitnesses), size=k, replace=False)
            winner_index = tournament[np.argmin(fitnesses[tournament])]
            parents[i] = self.population[winner_index]
        return parents

    def gaussian_mutation(self, parents, sigma):
        offspring = parents + np.random.normal(0, sigma, parents.shape)
        return offspring

```

### Rank 6 (Overall AOCC Score: 1.1760e-01 |             AOCC Score on Unimodal instances: 1.9870e-01 |             AOCC Score on Multimodal instances with a single component: 1.1192e-01 |             AOCC Score on Multimodal instances with multiple components: 4.2191e-02)
# Name: EnhancedArchiveGuidedDE
# Description: Seed from EnhancedArchiveGuidedDE
# Code:
```python
import numpy as np
import random
class EnhancedArchiveGuidedDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        self.population_size = 10 * self.dim  # common heuristic
        self.archive_size = 100
        self.archive = []
        self.population = None
        self.F_scale = 0.5 #initial scaling factor


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness)]
        self.best_fitness_overall = np.min(fitness)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring(self.population, fitness)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            # Update archive
            self.update_archive(offspring, offspring_fitness)

            # Select best solutions for next generation
            combined_population = np.concatenate((self.population, offspring))
            combined_fitness = np.concatenate((fitness, offspring_fitness))
            indices = np.argsort(combined_fitness)
            self.population = combined_population[indices[:self.population_size]]
            fitness = combined_fitness[indices[:self.population_size]]

            #Update best solution
            self.best_solution_overall = self.population[np.argmin(fitness)]
            self.best_fitness_overall = np.min(fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'archive_size': len(self.archive)
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self, population, fitness):
        offspring = np.zeros((self.population_size, self.dim))
        #Adaptive scaling factor
        self.F_scale = 0.5 + 0.3*np.random.rand() #scale factor with slight variation

        for i in range(self.population_size):
            # Select pbest from archive (if available)
            if self.archive:
                pbest_index = np.random.choice(len(self.archive))
                pbest = self.archive[pbest_index][0]
            else:
                pbest = population[np.argmin(fitness)]

            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            offspring[i] = population[i] + self.F_scale * (pbest - population[i] + population[a] - population[b])
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds) #Boundary handling

        return offspring

    def update_archive(self, offspring, offspring_fitness):
        for i in range(len(offspring)):
            if len(self.archive) < self.archive_size:
                self.archive.append((offspring[i], offspring_fitness[i]))
            else:
                #Prioritize diversity in archive
                worst_index = np.argmax([f for _, f in self.archive])
                if offspring_fitness[i] < self.archive[worst_index][1] or len(self.archive) < self.archive_size * 0.8 :
                    self.archive[worst_index] = (offspring[i], offspring_fitness[i])
```

### Rank 7 (Overall AOCC Score: 1.1370e-01 |             AOCC Score on Unimodal instances: 2.0798e-01 |             AOCC Score on Multimodal instances with a single component: 1.0968e-01 |             AOCC Score on Multimodal instances with multiple components: 2.3439e-02)
# Name: AdaptiveGaussianMutationDE
# Description: Seed from AdaptiveGaussianMutationDE
# Code:
```python
import numpy as np
import random

class AdaptiveGaussianMutationDE:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_scale = 0.8 # Initial mutation scale
        self.mutation_scale_decay = 0.99 #decay factor for the mutation scale

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)

        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population,self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []

            for i in range(self.population_size):
                # Differential Mutation
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.mutation_scale * (self.population[b] - self.population[c])

                #Adaptive Gaussian perturbation to escape local optima
                mutant += np.random.normal(0, self.mutation_scale/2, self.dim)  

                #Clipping
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                #Crossover
                trial = np.where(np.random.rand(self.dim) < 0.5, mutant, self.population[i])

                #Selection
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])
                
                best_solution,best_fitness = self._find_best(np.array(new_population), np.array(new_fitness_values))
                if best_fitness < self.best_fitness_overall:
                    self.best_solution_overall = best_solution
                    self.best_fitness_overall = best_fitness


            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.mutation_scale *= self.mutation_scale_decay #Decay mutation scale

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        population_reshaped = self.population.reshape(-1, self.dim)
        fitness = objective_function(population_reshaped)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self,population,fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
```

### Rank 8 (Overall AOCC Score: 1.0707e-01 |             AOCC Score on Unimodal instances: 2.0285e-01 |             AOCC Score on Multimodal instances with a single component: 1.1836e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizerImproved
# Description: Seed from AdaptiveMultimodalOptimizerImproved
# Code:
```python
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability








```

### Rank 9 (Overall AOCC Score: 9.9955e-02 |             AOCC Score on Unimodal instances: 1.8000e-01 |             AOCC Score on Multimodal instances with a single component: 1.1987e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: An evolutionary algorithm employing adaptive mutation and a niching mechanism to efficiently explore multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.mutation_rate = 0.1  # Initial mutation rate
        self.niche_radius = 0.5 # Initial niche radius
        self.sigma = 0.5 #Initial standard deviation for Gaussian mutation

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        self.fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall, self.best_fitness_overall = self.find_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            offspring = self.generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.population = np.concatenate((self.population, offspring))
            self.fitness_values = np.concatenate((self.fitness_values, offspring_fitness))

            self.population, self.fitness_values = self.selection(self.population, self.fitness_values)

            best_solution, best_fitness = self.find_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            self.adapt_parameters()


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate': self.mutation_rate,
            'niche_radius': self.niche_radius
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def generate_offspring(self):
        offspring = []
        for i in range(self.population_size // 2):
            parent1_index = random.randint(0, len(self.population) - 1)
            parent2_index = random.randint(0, len(self.population) - 1)
            while parent2_index == parent1_index: # Ensure different parents
                parent2_index = random.randint(0, len(self.population)-1)

            child1, child2 = self.crossover(self.population[parent1_index], self.population[parent2_index])
            child1 = self.mutate(child1)
            child2 = self.mutate(child2)
            offspring.extend([child1, child2])
        return np.array(offspring)

    def crossover(self, parent1, parent2):
        crossover_point = random.randint(1, self.dim -1)
        child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))
        child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))
        return child1, child2

    def mutate(self, individual):
        mutated_individual = individual + np.random.normal(0, self.sigma, size=self.dim)
        mutated_individual = np.clip(mutated_individual, self.lower_bounds, self.upper_bounds) #Keeps it within bounds
        return mutated_individual


    def selection(self, population, fitness_values):
      # Niche based selection.  Keep best individuals from each niche.
      sorted_indices = np.argsort(fitness_values)
      selected_population = []
      selected_fitness = []
      occupied_niches = set()

      for i in sorted_indices:
          individual = population[i]
          is_in_niche = False
          for j in occupied_niches:
              if np.linalg.norm(individual - population[j]) < self.niche_radius:
                  is_in_niche = True
                  break
          if not is_in_niche:
              selected_population.append(individual)
              selected_fitness.append(fitness_values[i])
              occupied_niches.add(len(selected_population) -1)
              if len(selected_population) == self.population_size:
                  break
      return np.array(selected_population), np.array(selected_fitness)

    def find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def adapt_parameters(self):
        #Simple adaptive mechanism: reduce mutation rate and niche radius over time
        self.mutation_rate *= 0.99
        self.sigma *= 0.99
        self.niche_radius *= 0.99


```

### Rank 10 (Overall AOCC Score: 9.7019e-02 |             AOCC Score on Unimodal instances: 1.7532e-01 |             AOCC Score on Multimodal instances with a single component: 1.1574e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalEvolutionaryStrategy
# Description: An evolutionary strategy employing adaptive mutation and a niching mechanism to efficiently explore and exploit multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalEvolutionaryStrategy:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.sigma = 0.5  # Initial mutation strength
        self.sigma_decay = 0.99 # Decay rate of mutation strength
        self.niche_radius = 20 # Radius for niching

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            self.population, fitness_values = self._selection(self.population, fitness_values, offspring, offspring_fitness)
            self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)
            self.sigma *= self.sigma_decay #Adaptive mutation

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'sigma': self.sigma
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _generate_offspring(self):
        offspring = []
        for i in range(self.population_size):
            offspring.append(self.population[i] + np.random.normal(0, self.sigma, self.dim))
        return np.array(offspring)
    

    def _selection(self, parents, parent_fitness, offspring, offspring_fitness):
        combined_population = np.vstack((parents, offspring))
        combined_fitness = np.concatenate((parent_fitness, offspring_fitness))

        #Niching to handle multimodality
        selected_population = []
        selected_fitness = []
        while len(selected_population) < self.population_size:
            best_index = np.argmin(combined_fitness)
            best_solution = combined_population[best_index]
            is_unique = True
            for sol in selected_population:
                if np.linalg.norm(sol- best_solution) < self.niche_radius:
                    is_unique = False
                    break
            if is_unique:
                selected_population.append(best_solution)
                selected_fitness.append(combined_fitness[best_index])
            combined_population = np.delete(combined_population, best_index, axis=0)
            combined_fitness = np.delete(combined_fitness, best_index)


        return np.array(selected_population), np.array(selected_fitness)

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]

        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return self.best_solution_overall, self.best_fitness_overall

```

### Rank 11 (Overall AOCC Score: 9.6664e-02 |             AOCC Score on Unimodal instances: 1.8056e-01 |             AOCC Score on Multimodal instances with a single component: 1.0944e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining differential evolution with adaptive local search to escape local optima in multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.local_search_iterations = 5 #Number of iterations for local search


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)


        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness = objective_function(new_population)
            self.eval_count += self.population_size

            self.population = np.concatenate((self.population, new_population))
            fitness = np.concatenate((fitness, new_fitness))
            
            self.population, fitness = self._selection(self.population, fitness)
            
            #Adaptive Local Search
            best_idx = np.argmin(fitness)
            best_solution = self.population[best_idx]
            
            improved = True
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(best_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))
                self.eval_count +=1
                if neighbor_fitness[0] < fitness[best_idx]:
                    best_solution = neighbor
                    fitness[best_idx] = neighbor_fitness[0]
                    improved = True
                else:
                    improved = False
            if improved:
                self.population[best_idx] = best_solution

            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        new_population = np.zeros_like(self.population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            v = self.population[a] + self.F * (self.population[b] - self.population[c])
            
            for j in range(self.dim):
                if random.random() < self.CR:
                    new_population[i, j] = v[j]
                else:
                    new_population[i, j] = self.population[i, j]
            
            new_population[i] = np.clip(new_population[i], self.lower_bounds, self.upper_bounds)
        return new_population
    
    def _selection(self, population, fitness):
        combined = np.column_stack((population, fitness))
        sorted_pop = combined[combined[:, -1].argsort()]
        return sorted_pop[:, :-1][:self.population_size], sorted_pop[:, -1][:self.population_size]
        

    def _find_best(self, population, fitness):
        best_idx = np.argmin(fitness)
        best_solution = population[best_idx]
        best_fitness = fitness[best_idx]
        return best_solution, best_fitness
    
    def _generate_neighbor(self, solution):
        neighbor = solution + np.random.normal(0, 0.1, self.dim) # Adjust the standard deviation as needed
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor
```

### Rank 12 (Overall AOCC Score: 9.6272e-02 |             AOCC Score on Unimodal instances: 1.7867e-01 |             AOCC Score on Multimodal instances with a single component: 1.1015e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining Differential Evolution with a local search and adaptive mutation strategy for efficient multimodal optimization.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.F = 0.8 #Differential Evolution scaling factor
        self.CR = 0.9 #Differential Evolution crossover rate
        self.mutation_rate = 0.1 #Initial mutation rate
        self.adaptive_factor = 0.95 #factor to adjust mutation rate


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                #Differential Evolution
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)

                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                trial = np.clip(np.random.rand(self.dim) < self.CR, 0, 1) * mutant + (1 - np.clip(np.random.rand(self.dim) < self.CR, 0, 1)) * self.population[i]
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds)

                #Local Search (Optional - add a simple local search here if needed)
                #Example: Simple hill climbing.  Comment this section out if you do not want it.
                # for j in range(self.dim):
                #     temp = trial.copy()
                #     temp[j] += 0.1
                #     temp_fitness = objective_function(temp.reshape(1,-1))[0]
                #     self.eval_count +=1
                #     if temp_fitness < objective_function(trial.reshape(1,-1))[0]:
                #         trial = temp

                trial_fitness = objective_function(trial.reshape(1,-1))[0]
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values[i] = trial_fitness
                    if trial_fitness < self.best_fitness_overall:
                        self.best_fitness_overall = trial_fitness
                        self.best_solution_overall = trial
                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)

            #Adaptive Mutation Rate based on convergence speed
            if self.best_fitness_overall < acceptance_threshold:
                self.mutation_rate *= self.adaptive_factor

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate_final' : self.mutation_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

```

### Rank 13 (Overall AOCC Score: 9.6122e-02 |             AOCC Score on Unimodal instances: 1.8027e-01 |             AOCC Score on Multimodal instances with a single component: 1.0809e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining differential evolution with adaptive mutation and a niching strategy to escape local optima and explore diverse regions of the search space.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.fitness_values = None
        self.F = 0.8 # Differential evolution scaling factor
        self.CR = 0.9 # Differential evolution crossover rate
        self.niche_radius = 0.1 #Adjust this parameter to control the niching effect


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        self.fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self.get_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = self.differential_evolution()
            new_fitness_values = objective_function(new_population)
            self.eval_count += len(new_fitness_values)
            self.population, self.fitness_values = self.selection(self.population, self.fitness_values, new_population, new_fitness_values)
            best_solution, best_fitness = self.get_best(self.population, self.fitness_values)
            if best_fitness < self.best_fitness_overall:
                self.best_solution_overall = best_solution
                self.best_fitness_overall = best_fitness
            
            #Adaptive Mutation: increase F and CR if stuck in local optima
            if self.eval_count > self.budget * 0.7 and self.best_fitness_overall > acceptance_threshold:
                self.F += 0.1 * (1 - self.F)
                self.CR += 0.1 * (1 - self.CR)
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def differential_evolution(self):
        offspring = np.zeros_like(self.population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
            trial = np.zeros_like(mutant)
            jrand = random.randint(0, self.dim -1)
            for j in range(self.dim):
                if random.random() < self.CR or j == jrand:
                    trial[j] = mutant[j]
                else:
                    trial[j] = self.population[i][j]
            offspring[i] = np.clip(trial, self.lower_bounds, self.upper_bounds)
        return offspring


    def selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_population = np.vstack((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fitness)
        return combined_population[sorted_indices[:self.population_size]], combined_fitness[sorted_indices[:self.population_size]]

    def get_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
```

### Rank 14 (Overall AOCC Score: 9.6096e-02 |             AOCC Score on Unimodal instances: 1.7849e-01 |             AOCC Score on Multimodal instances with a single component: 1.0980e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining Differential Evolution with a local search and adaptive mutation to escape local optima in high-dimensional multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate
        self.population = None


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size

        self.best_solution_overall = self.population[np.argmin(fitness_values)]
        self.best_fitness_overall = np.min(fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                # Differential Evolution
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                trial = np.clip(np.random.binomial(1,self.CR,size=self.dim)*mutant + (1-np.random.binomial(1,self.CR,size=self.dim))*self.population[i],self.lower_bounds,self.upper_bounds)

                #Adaptive Mutation - Increase mutation rate if stuck
                if self.best_fitness_overall == np.min(fitness_values) and self.eval_count>self.population_size*5:
                    self.mutation_rate = min(1.0, self.mutation_rate * 1.1)

                #Local Search - small perturbation if close to solution
                if abs(np.min(fitness_values) - self.best_fitness_overall)<0.01 and self.eval_count > self.population_size*5:
                    trial = trial + np.random.normal(0,0.1,size=self.dim)
                    trial = np.clip(trial,self.lower_bounds, self.upper_bounds)


                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values = np.concatenate((fitness_values[:i], trial_fitness, fitness_values[i+1:]))
                    if trial_fitness < self.best_fitness_overall:
                        self.best_solution_overall = trial
                        self.best_fitness_overall = trial_fitness
                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'mutation_rate':self.mutation_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

```

### Rank 15 (Overall AOCC Score: 9.6035e-02 |             AOCC Score on Unimodal instances: 1.8038e-01 |             AOCC Score on Multimodal instances with a single component: 1.0772e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining Differential Evolution with a niching technique to escape local optima and explore diverse regions of the search space effectively.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Differential Evolution crossover rate
        self.niche_radius = 0.5  # Adjust based on problem scale
        self.population = self.initialize_population()


    def initialize_population(self):
        return np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))

    def differential_evolution(self, population):
        new_population = np.zeros_like(population)
        for i in range(self.population_size):
            # Select three random individuals (excluding the current one)
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            # Mutation
            mutant = population[a] + self.F * (population[b] - population[c])

            # Boundary handling (clipping)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Crossover
            trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])

            new_population[i] = trial
        return new_population

    def niching(self, population, fitness_values):
        # Simple niching: remove solutions too close to existing ones
        unique_population = [population[0]]
        unique_fitness = [fitness_values[0]]
        for i in range(1, len(population)):
            too_close = False
            for j in range(len(unique_population)):
                distance = np.linalg.norm(population[i] - unique_population[j])
                if distance < self.niche_radius:
                    too_close = True
                    break
            if not too_close:
                unique_population.append(population[i])
                unique_fitness.append(fitness_values[i])
        return np.array(unique_population), np.array(unique_fitness)

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population = self.initialize_population()

        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        
        best_index = np.argmin(fitness_values)
        self.best_solution_overall = self.population[best_index]
        self.best_fitness_overall = fitness_values[best_index]

        while self.eval_count < self.budget:
            new_population = self.differential_evolution(self.population)
            new_fitness_values = objective_function(new_population)
            self.eval_count += self.population_size

            combined_population = np.concatenate((self.population, new_population))
            combined_fitness = np.concatenate((fitness_values, new_fitness_values))
            
            #Sort to find best solutions (before niching)
            sorted_indices = np.argsort(combined_fitness)
            combined_population = combined_population[sorted_indices]
            combined_fitness = combined_fitness[sorted_indices]

            # Niching
            self.population, fitness_values = self.niching(combined_population[:self.population_size], combined_fitness[:self.population_size])

            best_index = np.argmin(fitness_values)
            if fitness_values[best_index] < self.best_fitness_overall:
                self.best_fitness_overall = fitness_values[best_index]
                self.best_solution_overall = self.population[best_index]

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

```

### Rank 16 (Overall AOCC Score: 9.5467e-02 |             AOCC Score on Unimodal instances: 1.7934e-01 |             AOCC Score on Multimodal instances with a single component: 1.0707e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining differential evolution with a local search and adaptive mutation to escape local optima in high-dimensional multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 #  Adjust as needed for problem difficulty
        self.F = 0.8 # Differential Evolution scaling factor
        self.CR = 0.9 # Crossover rate

        self.mutation_rate = 0.1 # Initial mutation rate, adaptive
        self.local_search_radius = 0.5 * (self.upper_bounds[0] - self.lower_bounds[0]) # Initialize local search radius


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = float('inf')
        
        # Initialize population
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        # Main optimization loop
        while self.eval_count < self.budget:
            # Differential Evolution
            for i in range(self.population_size):
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)
                mutant = population[a] + self.F * (population[b] - population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.copy(population[i])
                for j in range(self.dim):
                    if random.random() < self.CR:
                        trial[j] = mutant[j]

                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < fitness_values[i]:
                    population[i] = trial
                    fitness_values[i] = trial_fitness

            # Adaptive Mutation and Local Search
            best_index = np.argmin(fitness_values)
            best_solution = population[best_index]
            best_fitness = fitness_values[best_index]


            if best_fitness < self.best_fitness_overall:
                self.best_fitness_overall = best_fitness
                self.best_solution_overall = best_solution

            # Adaptive components
            if self.eval_count % (self.population_size * 5) == 0:  # Adaptive every 5 generations
                if best_fitness > acceptance_threshold: # Increase exploration if far from optimum
                    self.mutation_rate = min(self.mutation_rate * 1.1, 0.5)
                    self.local_search_radius *= 1.1 #Wider exploration
                else:
                     self.mutation_rate *= 0.9 #Reduce exploration when approaching optimum
                     self.local_search_radius *=0.9  # Narrower exploration

            # Local Search (optional, to enhance exploitation)
            local_search_solution = self.local_search(best_solution, objective_function, self.local_search_radius)
            if local_search_solution[1] < self.best_fitness_overall:
                self.best_solution_overall = local_search_solution[0]
                self.best_fitness_overall = local_search_solution[1]


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def local_search(self, solution, objective_function, radius):
        best_local_solution = solution
        best_local_fitness = objective_function(solution.reshape(1, -1))[0]
        self.eval_count +=1
        for i in range(10): # number of local search iterations
            perturbation = np.random.uniform(-radius, radius, self.dim)
            neighbor = np.clip(solution + perturbation, self.lower_bounds, self.upper_bounds)
            neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
            self.eval_count += 1
            if neighbor_fitness < best_local_fitness:
                best_local_solution = neighbor
                best_local_fitness = neighbor_fitness
                
        return best_local_solution, best_local_fitness

```

### Rank 17 (Overall AOCC Score: 9.5294e-02 |             AOCC Score on Unimodal instances: 1.7951e-01 |             AOCC Score on Multimodal instances with a single component: 1.0637e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining differential evolution with adaptive local search to escape local optima in multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8 # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate

        self.population = np.random.uniform(low=self.lower_bounds, high=self.upper_bounds, size=(self.population_size, self.dim))


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')

        #Initial Evaluation
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        best_index = np.argmin(fitness_values)
        self.best_solution_overall = self.population[best_index].copy()
        self.best_fitness_overall = fitness_values[best_index]


        while self.eval_count < self.budget:
            # Differential Evolution
            offspring = np.zeros_like(self.population)
            for i in range(self.population_size):
                a, b, c = random.sample(range(self.population_size), 3)
                while a == i or b == i or c == i:
                    a, b, c = random.sample(range(self.population_size), 3)

                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)  #Keep within bounds

                trial = np.zeros_like(self.population[i])
                for j in range(self.dim):
                    if random.random() < self.CR:
                        trial[j] = mutant[j]
                    else:
                        trial[j] = self.population[i][j]

                offspring[i] = trial

            # Evaluation of offspring
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            # Selection
            for i in range(self.population_size):
                if offspring_fitness[i] < fitness_values[i]:
                    self.population[i] = offspring[i]
                    fitness_values[i] = offspring_fitness[i]

                    if fitness_values[i] < self.best_fitness_overall:
                        self.best_solution_overall = self.population[i].copy()
                        self.best_fitness_overall = fitness_values[i]

            #Adaptive Local Search (Optional, uncomment for enhanced performance)
            # if self.eval_count % (self.population_size * 5) == 0: #Perform Local Search every 5 generations
            #    self.local_search(objective_function)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    # def local_search(self, objective_function): #Optional Local Search (Hill Climbing)
    #     current_solution = self.best_solution_overall.copy()
    #     current_fitness = self.best_fitness_overall
    #     step_size = 0.1

    #     while self.eval_count < self.budget: #Check budget within the local search
    #         improved = False
    #         for i in range(self.dim):
    #             temp_solution = current_solution.copy()
    #             temp_solution[i] += step_size
    #             temp_solution = np.clip(temp_solution, self.lower_bounds, self.upper_bounds)
    #             temp_fitness = objective_function(temp_solution.reshape(1,-1))[0] #Evaluate single solution
    #             self.eval_count += 1
    #             if temp_fitness < current_fitness:
    #                 current_solution = temp_solution
    #                 current_fitness = temp_fitness
    #                 improved = True
    #             temp_solution[i] -= 2 * step_size
    #             temp_solution = np.clip(temp_solution, self.lower_bounds, self.upper_bounds)
    #             temp_fitness = objective_function(temp_solution.reshape(1,-1))[0]
    #             self.eval_count += 1
    #             if temp_fitness < current_fitness:
    #                 current_solution = temp_solution
    #                 current_fitness = temp_fitness
    #                 improved = True
    #         if not improved:
    #             break
    #         if current_fitness < self.best_fitness_overall:
    #             self.best_solution_overall = current_solution.copy()
    #             self.best_fitness_overall = current_fitness

```

### Rank 18 (Overall AOCC Score: 9.2799e-02 |             AOCC Score on Unimodal instances: 1.7556e-01 |             AOCC Score on Multimodal instances with a single component: 1.0284e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining Differential Evolution with a local search and adaptive mutation strategy to escape local optima in high-dimensional multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100 # Adjust as needed
        self.population = None
        self.F = 0.8 # Differential Evolution scaling factor
        self.CR = 0.9 # Differential Evolution crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness_values = objective_function(new_population)
            self.eval_count += self.population_size
            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, new_population, new_fitness_values)
            fitness_values = np.concatenate((fitness_values, new_fitness_values))
            
            # Adaptive Mutation: Increase mutation rate if stuck
            if self.eval_count > self.budget * 0.2 and self.best_fitness_overall > acceptance_threshold and self._is_stagnant(fitness_values):
                self.mutation_rate *= 1.1
                
        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'final_mutation_rate': self.mutation_rate
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        offspring = np.copy(self.population)
        for i in range(self.population_size):
            r1, r2, r3 = random.sample(range(self.population_size), 3)
            while r1 == i or r2 == i or r3 == i:
                r1, r2, r3 = random.sample(range(self.population_size), 3)
            mutant = self.population[r1] + self.F * (self.population[r2] - self.population[r3])
            
            #Clamp Values
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            
            trial = np.copy(self.population[i])
            crossover_points = np.random.rand(self.dim) < self.CR
            trial[crossover_points] = mutant[crossover_points]
            
            #Add Mutation
            mutation_points = np.random.rand(self.dim) < self.mutation_rate
            trial[mutation_points] = np.random.uniform(self.lower_bounds, self.upper_bounds, np.sum(mutation_points))
            
            offspring[i] = trial
        return offspring

    def _selection(self, population, fitness_values, offspring, offspring_fitness_values):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness_values))
        indices = np.argsort(combined_fitness)
        selected_population = combined_population[indices[:self.population_size]]
        selected_fitness = combined_fitness[indices[:self.population_size]]
        best_solution, best_fitness = self._update_best(selected_population, selected_fitness)
        return selected_population, best_solution, best_fitness

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness
    
    def _is_stagnant(self, fitness_values):
        # Check if the best fitness has not improved for a certain number of iterations.
        # Adjust the parameters (e.g., num_iterations) as needed
        num_iterations = 10
        if len(fitness_values) < num_iterations * self.population_size:
             return False
        return np.min(fitness_values[-num_iterations*self.population_size:]) == np.min(fitness_values)

```

### Rank 19 (Overall AOCC Score: 9.2257e-02 |             AOCC Score on Unimodal instances: 1.7529e-01 |             AOCC Score on Multimodal instances with a single component: 1.0149e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining Differential Evolution with a local search and adaptive mutation rate to efficiently explore and exploit multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.population = None
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9  # Crossover rate
        self.mutation_rate = 0.1 # Initial mutation rate
        

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self._update_best(self.population, fitness_values)

        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness_values = objective_function(new_population)
            self.eval_count += self.population_size
            self.population, self.best_solution_overall, self.best_fitness_overall = self._selection(self.population, fitness_values, new_population, new_fitness_values)
            fitness_values = np.concatenate((fitness_values, new_fitness_values))
            self._adapt_mutation_rate() # Adjust mutation rate based on performance
            
        if self.best_solution_overall is None and self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        new_population = np.zeros_like(self.population)
        for i in range(self.population_size):
            # Randomly select three different individuals
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)

            # Mutation
            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])

            # Crossover
            j_rand = random.randint(0, self.dim - 1)
            for j in range(self.dim):
                if random.random() < self.CR or j == j_rand:
                    new_population[i, j] = mutant[j]
                else:
                    new_population[i, j] = self.population[i, j]

            # Boundary handling
            new_population[i] = np.clip(new_population[i], self.lower_bounds, self.upper_bounds)

        return new_population

    def _selection(self, old_pop, old_fit, new_pop, new_fit):
        combined_pop = np.concatenate((old_pop, new_pop))
        combined_fit = np.concatenate((old_fit, new_fit))
        indices = np.argsort(combined_fit)
        selected_pop = combined_pop[indices[:self.population_size]]
        selected_fit = combined_fit[indices[:self.population_size]]
        best_solution, best_fitness = self._update_best(selected_pop, selected_fit)
        return selected_pop, best_solution, best_fitness


    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        best_solution = population[best_index]
        best_fitness = fitness_values[best_index]
        if best_fitness < self.best_fitness_overall:
            self.best_fitness_overall = best_fitness
            self.best_solution_overall = best_solution
        return best_solution, best_fitness

    def _adapt_mutation_rate(self):
        # Simple adaptive strategy: Reduce mutation rate if consistently no improvement
        if self.best_fitness_overall == float('inf'):
            self.mutation_rate = min(self.mutation_rate * 1.1, 1)
        elif self.best_fitness_overall > self.best_fitness_overall:
            self.mutation_rate = max(self.mutation_rate * 0.9, 0.01) # prevents mutation rate from dropping too low

```

### Rank 20 (Overall AOCC Score: 9.2223e-02 |             AOCC Score on Unimodal instances: 1.7546e-01 |             AOCC Score on Multimodal instances with a single component: 1.0121e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalOptimizer
# Description: A hybrid evolutionary algorithm combining differential evolution with adaptive mutation and a local search to efficiently explore and exploit multimodal landscapes.
# Code:
```python
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        
        self.population_size = 100 # Adjust as needed
        self.F = 0.8 # Differential evolution scaling factor
        self.CR = 0.9 # Differential evolution crossover rate
        self.mutation_rate = 0.1 # Adaptive mutation rate
        self.local_search_iterations = 5 #Number of iterations for local search

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self.best_solution_overall, self.best_fitness_overall = self.find_best(self.population, fitness_values)


        while self.eval_count < self.budget:
            new_population = []
            for i in range(self.population_size):
                #Differential Evolution
                a, b, c = self.select_different(i)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                trial = self.crossover(self.population[i], mutant)
                trial = np.clip(trial, self.lower_bounds, self.upper_bounds) #Bound constraints

                #Adaptive Mutation
                if random.random() < self.mutation_rate :
                    trial = self.mutate(trial)

                #Local Search
                trial = self.local_search(trial, objective_function)
                
                trial_fitness = objective_function(trial.reshape(1, -1))
                self.eval_count += 1
                
                if trial_fitness[0] < fitness_values[i]:
                    new_population.append(trial)
                    fitness_values[i] = trial_fitness[0]
                    if trial_fitness[0] < self.best_fitness_overall:
                         self.best_solution_overall, self.best_fitness_overall = trial, trial_fitness[0]

                else:
                    new_population.append(self.population[i])

            self.population = np.array(new_population)
            self.adaptive_mutation_rate()

        if self.best_solution_overall is None and self.dim > 0 : # Fallback
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
            self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def select_different(self, i):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == i or b == i or c == i:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def crossover(self, x, v):
        u = np.zeros_like(x)
        for i in range(self.dim):
            if random.random() < self.CR:
                u[i] = v[i]
            else:
                u[i] = x[i]
        return u

    def mutate(self, x):
        index = random.randint(0, self.dim - 1)
        x[index] += np.random.normal(0, 0.5)  #Small gaussian mutation
        return x

    def local_search(self, x, objective_function):
        for _ in range(self.local_search_iterations):
            best_neighbor = x.copy()
            best_neighbor_fitness = objective_function(best_neighbor.reshape(1,-1))
            for i in range(self.dim):
              for j in range(2): # explore both directions
                  neighbor = x.copy()
                  neighbor[i] += (-1)**j * 0.1 # Adjust step size as needed
                  neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
                  neighbor_fitness = objective_function(neighbor.reshape(1,-1))
                  if neighbor_fitness[0] < best_neighbor_fitness[0]:
                    best_neighbor = neighbor
                    best_neighbor_fitness = neighbor_fitness
            x = best_neighbor
            self.eval_count+=1
        return x


    def adaptive_mutation_rate(self):
        # Adjust mutation rate based on convergence - simple example
        if self.eval_count > 0.8 * self.budget:
            self.mutation_rate *= 0.5 # Reduce mutation closer to the end

    def find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]
```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-23 14:24:39 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveGaussianArchiveEA vs (worst) AdaptiveMultimodalOptimizer, we see that the best utilizes an archive to maintain diversity and avoid premature convergence, a feature missing in the worst.  The best also employs adaptive Gaussian mutation, which dynamically adjusts exploration and exploitation, unlike the worst's simpler mutation. (second best) AdaptiveGaussianSamplingEAwithArchive vs (second worst) AdaptiveMultimodalEvolutionaryStrategy shows similar trends: archives improve performance significantly. Comparing (1st) AdaptiveGaussianArchiveEA vs (2nd) AdaptiveGaussianSamplingEAwithArchive, we see the former's slightly more aggressive sigma decay and larger archive contributes to a marginal improvement in multimodal performance, especially in multi-component problems. (3rd) AdaptiveGaussianSamplingEA vs (4th) AdaptiveDifferentialEvolutionWithEnhancedInitialization: AdaptiveGaussianSamplingEA uses adaptive sigma and demonstrates superior multimodal performance, showcasing the importance of adaptive exploration. Comparing (second worst) AdaptiveMultimodalEvolutionaryStrategy vs (worst) AdaptiveMultimodalOptimizer, we see that the second-worst incorporates a rudimentary niching mechanism to diversify the search, yielding slightly better results than the basic adaptive mutation strategy of the worst. Overall:  Adaptive strategies (mutation, sigma decay), combined with archives and niching mechanisms, consistently improve performance across all GNBG groups.  A simple Differential Evolution implementation (without adaptation) performs poorly.


**Experience:**

Adaptive parameter tuning and mechanisms promoting population diversity (archives, niching) are crucial for solving high-dimensional multimodal problems.  Combining exploration and exploitation methods yields the best results.

2025-06-23 14:24:40 INFO Full response text: * **Keywords:**  Adaptive, Multimodal, Exploration-Exploitation, Diversity, Heuristics, Parameter Tuning.

* **Advice:** Focus on quantifiable metrics for evaluating exploration/exploitation balance and diversity.  Develop adaptive mechanisms that dynamically adjust based on these metrics.  Consider incorporating information theoretic measures to guide the search. Experiment with different diversity-promoting techniques (e.g., crowding, speciation) and analyze their impact.

* **Avoid:**  Generic statements about exploration/exploitation.  Avoid subjective assessments of "good" or "bad" without quantitative support.  Don't rely solely on single performance metrics; consider multiple aspects (convergence speed, solution quality, diversity).

* **Explanation:**  Effective self-reflection for heuristic design requires rigorous, data-driven analysis. By quantifying the performance and diversity aspects of your algorithm, you can identify weaknesses and systematically improve your heuristics, leading to a more robust and efficient search.

2025-06-23 14:24:40 INFO Generating offspring via Crossover...
2025-06-23 14:36:49 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

### Better code
AdaptiveMultimodalOptimizer
import numpy as np
import random

class AdaptiveMultimodalOptimizer:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100  # Adjust as needed
        self.F = 0.8  # Differential Evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.local_search_iterations = 5 #Number of iterations for local search


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        else:
            self.population = np.array([])
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)


        while self.eval_count < self.budget:
            new_population = self._differential_evolution()
            new_fitness = objective_function(new_population)
            self.eval_count += self.population_size

            self.population = np.concatenate((self.population, new_population))
            fitness = np.concatenate((fitness, new_fitness))
            
            self.population, fitness = self._selection(self.population, fitness)
            
            #Adaptive Local Search
            best_idx = np.argmin(fitness)
            best_solution = self.population[best_idx]
            
            improved = True
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(best_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))
                self.eval_count +=1
                if neighbor_fitness[0] < fitness[best_idx]:
                    best_solution = neighbor
                    fitness[best_idx] = neighbor_fitness[0]
                    improved = True
                else:
                    improved = False
            if improved:
                self.population[best_idx] = best_solution

            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, fitness)
            

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _differential_evolution(self):
        new_population = np.zeros_like(self.population)
        for i in range(self.population_size):
            a, b, c = random.sample(range(self.population_size), 3)
            while a == i or b == i or c == i:
                a, b, c = random.sample(range(self.population_size), 3)
            v = self.population[a] + self.F * (self.population[b] - self.population[c])
            
            for j in range(self.dim):
                if random.random() < self.CR:
                    new_population[i, j] = v[j]
                else:
                    new_population[i, j] = self.population[i, j]
            
            new_population[i] = np.clip(new_population[i], self.lower_bounds, self.upper_bounds)
        return new_population
    
    def _selection(self, population, fitness):
        combined = np.column_stack((population, fitness))
        sorted_pop = combined[combined[:, -1].argsort()]
        return sorted_pop[:, :-1][:self.population_size], sorted_pop[:, -1][:self.population_size]
        

    def _find_best(self, population, fitness):
        best_idx = np.argmin(fitness)
        best_solution = population[best_idx]
        best_fitness = fitness[best_idx]
        return best_solution, best_fitness
    
    def _generate_neighbor(self, solution):
        neighbor = solution + np.random.normal(0, 0.1, self.dim) # Adjust the standard deviation as needed
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

### Worse code
AdaptiveMultimodalOptimizerImproved
import numpy as np
import random
class AdaptiveMultimodalOptimizerImproved:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.tabu_list = []  # Tabu list to avoid revisiting recent solutions
        self.tabu_length = 10 # Length of the tabu list

        self.perturbation_strength = 0.5 # Initial perturbation strength, adaptive
        self.local_search_iterations = 10 # Number of iterations for local search
        self.temperature = 1.0 # Initial temperature for simulated annealing

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])

        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        while self.eval_count < self.budget:
            current_solution = self.best_solution_overall.copy()
                                
            # Local Search
            for _ in range(self.local_search_iterations):
                neighbor = self._generate_neighbor(current_solution)
                neighbor_fitness = objective_function(neighbor.reshape(1, -1))[0]
                self.eval_count += 1

                if self._accept(neighbor_fitness, self._fitness(current_solution, objective_function), self.temperature):
                    current_solution = neighbor

                if neighbor_fitness < self.best_fitness_overall:
                    self.best_fitness_overall = neighbor_fitness
                    self.best_solution_overall = neighbor
                    self.tabu_list = [] # Reset tabu list upon finding a new global best

            # Check for stagnation and apply perturbation
            if self._is_stagnant(current_solution):
                current_solution = self._perturb(current_solution)
            self.temperature *= 0.95 # Cool down the temperature

            # Add solution to tabu list
            self._update_tabu_list(current_solution)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall,
            'perturbation_strength': self.perturbation_strength,
            'final_temperature': self.temperature
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_neighbor(self, solution):
        neighbor = solution.copy()
        index = random.randint(0, self.dim - 1)
        neighbor[index] += np.random.normal(0, 0.1 * (self.upper_bounds[index] - self.lower_bounds[index]))  # Small Gaussian perturbation
        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
        return neighbor

    def _perturb(self, solution):
        perturbation = np.random.uniform(-self.perturbation_strength, self.perturbation_strength, self.dim) * (self.upper_bounds - self.lower_bounds)
        new_solution = solution + perturbation
        new_solution = np.clip(new_solution, self.lower_bounds, self.upper_bounds)
        self.perturbation_strength *= 1.1 # Increase perturbation strength adaptively
        return new_solution

    def _is_stagnant(self, solution):
        return np.allclose(solution, self.best_solution_overall, atol=1e-4)


    def _update_tabu_list(self, solution):
        self.tabu_list.append(tuple(solution))
        if len(self.tabu_list) > self.tabu_length:
            self.tabu_list.pop(0)

    def _fitness(self, solution, objective_function):
        return objective_function(solution.reshape(1, -1))[0]

    def _accept(self, new_fitness, current_fitness, temperature):
        if new_fitness < current_fitness:
            return True
        else:
            delta_e = new_fitness - current_fitness
            acceptance_probability = np.exp(-delta_e / temperature)
            return random.random() < acceptance_probability









### Analyze & experience
- Comparing (best) AdaptiveGaussianArchiveEA vs (worst) AdaptiveMultimodalOptimizer, we see that the best utilizes an archive to maintain diversity and avoid premature convergence, a feature missing in the worst.  The best also employs adaptive Gaussian mutation, which dynamically adjusts exploration and exploitation, unlike the worst's simpler mutation. (second best) AdaptiveGaussianSamplingEAwithArchive vs (second worst) AdaptiveMultimodalEvolutionaryStrategy shows similar trends: archives improve performance significantly. Comparing (1st) AdaptiveGaussianArchiveEA vs (2nd) AdaptiveGaussianSamplingEAwithArchive, we see the former's slightly more aggressive sigma decay and larger archive contributes to a marginal improvement in multimodal performance, especially in multi-component problems. (3rd) AdaptiveGaussianSamplingEA vs (4th) AdaptiveDifferentialEvolutionWithEnhancedInitialization: AdaptiveGaussianSamplingEA uses adaptive sigma and demonstrates superior multimodal performance, showcasing the importance of adaptive exploration. Comparing (second worst) AdaptiveMultimodalEvolutionaryStrategy vs (worst) AdaptiveMultimodalOptimizer, we see that the second-worst incorporates a rudimentary niching mechanism to diversify the search, yielding slightly better results than the basic adaptive mutation strategy of the worst. Overall:  Adaptive strategies (mutation, sigma decay), combined with archives and niching mechanisms, consistently improve performance across all GNBG groups.  A simple Differential Evolution implementation (without adaptation) performs poorly.
- * **Keywords:**  Adaptive, Multimodal, Exploration-Exploitation, Diversity, Heuristics, Parameter Tuning.

* **Advice:** Focus on quantifiable metrics for evaluating exploration/exploitation balance and diversity.  Develop adaptive mechanisms that dynamically adjust based on these metrics.  Consider incorporating information theoretic measures to guide the search. Experiment with different diversity-promoting techniques (e.g., crowding, speciation) and analyze their impact.

* **Avoid:**  Generic statements about exploration/exploitation.  Avoid subjective assessments of "good" or "bad" without quantitative support.  Don't rely solely on single performance metrics; consider multiple aspects (convergence speed, solution quality, diversity).

* **Explanation:**  Effective self-reflection for heuristic design requires rigorous, data-driven analysis. By quantifying the performance and diversity aspects of your algorithm, you can identify weaknesses and systematically improve your heuristics, leading to a more robust and efficient search.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-23 14:36:49 INFO Mutation prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

Current heuristics:
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- * **Keywords:**  Adaptive, Multimodal, Exploration-Exploitation, Diversity, Heuristics, Parameter Tuning.

* **Advice:** Focus on quantifiable metrics for evaluating exploration/exploitation balance and diversity.  Develop adaptive mechanisms that dynamically adjust based on these metrics.  Consider incorporating information theoretic measures to guide the search. Experiment with different diversity-promoting techniques (e.g., crowding, speciation) and analyze their impact.

* **Avoid:**  Generic statements about exploration/exploitation.  Avoid subjective assessments of "good" or "bad" without quantitative support.  Don't rely solely on single performance metrics; consider multiple aspects (convergence speed, solution quality, diversity).

* **Explanation:**  Effective self-reflection for heuristic design requires rigorous, data-driven analysis. By quantifying the performance and diversity aspects of your algorithm, you can identify weaknesses and systematically improve your heuristics, leading to a more robust and efficient search.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-23 14:46:44 INFO Perform Harmony Search...
2025-06-23 14:46:55 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:46:55 ERROR Can not run the algorithm
2025-06-23 14:46:55 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1736
2025-06-23 14:46:55 INFO FeHistory: [-701.29730166]
2025-06-23 14:46:55 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:46:55 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 930.0053150477364, archive_size: int = 104.98091414320167, sigma_decay: float = 0.9238666657403672, initial_sigma_multiplier: float = 0.8869308661459988):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:46:55 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:46:55 ERROR Can not run the algorithm
2025-06-23 14:46:55 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0919
2025-06-23 14:46:55 INFO FeHistory: [-222.21703097]
2025-06-23 14:46:55 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:46:55 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:46:55 ERROR Can not run the algorithm
2025-06-23 14:46:56 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:46:56 INFO FeHistory: [191.9188519]
2025-06-23 14:46:56 INFO Expected Optimum FE: -100
2025-06-23 14:46:56 INFO Unimodal AOCC mean: 0.1736
2025-06-23 14:46:56 INFO Multimodal (single component) AOCC mean: 0.0919
2025-06-23 14:46:56 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:46:56 INFO AOCC mean: 0.0885
2025-06-23 14:46:56 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:46:56 ERROR Can not run the algorithm
2025-06-23 14:46:56 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1748
2025-06-23 14:46:56 INFO FeHistory: [-701.34455511]
2025-06-23 14:46:56 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:46:56 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 686.6857251009523, archive_size: int = 883.9067822500798, sigma_decay: float = 0.9985746972204944, initial_sigma_multiplier: float = 0.8838212018280642):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:46:56 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:46:56 ERROR Can not run the algorithm
2025-06-23 14:46:56 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0911
2025-06-23 14:46:56 INFO FeHistory: [-222.00551804]
2025-06-23 14:46:56 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:46:56 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:46:56 ERROR Can not run the algorithm
2025-06-23 14:46:56 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:46:56 INFO FeHistory: [202.64134025]
2025-06-23 14:46:56 INFO Expected Optimum FE: -100
2025-06-23 14:46:56 INFO Unimodal AOCC mean: 0.1748
2025-06-23 14:46:56 INFO Multimodal (single component) AOCC mean: 0.0911
2025-06-23 14:46:56 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:46:56 INFO AOCC mean: 0.0886
2025-06-23 14:46:56 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:46:56 ERROR Can not run the algorithm
2025-06-23 14:46:56 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1745
2025-06-23 14:46:56 INFO FeHistory: [-701.33407421]
2025-06-23 14:46:56 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:46:56 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 416.29056078532176, archive_size: int = 191.7005012131621, sigma_decay: float = 0.9415478195383574, initial_sigma_multiplier: float = 0.6518489995146987):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:46:56 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:46:56 ERROR Can not run the algorithm
2025-06-23 14:46:56 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0915
2025-06-23 14:46:56 INFO FeHistory: [-222.11313176]
2025-06-23 14:46:56 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:46:56 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:46:56 ERROR Can not run the algorithm
2025-06-23 14:46:57 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:46:57 INFO FeHistory: [194.47186853]
2025-06-23 14:46:57 INFO Expected Optimum FE: -100
2025-06-23 14:46:57 INFO Unimodal AOCC mean: 0.1745
2025-06-23 14:46:57 INFO Multimodal (single component) AOCC mean: 0.0915
2025-06-23 14:46:57 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:46:57 INFO AOCC mean: 0.0887
2025-06-23 14:46:57 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:46:57 ERROR Can not run the algorithm
2025-06-23 14:46:57 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1745
2025-06-23 14:46:57 INFO FeHistory: [-701.33226348]
2025-06-23 14:46:57 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:46:57 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 219.67661056162515, archive_size: int = 831.1428750190842, sigma_decay: float = 0.9935777238244035, initial_sigma_multiplier: float = 0.5766379318642665):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:46:57 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:46:57 ERROR Can not run the algorithm
2025-06-23 14:46:57 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0918
2025-06-23 14:46:57 INFO FeHistory: [-222.20382739]
2025-06-23 14:46:57 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:46:57 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:46:57 ERROR Can not run the algorithm
2025-06-23 14:46:57 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:46:57 INFO FeHistory: [119.88914746]
2025-06-23 14:46:57 INFO Expected Optimum FE: -100
2025-06-23 14:46:57 INFO Unimodal AOCC mean: 0.1745
2025-06-23 14:46:57 INFO Multimodal (single component) AOCC mean: 0.0918
2025-06-23 14:46:57 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:46:57 INFO AOCC mean: 0.0888
2025-06-23 14:46:57 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:46:57 ERROR Can not run the algorithm
2025-06-23 14:46:57 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1739
2025-06-23 14:46:57 INFO FeHistory: [-701.30785555]
2025-06-23 14:46:57 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:46:57 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 277.81847845079477, archive_size: int = 308.1653845592917, sigma_decay: float = 0.9407963446004244, initial_sigma_multiplier: float = 0.3289463872134193):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:46:57 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:46:57 ERROR Can not run the algorithm
2025-06-23 14:46:57 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0898
2025-06-23 14:46:57 INFO FeHistory: [-221.63555585]
2025-06-23 14:46:57 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:46:58 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:46:58 ERROR Can not run the algorithm
2025-06-23 14:46:58 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:46:58 INFO FeHistory: [196.53625013]
2025-06-23 14:46:58 INFO Expected Optimum FE: -100
2025-06-23 14:46:58 INFO Unimodal AOCC mean: 0.1739
2025-06-23 14:46:58 INFO Multimodal (single component) AOCC mean: 0.0898
2025-06-23 14:46:58 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:46:58 INFO AOCC mean: 0.0879
2025-06-23 14:46:58 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:46:58 ERROR Can not run the algorithm
2025-06-23 14:46:58 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1739
2025-06-23 14:46:58 INFO FeHistory: [-701.30988864]
2025-06-23 14:46:58 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:46:58 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 875.5572325126533, archive_size: int = 883.9067822500798, sigma_decay: float = 0.9935777238244035, initial_sigma_multiplier: float = 0.40012502471899825):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:46:58 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:46:58 ERROR Can not run the algorithm
2025-06-23 14:46:58 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0932
2025-06-23 14:46:58 INFO FeHistory: [-222.58569964]
2025-06-23 14:46:58 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:46:58 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:46:58 ERROR Can not run the algorithm
2025-06-23 14:46:58 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:46:58 INFO FeHistory: [187.17499483]
2025-06-23 14:46:58 INFO Expected Optimum FE: -100
2025-06-23 14:46:58 INFO Unimodal AOCC mean: 0.1739
2025-06-23 14:46:58 INFO Multimodal (single component) AOCC mean: 0.0932
2025-06-23 14:46:58 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:46:58 INFO AOCC mean: 0.0890
2025-06-23 14:46:58 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:46:58 ERROR Can not run the algorithm
2025-06-23 14:46:58 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1743
2025-06-23 14:46:58 INFO FeHistory: [-701.32526957]
2025-06-23 14:46:58 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:46:58 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 306.13102574980957, archive_size: int = 689.6607424106114, sigma_decay: float = 0.989954352717201, initial_sigma_multiplier: float = 0.40012502471899825):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:46:58 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:46:58 ERROR Can not run the algorithm
2025-06-23 14:46:59 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0895
2025-06-23 14:46:59 INFO FeHistory: [-221.54320081]
2025-06-23 14:46:59 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:46:59 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:46:59 ERROR Can not run the algorithm
2025-06-23 14:46:59 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:46:59 INFO FeHistory: [221.41159635]
2025-06-23 14:46:59 INFO Expected Optimum FE: -100
2025-06-23 14:46:59 INFO Unimodal AOCC mean: 0.1743
2025-06-23 14:46:59 INFO Multimodal (single component) AOCC mean: 0.0895
2025-06-23 14:46:59 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:46:59 INFO AOCC mean: 0.0879
2025-06-23 14:46:59 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:46:59 ERROR Can not run the algorithm
2025-06-23 14:46:59 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1736
2025-06-23 14:46:59 INFO FeHistory: [-701.2976855]
2025-06-23 14:46:59 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:46:59 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 875.5572325126533, archive_size: int = 304.9520124707951, sigma_decay: float = 0.9814565527917656, initial_sigma_multiplier: float = 0.25407188204869463):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:46:59 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:46:59 ERROR Can not run the algorithm
2025-06-23 14:46:59 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0929
2025-06-23 14:46:59 INFO FeHistory: [-222.49645812]
2025-06-23 14:46:59 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:46:59 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:46:59 ERROR Can not run the algorithm
2025-06-23 14:46:59 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:46:59 INFO FeHistory: [203.33364983]
2025-06-23 14:46:59 INFO Expected Optimum FE: -100
2025-06-23 14:46:59 INFO Unimodal AOCC mean: 0.1736
2025-06-23 14:46:59 INFO Multimodal (single component) AOCC mean: 0.0929
2025-06-23 14:46:59 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:46:59 INFO AOCC mean: 0.0888
2025-06-23 14:46:59 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:46:59 ERROR Can not run the algorithm
2025-06-23 14:46:59 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1737
2025-06-23 14:46:59 INFO FeHistory: [-701.2996905]
2025-06-23 14:46:59 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:46:59 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 527.918502409407, archive_size: int = 831.2473387670606, sigma_decay: float = 0.9254629071021369, initial_sigma_multiplier: float = 0.767653792470302):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:46:59 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:46:59 ERROR Can not run the algorithm
2025-06-23 14:47:00 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0907
2025-06-23 14:47:00 INFO FeHistory: [-221.88403276]
2025-06-23 14:47:00 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:47:00 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:47:00 ERROR Can not run the algorithm
2025-06-23 14:47:00 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:47:00 INFO FeHistory: [193.54240449]
2025-06-23 14:47:00 INFO Expected Optimum FE: -100
2025-06-23 14:47:00 INFO Unimodal AOCC mean: 0.1737
2025-06-23 14:47:00 INFO Multimodal (single component) AOCC mean: 0.0907
2025-06-23 14:47:00 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:47:00 INFO AOCC mean: 0.0881
2025-06-23 14:47:00 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 14:47:00 ERROR Can not run the algorithm
2025-06-23 14:47:00 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1735
2025-06-23 14:47:00 INFO FeHistory: [-701.29322376]
2025-06-23 14:47:00 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 14:47:00 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 130.9438467894551, archive_size: int = 692.5728517449193, sigma_decay: float = 0.9309375514490694, initial_sigma_multiplier: float = 0.5693717018074013):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 14:47:00 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 14:47:00 ERROR Can not run the algorithm
2025-06-23 14:47:00 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0900
2025-06-23 14:47:00 INFO FeHistory: [-221.69405278]
2025-06-23 14:47:00 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 14:47:00 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 14:47:00 ERROR Can not run the algorithm
2025-06-23 14:47:00 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 14:47:00 INFO FeHistory: [197.85692872]
2025-06-23 14:47:00 INFO Expected Optimum FE: -100
2025-06-23 14:47:00 INFO Unimodal AOCC mean: 0.1735
2025-06-23 14:47:00 INFO Multimodal (single component) AOCC mean: 0.0900
2025-06-23 14:47:00 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 14:47:00 INFO AOCC mean: 0.0878
2025-06-23 14:47:00 INFO Generation 1, best so far: 0.13788948561334638
2025-06-23 14:47:00 INFO Population length is: 12
2025-06-23 14:47:00 INFO --- Performing Long-Term Reflection at Generation 2 ---
2025-06-23 14:47:00 INFO Reflection Prompt: ### Problem Description
Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    
### List heuristics
Below is a list of design heuristics ranked from best to worst based on their average AOCC score, where higher is better, across a subset of the GNBG benchmark. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score: 9.5866e-02 |             AOCC Score on Unimodal instances: 1.7964e-01 |             AOCC Score on Multimodal instances with a single component: 1.0796e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDEwithArchiveAndGaussianPerturbation
# Description: Combines Differential Evolution, an archive for diversity, and Gaussian perturbation to escape local optima.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndGaussianPerturbation
# Description: Combines Differential Evolution, an archive for diversity, and Gaussian perturbation to escape local optima.
class AdaptiveDEwithArchiveAndGaussianPerturbation:
    """
    Combines Differential Evolution (DE), an archive to maintain diversity, and Gaussian perturbation to escape local optima in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
        self.archive = self._update_archive(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                
                # Gaussian perturbation
                mutant += np.random.normal(0, self.mutation_scale, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
            self.archive = self._update_archive(np.vstack((self.population,self.population)), np.concatenate((self.fitness_values, self.fitness_values)))
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

```

### Rank 2 (Overall AOCC Score: 9.5192e-02 |             AOCC Score on Unimodal instances: 1.7790e-01 |             AOCC Score on Multimodal instances with a single component: 1.0768e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A differential evolution algorithm enhanced with adaptive mutation and clustering for efficient multimodal optimization.
# Code:
```python
import numpy as np
from scipy.stats import multivariate_normal

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: A differential evolution algorithm enhanced with adaptive mutation and clustering for efficient multimodal optimization.
# Code:

class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.archive = []
        self.archive_size = 200
        self.cluster_threshold = 0.1 # Threshold for determining clusters


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)


        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_distinct(population, i)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_three_distinct(self, population, exclude_index):
        indices = np.random.choice(len(population), 3, replace=False)
        while exclude_index in indices:
            indices = np.random.choice(len(population), 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
        u = np.copy(x)
        mask = np.random.rand(self.dim) < self.CR
        u[mask] = v[mask]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        indices = np.argsort(combined_fit)
        return combined_pop[indices[:self.population_size]], combined_fit[indices[:self.population_size]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptive strategy: Reduce F and CR if solutions are clustered.
        distances = np.linalg.norm(population[:, np.newaxis, :] - population[np.newaxis, :, :], axis=2)
        avg_distance = np.mean(distances)
        if avg_distance < self.cluster_threshold:
            self.F *= 0.95
            self.CR *= 0.95
        else:
            self.F = min(self.F * 1.05, 1.0) #Avoid overly large F
            self.CR = min(self.CR * 1.05, 1.0) #Avoid overly large CR


```

### Rank 3 (Overall AOCC Score: 9.4531e-02 |             AOCC Score on Unimodal instances: 1.7551e-01 |             AOCC Score on Multimodal instances with a single component: 1.0809e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveLevyFlightArchiveEA
# Description: An evolutionary algorithm combining adaptive Levy flights with an archive for efficient multimodal optimization.
# Code:
```python
import numpy as np
from scipy.stats import levy

# Name: AdaptiveLevyFlightArchiveEA
# Description: An evolutionary algorithm combining adaptive Levy flights with an archive for efficient multimodal optimization.
# Code:

class AdaptiveLevyFlightArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.beta = 1.5  # Levy flight parameter
        self.step_size = 0.1 * (self.upper_bounds - self.lower_bounds) # Adaptive step size initialization
        self.step_size_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._levy_flight_mutation(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._replacement_selection(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.step_size *= self.step_size_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _levy_flight_mutation(self, population, fitness_values):
        step = levy.rvs(self.beta, size=(self.population_size, self.dim))
        offspring = population + self.step_size * step
        return np.clip(offspring, self.lower_bounds, self.upper_bounds)

    def _replacement_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

```

### Rank 4 (Overall AOCC Score: 9.4091e-02 |             AOCC Score on Unimodal instances: 1.7578e-01 |             AOCC Score on Multimodal instances with a single component: 1.0649e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveMultimodalExplorationEA
# Description: An evolutionary algorithm employing adaptive Gaussian sampling, niching, and a diversity metric to efficiently explore multimodal landscapes.
# Code:
```python
import numpy as np
from scipy.stats import multivariate_normal

# Name: AdaptiveMultimodalExplorationEA
# Description: An evolutionary algorithm employing adaptive Gaussian sampling, niching, and a diversity metric to efficiently explore multimodal landscapes.
# Code:

class AdaptiveMultimodalExplorationEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.niche_radius = 0.5  # Initial niche radius
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.99
        self.population = None
        self.fitness_values = None
        self.niches = []


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1
        self.population = self._initialize_population()
        self.fitness_values = objective_function(self.population)
        self.eval_count += self.population_size
        self._update_best(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring()
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)
            self._update_best(offspring, offspring_fitness)
            self.population, self.fitness_values = self._selection(offspring, offspring_fitness)
            self._update_niches()
            self.sigma *= self.sigma_decay
            self.niche_radius *= self.sigma_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self):
        offspring = []
        for i in range(self.population_size):
            parent = self.population[i]
            mutation = np.random.multivariate_normal(np.zeros(self.dim), np.diag(self.sigma**2))
            child = np.clip(parent + mutation, self.lower_bounds, self.upper_bounds)
            offspring.append(child)
        return np.array(offspring)

    def _selection(self, offspring, offspring_fitness):
        combined_pop = np.vstack((self.population, offspring))
        combined_fit = np.concatenate((self.fitness_values, offspring_fitness))
        
        #Niching Selection
        selected_indices = []
        for i in range(self.population_size):
            best_index = np.argmin(combined_fit)
            selected_indices.append(best_index)
            combined_fit[best_index] = np.inf #remove selected individual
            
        next_gen = combined_pop[selected_indices]
        next_fit = combined_fit[selected_indices]
        
        return next_gen, next_fit

    def _update_best(self, solutions, fitnesses):
        for i, fitness in enumerate(fitnesses):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = solutions[i]

    def _update_niches(self):
      self.niches = []
      for i in range(len(self.population)):
        niche_found = False
        for j in range(len(self.niches)):
          if np.linalg.norm(self.population[i] - self.niches[j][0]) < self.niche_radius:
            self.niches[j].append(self.population[i])
            niche_found = True
            break
        if not niche_found:
          self.niches.append([self.population[i]])
```

### Rank 5 (Overall AOCC Score: 9.3828e-02 |             AOCC Score on Unimodal instances: 1.7575e-01 |             AOCC Score on Multimodal instances with a single component: 1.0574e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveGaussianArchiveEAwithNiching
# Description: Improves exploration and exploitation in multimodal landscapes using adaptive Gaussian sampling, an archive, and niching.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEAwithNiching
# Description: Improves exploration and exploitation in multimodal landscapes using adaptive Gaussian sampling, an archive, and niching.
# Code:
class AdaptiveGaussianArchiveEAwithNiching:
    """
    Combines adaptive Gaussian sampling with an archive and niching to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.98
        self.archive = []
        self.niche_radius = 0.25 * np.linalg.norm(self.upper_bounds - self.lower_bounds) #Adaptive niche radius


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._niche_preservation_selection(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay
            self.niche_radius *= self.sigma_decay #Adapt niche radius


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)


    def _niche_preservation_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        next_gen = []
        next_fit = []
        
        for i in range(self.population_size):
            best_individual = None
            best_fitness = float('inf')
            for j in range(len(combined_pop)):
                distance = np.linalg.norm(combined_pop[j] - (population[i] if i < len(population) else np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim))) #Distance to existing niche
                if distance < self.niche_radius and combined_fit[j] < best_fitness:
                    best_individual = combined_pop[j]
                    best_fitness = combined_fit[j]
            if best_individual is not None:
                next_gen.append(best_individual)
                next_fit.append(best_fitness)

        next_gen = np.array(next_gen)
        next_fit = np.array(next_fit)
        
        return next_gen, next_fit


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

```

### Rank 6 (Overall AOCC Score: 9.2467e-02 |             AOCC Score on Unimodal instances: 1.7506e-01 |             AOCC Score on Multimodal instances with a single component: 1.0234e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveGaussianArchiveEAwithLocalSearch
# Description: Combines adaptive Gaussian mutation, an archive for diversity, and local search to escape local optima in multimodal landscapes.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEAwithLocalSearch
# Description: Combines adaptive Gaussian mutation, an archive for diversity, and local search to escape local optima in multimodal landscapes.
class AdaptiveGaussianArchiveEAwithLocalSearch:
    """Combines adaptive Gaussian mutation, an archive for diversity, and local search to escape local optima."""
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution = None
        self.best_fitness = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.sigma = 0.5  # Initial standard deviation for Gaussian mutation
        self.sigma_decay = 0.99 # Decay rate for sigma

        self.local_search_iterations = 5

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(population)
        self.eval_count += self.population_size

        for i in range(self.population_size):
            self._update_best(population[i], fitness[i])
            self._add_to_archive(population[i], fitness[i])

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            for i in range(self.population_size):
                self._update_best(offspring[i], offspring_fitness[i])
                self._add_to_archive(offspring[i], offspring_fitness[i])

            population = self._select_population(population, fitness, offspring, offspring_fitness)
            fitness = np.concatenate((fitness,offspring_fitness))
            fitness = fitness[fitness.argsort()][:self.population_size]
            population = population[fitness.argsort()][:self.population_size]

            # Adaptive Local Search on the best solution
            self.best_solution = self._local_search(self.best_solution, objective_function)
            
            self.sigma *= self.sigma_decay


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness
        }
        return self.best_solution, self.best_fitness, optimization_info

    def _generate_offspring(self, population):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            parent = population[i]
            mutation = np.random.normal(0, self.sigma, self.dim)
            offspring[i] = np.clip(parent + mutation, self.lower_bounds, self.upper_bounds)
        return offspring

    def _update_best(self, solution, fitness):
        if fitness < self.best_fitness:
            self.best_fitness = fitness
            self.best_solution = solution

    def _add_to_archive(self, solution, fitness):
        if len(self.archive) < self.archive_size:
            self.archive.append((solution, fitness))
        else:
            worst_solution, worst_fitness = max(self.archive, key=lambda item: item[1])
            if fitness < worst_fitness:
                self.archive.remove((worst_solution, worst_fitness))
                self.archive.append((solution, fitness))

    def _select_population(self, population, fitness, offspring, offspring_fitness):
        combined_population = np.concatenate((population, offspring))
        combined_fitness = np.concatenate((fitness, offspring_fitness))
        sorted_indices = np.argsort(combined_fitness)
        return combined_population[sorted_indices[:self.population_size]]

    def _local_search(self, solution, objective_function):
        current_solution = solution.copy()
        current_fitness = objective_function(current_solution.reshape(1, -1))[0]
        for _ in range(self.local_search_iterations):
            neighbor = current_solution + np.random.normal(0, 0.01, self.dim)
            neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)
            neighbor_fitness = objective_function(neighbor.reshape(1,-1))[0]
            if neighbor_fitness < current_fitness:
                current_solution = neighbor
                current_fitness = neighbor_fitness
        return current_solution
```

### Rank 7 (Overall AOCC Score: 9.1278e-02 |             AOCC Score on Unimodal instances: 1.7483e-01 |             AOCC Score on Multimodal instances with a single component: 9.9005e-02 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveGaussianArchiveEA_Enhanced
# Description: Combines adaptive Gaussian mutation with an archive for multimodal optimization.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEA_Enhanced
# Description: Combines adaptive Gaussian mutation with an archive for multimodal optimization.
# Code:

class AdaptiveGaussianArchiveEA_Enhanced:
    """
    Combines adaptive Gaussian mutation with an archive to maintain diversity and escape local optima in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = budget
        self.dim = dim
        self.lower_bounds = np.array(lower_bounds)
        self.upper_bounds = np.array(upper_bounds)
        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  # Increased archive size for better diversity
        self.archive = []
        self.sigma = 20.0  # Initial standard deviation for Gaussian mutation
        self.sigma_decay = 0.98 # Adjust decay rate for faster convergence near optima.


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        population = np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))
        fitness = objective_function(population)
        self.eval_count += self.population_size

        self.best_solution_overall, self.best_fitness_overall = self._find_best(population, fitness)
        self.archive = self._update_archive(self.archive, population, fitness, self.archive_size)


        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, self.sigma)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population = np.concatenate((population, offspring))
            fitness = np.concatenate((fitness, offspring_fitness))
            
            self.archive = self._update_archive(self.archive, population, fitness, self.archive_size)
            population, fitness = self._selection(population, fitness)
            
            self.best_solution_overall, self.best_fitness_overall = self._find_best(population, fitness)
            self.sigma *= self.sigma_decay # Adaptive sigma decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _generate_offspring(self, population, sigma):
        offspring = np.copy(population)
        for i in range(self.population_size):
            offspring[i] += np.random.normal(0, sigma, self.dim)
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)
        return offspring
    
    def _update_archive(self, archive, population, fitness, max_size):
        combined = np.column_stack((population, fitness))
        combined = combined[combined[:, -1].argsort()] # Sort by fitness
        
        updated_archive = []
        for sol, fit in combined:
            if len(updated_archive) < max_size:
                updated_archive.append((sol,fit))
            else:
                break
        return updated_archive
    

    def _selection(self, population, fitness):
        combined = np.column_stack((population, fitness))
        sorted_pop = combined[combined[:, -1].argsort()]
        return sorted_pop[:, :-1][:self.population_size], sorted_pop[:, -1][:self.population_size]


    def _find_best(self, population, fitness):
        best_idx = np.argmin(fitness)
        best_solution = population[best_idx]
        best_fitness = fitness[best_idx]
        return best_solution, best_fitness

```

### Rank 8 (Overall AOCC Score: 8.9041e-02 |             AOCC Score on Unimodal instances: 1.7392e-01 |             AOCC Score on Multimodal instances with a single component: 9.3201e-02 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveGaussianArchiveEA
# Description: 
# Code:
```python
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float], population_size: int = 875.5572325126533, archive_size: int = 883.9067822500798, sigma_decay: float = 0.9935777238244035, initial_sigma_multiplier: float = 0.40012502471899825):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size:int = 5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size: int = 5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 9 (Overall AOCC Score: 0.0000e+00 |             AOCC Score on Unimodal instances: 0.0000e+00 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: UnnamedAlgorithm
# Description: Combines adaptive Gaussian sampling, an archive, and improved initialization for multimodal optimization.
# Code:
```python
import numpy as np

# Name: AdaptiveGaussianArchiveEAwithImprovedInitialization
# Description: Combines adaptive Gaussian sampling, an archive, and improved initialization for multimodal optimization.
# Code:
```

### Rank 10 (Overall AOCC Score: 0.0000e+00 |             AOCC Score on Unimodal instances: 0.0000e+00 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: UnnamedAlgorithm
# Description: Combines adaptive Gaussian sampling, an archive, and niching for robust multimodal optimization.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEAwithNiching
# Description: Combines adaptive Gaussian sampling, an archive, and niching for robust multimodal optimization.
# Code:
```

### Rank 11 (Overall AOCC Score: 0.0000e+00 |             AOCC Score on Unimodal instances: 0.0000e+00 |             AOCC Score on Multimodal instances with a single component: 0.0000e+00 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: UnnamedAlgorithm
# Description: Combines Differential Evolution with an archive and adaptive mutation for robust multimodal optimization.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveDifferentialEvolutionWithArchive
# Description: Combines Differential Evolution with an archive and adaptive mutation for robust multimodal optimization.
# Code:
```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-23 14:47:06 INFO Full response text: **Analysis:**

Comparing (best) `AdaptiveDEwithArchiveAndGaussianPerturbation` vs (worst) `UnnamedAlgorithm` (AdaptiveGaussianArchiveEAwithImprovedInitialization), we see that the best utilizes Differential Evolution (DE), a powerful optimization technique, combined with an archive and Gaussian perturbation to handle multimodal landscapes.  The worst is only described and lacks implementation. `AdaptiveDEwithArchiveAndGaussianPerturbation` shows better performance because of its explicit use of DE and a well-defined mechanism to escape local optima.

(second best) `AdaptiveDifferentialEvolutionWithClustering` vs (second worst) `UnnamedAlgorithm` (AdaptiveGaussianArchiveEAwithNiching).  Again, the second best uses DE with clustering to handle multiple optima, while the second worst lacks an implementation.  Clustering helps to focus the search around promising regions, increasing efficiency.

Comparing (1st) `AdaptiveDEwithArchiveAndGaussianPerturbation` vs (2nd) `AdaptiveDifferentialEvolutionWithClustering`, we see that Gaussian perturbation provides an additional mechanism to escape local optima compared to the clustering-based approach in the second-best algorithm.  The Gaussian noise injects randomness into the search that can help the algorithm navigate rugged search spaces.

(3rd) `AdaptiveLevyFlightArchiveEA` vs (4th) `AdaptiveMultimodalExplorationEA`. Both use archives.  `AdaptiveLevyFlightArchiveEA` uses Levy flights, known for their long jumps, providing exploration capabilities, which `AdaptiveMultimodalExplorationEA` lacks. However, the adaptive Gaussian sampling in the latter still offers decent exploration.

Comparing (second worst) `UnnamedAlgorithm` (AdaptiveGaussianArchiveEAwithNiching) vs (worst) `UnnamedAlgorithm` (AdaptiveDifferentialEvolutionWithArchive), both are incomplete. However, the niching concept in the second-worst suggests a potential to handle multiple optima, which might lead to better results if implemented fully.

Overall: The top-performing algorithms consistently incorporate mechanisms for escaping local optima (Gaussian perturbation, Levy flights, clustering) and maintain diversity (archives).  The poorly performing algorithms are merely conceptual.

**Experience:**

Effective heuristics require robust exploration and exploitation.  Combining well-established optimization methods (DE, Levy flights) with diversity-preserving strategies (archives) and escape mechanisms (perturbation, clustering) is crucial for handling high-dimensional, multimodal problems.  Complete implementations are essential for evaluation.

2025-06-23 14:47:07 INFO Full response text: * **Keywords:**  Heuristic design, multimodal optimization, high-dimensionality, exploration-exploitation, diversity preservation.

* **Advice:** Focus on rigorous empirical evaluation using complete implementations.  Prioritize specific escape mechanisms (e.g.,  detailed perturbation strategies) and rigorously define diversity metrics beyond simple population size.  Analyze the interplay between exploration, exploitation, and diversity preservation quantitatively.

* **Avoid:** Vague claims about "best results" or "crucial" components without quantitative support.  Avoid generic statements about adaptive parameter tuning and population diversity without specifying the mechanisms used.

* **Explanation:** The current reflection lacks concrete details and quantitative measures.  The proposed improvements emphasize measurable outcomes and specific design choices, leading to more effective heuristic development and a clearer path toward better performance in high-dimensional multimodal optimization.

2025-06-23 14:47:07 INFO Generating offspring via Crossover...
2025-06-23 15:07:37 INFO Crossover Prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

### Better code
AdaptiveGaussianArchiveEAwithNiching
import numpy as np
import random

# Name: AdaptiveGaussianArchiveEAwithNiching
# Description: Improves exploration and exploitation in multimodal landscapes using adaptive Gaussian sampling, an archive, and niching.
# Code:
class AdaptiveGaussianArchiveEAwithNiching:
    """
    Combines adaptive Gaussian sampling with an archive and niching to enhance exploration and exploitation in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = 0.98
        self.archive = []
        self.niche_radius = 0.25 * np.linalg.norm(self.upper_bounds - self.lower_bounds) #Adaptive niche radius


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._niche_preservation_selection(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay
            self.niche_radius *= self.sigma_decay #Adapt niche radius


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)


    def _niche_preservation_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        
        next_gen = []
        next_fit = []
        
        for i in range(self.population_size):
            best_individual = None
            best_fitness = float('inf')
            for j in range(len(combined_pop)):
                distance = np.linalg.norm(combined_pop[j] - (population[i] if i < len(population) else np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim))) #Distance to existing niche
                if distance < self.niche_radius and combined_fit[j] < best_fitness:
                    best_individual = combined_pop[j]
                    best_fitness = combined_fit[j]
            if best_individual is not None:
                next_gen.append(best_individual)
                next_fit.append(best_fitness)

        next_gen = np.array(next_gen)
        next_fit = np.array(next_fit)
        
        return next_gen, next_fit


    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])


### Worse code
AdaptiveDEwithArchiveAndGaussianPerturbation
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndGaussianPerturbation
# Description: Combines Differential Evolution, an archive for diversity, and Gaussian perturbation to escape local optima.
class AdaptiveDEwithArchiveAndGaussianPerturbation:
    """
    Combines Differential Evolution (DE), an archive to maintain diversity, and Gaussian perturbation to escape local optima in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.mutation_scale = 0.8
        self.mutation_scale_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
        self.archive = self._update_archive(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])
                
                # Gaussian perturbation
                mutant += np.random.normal(0, self.mutation_scale, self.dim)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
            self.archive = self._update_archive(np.vstack((self.population,self.population)), np.concatenate((self.fitness_values, self.fitness_values)))
            self.mutation_scale *= self.mutation_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])


### Analyze & experience
- Comparing (best) `AdaptiveDEwithArchiveAndGaussianPerturbation` vs (worst) `UnnamedAlgorithm` (AdaptiveGaussianArchiveEAwithImprovedInitialization), we see that the best utilizes Differential Evolution (DE), a powerful optimization technique, combined with an archive and Gaussian perturbation to handle multimodal landscapes.  The worst is only described and lacks implementation. `AdaptiveDEwithArchiveAndGaussianPerturbation` shows better performance because of its explicit use of DE and a well-defined mechanism to escape local optima.

(second best) `AdaptiveDifferentialEvolutionWithClustering` vs (second worst) `UnnamedAlgorithm` (AdaptiveGaussianArchiveEAwithNiching).  Again, the second best uses DE with clustering to handle multiple optima, while the second worst lacks an implementation.  Clustering helps to focus the search around promising regions, increasing efficiency.

Comparing (1st) `AdaptiveDEwithArchiveAndGaussianPerturbation` vs (2nd) `AdaptiveDifferentialEvolutionWithClustering`, we see that Gaussian perturbation provides an additional mechanism to escape local optima compared to the clustering-based approach in the second-best algorithm.  The Gaussian noise injects randomness into the search that can help the algorithm navigate rugged search spaces.

(3rd) `AdaptiveLevyFlightArchiveEA` vs (4th) `AdaptiveMultimodalExplorationEA`. Both use archives.  `AdaptiveLevyFlightArchiveEA` uses Levy flights, known for their long jumps, providing exploration capabilities, which `AdaptiveMultimodalExplorationEA` lacks. However, the adaptive Gaussian sampling in the latter still offers decent exploration.

Comparing (second worst) `UnnamedAlgorithm` (AdaptiveGaussianArchiveEAwithNiching) vs (worst) `UnnamedAlgorithm` (AdaptiveDifferentialEvolutionWithArchive), both are incomplete. However, the niching concept in the second-worst suggests a potential to handle multiple optima, which might lead to better results if implemented fully.

Overall: The top-performing algorithms consistently incorporate mechanisms for escaping local optima (Gaussian perturbation, Levy flights, clustering) and maintain diversity (archives).  The poorly performing algorithms are merely conceptual.
- * **Keywords:**  Heuristic design, multimodal optimization, high-dimensionality, exploration-exploitation, diversity preservation.

* **Advice:** Focus on rigorous empirical evaluation using complete implementations.  Prioritize specific escape mechanisms (e.g.,  detailed perturbation strategies) and rigorously define diversity metrics beyond simple population size.  Analyze the interplay between exploration, exploitation, and diversity preservation quantitatively.

* **Avoid:** Vague claims about "best results" or "crucial" components without quantitative support.  Avoid generic statements about adaptive parameter tuning and population diversity without specifying the mechanisms used.

* **Explanation:** The current reflection lacks concrete details and quantitative measures.  The proposed improvements emphasize measurable outcomes and specific design choices, leading to more effective heuristic development and a clearer path toward better performance in high-dimensional multimodal optimization.


Your task is to write an improved function by COMBINING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-23 15:07:37 INFO Mutation prompt: Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    

Current heuristics:
AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200  #Increased archive size for better diversity
        self.sigma = 0.5 * (self.upper_bounds - self.lower_bounds) #Increased initial sigma
        self.sigma_decay = 0.98 # Slightly faster decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values):
        tournament_size = 5
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

Now, think outside the box write a mutated function better than current version.
You can use some hints below:
- * **Keywords:**  Heuristic design, multimodal optimization, high-dimensionality, exploration-exploitation, diversity preservation.

* **Advice:** Focus on rigorous empirical evaluation using complete implementations.  Prioritize specific escape mechanisms (e.g.,  detailed perturbation strategies) and rigorously define diversity metrics beyond simple population size.  Analyze the interplay between exploration, exploitation, and diversity preservation quantitatively.

* **Avoid:** Vague claims about "best results" or "crucial" components without quantitative support.  Avoid generic statements about adaptive parameter tuning and population diversity without specifying the mechanisms used.

* **Explanation:** The current reflection lacks concrete details and quantitative measures.  The proposed improvements emphasize measurable outcomes and specific design choices, leading to more effective heuristic development and a clearer path toward better performance in high-dimensional multimodal optimization.


Output code only and enclose your code with Python code block: ```python ... ```.
I'm going to tip $999K for a better solution!
2025-06-23 15:10:54 INFO Perform Harmony Search...
2025-06-23 15:11:05 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:05 ERROR Can not run the algorithm
2025-06-23 15:11:05 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1737
2025-06-23 15:11:05 INFO FeHistory: [-701.30119249]
2025-06-23 15:11:05 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:05 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 358.2108927115551, archive_size: int = 367.0255493829568, sigma_decay: float = 0.9709158633343213,
                 initial_sigma_multiplier: float = 0.8955723286062258):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:05 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:05 ERROR Can not run the algorithm
2025-06-23 15:11:05 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0928
2025-06-23 15:11:05 INFO FeHistory: [-222.47494215]
2025-06-23 15:11:05 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:05 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:05 ERROR Can not run the algorithm
2025-06-23 15:11:05 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:05 INFO FeHistory: [175.40264289]
2025-06-23 15:11:05 INFO Expected Optimum FE: -100
2025-06-23 15:11:05 INFO Unimodal AOCC mean: 0.1737
2025-06-23 15:11:05 INFO Multimodal (single component) AOCC mean: 0.0928
2025-06-23 15:11:05 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:05 INFO AOCC mean: 0.0888
2025-06-23 15:11:05 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:05 ERROR Can not run the algorithm
2025-06-23 15:11:06 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1733
2025-06-23 15:11:06 INFO FeHistory: [-701.2818469]
2025-06-23 15:11:06 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:06 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 458.30915581690067, archive_size: int = 797.9019182718499, sigma_decay: float = 0.9950089938720218,
                 initial_sigma_multiplier: float = 0.9535644827270598):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:06 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:06 ERROR Can not run the algorithm
2025-06-23 15:11:06 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0923
2025-06-23 15:11:06 INFO FeHistory: [-222.34303627]
2025-06-23 15:11:06 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:06 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:06 ERROR Can not run the algorithm
2025-06-23 15:11:06 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:06 INFO FeHistory: [186.90678796]
2025-06-23 15:11:06 INFO Expected Optimum FE: -100
2025-06-23 15:11:06 INFO Unimodal AOCC mean: 0.1733
2025-06-23 15:11:06 INFO Multimodal (single component) AOCC mean: 0.0923
2025-06-23 15:11:06 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:06 INFO AOCC mean: 0.0885
2025-06-23 15:11:06 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:06 ERROR Can not run the algorithm
2025-06-23 15:11:06 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1740
2025-06-23 15:11:06 INFO FeHistory: [-701.31468437]
2025-06-23 15:11:06 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:06 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 166.96882751621482, archive_size: int = 567.2985495108148, sigma_decay: float = 0.87383767981754,
                 initial_sigma_multiplier: float = 0.21500001270712066):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:06 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:06 ERROR Can not run the algorithm
2025-06-23 15:11:06 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0940
2025-06-23 15:11:06 INFO FeHistory: [-222.80746512]
2025-06-23 15:11:06 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:06 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:06 ERROR Can not run the algorithm
2025-06-23 15:11:06 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:06 INFO FeHistory: [195.78034064]
2025-06-23 15:11:06 INFO Expected Optimum FE: -100
2025-06-23 15:11:06 INFO Unimodal AOCC mean: 0.1740
2025-06-23 15:11:06 INFO Multimodal (single component) AOCC mean: 0.0940
2025-06-23 15:11:06 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:06 INFO AOCC mean: 0.0894
2025-06-23 15:11:06 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:06 ERROR Can not run the algorithm
2025-06-23 15:11:07 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1744
2025-06-23 15:11:07 INFO FeHistory: [-701.33181245]
2025-06-23 15:11:07 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:07 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 165.33856808793323, archive_size: int = 27.863997665693716, sigma_decay: float = 0.9822430358929689,
                 initial_sigma_multiplier: float = 0.7005694277293985):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:07 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:07 ERROR Can not run the algorithm
2025-06-23 15:11:07 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0915
2025-06-23 15:11:07 INFO FeHistory: [-222.10857854]
2025-06-23 15:11:07 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:07 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:07 ERROR Can not run the algorithm
2025-06-23 15:11:07 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:07 INFO FeHistory: [196.44162238]
2025-06-23 15:11:07 INFO Expected Optimum FE: -100
2025-06-23 15:11:07 INFO Unimodal AOCC mean: 0.1744
2025-06-23 15:11:07 INFO Multimodal (single component) AOCC mean: 0.0915
2025-06-23 15:11:07 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:07 INFO AOCC mean: 0.0886
2025-06-23 15:11:07 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:07 ERROR Can not run the algorithm
2025-06-23 15:11:07 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1739
2025-06-23 15:11:07 INFO FeHistory: [-701.30883872]
2025-06-23 15:11:07 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:07 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 288.8670893955986, archive_size: int = 749.8673405102672, sigma_decay: float = 0.8130013638752185,
                 initial_sigma_multiplier: float = 0.7842986224441842):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:07 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:07 ERROR Can not run the algorithm
2025-06-23 15:11:07 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0881
2025-06-23 15:11:07 INFO FeHistory: [-221.12647824]
2025-06-23 15:11:07 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:07 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:07 ERROR Can not run the algorithm
2025-06-23 15:11:07 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:07 INFO FeHistory: [225.58610291]
2025-06-23 15:11:07 INFO Expected Optimum FE: -100
2025-06-23 15:11:07 INFO Unimodal AOCC mean: 0.1739
2025-06-23 15:11:07 INFO Multimodal (single component) AOCC mean: 0.0881
2025-06-23 15:11:07 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:07 INFO AOCC mean: 0.0873
2025-06-23 15:11:07 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:07 ERROR Can not run the algorithm
2025-06-23 15:11:08 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1745
2025-06-23 15:11:08 INFO FeHistory: [-701.33240099]
2025-06-23 15:11:08 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:08 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 332.45845956331476, archive_size: int = 27.863997665693716, sigma_decay: float = 0.9950089938720218,
                 initial_sigma_multiplier: float = 0.7928151563010645):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:08 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:08 ERROR Can not run the algorithm
2025-06-23 15:11:08 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0899
2025-06-23 15:11:08 INFO FeHistory: [-221.65201219]
2025-06-23 15:11:08 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:08 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:08 ERROR Can not run the algorithm
2025-06-23 15:11:08 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:08 INFO FeHistory: [209.54254419]
2025-06-23 15:11:08 INFO Expected Optimum FE: -100
2025-06-23 15:11:08 INFO Unimodal AOCC mean: 0.1745
2025-06-23 15:11:08 INFO Multimodal (single component) AOCC mean: 0.0899
2025-06-23 15:11:08 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:08 INFO AOCC mean: 0.0881
2025-06-23 15:11:08 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:08 ERROR Can not run the algorithm
2025-06-23 15:11:08 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1735
2025-06-23 15:11:08 INFO FeHistory: [-701.29083083]
2025-06-23 15:11:08 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:08 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 458.30915581690067, archive_size: int = 142.40347829090143, sigma_decay: float = 0.9908097132710002,
                 initial_sigma_multiplier: float = 0.26785539645249):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:08 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:08 ERROR Can not run the algorithm
2025-06-23 15:11:08 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0929
2025-06-23 15:11:08 INFO FeHistory: [-222.50952622]
2025-06-23 15:11:08 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:08 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:08 ERROR Can not run the algorithm
2025-06-23 15:11:09 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:09 INFO FeHistory: [172.58508519]
2025-06-23 15:11:09 INFO Expected Optimum FE: -100
2025-06-23 15:11:09 INFO Unimodal AOCC mean: 0.1735
2025-06-23 15:11:09 INFO Multimodal (single component) AOCC mean: 0.0929
2025-06-23 15:11:09 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:09 INFO AOCC mean: 0.0888
2025-06-23 15:11:09 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:09 ERROR Can not run the algorithm
2025-06-23 15:11:09 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1743
2025-06-23 15:11:09 INFO FeHistory: [-701.32629221]
2025-06-23 15:11:09 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:09 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 367.30521382549574, archive_size: int = 763.8609776266115, sigma_decay: float = 0.897254140476471,
                 initial_sigma_multiplier: float = 0.33139418006852284):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:09 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:09 ERROR Can not run the algorithm
2025-06-23 15:11:09 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0989
2025-06-23 15:11:09 INFO FeHistory: [-224.02213014]
2025-06-23 15:11:09 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:09 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:09 ERROR Can not run the algorithm
2025-06-23 15:11:09 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:09 INFO FeHistory: [189.08511798]
2025-06-23 15:11:09 INFO Expected Optimum FE: -100
2025-06-23 15:11:09 INFO Unimodal AOCC mean: 0.1743
2025-06-23 15:11:09 INFO Multimodal (single component) AOCC mean: 0.0989
2025-06-23 15:11:09 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:09 INFO AOCC mean: 0.0911
2025-06-23 15:11:09 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:09 ERROR Can not run the algorithm
2025-06-23 15:11:09 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1737
2025-06-23 15:11:09 INFO FeHistory: [-701.30155827]
2025-06-23 15:11:09 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:09 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 909.682619624748, archive_size: int = 902.9108219383567, sigma_decay: float = 0.897254140476471,
                 initial_sigma_multiplier: float = 0.33139418006852284):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:09 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:09 ERROR Can not run the algorithm
2025-06-23 15:11:09 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0893
2025-06-23 15:11:09 INFO FeHistory: [-221.49477774]
2025-06-23 15:11:09 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:09 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:09 ERROR Can not run the algorithm
2025-06-23 15:11:10 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:10 INFO FeHistory: [199.88949075]
2025-06-23 15:11:10 INFO Expected Optimum FE: -100
2025-06-23 15:11:10 INFO Unimodal AOCC mean: 0.1737
2025-06-23 15:11:10 INFO Multimodal (single component) AOCC mean: 0.0893
2025-06-23 15:11:10 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:10 INFO AOCC mean: 0.0877
2025-06-23 15:11:10 INFO --- GNBG Problem Parameters for f2 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -703.132815
  Lambda (Curvature): [0.05]
  Mu (Asymmetry/Depth): [0 0]
----------------------------------------
2025-06-23 15:11:10 ERROR Can not run the algorithm
2025-06-23 15:11:10 INFO Run function 2 complete. FEHistory len: 1, AOCC: 0.1737
2025-06-23 15:11:10 INFO FeHistory: [-701.30053574]
2025-06-23 15:11:10 INFO Expected Optimum FE: -703.1328146165181
2025-06-23 15:11:10 INFO Good algorithm:
Algorithm Name: AdaptiveGaussianArchiveEA
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 165.33856808793323, archive_size: int = 220.80837904944948, sigma_decay: float = 0.8016482858237894,
                 initial_sigma_multiplier: float = 0.8955723286062258):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
2025-06-23 15:11:10 INFO --- GNBG Problem Parameters for f15 ---
  Dimension: 30, MaxEvals: 500000
  Search Bounds: [-100, 100]
  Number of Components: 1
  Known Optimum Value: -234.280428
  Lambda (Curvature): [0.1]
  Mu (Asymmetry/Depth): [1 1]
----------------------------------------
2025-06-23 15:11:10 ERROR Can not run the algorithm
2025-06-23 15:11:10 INFO Run function 15 complete. FEHistory len: 1, AOCC: 0.0899
2025-06-23 15:11:10 INFO FeHistory: [-221.66663168]
2025-06-23 15:11:10 INFO Expected Optimum FE: -234.28042789139022
2025-06-23 15:11:10 INFO --- GNBG Problem Parameters for f24 ---
  Dimension: 30, MaxEvals: 1000000
  Search Bounds: [-100, 100]
  Number of Components: 5
  Known Optimum Value: -100.000000
  Lambda (Curvature): [0.25 0.25 0.25 0.25 0.25]
  Mu (Asymmetry/Depth): [0.44142637 0.27898903 0.25803028 0.21978833 0.39183826 0.42051979
 0.35740109 0.43165341 0.47744239 0.47234476]
----------------------------------------
2025-06-23 15:11:10 ERROR Can not run the algorithm
2025-06-23 15:11:10 INFO Run function 24 complete. FEHistory len: 1, AOCC: 0.0000
2025-06-23 15:11:10 INFO FeHistory: [165.07493393]
2025-06-23 15:11:10 INFO Expected Optimum FE: -100
2025-06-23 15:11:10 INFO Unimodal AOCC mean: 0.1737
2025-06-23 15:11:10 INFO Multimodal (single component) AOCC mean: 0.0899
2025-06-23 15:11:10 INFO Multimodal (multiple components) AOCC mean: 0.0000
2025-06-23 15:11:10 INFO AOCC mean: 0.0879
2025-06-23 15:11:10 INFO Generation 2, best so far: 0.13788948561334638
2025-06-23 15:11:10 INFO Population length is: 12
2025-06-23 15:11:10 INFO --- Performing Long-Term Reflection at Generation 3 ---
2025-06-23 15:11:10 INFO Reflection Prompt: ### Problem Description
Your objective is to design a novel and sophisticated population function in Python, solving 24 GNBG benchmark functions, particularly:

Unimodal Group (f1-f6): Problems with a single optimum, but often with ill-conditioned (narrow, rotated) landscapes that test an algorithm's exploitation and convergence efficiency.

Multimodal Single-Component Group (f7-f15): Problems with a single main basin of attraction that is filled with numerous, often deep and rugged, local optima. This tests an algorithm's ability to escape local traps.

Multimodal Multi-Component Group (f16-f24): The most difficult problems, featuring multiple, separate, and often deceptive basins of attraction. This rigorously tests an algorithm's global exploration capability.

This function is for an Evolutionary Algorithm that will solve the GNBG benchmark. The key challenge is creating a good population for a high-dimensional search space (30D) with wide bounds (typically [-100, 100]).

    
### List heuristics
Below is a list of design heuristics ranked from best to worst based on their average AOCC score, where higher is better, across a subset of the GNBG benchmark. To enable a detailed analysis of their specializations, the performance breakdown on each of the three GNBG function groups is also provided.
### Rank 1 (Overall AOCC Score: 9.6193e-02 |             AOCC Score on Unimodal instances: 1.7840e-01 |             AOCC Score on Multimodal instances with a single component: 1.1017e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDEwithArchiveAndGaussianPerturbation
# Description: Differential Evolution with archive, Gaussian perturbation, and adaptive mutation for multimodal optimization.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveDEwithArchiveAndGaussianPerturbation
# Description: Differential Evolution with archive, Gaussian perturbation, and adaptive mutation for multimodal optimization.
# Code:
class AdaptiveDEwithArchiveAndGaussianPerturbation:
    """
    Combines Differential Evolution (DE) with an archive, Gaussian perturbation, and adaptive mutation 
    to efficiently explore and exploit multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.sigma = 0.1 #Initial Gaussian perturbation
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring = self._gaussian_perturbation(offspring) #Add Gaussian noise
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_distinct(population, i)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_three_distinct(self, population, exclude_index):
        indices = np.random.choice(len(population), 3, replace=False)
        while exclude_index in indices:
            indices = np.random.choice(len(population), 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
        u = np.copy(x)
        mask = np.random.rand(self.dim) < self.CR
        u[mask] = v[mask]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        indices = np.argsort(combined_fit)
        return combined_pop[indices[:self.population_size]], combined_fit[indices[:self.population_size]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        #Simple adaptive strategy: Reduce F and CR if solutions are clustered.  Increase sigma for exploration
        distances = np.linalg.norm(population[:, np.newaxis, :] - population[np.newaxis, :, :], axis=2)
        avg_distance = np.mean(distances)
        if avg_distance < 0.1: #Threshold for clustering
            self.F *= 0.95
            self.CR *= 0.95
            self.sigma *= 1.1 #Increase sigma to enhance exploration
        else:
            self.sigma *= 0.9 #Reduce sigma for exploitation
            self.F = min(self.F * 1.05, 1.0)
            self.CR = min(self.CR * 1.05, 1.0)

    def _gaussian_perturbation(self, offspring):
      noise = np.random.normal(0, self.sigma, size=offspring.shape)
      perturbed_offspring = np.clip(offspring + noise, self.lower_bounds, self.upper_bounds)
      return perturbed_offspring

```

### Rank 2 (Overall AOCC Score: 9.6128e-02 |             AOCC Score on Unimodal instances: 1.8035e-01 |             AOCC Score on Multimodal instances with a single component: 1.0803e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDEwithLevyFlightsAndArchive
# Description: Differential Evolution with Levy flights and an archive for robust multimodal optimization.
# Code:
```python
import numpy as np
from scipy.stats import levy, multivariate_normal

# Name: AdaptiveDEwithLevyFlightsAndArchive
# Description: Differential Evolution with Levy flights and an archive for robust multimodal optimization.
# Code:

class AdaptiveDEwithLevyFlightsAndArchive:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.beta = 1.5 # Levy flight parameter
        self.levy_probability = 0.1 # Probability of applying levy flight mutation


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(offspring, offspring_fitness)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            if np.random.rand() < self.levy_probability:
                offspring[i] = self._levy_flight_mutation(population[i])
            else:
                a, b, c = self._select_three_distinct(population, i)
                mutant = a + self.F * (b - c)
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
                trial = self._crossover(population[i], mutant)
                offspring[i] = trial
        return offspring

    def _levy_flight_mutation(self, solution):
        step = levy.rvs(self.beta, size=self.dim)
        mutant = solution + 0.1*(self.upper_bounds - self.lower_bounds) * step #Adaptive step size
        return np.clip(mutant, self.lower_bounds, self.upper_bounds)


    def _select_three_distinct(self, population, exclude_index):
        indices = np.random.choice(len(population), 3, replace=False)
        while exclude_index in indices:
            indices = np.random.choice(len(population), 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
        u = np.copy(x)
        mask = np.random.rand(self.dim) < self.CR
        u[mask] = v[mask]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        return combined_pop[sorted_indices[:self.population_size]], combined_fit[sorted_indices[:self.population_size]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 3 (Overall AOCC Score: 9.6059e-02 |             AOCC Score on Unimodal instances: 1.7936e-01 |             AOCC Score on Multimodal instances with a single component: 1.0882e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDEwithArchiveAndClustering
# Description: Differential Evolution with archive and clustering to escape local optima in multimodal landscapes.
# Code:
```python
import numpy as np
from scipy.spatial.distance import cdist

# Name: AdaptiveDEwithArchiveAndClustering
# Description: Differential Evolution with archive and clustering to escape local optima in multimodal landscapes.
# Code:
class AdaptiveDEwithArchiveAndClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.F = 0.8
        self.CR = 0.9
        self.archive = []
        self.cluster_threshold = 0.1 # Distance threshold for clustering


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))

            self._update_best(offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_distinct(population, i)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_three_distinct(self, population, exclude_index):
        indices = np.random.choice(len(population), 3, replace=False)
        while exclude_index in indices:
            indices = np.random.choice(len(population), 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
        u = np.copy(x)
        mask = np.random.rand(self.dim) < self.CR
        u[mask] = v[mask]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        indices = np.argsort(combined_fit)
        return combined_pop[indices[:self.population_size]], combined_fit[indices[:self.population_size]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _adapt_parameters(self, population, fitness_values):
        distances = cdist(population, population)
        np.fill_diagonal(distances, np.inf)  # Ignore self-distances
        min_distance = np.min(distances)

        if min_distance < self.cluster_threshold:
            self.F *= 0.95
            self.CR *= 0.95
        else:
            self.F = min(self.F * 1.05, 1.0)
            self.CR = min(self.CR * 1.05, 1.0)

```

### Rank 4 (Overall AOCC Score: 9.5856e-02 |             AOCC Score on Unimodal instances: 1.7963e-01 |             AOCC Score on Multimodal instances with a single component: 1.0794e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDEwithLevyFlightAndArchive
# Description: Combines Differential Evolution, Levy flights, and an archive for robust multimodal optimization.
# Code:
```python
import numpy as np
import random

# Name: AdaptiveDEwithLevyFlightAndArchive
# Description: Combines Differential Evolution, Levy flights, and an archive for robust multimodal optimization.
class AdaptiveDEwithLevyFlightAndArchive:
    """
    Combines Differential Evolution (DE), Levy flights for exploration, and an archive to maintain diversity in multimodal landscapes.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.levy_scale = 0.1 # Initial Levy flight scale
        self.levy_scale_decay = 0.99 # Decay for Levy scale


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
        self.archive = self._update_archive(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])

                # Levy flight perturbation
                levy_step = self._levy_flight(self.dim) * self.levy_scale
                mutant += levy_step
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
            self.archive = self._update_archive(np.vstack((self.population, self.population)), np.concatenate((self.fitness_values, self.fitness_values)))
            self.levy_scale *= self.levy_scale_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = random.sample(range(self.population_size), 3)
        while a == index or b == index or c == index or a == b or a == c or b == c:
            a, b, c = random.sample(range(self.population_size), 3)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

    def _levy_flight(self, dim):
        # Levy flight using Mantegna's algorithm
        beta = 3/2
        u = np.random.normal(0, 1, dim)
        v = np.random.normal(0, 1, dim)
        step = u / (np.abs(v)**(1/beta))
        return step

```

### Rank 5 (Overall AOCC Score: 9.5415e-02 |             AOCC Score on Unimodal instances: 1.7659e-01 |             AOCC Score on Multimodal instances with a single component: 1.0966e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDELevyFlightArchiveEA
# Description: Combines Differential Evolution, Levy flights, and an archive for robust multimodal optimization.
# Code:
```python
import numpy as np
from scipy.stats import levy, norm

# Name: AdaptiveDELevyFlightArchiveEA
# Description: Combines Differential Evolution, Levy flights, and an archive for robust multimodal optimization.
# Code:

class AdaptiveDELevyFlightArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.5  # Differential Evolution scaling factor
        self.CR = 0.9  # Differential Evolution crossover rate
        self.beta = 1.5  # Levy flight parameter
        self.step_size = 0.1 * (self.upper_bounds - self.lower_bounds)
        self.step_size_decay = 0.99
        self.sigma = 0.1 # Gaussian perturbation

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._differential_evolution_mutation(population, fitness_values)
            offspring = self._levy_flight_perturbation(offspring) #Add Levy Flight perturbation
            offspring = self._gaussian_perturbation(offspring) #Add Gaussian perturbation for local optima escape

            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._replacement_selection(
                population, fitness_values, offspring, offspring_fitness
            )
            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )
            self._update_best(offspring, offspring_fitness)
            self.step_size *= self.step_size_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _differential_evolution_mutation(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = np.random.choice(np.arange(self.population_size), size=3, replace=False)
            while a == i or b == i or c == i:
              a, b, c = np.random.choice(np.arange(self.population_size), size=3, replace=False)
            mutant = population[a] + self.F * (population[b] - population[c])
            offspring[i] = np.clip(mutant, self.lower_bounds, self.upper_bounds)
        return offspring

    def _levy_flight_perturbation(self, population):
        step = levy.rvs(self.beta, size=(self.population_size, self.dim))
        perturbed_pop = population + self.step_size * step
        return np.clip(perturbed_pop, self.lower_bounds, self.upper_bounds)

    def _gaussian_perturbation(self, population):
        noise = norm.rvs(loc=0, scale=self.sigma, size=population.shape)
        perturbed_pop = population + noise
        return np.clip(perturbed_pop, self.lower_bounds, self.upper_bounds)

    def _replacement_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 6 (Overall AOCC Score: 9.5113e-02 |             AOCC Score on Unimodal instances: 1.7670e-01 |             AOCC Score on Multimodal instances with a single component: 1.0864e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDEwithLevyFlightAndArchive
# Description: Differential Evolution combined with Levy flights and an archive for escaping local optima in multimodal landscapes.
# Code:
```python
import numpy as np
from scipy.stats import levy

# Name: AdaptiveDEwithLevyFlightAndArchive
# Description: Differential Evolution combined with Levy flights and an archive for escaping local optima in multimodal landscapes.
# Code:
class AdaptiveDEwithLevyFlightAndArchive:
    """
    Combines Differential Evolution (DE) with Levy flights for exploration and an archive to maintain diversity in multimodal optimization.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.beta = 1.5 # Levy flight parameter
        self.step_size = 0.1 * (self.upper_bounds - self.lower_bounds) # Adaptive step size initialization
        self.step_size_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        self.population = self._initialize_population()
        self.fitness_values = self._evaluate_population(objective_function)
        self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
        self.archive = self._update_archive(self.population, self.fitness_values)

        while self.eval_count < self.budget:
            new_population = []
            new_fitness_values = []
            for i in range(self.population_size):
                a, b, c = self._select_different(i)
                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])

                # Levy flight perturbation
                levy_step = levy.rvs(self.beta, size=self.dim)
                mutant += self.step_size * levy_step
                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])
                trial_fitness = objective_function(trial.reshape(1, -1))[0]
                self.eval_count += 1
                if trial_fitness < self.fitness_values[i]:
                    new_population.append(trial)
                    new_fitness_values.append(trial_fitness)
                else:
                    new_population.append(self.population[i])
                    new_fitness_values.append(self.fitness_values[i])

            self.population = np.array(new_population)
            self.fitness_values = np.array(new_fitness_values)
            self.best_solution_overall, self.best_fitness_overall = self._find_best(self.population, self.fitness_values)
            self.archive = self._update_archive(np.vstack((self.population, self.population)), np.concatenate((self.fitness_values, self.fitness_values)))
            self.step_size *= self.step_size_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _evaluate_population(self, objective_function):
        fitness = objective_function(self.population)
        self.eval_count += self.population_size
        return fitness

    def _select_different(self, index):
        a, b, c = np.random.choice(self.population_size, 3, replace=False)
        while a == index or b == index or c == index:
            a, b, c = np.random.choice(self.population_size, 3, replace=False)
        return a, b, c

    def _find_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        return population[best_index], fitness_values[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])

```

### Rank 7 (Overall AOCC Score: 9.4693e-02 |             AOCC Score on Unimodal instances: 1.7773e-01 |             AOCC Score on Multimodal instances with a single component: 1.0635e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDELevyFlightArchiveEA
# Description: Combines Differential Evolution, Levy flights, and an archive for robust multimodal optimization.
# Code:
```python
import numpy as np
from scipy.stats import levy, norm

# Name: AdaptiveDELevyFlightArchiveEA
# Description: Combines Differential Evolution, Levy flights, and an archive for robust multimodal optimization.
# Code:

class AdaptiveDELevyFlightArchiveEA:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.archive_size = 200
        self.archive = []
        self.beta = 1.5  # Levy flight parameter
        self.F = 0.5 # Differential evolution scaling factor
        self.CR = 0.9 # Crossover rate
        self.step_size = 0.1 * (self.upper_bounds - self.lower_bounds)
        self.step_size_decay = 0.99


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._replacement_selection(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.step_size *= self.step_size_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_parents(i, population)
            mutant = self._differential_mutation(a, b, c)
            offspring[i] = self._crossover(population[i], mutant)
            offspring[i] += self._levy_flight_perturbation()
            offspring[i] = np.clip(offspring[i], self.lower_bounds, self.upper_bounds)
        return offspring

    def _select_parents(self, i, population):
        indices = np.random.choice(self.population_size, 3, replace=False)
        while i in indices:
            indices = np.random.choice(self.population_size, 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]
    
    def _differential_mutation(self, a, b, c):
        return a + self.F * (b - c)

    def _crossover(self, x, v):
        jrand = np.random.randint(0, self.dim)
        u = np.copy(x)
        for j in range(self.dim):
            if np.random.rand() < self.CR or j == jrand:
                u[j] = v[j]
        return u

    def _levy_flight_perturbation(self):
        step = levy.rvs(self.beta, size=self.dim)
        return self.step_size * step

    def _replacement_selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```

### Rank 8 (Overall AOCC Score: 9.3324e-02 |             AOCC Score on Unimodal instances: 1.7512e-01 |             AOCC Score on Multimodal instances with a single component: 1.0485e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: MultimodalAdaptiveDifferentialEvolution
# Description: A differential evolution algorithm with adaptive mutation and a niching strategy to escape local optima in multimodal landscapes.
# Code:
```python
import numpy as np
from scipy.spatial.distance import pdist, squareform

# Name: MultimodalAdaptiveDifferentialEvolution
# Description: A differential evolution algorithm with adaptive mutation and a niching strategy to escape local optima in multimodal landscapes.
# Code:
class MultimodalAdaptiveDifferentialEvolution:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.niche_radius = 0.2 * np.linalg.norm(self.upper_bounds - self.lower_bounds) # Adaptive niching
        self.archive = [] #Store diverse solutions
        self.archive_size = 200


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)

            self._update_archive(population, fitness_values)
            self._update_best(population, fitness_values)

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            # Mutation
            a, b, c = self._select_different_individuals(i, population)
            mutant = a + self.F * (b - c)
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)

            # Crossover
            crosspoints = np.random.rand(self.dim) < self.CR
            offspring[i] = np.where(crosspoints, mutant, population[i])
        return offspring

    def _select_different_individuals(self, i, population):
        indices = np.random.choice(self.population_size, 3, replace=False)
        while i in indices:
            indices = np.random.choice(self.population_size, 3, replace=False)
        return population[indices]

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_population = np.vstack((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fitness)
        return combined_population[sorted_indices[:self.population_size]], combined_fitness[sorted_indices[:self.population_size]]

    def _update_best(self, population, fitness_values):
        best_index = np.argmin(fitness_values)
        if fitness_values[best_index] < self.best_fitness_overall:
            self.best_fitness_overall = fitness_values[best_index]
            self.best_solution_overall = population[best_index]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        for sol in combined:
            too_close = False
            if len(self.archive) > 0:
                distances = np.linalg.norm(self.archive[:, :-1] - sol[:-1], axis=1)
                too_close = np.any(distances < self.niche_radius)

            if not too_close:
                self.archive.append(sol)

        self.archive.sort(key=lambda x: x[-1]) #Keep best
        self.archive = np.array(self.archive[:self.archive_size])

```

### Rank 9 (Overall AOCC Score: 9.1835e-02 |             AOCC Score on Unimodal instances: 1.7564e-01 |             AOCC Score on Multimodal instances with a single component: 9.9868e-02 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: Differential Evolution enhanced with adaptive mutation and clustering for efficient multimodal optimization.
# Code:
```python
import numpy as np
from scipy.spatial.distance import pdist, squareform

# Name: AdaptiveDifferentialEvolutionWithClustering
# Description: Differential Evolution enhanced with adaptive mutation and clustering for efficient multimodal optimization.
# Code:
class AdaptiveDifferentialEvolutionWithClustering:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9  # Crossover rate
        self.archive = []
        self.archive_size = 200
        self.cluster_threshold = 0.1 # controls clustering granularity


    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size

            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self.archive = self._update_archive(np.vstack((population, offspring)), np.concatenate((fitness_values, offspring_fitness)))
            self._update_best(offspring, offspring_fitness)
            self._adaptive_parameter_tuning(population, fitness_values) #Adaptive Parameter tuning


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info


    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_different_individuals(i, population)
            v = a + self.F * (b - c)
            v = np.clip(v, self.lower_bounds, self.upper_bounds)
            offspring[i] = self._crossover(population[i], v)
        return offspring

    def _select_different_individuals(self, i, population):
        indices = np.random.choice(self.population_size, 3, replace=False)
        while i in indices:
            indices = np.random.choice(self.population_size, 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
      jrand = np.random.randint(0, self.dim)
      y = np.copy(x)
      for j in range(self.dim):
        if np.random.rand() < self.CR or j == jrand:
          y[j] = v[j]
      return y

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_population = np.vstack((population, offspring))
        combined_fitness = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fitness)
        return combined_population[sorted_indices[:self.population_size]], combined_fitness[sorted_indices[:self.population_size]]

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        #Simple distance based archive update
        if len(self.archive) < self.archive_size:
            for sol in combined:
                already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
                if not already_present:
                    self.archive.append(sol)
        else:
            # Replace worst with better solutions
            for sol in combined:
                worst_index = np.argmax([s[-1] for s in self.archive])
                if sol[-1] < self.archive[worst_index][-1]:
                    self.archive[worst_index] = sol
        return np.array(self.archive)

    def _adaptive_parameter_tuning(self, population, fitness_values):
        # Simple adaptive strategy: Adjust F and CR based on convergence
        mean_fitness = np.mean(fitness_values)
        if mean_fitness < 0.1 * self.best_fitness_overall:  # Adjust parameters if converging fast
            self.F *= 0.9
            self.CR *= 0.95
        elif mean_fitness > 0.8 * self.best_fitness_overall:  #Explore more if slow convergence
            self.F *=1.1
            self.CR *=1.05


        self.F = np.clip(self.F, 0.1, 1.0)  # Keep F within bounds
        self.CR = np.clip(self.CR, 0.1, 1.0) # Keep CR within bounds

```

### Rank 10 (Overall AOCC Score: 9.1792e-02 |             AOCC Score on Unimodal instances: 1.7499e-01 |             AOCC Score on Multimodal instances with a single component: 1.0038e-01 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveDEwithClusteringAndGaussianPerturbation
# Description: Differential Evolution with clustering and Gaussian perturbation for multimodal optimization.
# Code:
```python
import numpy as np
from scipy.spatial.distance import cdist

# Name: AdaptiveDEwithClusteringAndGaussianPerturbation
# Description: Differential Evolution with clustering and Gaussian perturbation for multimodal optimization.
# Code:

class AdaptiveDEwithClusteringAndGaussianPerturbation:
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = 100
        self.F = 0.8  # Differential weight
        self.CR = 0.9 # Crossover rate
        self.cluster_threshold = 0.1
        self.sigma = 0.1 * (self.upper_bounds - self.lower_bounds) # Initial Gaussian perturbation
        self.sigma_decay = 0.99

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size
        self._update_best(population, fitness_values)

        while self.eval_count < self.budget:
            offspring = self._generate_offspring(population, fitness_values)
            offspring_fitness = objective_function(offspring)
            self.eval_count += self.population_size
            self._update_best(offspring, offspring_fitness)
            population, fitness_values = self._selection(population, fitness_values, offspring, offspring_fitness)
            self._adapt_parameters(population, fitness_values)


        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }
        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        return np.random.uniform(self.lower_bounds, self.upper_bounds, size=(self.population_size, self.dim))

    def _generate_offspring(self, population, fitness_values):
        offspring = np.zeros_like(population)
        for i in range(self.population_size):
            a, b, c = self._select_three_distinct(population, i)
            mutant = a + self.F * (b - c) + np.random.multivariate_normal(np.zeros(self.dim), np.diag(self.sigma**2))
            mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)
            trial = self._crossover(population[i], mutant)
            offspring[i] = trial
        return offspring

    def _select_three_distinct(self, population, exclude_index):
        indices = np.random.choice(len(population), 3, replace=False)
        while exclude_index in indices:
            indices = np.random.choice(len(population), 3, replace=False)
        return population[indices[0]], population[indices[1]], population[indices[2]]

    def _crossover(self, x, v):
        u = np.copy(x)
        mask = np.random.rand(self.dim) < self.CR
        u[mask] = v[mask]
        return u

    def _selection(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        indices = np.argsort(combined_fit)
        return combined_pop[indices[:self.population_size]], combined_fit[indices[:self.population_size]]

    def _update_best(self, solutions, fitnesses):
        for i, fitness in enumerate(fitnesses):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = solutions[i]

    def _adapt_parameters(self, population, fitness_values):
        distances = cdist(population, population, 'euclidean')
        avg_distance = np.mean(distances[np.triu_indices(len(population), k=1)]) #Average distance between all pairs

        if avg_distance < self.cluster_threshold:
            self.F *= 0.95
            self.CR *= 0.95
            self.sigma *= self.sigma_decay #Reduce Gaussian perturbation
        else:
            self.F = min(self.F * 1.05, 1.0)
            self.CR = min(self.CR * 1.05, 1.0)
            self.sigma = min(self.sigma * 1.05, 0.5 * (self.upper_bounds - self.lower_bounds)) #Increase Gaussian perturbation, but limit to avoid over-exploration.

```

### Rank 11 (Overall AOCC Score: 9.1069e-02 |             AOCC Score on Unimodal instances: 1.7432e-01 |             AOCC Score on Multimodal instances with a single component: 9.8892e-02 |             AOCC Score on Multimodal instances with multiple components: 0.0000e+00)
# Name: AdaptiveGaussianArchiveEA
# Description: 
# Code:
```python
import numpy as np
class AdaptiveGaussianArchiveEA:
    """
    Combines adaptive Gaussian sampling with an archive to enhance exploration and exploitation in multimodal landscapes.  Employs a simple Gaussian mutation strategy and tournament selection for efficiency.
    """
    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float],
                 population_size: int = 367.30521382549574, archive_size: int = 763.8609776266115, sigma_decay: float = 0.897254140476471,
                 initial_sigma_multiplier: float = 0.33139418006852284):
        self.budget = int(budget)
        self.dim = int(dim)
        self.lower_bounds = np.array(lower_bounds, dtype=float)
        self.upper_bounds = np.array(upper_bounds, dtype=float)

        self.eval_count = 0
        self.best_solution_overall = None
        self.best_fitness_overall = float('inf')
        self.population_size = population_size
        self.archive_size = archive_size
        self.sigma = initial_sigma_multiplier * (self.upper_bounds - self.lower_bounds)
        self.sigma_decay = sigma_decay
        self.archive = []

    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8, tournament_size: int =5) -> tuple:
        self.eval_count = 0
        if self.dim > 0:
            self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        else:
            self.best_solution_overall = np.array([])
        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1, -1))[0]
        self.eval_count += 1

        population = self._initialize_population()
        fitness_values = objective_function(population)
        self.eval_count += self.population_size

        self.archive = self._update_archive(population, fitness_values)

        while self.eval_count < self.budget:
            parents = self._tournament_selection(population, fitness_values, tournament_size=tournament_size)
            offspring = self._gaussian_recombination(parents)
            offspring = self._adaptive_mutation(offspring)
            offspring_fitness = objective_function(offspring)
            self.eval_count += len(offspring)

            population, fitness_values = self._select_next_generation(
                population, fitness_values, offspring, offspring_fitness
            )

            self.archive = self._update_archive(
                np.vstack((population, offspring)),
                np.concatenate((fitness_values, offspring_fitness))
            )

            self._update_best(offspring, offspring_fitness)
            self.sigma *= self.sigma_decay

        optimization_info = {
            'function_evaluations_used': self.eval_count,
            'final_best_fitness': self.best_fitness_overall
        }

        return self.best_solution_overall, self.best_fitness_overall, optimization_info

    def _initialize_population(self):
        center = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)
        population = np.random.normal(center, self.sigma, size=(self.population_size, self.dim))
        return np.clip(population, self.lower_bounds, self.upper_bounds)

    def _tournament_selection(self, population, fitness_values, tournament_size:int =5):
        num_parents = self.population_size // 2
        selected_parents = []

        for _ in range(num_parents):
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            winner_index = tournament[np.argmin(fitness_values[tournament])]
            selected_parents.append(population[winner_index])

        return np.array(selected_parents)

    def _gaussian_recombination(self, parents):
        offspring = []
        for i in range(0, len(parents), 2):
            parent1 = parents[i]
            parent2 = parents[i + 1]
            midpoint = (parent1 + parent2) / 2
            child1 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            child2 = midpoint + np.random.normal(0, self.sigma / 2, self.dim)
            offspring.extend([child1, child2])
        return np.clip(np.array(offspring), self.lower_bounds, self.upper_bounds)

    def _adaptive_mutation(self, offspring):
        mutated = offspring + np.random.normal(0, self.sigma, size=offspring.shape)
        return np.clip(mutated, self.lower_bounds, self.upper_bounds)

    def _select_next_generation(self, population, fitness_values, offspring, offspring_fitness):
        combined_pop = np.vstack((population, offspring))
        combined_fit = np.concatenate((fitness_values, offspring_fitness))
        sorted_indices = np.argsort(combined_fit)
        next_gen = combined_pop[sorted_indices[:self.population_size]]
        next_fit = combined_fit[sorted_indices[:self.population_size]]
        return next_gen, next_fit

    def _update_best(self, offspring, offspring_fitness):
        for i, fitness in enumerate(offspring_fitness):
            if fitness < self.best_fitness_overall:
                self.best_fitness_overall = fitness
                self.best_solution_overall = offspring[i]

    def _update_archive(self, population, fitness_values):
        combined = np.column_stack((population, fitness_values))
        new_archive = []
        for sol in combined:
            already_present = any(np.allclose(sol[:-1], arch[:-1], atol=1e-6) for arch in self.archive)
            if not already_present:
                new_archive.append(sol)
        new_archive.sort(key=lambda x: x[-1])
        return np.array(new_archive[:self.archive_size])
```



### Guide
Keep in mind, this is a list of design heuristics ranked from best to worst. The first algorithm in the list is the best performing on average, and the last is the worst.
Your response must be in Markdown format and contain nothing else. It must have the following structure:

"**Analysis:**
**Experience:**"

In there:
+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.
Example: "Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:"

+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).

I'm going to tip $999K for a better heuristics! Let's think step by step.
2025-06-23 15:11:16 INFO Full response text: **Analysis:**

Comparing (best) AdaptiveDEwithArchiveAndGaussianPerturbation vs (worst) AdaptiveDEwithClusteringAndGaussianPerturbation, we see that the best utilizes an adaptive strategy for its differential weight (F), crossover rate (CR), and Gaussian perturbation (sigma), adjusting these based on population clustering.  The worst also uses Gaussian perturbation but its adaptive strategy is less sophisticated, only adjusting F and CR based on a simple convergence criterion and not taking into account the spread of the population. The best also incorporates an archive to maintain diversity, while the worst does not.

(second best) AdaptiveDEwithLevyFlightsAndArchive vs (second worst) AdaptiveDifferentialEvolutionWithClustering: The second best uses Levy flights to enhance exploration, introducing a significant diversity benefit over DE alone.  The second worst uses clustering to detect convergence but lacks an archive to explicitly maintain diversity. Its adaptive strategy is also less refined.

Comparing (1st) AdaptiveDEwithArchiveAndGaussianPerturbation vs (2nd) AdaptiveDEwithLevyFlightsAndArchive, we see both use archives, but the best performs Gaussian perturbation, leading to more refined local exploitation.  The second-best uses Levy flights, better for global exploration but potentially less effective for exploitation.

(3rd) AdaptiveDEwithArchiveAndClustering vs (4th) AdaptiveDEwithLevyFlightAndArchive: Both use archives, but the third uses clustering to inform parameter adaptation, while the fourth has a more basic adaptive scheme with Levy flight. The third's adaptation is more direct and less reliant on an explicit decay rate, but lacks the global exploration potential of Levy flights in the fourth.

Comparing (second worst) AdaptiveDifferentialEvolutionWithClustering vs (worst) AdaptiveDEwithClusteringAndGaussianPerturbation, we see both use clustering, but the second worst lacks explicit Gaussian perturbation. This contributes to the better performance of the second worst heuristic, as the addition of Gaussian noise in the worst heuristic makes the optimization unstable.

Overall: The top-performing heuristics consistently leverage archives to preserve diversity, adapt parameters based on population dynamics (not just convergence), and employ strategies (Gaussian perturbation, Levy flights) tailored to either enhance exploration or exploitation.  The lower-ranked methods often use simpler adaptive mechanisms and less sophisticated exploration/exploitation balance.


**Experience:**

Combining archive-based diversity maintenance with adaptive parameter control (especially mutation strength) based on both population distribution and convergence indicators is crucial for high-dimensional multimodal optimization.  Levy flights offer excellent global exploration, but Gaussian perturbations can be superior for local refinement.

2025-06-23 15:11:18 INFO Full response text: * **Keywords:**  Adaptive mutation,  archive-based diversity,  convergence indicators,  population distribution,  Levy flights vs. Gaussian perturbations,  high-dimensional multimodal optimization.

* **Advice:** Focus on *quantifiable* metrics for assessing population distribution and convergence.  Develop a dynamic switching mechanism between Levy flights and Gaussian perturbations based on these metrics.  Rigorously test the archive's impact on diversity and its interaction with the adaptive mutation strength.

* **Avoid:** Vague terms like "exploration" and "exploitation."  Avoid generic statements about combining methods without specifying the *conditions* under which one method is preferred over another.  Don't claim "best results" without rigorous empirical evidence.

* **Explanation:**  The current self-reflection needs sharpening.  It identifies key components but lacks the precise, measurable criteria needed for designing effective heuristics.  The improved approach emphasizes data-driven adaptation and avoids unsubstantiated generalizations.

2025-06-23 15:11:18 INFO Generating offspring via Crossover...
