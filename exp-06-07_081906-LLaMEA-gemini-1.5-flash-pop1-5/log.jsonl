{"id": "feaa7a42-6bc8-4943-b458-17c887012b5d", "fitness": 0.025344840781882375, "name": "AdaptiveDEMGM", "description": "A hybrid algorithm combining Differential Evolution with a novel adaptive mutation strategy based on a Gaussian Mixture Model to dynamically adjust exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGM:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = 10 * self.dim\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): # Limit generations to avoid excessive runtime\n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            self.gmm.fit(self.population)\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c])\n            \n            #Adaptive Mutation using GMM\n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n            \n\n        return mutated_pop\n", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDEMGM got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07603452234564713]}}
{"id": "480562bf-d18d-4200-bd87-2e32576f7cb3", "fitness": 0.02563941659913418, "name": "AdaptiveDEMGMImproved", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation and enhanced diversity preservation.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = 10 * self.dim\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): # Limit generations to avoid excessive runtime\n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c])\n            \n            #Adaptive Mutation using GMM\n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n            \n\n        return mutated_pop", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDEMGMImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["feaa7a42-6bc8-4943-b458-17c887012b5d"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07691824979740254]}}
{"id": "f1b3442e-818d-4152-ae2d-60ac65f0f921", "fitness": 0.025403759500330816, "name": "AdaptiveDEMGMImprovedLevy", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved exploration via Levy flights.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal, levy_stable\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedLevy:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = 10 * self.dim\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): # Limit generations to avoid excessive runtime\n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c])\n            \n            #Adaptive Mutation using GMM\n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            #Levy Flight Modification\n            levy_step = levy_stable.rvs(alpha=1.5, beta=0, loc=0, scale=0.1, size=self.dim) #Added Levy flight\n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]) + levy_step, self.lower_bounds, self.upper_bounds)\n            \n            \n\n        return mutated_pop\n", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEMGMImprovedLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["480562bf-d18d-4200-bd87-2e32576f7cb3"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07621127850099245]}}
{"id": "75f878c5-0224-4a19-9f8c-398f7cd29001", "fitness": 0.02550746389013898, "name": "AdaptiveDEMGMImprovedEnhanced", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved crossover.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhanced:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = 10 * self.dim\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): # Limit generations to avoid excessive runtime\n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c]) #Improved Crossover\n            \n            #Adaptive Mutation using GMM\n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n            \n\n        return mutated_pop", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["480562bf-d18d-4200-bd87-2e32576f7cb3"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07652239167041694]}}
{"id": "ed6ad25b-683b-48bc-bcc1-fbffc4a2384c", "fitness": 0.025789868666541704, "name": "AdaptiveDEMGMImprovedEnhanced", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhanced:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) #Improved scaling for high dimensions\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): # Limit generations to avoid excessive runtime\n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c])\n            \n            #Adaptive Mutation using GMM\n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n            \n\n        return mutated_pop", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["480562bf-d18d-4200-bd87-2e32576f7cb3"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07736960599962511]}}
{"id": "745de4ed-6618-4b1b-a592-fe64124eec8b", "fitness": 0.025089157246124403, "name": "AdaptiveDEMGMImprovedEnhancedElitist", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation and enhanced diversity, incorporating an elitist selection strategy for improved convergence.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhancedElitist:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            \n            #Elitist selection: always keep the best solution\n            self.population = np.vstack((combined_pop[indices[0]], combined_pop[indices[1:pop_size]]))\n            self.fitness = np.concatenate(([combined_fitness[indices[0]]], combined_fitness[indices[1:pop_size]]))\n\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c])\n            \n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n            \n\n        return mutated_pop", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhancedElitist got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["ed6ad25b-683b-48bc-bcc1-fbffc4a2384c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07526747173837321]}}
{"id": "26d4f863-3b39-4f92-b2d7-18fd2f7eef60", "fitness": 0.02558561410442751, "name": "AdaptiveDEMGMImprovedEnhanced", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, incorporating a scaling factor for adaptive mutation strength.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhanced:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c])\n            \n            #Adaptive Mutation using GMM\n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            # Modification: Adaptive scaling factor\n            scaling_factor = 1.0 + 0.5 * (1 - (self.eval_count / self.budget)) # Decreases over time\n\n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index] * scaling_factor), self.lower_bounds, self.upper_bounds)\n            \n\n        return mutated_pop", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["ed6ad25b-683b-48bc-bcc1-fbffc4a2384c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07675684231328253]}}
{"id": "7946dc6e-a24e-4a19-bdee-29ca691a30bc", "fitness": 0.02496772821058087, "name": "AdaptiveDEMGMImprovedEnhancedRegularized", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, using a more robust covariance matrix regularization.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhancedRegularized:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100)\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)):\n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c])\n            \n            #Adaptive Mutation using GMM\n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            #Regularization for numerical stability\n            covariances[best_mean_index] += 0.01 * np.eye(self.dim) # Added regularization\n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n            \n\n        return mutated_pop", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhancedRegularized got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.04.", "error": "", "parent_ids": ["ed6ad25b-683b-48bc-bcc1-fbffc4a2384c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07490318463174261]}}
{"id": "fae72d3e-eaf0-4642-b46b-72862c99d1d9", "fitness": 0.025079865668096533, "name": "AdaptiveDEMGMImprovedEnhancedLevy", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, incorporating a Levy flight mutation strategy for enhanced exploration.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal, levy_stable\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhancedLevy:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c])\n            \n            #Adaptive Mutation using GMM and Levy Flight\n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            levy_step = levy_stable.rvs(alpha=1.5, beta=0, loc=0, scale=0.1, size=self.dim) #Levy Flight added here\n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]) + levy_step, self.lower_bounds, self.upper_bounds)\n            \n\n        return mutated_pop\n", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhancedLevy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["ed6ad25b-683b-48bc-bcc1-fbffc4a2384c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.0752395970042896]}}
{"id": "248061d6-8360-4191-a0aa-0391ea7e724c", "fitness": 0.02578228234947545, "name": "AdaptiveDEMGMImprovedEnhancedWeighted", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, using a weighted average for mutation.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhancedWeighted:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            # Modification: Weighted average for mutation vector\n            v = 0.5 * self.population[a] + 0.25 * self.population[b] + 0.25 * self.population[c]  \n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n            \n\n        return mutated_pop", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhancedWeighted got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["ed6ad25b-683b-48bc-bcc1-fbffc4a2384c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07734684704842634]}}
{"id": "0678a671-503d-4b99-81a3-becd4d39f67b", "fitness": 0.02532734850445363, "name": "AdaptiveDEMGMImprovedEnhancedWeighted", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, using a weighted average for mutation.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhancedWeighted:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            #Change: Weighted average instead of 0.5\n            v = self.population[a] + 0.6*(self.population[b] - self.population[c]) + 0.4 * (self.population[a] - self.population[c])\n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n\n        return mutated_pop", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhancedWeighted got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["ed6ad25b-683b-48bc-bcc1-fbffc4a2384c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.0759820455133609]}}
{"id": "3d935c46-4abe-402f-90ea-56a8af73f4bc", "fitness": 0.024583530108292617, "name": "AdaptiveDEMGMImprovedEnhanced", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, incorporating a linear scaling factor for mutation strength.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhanced:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            v = self.population[a] + 0.5*(self.population[b] - self.population[c]) #Original line\n            #v = self.population[a] + (gen + 1) / 100 * 0.5*(self.population[b] - self.population[c]) #New line with linear scaling factor\n            \n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n            \n\n        return mutated_pop", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.03.", "error": "", "parent_ids": ["ed6ad25b-683b-48bc-bcc1-fbffc4a2384c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07375059032487785]}}
{"id": "148ed3c3-4fa0-4294-ab7f-517fb2ad215c", "fitness": 0.026131594743449425, "name": "AdaptiveDEMGMImprovedEnhanced", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, using a weighted average for mutation.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhanced:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            # Modification: Weighted average for mutation vector\n            v = 0.5 * self.population[a] + 0.25 * self.population[b] + 0.25 * self.population[c]\n            \n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n\n        return mutated_pop", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["ed6ad25b-683b-48bc-bcc1-fbffc4a2384c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07839478423034828]}}
{"id": "9324f710-cb6f-4418-bcb1-5b8a3b65f44d", "fitness": 0.025047295120360677, "name": "AdaptiveDEMGMImprovedEnhancedRefined", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, using a weighted average for mutation and a refined GMM component selection.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhancedRefined:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            # Modification: Weighted average for mutation vector\n            v = 0.5 * self.population[a] + 0.25 * self.population[b] + 0.25 * self.population[c]\n            \n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            #Refined GMM component selection:  select component based on probability density, not just distance\n            probabilities = np.array([multivariate_normal.pdf(v, mean=mean, cov=cov) for mean, cov in zip(means, covariances)])\n            best_mean_index = np.argmax(probabilities)\n\n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n\n        return mutated_pop\n", "configspace": "", "generation": 13, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhancedRefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["148ed3c3-4fa0-4294-ab7f-517fb2ad215c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07514188536108203]}}
{"id": "a37fe9a2-f5f6-438a-93c5-9f5202187435", "fitness": 0.0255023386037833, "name": "AdaptiveDEMGMImprovedEnhanced", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, using a weighted average for mutation and incorporating a dynamic scaling factor.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhanced:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            # Modification: Weighted average for mutation vector\n            v = 0.5 * self.population[a] + 0.25 * self.population[b] + 0.25 * self.population[c]\n            \n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            #Change: Dynamic scaling factor\n            F = 0.5 + 0.5 * np.random.rand() * (1 - (self.eval_count / self.budget)) #Decreases over time\n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n\n        return mutated_pop", "configspace": "", "generation": 14, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["148ed3c3-4fa0-4294-ab7f-517fb2ad215c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.0765070158113499]}}
{"id": "5b254e2b-1bf5-4724-82f5-e9908cebd09e", "fitness": 0.025269012455744463, "name": "AdaptiveDEMGMImprovedEnhanced", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, using a weighted average for mutation and incorporating a self-adaptive scaling factor.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhanced:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            # Modification: Weighted average for mutation vector and self-adaptive scaling factor\n            F = 0.5 + 0.5 * np.random.rand() #Self-adaptive scaling factor\n            v = F * (0.5 * self.population[a] + 0.25 * self.population[b] + 0.25 * self.population[c])\n            \n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n\n        return mutated_pop\n", "configspace": "", "generation": 15, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["148ed3c3-4fa0-4294-ab7f-517fb2ad215c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07580703736723339]}}
{"id": "dc815afe-7b6d-4a9b-b968-14ca80abc731", "fitness": 0.025705200522939422, "name": "AdaptiveDEMGMImprovedEnhancedRefined", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, using a weighted average for mutation and incorporating a self-adaptive scaling factor.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhancedRefined:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n        self.F = 0.5 #initial scaling factor\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            # Modification: Weighted average for mutation vector and self-adaptive scaling factor\n            v = self.F * (self.population[a] + 0.5 * (self.population[b] - self.population[c]))\n            \n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index], cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            self.F = self.F * 0.99 + np.random.normal(0, 0.1)\n            self.F = max(0.1,min(self.F,1.5)) #clamp between 0.1 and 1.5\n\n        return mutated_pop", "configspace": "", "generation": 16, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhancedRefined got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["148ed3c3-4fa0-4294-ab7f-517fb2ad215c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07711560156881826]}}
{"id": "772598f1-2254-411b-bd67-061b207ccc03", "fitness": 0.025762091010645933, "name": "AdaptiveDEMGMImprovedEnhanced", "description": "Adaptive Differential Evolution with Gaussian Mixture Model-guided mutation, enhanced diversity, and improved scaling for high-dimensional problems, using a weighted average for mutation and an adaptive scaling factor.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveDEMGMImprovedEnhanced:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population = None\n        self.fitness = None\n        self.gmm = None\n        self.F = 0.8 #initial scaling factor\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(self.best_solution_overall.reshape(1,-1))[0]\n        self.eval_count +=1\n        \n        pop_size = max(10 * self.dim, 100) \n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (pop_size, self.dim))\n        self.fitness = objective_function(self.population)\n        self.eval_count += pop_size\n        \n        self.best_solution_overall = self.population[np.argmin(self.fitness)]\n        self.best_fitness_overall = np.min(self.fitness)\n        \n        self.gmm = GaussianMixture(n_components=min(pop_size//2,5), random_state=0)\n\n\n        for gen in range(min(self.budget // pop_size, 100)): \n            mutated_pop = self.mutate()\n            offspring = np.clip(mutated_pop, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += pop_size\n            \n            combined_pop = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n            \n            indices = np.argsort(combined_fitness)\n            self.population = combined_pop[indices[:pop_size]]\n            self.fitness = combined_fitness[indices[:pop_size]]\n\n            if np.min(self.fitness) < self.best_fitness_overall:\n                self.best_fitness_overall = np.min(self.fitness)\n                self.best_solution_overall = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n            #Enhance diversity by adding a small amount of random noise to the best solution before fitting the GMM\n            self.gmm.fit(np.clip(self.population + np.random.normal(0, 0.1, size=self.population.shape), self.lower_bounds, self.upper_bounds))\n            #Adaptive scaling factor based on convergence\n            self.F = max(0.1, self.F * (1 - np.exp(-gen/10)))\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.population)\n        for i in range(len(self.population)):\n            a, b, c = np.random.choice(len(self.population), 3, replace=False)\n            # Modification: Weighted average for mutation vector\n            v = 0.5 * self.population[a] + 0.25 * self.population[b] + 0.25 * self.population[c]\n            \n            means = self.gmm.means_\n            covariances = self.gmm.covariances_\n            weights = self.gmm.weights_\n\n            best_mean_index = np.argmin(np.sum((means - v)**2, axis=1))\n            \n            mutated_pop[i] = np.clip(multivariate_normal.rvs(mean=means[best_mean_index] + self.F * (v - means[best_mean_index]), cov=covariances[best_mean_index]), self.lower_bounds, self.upper_bounds)\n            \n\n        return mutated_pop\n", "configspace": "", "generation": 17, "feedback": "The algorithm AdaptiveDEMGMImprovedEnhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["148ed3c3-4fa0-4294-ab7f-517fb2ad215c"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.0772862730319378]}}
