{"id": "21cc6fb5-0378-42df-909f-0f7afe37a5c3", "fitness": 0.025684225106523167, "name": "AdaptiveDE", "description": "A hybrid algorithm combining Differential Evolution with a novel adaptive mutation strategy based on a dynamically adjusted covariance matrix to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover rate\n        self.covariance_matrix = np.eye(self.dim)  # Initialize covariance matrix\n        self.learning_rate = 0.1 # Learning rate for covariance matrix adaptation\n\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        if self.dim > 0:\n            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        else:\n            self.population = np.array([])\n        fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        best_index = np.argmin(fitness_values)\n        self.best_solution_overall = self.population[best_index].copy()\n        self.best_fitness_overall = fitness_values[best_index]\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                #Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                #Adaptive Mutation using covariance matrix\n                mutated_vector = np.random.multivariate_normal(mutant, self.covariance_matrix)\n                mutated_vector = np.clip(mutated_vector, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutated_vector, self.population[i])\n\n                #Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness[0] < fitness_values[i]:\n                    self.population[i] = trial\n                    fitness_values[i] = trial_fitness[0]\n                    if trial_fitness[0] < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness[0]\n                        self.best_solution_overall = trial.copy()\n\n            #Covariance Matrix Adaptation\n            mean_diff = np.mean(self.population - np.mean(self.population, axis=0),axis=0)\n            self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.outer(mean_diff, mean_diff)\n            self.covariance_matrix = (self.covariance_matrix + self.covariance_matrix.T) / 2 #ensure symmetry\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.0770526753195695]}}
{"id": "ed0dc525-6733-4d21-8c7d-1612fce25dcb", "fitness": 0.026225896389332412, "name": "AdaptiveGMDE", "description": "A hybrid algorithm combining Differential Evolution with a novel adaptive mutation based on a Gaussian Mixture Model to handle multimodality and ill-conditioning.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass AdaptiveGMDE:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential Evolution scaling factor\n        self.CR = 0.9  # Crossover rate\n        self.population = None\n        self.fitness_values = None\n        self.gmm_means = None\n        self.gmm_covariances = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        self.best_solution_overall = self.population[np.argmin(self.fitness_values)]\n        self.best_fitness_overall = np.min(self.fitness_values)\n\n        # Initialize GMM parameters (2 components initially)\n        self.gmm_means = np.array([self.best_solution_overall, np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)])\n        self.gmm_covariances = np.array([np.eye(self.dim), np.eye(self.dim)])\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                # Adaptive Gaussian Mixture Model Mutation\n                if np.random.rand() < 0.2:  # Probability of using GMM mutation\n                    weights = np.array([multivariate_normal.pdf(mutant, mean=mean, cov=cov) for mean, cov in zip(self.gmm_means, self.gmm_covariances)])\n                    weights /= np.sum(weights)\n                    chosen_component = np.random.choice(len(self.gmm_means), p=weights)\n                    mutant = np.random.multivariate_normal(self.gmm_means[chosen_component], self.gmm_covariances[chosen_component])\n\n\n                # Boundary handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n\n                # Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness < self.fitness_values[i]:\n                    self.population[i] = trial\n                    self.fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial\n\n            # Update GMM parameters (simple update - can be improved)\n            top_indices = np.argsort(self.fitness_values)[:2]  #Take top 2 solutions\n            self.gmm_means = self.population[top_indices]\n            self.gmm_covariances = np.array([np.cov(self.population.T), np.cov(self.population.T)])\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveGMDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07867768916799724]}}
{"id": "281380f5-2014-4536-8383-15350b981f53", "fitness": 0.025485820387292083, "name": "AdaptiveDEwithLocalSearch", "description": "A hybrid algorithm combining Differential Evolution with a novel adaptive mutation strategy based on the population's diversity and a local search phase.", "code": "import numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\nclass AdaptiveDEwithLocalSearch:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim #Adaptive population size\n        self.F = 0.8  # Differential Evolution scaling factor\n        self.CR = 0.9 # Crossover rate\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        \n        self.best_solution_overall = self.population[np.argmin(fitness_values)]\n        self.best_fitness_overall = np.min(fitness_values)\n\n        for _ in range(int(self.budget / self.population_size)):\n          \n            #Adaptive Mutation Strategy\n            diversity = np.mean(pdist(self.population))\n            if diversity < 0.1: # Adjust this threshold based on problem's complexity\n                self.F = 0.2 # Reduce exploration in high diversity\n            else:\n                self.F = 0.8 # Increase exploration in low diversity\n                \n            offspring = self.mutate(self.population, self.F, self.CR)\n            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += self.population_size\n\n\n            # Selection\n            for i in range(self.population_size):\n                if offspring_fitness[i] < fitness_values[i]:\n                    self.population[i] = offspring[i]\n                    fitness_values[i] = offspring_fitness[i]\n                    if fitness_values[i] < self.best_fitness_overall:\n                        self.best_fitness_overall = fitness_values[i]\n                        self.best_solution_overall = self.population[i]\n\n\n            # Local search around best solution\n            if self.eval_count < self.budget:\n              self.local_search(objective_function)\n              \n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n    \n    def mutate(self, population, F, CR):\n        offspring = np.copy(population)\n        for i in range(self.population_size):\n            r1, r2, r3 = np.random.choice(np.arange(self.population_size), 3, replace=False)\n            while r1 == i or r2 == i or r3 == i: # ensure no self-selection\n                r1, r2, r3 = np.random.choice(np.arange(self.population_size), 3, replace=False)\n            mutant = population[r1] + F * (population[r2] - population[r3])\n            cross_points = np.random.rand(self.dim) < CR\n            offspring[i] = np.where(cross_points, mutant, offspring[i])\n        return offspring\n        \n    def local_search(self, objective_function):\n      if self.eval_count < self.budget:\n        step_size = 0.1*(self.upper_bounds - self.lower_bounds)\n        neighbor = self.best_solution_overall + np.random.uniform(-step_size, step_size)\n        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)\n        neighbor_fitness = objective_function(neighbor.reshape(1, -1))\n        self.eval_count += 1\n        if neighbor_fitness[0] < self.best_fitness_overall:\n          self.best_fitness_overall = neighbor_fitness[0]\n          self.best_solution_overall = neighbor", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDEwithLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07645746116187625]}}
{"id": "08e7a677-9be6-46a4-8fea-90803110a680", "fitness": 0.026859550267983978, "name": "AdaptiveDE", "description": "A hybrid algorithm combining Differential Evolution with a novel adaptive mutation strategy based on local search and population diversity.", "code": "import numpy as np\nimport random\n\nclass AdaptiveDE:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Scaling factor for mutation\n        self.CR = 0.9  # Crossover rate\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(np.array([self.best_solution_overall]))[0]\n        self.eval_count += 1\n\n        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        fitness_values = objective_function(population)\n        self.eval_count += self.population_size\n\n        for i in range(self.population_size):\n            if fitness_values[i] < self.best_fitness_overall:\n                self.best_fitness_overall = fitness_values[i]\n                self.best_solution_overall = population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            new_population = np.zeros_like(population)\n            for i in range(self.population_size):\n                # Adaptive Mutation Strategy\n                a, b, c = random.sample(range(self.population_size), 3)\n                while a == i or b == i or c == i:\n                    a, b, c = random.sample(range(self.population_size), 3)\n\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Local Search Enhancement:  Adjust mutation based on local fitness\n                if fitness_values[a] < fitness_values[b] and fitness_values[a] < fitness_values[c]:\n                    mutant = population[a] + 0.2*(np.random.normal(0,1, self.dim)) #small perturbation near better solution\n                elif fitness_values[b] < fitness_values[c]:\n                    mutant = population[b] + 0.2*(np.random.normal(0,1, self.dim))\n                else:\n                    mutant = population[c] + 0.2*(np.random.normal(0,1, self.dim))\n\n                #Boundary Handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                #Selection\n                trial_fitness = objective_function(np.array([trial]))[0]\n                self.eval_count += 1\n                if trial_fitness < fitness_values[i]:\n                    new_population[i] = trial\n                    fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial.copy()\n\n            population = new_population\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.08057865080395193]}}
{"id": "8f13a12d-6863-4b0c-8b88-5fdd157e1bb5", "fitness": 0.0241369179706632, "name": "AdaptiveGMMDE", "description": "A hybrid algorithm combining Differential Evolution with a novel adaptive mutation strategy based on a Gaussian Mixture Model to model the search space and guide exploration/exploitation.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass AdaptiveGMMDE:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.population = None\n        self.fitness_values = None\n        self.F = 0.8 # Differential Evolution scaling factor\n        self.CR = 0.9 # Crossover rate\n        self.gmm_components = 3 # Number of GMM components\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = np.full(self.population_size, np.inf)\n\n    def bound_solution(self, solution):\n        solution = np.clip(solution, self.lower_bounds, self.upper_bounds)\n        return solution\n\n    def differential_evolution_step(self, individual_index):\n        a, b, c = np.random.choice(np.delete(np.arange(self.population_size), individual_index), 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        mutant = self.bound_solution(mutant)\n        trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[individual_index])\n        return trial\n\n    def gmm_adaptive_mutation(self, individual):\n        if self.eval_count > self.population_size: # only after initial evaluation\n            X = self.population[self.fitness_values < np.inf] # Avoid NaN\n            if X.shape[0] > self.gmm_components: # prevent errors in fit\n                gmm = GaussianMixture(n_components=self.gmm_components, covariance_type='full', random_state=0).fit(X)\n                means = gmm.means_\n                covariances = gmm.covariances_\n                weights = gmm.weights_\n\n                component_index = np.random.choice(self.gmm_components, p=weights)\n                new_point = multivariate_normal.rvs(mean=means[component_index], cov=covariances[component_index])\n                new_point = self.bound_solution(new_point)\n\n                return new_point\n            else: return individual\n        else: return individual\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.initialize_population()\n\n        for i in range(self.population_size):\n            if self.eval_count < self.budget:\n                fitness = objective_function(self.population[i:i+1, :])[0]\n                self.fitness_values[i] = fitness\n                self.eval_count += 1\n                if fitness < self.best_fitness_overall:\n                    self.best_fitness_overall = fitness\n                    self.best_solution_overall = self.population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count < self.budget:\n                    trial_vector = self.differential_evolution_step(i)\n                    trial_vector = self.gmm_adaptive_mutation(trial_vector) # Adaptive mutation\n                    trial_fitness = objective_function(trial_vector.reshape(1,-1))[0]\n                    self.eval_count += 1\n\n                    if trial_fitness < self.fitness_values[i]:\n                        self.fitness_values[i] = trial_fitness\n                        self.population[i, :] = trial_vector.copy()\n                        if trial_fitness < self.best_fitness_overall:\n                            self.best_fitness_overall = trial_fitness\n                            self.best_solution_overall = trial_vector.copy()\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\nfrom sklearn.mixture import GaussianMixture\n", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveGMMDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.03.", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.0724107539119896]}}
{"id": "9f2d8945-57f6-4850-8fe4-4e491e788824", "fitness": 0.02603823998815619, "name": "AdaptiveDEImproved", "description": "Adaptive Differential Evolution with enhanced covariance matrix adaptation using rank-based updates and population diversity control.", "code": "import numpy as np\n\nclass AdaptiveDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover rate\n        self.covariance_matrix = np.eye(self.dim)  # Initialize covariance matrix\n        self.learning_rate = 0.1 # Learning rate for covariance matrix adaptation\n        self.diversity_threshold = 0.1 #Threshold for diversity control\n\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        if self.dim > 0:\n            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        else:\n            self.population = np.array([])\n        fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        best_index = np.argmin(fitness_values)\n        self.best_solution_overall = self.population[best_index].copy()\n        self.best_fitness_overall = fitness_values[best_index]\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                #Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                #Adaptive Mutation using covariance matrix\n                mutated_vector = np.random.multivariate_normal(mutant, self.covariance_matrix)\n                mutated_vector = np.clip(mutated_vector, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutated_vector, self.population[i])\n\n                #Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness[0] < fitness_values[i]:\n                    self.population[i] = trial\n                    fitness_values[i] = trial_fitness[0]\n                    if trial_fitness[0] < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness[0]\n                        self.best_solution_overall = trial.copy()\n\n            #Covariance Matrix Adaptation with rank-based update\n            ranked_indices = np.argsort(fitness_values)\n            top_individuals = self.population[ranked_indices[:int(0.2*self.population_size)]] #consider top 20%\n            mean_diff = np.mean(top_individuals - np.mean(top_individuals, axis=0),axis=0)\n            self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.outer(mean_diff, mean_diff)\n            self.covariance_matrix = (self.covariance_matrix + self.covariance_matrix.T) / 2 #ensure symmetry\n\n            #Diversity Control: If diversity is too low, increase exploration\n            diversity = np.mean(np.std(self.population, axis=0))\n            if diversity < self.diversity_threshold:\n                self.F = min(1.5, self.F + 0.1) #increase mutation strength\n                self.CR = min(1.0, self.CR + 0.1) #increase crossover rate\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["21cc6fb5-0378-42df-909f-0f7afe37a5c3"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07811471996446857]}}
{"id": "d40369ab-cf74-4de0-b6ac-b7426685e7d1", "fitness": 0.025871608632663426, "name": "AdaptiveGMDE", "description": "AdaptiveGMDE enhanced with a more sophisticated GMM update strategy focusing on diverse high-performing solutions.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass AdaptiveGMDE:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential Evolution scaling factor\n        self.CR = 0.9  # Crossover rate\n        self.population = None\n        self.fitness_values = None\n        self.gmm_means = None\n        self.gmm_covariances = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        self.best_solution_overall = self.population[np.argmin(self.fitness_values)]\n        self.best_fitness_overall = np.min(self.fitness_values)\n\n        # Initialize GMM parameters (2 components initially)\n        self.gmm_means = np.array([self.best_solution_overall, np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)])\n        self.gmm_covariances = np.array([np.eye(self.dim), np.eye(self.dim)])\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                # Adaptive Gaussian Mixture Model Mutation\n                if np.random.rand() < 0.2:  # Probability of using GMM mutation\n                    weights = np.array([multivariate_normal.pdf(mutant, mean=mean, cov=cov) for mean, cov in zip(self.gmm_means, self.gmm_covariances)])\n                    weights /= np.sum(weights)\n                    chosen_component = np.random.choice(len(self.gmm_means), p=weights)\n                    mutant = np.random.multivariate_normal(self.gmm_means[chosen_component], self.gmm_covariances[chosen_component])\n\n\n                # Boundary handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n\n                # Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness < self.fitness_values[i]:\n                    self.population[i] = trial\n                    self.fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial\n\n            # Update GMM parameters (improved update)\n            top_indices = np.argsort(self.fitness_values)[:5]  # Take top 5 diverse solutions\n            self.gmm_means = self.population[top_indices]\n            self.gmm_covariances = np.array([np.cov(self.population[idx].reshape(1,-1).T) for idx in top_indices])\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGMDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["ed0dc525-6733-4d21-8c7d-1612fce25dcb"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07761482589799028]}}
{"id": "94936f06-fcfd-4860-ac3f-f1154eed2975", "fitness": 0.02495915356565009, "name": "AdaptiveDEwithLocalSearchImproved", "description": "Adaptive Differential Evolution with Local Search and improved population size scaling.", "code": "import numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\nclass AdaptiveDEwithLocalSearchImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 20 * self.dim #Improved population size scaling\n        self.F = 0.8  # Differential Evolution scaling factor\n        self.CR = 0.9 # Crossover rate\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        \n        self.best_solution_overall = self.population[np.argmin(fitness_values)]\n        self.best_fitness_overall = np.min(fitness_values)\n\n        for _ in range(int(self.budget / self.population_size)):\n          \n            #Adaptive Mutation Strategy\n            diversity = np.mean(pdist(self.population))\n            if diversity < 0.1: # Adjust this threshold based on problem's complexity\n                self.F = 0.2 # Reduce exploration in high diversity\n            else:\n                self.F = 0.8 # Increase exploration in low diversity\n                \n            offspring = self.mutate(self.population, self.F, self.CR)\n            offspring = np.clip(offspring, self.lower_bounds, self.upper_bounds)\n            offspring_fitness = objective_function(offspring)\n            self.eval_count += self.population_size\n\n\n            # Selection\n            for i in range(self.population_size):\n                if offspring_fitness[i] < fitness_values[i]:\n                    self.population[i] = offspring[i]\n                    fitness_values[i] = offspring_fitness[i]\n                    if fitness_values[i] < self.best_fitness_overall:\n                        self.best_fitness_overall = fitness_values[i]\n                        self.best_solution_overall = self.population[i]\n\n\n            # Local search around best solution\n            if self.eval_count < self.budget:\n              self.local_search(objective_function)\n              \n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n    \n    def mutate(self, population, F, CR):\n        offspring = np.copy(population)\n        for i in range(self.population_size):\n            r1, r2, r3 = np.random.choice(np.arange(self.population_size), 3, replace=False)\n            while r1 == i or r2 == i or r3 == i: # ensure no self-selection\n                r1, r2, r3 = np.random.choice(np.arange(self.population_size), 3, replace=False)\n            mutant = population[r1] + F * (population[r2] - population[r3])\n            cross_points = np.random.rand(self.dim) < CR\n            offspring[i] = np.where(cross_points, mutant, offspring[i])\n        return offspring\n        \n    def local_search(self, objective_function):\n      if self.eval_count < self.budget:\n        step_size = 0.1*(self.upper_bounds - self.lower_bounds)\n        neighbor = self.best_solution_overall + np.random.uniform(-step_size, step_size)\n        neighbor = np.clip(neighbor, self.lower_bounds, self.upper_bounds)\n        neighbor_fitness = objective_function(neighbor.reshape(1, -1))\n        self.eval_count += 1\n        if neighbor_fitness[0] < self.best_fitness_overall:\n          self.best_fitness_overall = neighbor_fitness[0]\n          self.best_solution_overall = neighbor", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDEwithLocalSearchImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.04.", "error": "", "parent_ids": ["281380f5-2014-4536-8383-15350b981f53"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07487746069695027]}}
{"id": "f2b4bfc2-15d5-4eb1-a08c-854846b68e87", "fitness": 0.024969079939165098, "name": "AdaptiveGMDEImproved", "description": "Adaptive Gaussian Mixture Differential Evolution with enhanced covariance matrix update using a weighted average of individual covariances.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass AdaptiveGMDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential Evolution scaling factor\n        self.CR = 0.9  # Crossover rate\n        self.population = None\n        self.fitness_values = None\n        self.gmm_means = None\n        self.gmm_covariances = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        self.best_solution_overall = self.population[np.argmin(self.fitness_values)]\n        self.best_fitness_overall = np.min(self.fitness_values)\n\n        # Initialize GMM parameters (2 components initially)\n        self.gmm_means = np.array([self.best_solution_overall, np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)])\n        self.gmm_covariances = np.array([np.eye(self.dim), np.eye(self.dim)])\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                # Adaptive Gaussian Mixture Model Mutation\n                if np.random.rand() < 0.2:  # Probability of using GMM mutation\n                    weights = np.array([multivariate_normal.pdf(mutant, mean=mean, cov=cov) for mean, cov in zip(self.gmm_means, self.gmm_covariances)])\n                    weights /= np.sum(weights)\n                    chosen_component = np.random.choice(len(self.gmm_means), p=weights)\n                    mutant = np.random.multivariate_normal(self.gmm_means[chosen_component], self.gmm_covariances[chosen_component])\n\n\n                # Boundary handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n\n                # Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness < self.fitness_values[i]:\n                    self.population[i] = trial\n                    self.fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial\n\n            # Update GMM parameters (improved update)\n            top_indices = np.argsort(self.fitness_values)[:self.population_size] #Take top population_size solutions\n            weights = np.exp(-self.fitness_values[top_indices]/np.max(self.fitness_values[top_indices]))\n            weights /= np.sum(weights)\n            self.gmm_means = np.average(self.population[top_indices], axis=0, weights=weights)\n            self.gmm_covariances = np.average(np.array([np.cov(self.population[i].reshape(1,-1).T) for i in top_indices]), axis=0, weights=weights)\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGMDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.04.", "error": "", "parent_ids": ["ed0dc525-6733-4d21-8c7d-1612fce25dcb"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.0749072398174953]}}
{"id": "f36c14ad-5de3-4181-a3bf-532d3aebf4f4", "fitness": 0.025008270098881857, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with enhanced local search guided by fitness-based probability and dynamic CR adjustment.", "code": "import numpy as np\nimport random\n\nclass AdaptiveDE:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Scaling factor for mutation\n        self.CR = 0.9  # Crossover rate\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(np.array([self.best_solution_overall]))[0]\n        self.eval_count += 1\n\n        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        fitness_values = objective_function(population)\n        self.eval_count += self.population_size\n\n        for i in range(self.population_size):\n            if fitness_values[i] < self.best_fitness_overall:\n                self.best_fitness_overall = fitness_values[i]\n                self.best_solution_overall = population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            new_population = np.zeros_like(population)\n            for i in range(self.population_size):\n                # Adaptive Mutation Strategy\n                a, b, c = random.sample(range(self.population_size), 3)\n                while a == i or b == i or c == i:\n                    a, b, c = random.sample(range(self.population_size), 3)\n\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Local Search Enhancement:  Adjust mutation based on local fitness\n                fitness_probs = fitness_values[[a,b,c]] / np.sum(fitness_values[[a,b,c]])\n                best_index = np.argmin(fitness_values[[a,b,c]])\n                if best_index == 0:\n                    mutant = population[a] + 0.2 * (np.random.normal(0, 1, self.dim))\n                elif best_index == 1:\n                    mutant = population[b] + 0.2 * (np.random.normal(0, 1, self.dim))\n                else:\n                    mutant = population[c] + 0.2 * (np.random.normal(0, 1, self.dim))\n\n                #Boundary Handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n                #Crossover with dynamic CR\n                self.CR = 0.9 + 0.1 * np.exp(-self.eval_count / self.budget) #Dynamic CR\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                #Selection\n                trial_fitness = objective_function(np.array([trial]))[0]\n                self.eval_count += 1\n                if trial_fitness < fitness_values[i]:\n                    new_population[i] = trial\n                    fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial.copy()\n\n            population = new_population\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["08e7a677-9be6-46a4-8fea-90803110a680"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07502481029664557]}}
{"id": "cdfdbd89-1d66-4d80-91fa-acce86717cd8", "fitness": 0.025523622488174635, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with enhanced local search triggered by population diversity and stagnation detection.", "code": "import numpy as np\nimport random\n\nclass AdaptiveDE:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Scaling factor for mutation\n        self.CR = 0.9  # Crossover rate\n        self.stagnation_counter = 0 #New: Stagnation counter\n        self.stagnation_threshold = 10 * self.dim #New: Stagnation threshold\n\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(np.array([self.best_solution_overall]))[0]\n        self.eval_count += 1\n\n        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        fitness_values = objective_function(population)\n        self.eval_count += self.population_size\n\n        for i in range(self.population_size):\n            if fitness_values[i] < self.best_fitness_overall:\n                self.best_fitness_overall = fitness_values[i]\n                self.best_solution_overall = population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            new_population = np.zeros_like(population)\n            for i in range(self.population_size):\n                # Adaptive Mutation Strategy\n                a, b, c = random.sample(range(self.population_size), 3)\n                while a == i or b == i or c == i:\n                    a, b, c = random.sample(range(self.population_size), 3)\n\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Local Search Enhancement:  Adjust mutation based on local fitness and diversity\n                if fitness_values[a] < fitness_values[b] and fitness_values[a] < fitness_values[c] or self.stagnation_counter > self.stagnation_threshold: #NEW: Check for stagnation\n                    mutant = population[a] + 0.2*(np.random.normal(0,1, self.dim)) #small perturbation near better solution\n                elif fitness_values[b] < fitness_values[c]:\n                    mutant = population[b] + 0.2*(np.random.normal(0,1, self.dim))\n                else:\n                    mutant = population[c] + 0.2*(np.random.normal(0,1, self.dim))\n\n                #Boundary Handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                #Selection\n                trial_fitness = objective_function(np.array([trial]))[0]\n                self.eval_count += 1\n                if trial_fitness < fitness_values[i]:\n                    new_population[i] = trial\n                    fitness_values[i] = trial_fitness\n                    self.stagnation_counter = 0 #Reset stagnation counter\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial.copy()\n                else:\n                    self.stagnation_counter +=1 #Increment stagnation counter\n\n            population = new_population\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["08e7a677-9be6-46a4-8fea-90803110a680"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.0765708674645239]}}
{"id": "ce2e8b0f-fa92-4713-a235-e9de6b5862ad", "fitness": 0.02558240460543067, "name": "AdaptiveGMMDE", "description": "Adaptive Differential Evolution with enhanced local search guided by a Gaussian Mixture Model of the population's fitness landscape.", "code": "import numpy as np\nimport random\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveGMMDE:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Scaling factor for mutation\n        self.CR = 0.9  # Crossover rate\n        self.gmm = GaussianMixture(n_components=3, random_state=0) # Initialize GMM\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(np.array([self.best_solution_overall]))[0]\n        self.eval_count += 1\n\n        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        fitness_values = objective_function(population)\n        self.eval_count += self.population_size\n\n        for i in range(self.population_size):\n            if fitness_values[i] < self.best_fitness_overall:\n                self.best_fitness_overall = fitness_values[i]\n                self.best_solution_overall = population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            new_population = np.zeros_like(population)\n            for i in range(self.population_size):\n                # Adaptive Mutation Strategy\n                a, b, c = random.sample(range(self.population_size), 3)\n                while a == i or b == i or c == i:\n                    a, b, c = random.sample(range(self.population_size), 3)\n\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # GMM-guided Local Search Enhancement\n                self.gmm.fit(population, sample_weight=fitness_values) # Fit GMM to current population and fitness\n                means = self.gmm.means_\n                covariances = self.gmm.covariances_\n                weights = self.gmm.weights_\n                \n                best_component = np.argmin(means[:,0]) #Crude best component selection\n                mutant = np.random.multivariate_normal(means[best_component], covariances[best_component])\n\n                #Boundary Handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                #Selection\n                trial_fitness = objective_function(np.array([trial]))[0]\n                self.eval_count += 1\n                if trial_fitness < fitness_values[i]:\n                    new_population[i] = trial\n                    fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial.copy()\n\n            population = new_population\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGMMDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["08e7a677-9be6-46a4-8fea-90803110a680"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07674721381629201]}}
{"id": "3723100b-d3dc-4064-9c19-688676291814", "fitness": 0.02434379095954445, "name": "AdaptiveGMMDEImproved", "description": "Adaptive GMM-DE with enhanced Gaussian Mixture Model component adaptation and diversity preservation.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveGMMDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.population = None\n        self.fitness_values = None\n        self.F = 0.8 # Differential Evolution scaling factor\n        self.CR = 0.9 # Crossover rate\n        self.gmm_components = 3 # Number of GMM components\n        self.diversity_threshold = 0.1 # Threshold for diversity preservation\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = np.full(self.population_size, np.inf)\n\n    def bound_solution(self, solution):\n        solution = np.clip(solution, self.lower_bounds, self.upper_bounds)\n        return solution\n\n    def differential_evolution_step(self, individual_index):\n        a, b, c = np.random.choice(np.delete(np.arange(self.population_size), individual_index), 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        mutant = self.bound_solution(mutant)\n        trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[individual_index])\n        return trial\n\n    def gmm_adaptive_mutation(self, individual):\n        if self.eval_count > self.population_size: \n            X = self.population[self.fitness_values < np.inf] \n            if X.shape[0] > self.gmm_components: \n                # Adaptive component selection based on fitness\n                fitness_subset = self.fitness_values[self.fitness_values < np.inf]\n                weights = fitness_subset / np.sum(fitness_subset)\n                gmm = GaussianMixture(n_components=self.gmm_components, covariance_type='full', weights_init=weights, random_state=0).fit(X)\n                means = gmm.means_\n                covariances = gmm.covariances_\n                weights = gmm.weights_\n\n                component_index = np.random.choice(self.gmm_components, p=weights)\n                new_point = multivariate_normal.rvs(mean=means[component_index], cov=covariances[component_index])\n                new_point = self.bound_solution(new_point)\n                return new_point\n            else: return individual\n        else: return individual\n\n    def preserve_diversity(self):\n        # Calculate population diversity\n        diversity = np.mean(np.std(self.population, axis=0))\n        if diversity < self.diversity_threshold:\n            # Inject new random individuals to increase diversity\n            num_new = int(0.1*self.population_size)\n            new_individuals = np.random.uniform(self.lower_bounds, self.upper_bounds,(num_new, self.dim))\n            self.population = np.concatenate((self.population, new_individuals), axis=0)\n            self.fitness_values = np.concatenate((self.fitness_values, np.full(num_new, np.inf)))\n            \n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.initialize_population()\n\n        for i in range(self.population_size):\n            if self.eval_count < self.budget:\n                fitness = objective_function(self.population[i:i+1, :])[0]\n                self.fitness_values[i] = fitness\n                self.eval_count += 1\n                if fitness < self.best_fitness_overall:\n                    self.best_fitness_overall = fitness\n                    self.best_solution_overall = self.population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count < self.budget:\n                    trial_vector = self.differential_evolution_step(i)\n                    trial_vector = self.gmm_adaptive_mutation(trial_vector) \n                    trial_fitness = objective_function(trial_vector.reshape(1,-1))[0]\n                    self.eval_count += 1\n\n                    if trial_fitness < self.fitness_values[i]:\n                        self.fitness_values[i] = trial_fitness\n                        self.population[i, :] = trial_vector.copy()\n                        if trial_fitness < self.best_fitness_overall:\n                            self.best_fitness_overall = trial_fitness\n                            self.best_solution_overall = trial_vector.copy()\n            self.preserve_diversity() # Preserve diversity after each generation\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\nfrom sklearn.mixture import GaussianMixture", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGMMDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.03.", "error": "", "parent_ids": ["8f13a12d-6863-4b0c-8b88-5fdd157e1bb5"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07303137287863336]}}
{"id": "d5ad26b7-d774-48b8-8698-260d4bec77a8", "fitness": 0.024043491022502236, "name": "AdaptiveGMMDE_Archive", "description": "A hybrid algorithm combining Differential Evolution with a novel adaptive mutation strategy based on a Gaussian Mixture Model and an archive to enhance exploration and exploitation.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveGMMDE_Archive:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.population = None\n        self.fitness_values = None\n        self.F = 0.8 # Differential Evolution scaling factor\n        self.CR = 0.9 # Crossover rate\n        self.gmm_components = 3 # Number of GMM components\n        self.archive_size = 100\n        self.archive = None\n        self.archive_fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = np.full(self.population_size, np.inf)\n        self.archive = np.empty((0, self.dim))\n        self.archive_fitness = np.array([])\n\n    def bound_solution(self, solution):\n        solution = np.clip(solution, self.lower_bounds, self.upper_bounds)\n        return solution\n\n    def differential_evolution_step(self, individual_index):\n        a, b, c = np.random.choice(np.delete(np.arange(self.population_size), individual_index), 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        mutant = self.bound_solution(mutant)\n        trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[individual_index])\n        return trial\n\n    def gmm_adaptive_mutation(self, individual):\n        if self.eval_count > self.population_size:\n            X = np.concatenate((self.population[self.fitness_values < np.inf], self.archive))\n            if X.shape[0] > self.gmm_components:\n                gmm = GaussianMixture(n_components=self.gmm_components, covariance_type='full', random_state=0).fit(X)\n                means = gmm.means_\n                covariances = gmm.covariances_\n                weights = gmm.weights_\n\n                component_index = np.random.choice(self.gmm_components, p=weights)\n                new_point = multivariate_normal.rvs(mean=means[component_index], cov=covariances[component_index])\n                new_point = self.bound_solution(new_point)\n                return new_point\n            else: return individual\n        else: return individual\n\n    def update_archive(self, solution, fitness):\n        if self.archive.shape[0] < self.archive_size:\n            self.archive = np.vstack((self.archive, solution))\n            self.archive_fitness = np.append(self.archive_fitness, fitness)\n        else:\n            worst_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[worst_index]:\n                self.archive[worst_index] = solution\n                self.archive_fitness[worst_index] = fitness\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.initialize_population()\n\n        for i in range(self.population_size):\n            if self.eval_count < self.budget:\n                fitness = objective_function(self.population[i:i+1, :])[0]\n                self.fitness_values[i] = fitness\n                self.eval_count += 1\n                self.update_archive(self.population[i,:], fitness)\n                if fitness < self.best_fitness_overall:\n                    self.best_fitness_overall = fitness\n                    self.best_solution_overall = self.population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count < self.budget:\n                    trial_vector = self.differential_evolution_step(i)\n                    trial_vector = self.gmm_adaptive_mutation(trial_vector)\n                    trial_fitness = objective_function(trial_vector.reshape(1,-1))[0]\n                    self.eval_count += 1\n                    self.update_archive(trial_vector, trial_fitness)\n\n                    if trial_fitness < self.fitness_values[i]:\n                        self.fitness_values[i] = trial_fitness\n                        self.population[i, :] = trial_vector.copy()\n                        if trial_fitness < self.best_fitness_overall:\n                            self.best_fitness_overall = trial_fitness\n                            self.best_solution_overall = trial_vector.copy()\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\nfrom sklearn.mixture import GaussianMixture", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGMMDE_Archive got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.03.", "error": "", "parent_ids": ["8f13a12d-6863-4b0c-8b88-5fdd157e1bb5"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07213047306750671]}}
{"id": "787695b4-7787-43af-afab-96dea84a71fa", "fitness": 0.023419601749147104, "name": "AdaptiveGMMDE_Enhanced", "description": "AdaptiveGMMDE enhanced with an archive and selection pressure to maintain diversity and focus exploration.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom sklearn.mixture import GaussianMixture\n\nclass AdaptiveGMMDE_Enhanced:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.population = None\n        self.fitness_values = None\n        self.F = 0.8 # Differential Evolution scaling factor\n        self.CR = 0.9 # Crossover rate\n        self.gmm_components = 3 # Number of GMM components\n        self.archive = [] #New: Archive to store diverse solutions\n        self.archive_size = 50 #New: Max size of the archive\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = np.full(self.population_size, np.inf)\n\n    def bound_solution(self, solution):\n        solution = np.clip(solution, self.lower_bounds, self.upper_bounds)\n        return solution\n\n    def differential_evolution_step(self, individual_index):\n        a, b, c = np.random.choice(np.delete(np.arange(self.population_size), individual_index), 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        mutant = self.bound_solution(mutant)\n        trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[individual_index])\n        return trial\n\n    def gmm_adaptive_mutation(self, individual):\n        if self.eval_count > self.population_size: # only after initial evaluation\n            X = self.population[self.fitness_values < np.inf] # Avoid NaN\n            if X.shape[0] > self.gmm_components: # prevent errors in fit\n                gmm = GaussianMixture(n_components=self.gmm_components, covariance_type='full', random_state=0).fit(X)\n                means = gmm.means_\n                covariances = gmm.covariances_\n                weights = gmm.weights_\n\n                component_index = np.random.choice(self.gmm_components, p=weights)\n                new_point = multivariate_normal.rvs(mean=means[component_index], cov=covariances[component_index])\n                new_point = self.bound_solution(new_point)\n\n                return new_point\n            else: return individual\n        else: return individual\n\n    def update_archive(self, solution, fitness): #New: Archive update\n        if len(self.archive) < self.archive_size:\n            self.archive.append((solution, fitness))\n        else:\n            worst_solution, worst_fitness = max(self.archive, key=lambda item: item[1])\n            if fitness < worst_fitness:\n                self.archive.remove((worst_solution, worst_fitness))\n                self.archive.append((solution, fitness))\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.initialize_population()\n\n        for i in range(self.population_size):\n            if self.eval_count < self.budget:\n                fitness = objective_function(self.population[i:i+1, :])[0]\n                self.fitness_values[i] = fitness\n                self.eval_count += 1\n                self.update_archive(self.population[i,:].copy(), fitness) #New\n                if fitness < self.best_fitness_overall:\n                    self.best_fitness_overall = fitness\n                    self.best_solution_overall = self.population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count < self.budget:\n                    trial_vector = self.differential_evolution_step(i)\n                    trial_vector = self.gmm_adaptive_mutation(trial_vector) # Adaptive mutation\n                    trial_fitness = objective_function(trial_vector.reshape(1,-1))[0]\n                    self.eval_count += 1\n                    self.update_archive(trial_vector.copy(), trial_fitness) #New\n\n                    if trial_fitness < self.fitness_values[i]:\n                        self.fitness_values[i] = trial_fitness\n                        self.population[i, :] = trial_vector.copy()\n                        if trial_fitness < self.best_fitness_overall:\n                            self.best_fitness_overall = trial_fitness\n                            self.best_solution_overall = trial_vector.copy()\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info\n\nfrom sklearn.mixture import GaussianMixture", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGMMDE_Enhanced got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.03.", "error": "", "parent_ids": ["8f13a12d-6863-4b0c-8b88-5fdd157e1bb5"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07025880524744131]}}
{"id": "310346e9-a48d-4ccc-b707-da725acfcfbb", "fitness": 0.025358721579099405, "name": "AdaptiveDEImproved", "description": "Adaptive Differential Evolution with improved covariance matrix adaptation using a weighted average of past successful solutions.", "code": "import numpy as np\n\nclass AdaptiveDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover rate\n        self.covariance_matrix = np.eye(self.dim)  # Initialize covariance matrix\n        self.learning_rate = 0.1 # Learning rate for covariance matrix adaptation\n        self.successful_solutions = [] #Store successful solutions for weighted average\n\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        if self.dim > 0:\n            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        else:\n            self.population = np.array([])\n        fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        best_index = np.argmin(fitness_values)\n        self.best_solution_overall = self.population[best_index].copy()\n        self.best_fitness_overall = fitness_values[best_index]\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                #Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                #Adaptive Mutation using covariance matrix\n                mutated_vector = np.random.multivariate_normal(mutant, self.covariance_matrix)\n                mutated_vector = np.clip(mutated_vector, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutated_vector, self.population[i])\n\n                #Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness[0] < fitness_values[i]:\n                    self.population[i] = trial\n                    fitness_values[i] = trial_fitness[0]\n                    if trial_fitness[0] < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness[0]\n                        self.best_solution_overall = trial.copy()\n                        self.successful_solutions.append(trial)\n\n            #Covariance Matrix Adaptation using weighted average of successful solutions\n            if len(self.successful_solutions) > 0:\n                weights = np.exp(-np.array([np.linalg.norm(s - self.best_solution_overall) for s in self.successful_solutions]))\n                weights = weights / np.sum(weights)\n                weighted_mean = np.average(np.array(self.successful_solutions), axis=0, weights=weights)\n                mean_diff = weighted_mean - np.mean(self.population, axis=0)\n                self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.outer(mean_diff, mean_diff)\n                self.covariance_matrix = (self.covariance_matrix + self.covariance_matrix.T) / 2 #ensure symmetry\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["21cc6fb5-0378-42df-909f-0f7afe37a5c3"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07607616473729821]}}
{"id": "2e793b58-d209-47b3-9815-8f90669a72b6", "fitness": 0.024537412702493443, "name": "AdaptiveDEImproved", "description": "Adaptive Differential Evolution with enhanced covariance matrix adaptation using a weighted average of recent successful solutions and population diversity control.", "code": "import numpy as np\n\nclass AdaptiveDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover rate\n        self.covariance_matrix = np.eye(self.dim)  # Initialize covariance matrix\n        self.learning_rate = 0.1 # Learning rate for covariance matrix adaptation\n        self.success_archive = [] #Added line for success archive\n        self.archive_size = 10 #Added line for archive size\n\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        if self.dim > 0:\n            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        else:\n            self.population = np.array([])\n        fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        best_index = np.argmin(fitness_values)\n        self.best_solution_overall = self.population[best_index].copy()\n        self.best_fitness_overall = fitness_values[best_index]\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                #Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                #Adaptive Mutation using covariance matrix\n                mutated_vector = np.random.multivariate_normal(mutant, self.covariance_matrix)\n                mutated_vector = np.clip(mutated_vector, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutated_vector, self.population[i])\n\n                #Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness[0] < fitness_values[i]:\n                    self.population[i] = trial\n                    fitness_values[i] = trial_fitness[0]\n                    if trial_fitness[0] < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness[0]\n                        self.best_solution_overall = trial.copy()\n                        self.success_archive.append(trial) #Added line to add successful solutions to archive.\n\n            #Covariance Matrix Adaptation - modified to include archive\n            if len(self.success_archive) > self.archive_size:\n                self.success_archive = self.success_archive[-self.archive_size:]\n            mean_diff = np.mean(np.array(self.success_archive) - np.mean(np.array(self.success_archive), axis=0),axis=0)\n            self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.outer(mean_diff, mean_diff)\n            self.covariance_matrix = (self.covariance_matrix + self.covariance_matrix.T) / 2 #ensure symmetry\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.03.", "error": "", "parent_ids": ["21cc6fb5-0378-42df-909f-0f7afe37a5c3"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07361223810748033]}}
{"id": "4057fce3-2004-413d-9748-a6ba738d6488", "fitness": 0.02524638415050939, "name": "AdaptiveDEImproved2", "description": "Adaptive Differential Evolution with enhanced covariance matrix adaptation using a weighted average of past successful solutions and a diversity-promoting mutation operator.", "code": "import numpy as np\n\nclass AdaptiveDEImproved2:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover rate\n        self.covariance_matrix = np.eye(self.dim)  # Initialize covariance matrix\n        self.learning_rate = 0.1 # Learning rate for covariance matrix adaptation\n        self.successful_solutions = [] #Archive of successful solutions\n        self.success_weight = 0.5 #Weight given to successful solutions\n\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        if self.dim > 0:\n            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        else:\n            self.population = np.array([])\n        fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        best_index = np.argmin(fitness_values)\n        self.best_solution_overall = self.population[best_index].copy()\n        self.best_fitness_overall = fitness_values[best_index]\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                #Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                #Adaptive Mutation using covariance matrix and successful solutions\n                if self.successful_solutions:\n                    weighted_mean = self.success_weight * np.mean(self.successful_solutions, axis=0) + (1-self.success_weight) * mutant\n                    mutated_vector = np.random.multivariate_normal(weighted_mean, self.covariance_matrix)\n                else:\n                    mutated_vector = np.random.multivariate_normal(mutant, self.covariance_matrix)\n\n                mutated_vector = np.clip(mutated_vector, self.lower_bounds, self.upper_bounds)\n\n                #Diversity Promoting Mutation (small random perturbation)\n                mutated_vector += np.random.normal(0, 0.1, self.dim) #Added line\n                mutated_vector = np.clip(mutated_vector, self.lower_bounds, self.upper_bounds) #Added line\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutated_vector, self.population[i])\n\n                #Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness[0] < fitness_values[i]:\n                    self.population[i] = trial\n                    fitness_values[i] = trial_fitness[0]\n                    if trial_fitness[0] < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness[0]\n                        self.best_solution_overall = trial.copy()\n                        self.successful_solutions.append(trial)\n\n            #Covariance Matrix Adaptation\n            mean_diff = np.mean(self.population - np.mean(self.population, axis=0),axis=0)\n            self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.outer(mean_diff, mean_diff)\n            self.covariance_matrix = (self.covariance_matrix + self.covariance_matrix.T) / 2 #ensure symmetry\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEImproved2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["21cc6fb5-0378-42df-909f-0f7afe37a5c3"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07573915245152817]}}
{"id": "003ac914-5a95-4267-8d3c-25da3e92e953", "fitness": 0.02559364473217757, "name": "AdaptiveGMDEImproved", "description": "Adaptive Differential Evolution with Gaussian Mixture Model mutation and covariance matrix adaptation based on a weighted average of top performers for improved exploration and exploitation.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass AdaptiveGMDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential Evolution scaling factor\n        self.CR = 0.9  # Crossover rate\n        self.population = None\n        self.fitness_values = None\n        self.gmm_means = None\n        self.gmm_covariances = None\n        self.weights = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        self.best_solution_overall = self.population[np.argmin(self.fitness_values)]\n        self.best_fitness_overall = np.min(self.fitness_values)\n\n        # Initialize GMM parameters (2 components initially)\n        self.gmm_means = np.array([self.best_solution_overall, np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)])\n        self.gmm_covariances = np.array([np.eye(self.dim), np.eye(self.dim)])\n        self.weights = np.array([0.7,0.3]) #Initialize weights\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                # Adaptive Gaussian Mixture Model Mutation\n                if np.random.rand() < 0.2:  # Probability of using GMM mutation\n                    weights = np.array([multivariate_normal.pdf(mutant, mean=mean, cov=cov) for mean, cov in zip(self.gmm_means, self.gmm_covariances)])\n                    weights /= np.sum(weights)\n                    chosen_component = np.random.choice(len(self.gmm_means), p=weights)\n                    mutant = np.random.multivariate_normal(self.gmm_means[chosen_component], self.gmm_covariances[chosen_component])\n\n\n                # Boundary handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n\n                # Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness < self.fitness_values[i]:\n                    self.population[i] = trial\n                    self.fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial\n\n            # Update GMM parameters (weighted average of top performers)\n            top_indices = np.argsort(self.fitness_values)[:2]  #Take top 2 solutions\n            self.gmm_means = np.average(self.population[top_indices], axis=0, weights=self.weights)\n            self.gmm_covariances = np.array([np.cov(self.population.T), np.cov(self.population.T)])\n            self.weights = np.array([0.6, 0.4])\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGMDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["ed0dc525-6733-4d21-8c7d-1612fce25dcb"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07678093419653272]}}
{"id": "a33ec985-be93-4db2-8896-5af1ea5e8f4a", "fitness": 0.02495376173155311, "name": "AdaptiveDEImproved", "description": "Adaptive Differential Evolution with enhanced covariance matrix adaptation using a weighted average of past successful solutions.", "code": "import numpy as np\n\nclass AdaptiveDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover rate\n        self.covariance_matrix = np.eye(self.dim)  # Initialize covariance matrix\n        self.learning_rate = 0.1 # Learning rate for covariance matrix adaptation\n        self.successful_solutions = [] # added to store successful solutions\n\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        if self.dim > 0:\n            self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        else:\n            self.population = np.array([])\n        fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        best_index = np.argmin(fitness_values)\n        self.best_solution_overall = self.population[best_index].copy()\n        self.best_fitness_overall = fitness_values[best_index]\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                #Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                #Adaptive Mutation using covariance matrix\n                mutated_vector = np.random.multivariate_normal(mutant, self.covariance_matrix)\n                mutated_vector = np.clip(mutated_vector, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutated_vector, self.population[i])\n\n                #Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness[0] < fitness_values[i]:\n                    self.population[i] = trial\n                    fitness_values[i] = trial_fitness[0]\n                    self.successful_solutions.append(trial) #Store successful solutions\n\n                    if trial_fitness[0] < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness[0]\n                        self.best_solution_overall = trial.copy()\n\n            #Covariance Matrix Adaptation using weighted average of successful solutions\n            if len(self.successful_solutions)>0:\n                weights = np.exp(-np.array([objective_function(sol.reshape(1,-1))[0] for sol in self.successful_solutions]))\n                weights /= np.sum(weights)\n                weighted_avg = np.average(self.successful_solutions, axis=0, weights=weights)\n                mean_diff = weighted_avg - np.mean(self.population,axis=0)\n                self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.outer(mean_diff, mean_diff)\n            else:\n                mean_diff = np.mean(self.population - np.mean(self.population, axis=0),axis=0)\n                self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.outer(mean_diff, mean_diff)\n            self.covariance_matrix = (self.covariance_matrix + self.covariance_matrix.T) / 2 #ensure symmetry\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.04.", "error": "", "parent_ids": ["21cc6fb5-0378-42df-909f-0f7afe37a5c3"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07486128519465933]}}
{"id": "f20be657-73b1-41de-ba99-ade27992c073", "fitness": 0.025290756601359335, "name": "AdaptiveGMDEImproved", "description": "Adaptive Gaussian Mixture Differential Evolution with enhanced covariance matrix adaptation and elitism for improved convergence.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass AdaptiveGMDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential Evolution scaling factor\n        self.CR = 0.9  # Crossover rate\n        self.population = None\n        self.fitness_values = None\n        self.gmm_means = None\n        self.gmm_covariances = None\n        self.elite_solutions = []\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        self.best_solution_overall = self.population[np.argmin(self.fitness_values)]\n        self.best_fitness_overall = np.min(self.fitness_values)\n\n        # Initialize GMM parameters (2 components initially)\n        self.gmm_means = np.array([self.best_solution_overall, np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)])\n        self.gmm_covariances = np.array([np.eye(self.dim), np.eye(self.dim)])\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                # Adaptive Gaussian Mixture Model Mutation\n                if np.random.rand() < 0.3:  # Increased probability of using GMM mutation\n                    weights = np.array([multivariate_normal.pdf(mutant, mean=mean, cov=cov) for mean, cov in zip(self.gmm_means, self.gmm_covariances)])\n                    weights /= np.sum(weights)\n                    chosen_component = np.random.choice(len(self.gmm_means), p=weights)\n                    mutant = np.random.multivariate_normal(self.gmm_means[chosen_component], self.gmm_covariances[chosen_component])\n\n\n                # Boundary handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n\n                # Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness < self.fitness_values[i]:\n                    self.population[i] = trial\n                    self.fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial\n                        self.elite_solutions.append((self.best_solution_overall, self.best_fitness_overall)) #Elite solutions tracking\n\n\n            # Update GMM parameters (improved update - using top 5 solutions and covariance matrix adaptation)\n            top_indices = np.argsort(self.fitness_values)[:5]  #Take top 5 solutions\n            self.gmm_means = self.population[top_indices]\n            C = np.cov(self.population[top_indices].T) #Covariance matrix of top solutions\n            self.gmm_covariances = np.array([C,C]) #Adapt covariance matrix\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGMDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["ed0dc525-6733-4d21-8c7d-1612fce25dcb"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.075872269804078]}}
{"id": "d80f1be5-c247-4d74-8c90-4ce922d40a54", "fitness": 0.02592536553708845, "name": "AdaptiveGMDEImproved", "description": "AdaptiveGMDE enhanced with a more sophisticated GMM update strategy focusing on diverse high-performing solutions and elitism.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass AdaptiveGMDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential Evolution scaling factor\n        self.CR = 0.9  # Crossover rate\n        self.population = None\n        self.fitness_values = None\n        self.gmm_means = None\n        self.gmm_covariances = None\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        self.best_solution_overall = self.population[np.argmin(self.fitness_values)]\n        self.best_fitness_overall = np.min(self.fitness_values)\n\n        # Initialize GMM parameters (2 components initially)\n        self.gmm_means = np.array([self.best_solution_overall, np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)])\n        self.gmm_covariances = np.array([np.eye(self.dim), np.eye(self.dim)])\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                # Adaptive Gaussian Mixture Model Mutation\n                if np.random.rand() < 0.3:  # Increased probability of using GMM mutation\n                    weights = np.array([multivariate_normal.pdf(mutant, mean=mean, cov=cov) for mean, cov in zip(self.gmm_means, self.gmm_covariances)])\n                    weights /= np.sum(weights)\n                    chosen_component = np.random.choice(len(self.gmm_means), p=weights)\n                    mutant = np.random.multivariate_normal(self.gmm_means[chosen_component], self.gmm_covariances[chosen_component])\n\n\n                # Boundary handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n\n                # Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness < self.fitness_values[i]:\n                    self.population[i] = trial\n                    self.fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial\n\n            # Update GMM parameters (improved update)\n            top_indices = np.argsort(self.fitness_values)[:10]  # Take top 10 diverse solutions\n            self.gmm_means = self.population[top_indices]\n            self.gmm_covariances = np.array([np.cov(self.population[idx].reshape(1,-1).T) + 0.1 * np.eye(self.dim) for idx in top_indices]) #Regularization\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGMDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["d40369ab-cf74-4de0-b6ac-b7426685e7d1"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07777609661126535]}}
{"id": "a21f30f1-9e5b-43e4-903d-9946aa3fa0cf", "fitness": 0.025009290527428507, "name": "AdaptiveGMDEImproved", "description": "Adaptive Gaussian Mixture Differential Evolution with elitism and adaptive covariance matrix scaling based on solution diversity.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass AdaptiveGMDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Differential Evolution scaling factor\n        self.CR = 0.9  # Crossover rate\n        self.population = None\n        self.fitness_values = None\n        self.gmm_means = None\n        self.gmm_covariances = None\n        self.archive = [] #added\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        self.fitness_values = objective_function(self.population)\n        self.eval_count += self.population_size\n        self.best_solution_overall = self.population[np.argmin(self.fitness_values)]\n        self.best_fitness_overall = np.min(self.fitness_values)\n\n        # Initialize GMM parameters (2 components initially)\n        self.gmm_means = np.array([self.best_solution_overall, np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)])\n        self.gmm_covariances = np.array([np.eye(self.dim), np.eye(self.dim)])\n\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                # Differential Evolution Mutation\n                a, b, c = np.random.choice(np.delete(np.arange(self.population_size), i), 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                # Adaptive Gaussian Mixture Model Mutation\n                if np.random.rand() < 0.2:  # Probability of using GMM mutation\n                    weights = np.array([multivariate_normal.pdf(mutant, mean=mean, cov=cov) for mean, cov in zip(self.gmm_means, self.gmm_covariances)])\n                    weights /= np.sum(weights)\n                    chosen_component = np.random.choice(len(self.gmm_means), p=weights)\n                    mutant = np.random.multivariate_normal(self.gmm_means[chosen_component], self.gmm_covariances[chosen_component])\n\n\n                # Boundary handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n\n                # Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])\n\n                # Selection\n                trial_fitness = objective_function(trial.reshape(1, -1))\n                self.eval_count += 1\n                if trial_fitness < self.fitness_values[i]:\n                    self.population[i] = trial\n                    self.fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial\n                        \n            # Update GMM parameters (improved update)\n            top_indices = np.argsort(self.fitness_values)[:int(0.1 * self.population_size)] #top 10%\n            self.gmm_means = self.population[top_indices]\n            #Scale covariance matrices based on diversity.  Higher diversity, smaller scaling.\n            diversity = np.mean(np.linalg.norm(self.gmm_means - np.mean(self.gmm_means, axis=0), axis=1))\n            scaling_factor = 1.0 / (1 + diversity)\n            self.gmm_covariances = np.array([np.cov(self.population[idx].reshape(1,-1).T) * scaling_factor for idx in top_indices])\n            \n            #Add top solutions to archive\n            self.archive.extend(self.population[top_indices])\n            #if len(self.archive)>self.population_size*2:\n            #    self.archive = self.archive[-self.population_size*2:]\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGMDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["d40369ab-cf74-4de0-b6ac-b7426685e7d1"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07502787158228552]}}
{"id": "a1e920c3-be22-48fb-bee5-ff2e3b010450", "fitness": 0.025510906712052625, "name": "AdaptiveDEImprovedLocalSearch", "description": "Adaptive Differential Evolution with enhanced local search using a weighted average of best solutions and Gaussian perturbation.", "code": "import numpy as np\nimport random\n\nclass AdaptiveDEImprovedLocalSearch:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Scaling factor for mutation\n        self.CR = 0.9  # Crossover rate\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(np.array([self.best_solution_overall]))[0]\n        self.eval_count += 1\n\n        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        fitness_values = objective_function(population)\n        self.eval_count += self.population_size\n\n        for i in range(self.population_size):\n            if fitness_values[i] < self.best_fitness_overall:\n                self.best_fitness_overall = fitness_values[i]\n                self.best_solution_overall = population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            new_population = np.zeros_like(population)\n            for i in range(self.population_size):\n                # Adaptive Mutation Strategy\n                a, b, c = random.sample(range(self.population_size), 3)\n                while a == i or b == i or c == i:\n                    a, b, c = random.sample(range(self.population_size), 3)\n\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Improved Local Search Enhancement: Weighted average of best solutions and Gaussian perturbation\n                best_indices = np.argsort(fitness_values)[:3]  #Consider top 3\n                weighted_avg = np.average(population[best_indices], axis=0, weights=np.array([0.5, 0.3, 0.2]))\n                mutant = 0.7*mutant + 0.3*weighted_avg + 0.2*(np.random.normal(0,1, self.dim))\n\n\n                #Boundary Handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                #Selection\n                trial_fitness = objective_function(np.array([trial]))[0]\n                self.eval_count += 1\n                if trial_fitness < fitness_values[i]:\n                    new_population[i] = trial\n                    fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial.copy()\n\n            population = new_population\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEImprovedLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.03 with standard deviation 0.04.", "error": "", "parent_ids": ["08e7a677-9be6-46a4-8fea-90803110a680"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07653272013615788]}}
{"id": "1996662a-e029-4572-8b08-f9b6cbc22c1f", "fitness": 0.024887651498576375, "name": "AdaptiveDEImproved", "description": "Adaptive Differential Evolution with enhanced local search, diversity preservation, and adaptive control of mutation strength based on solution density.", "code": "import numpy as np\nimport random\n\nclass AdaptiveDEImproved:\n    def __init__(self, budget: int, dim: int, lower_bounds: list[float], upper_bounds: list[float]):\n        self.budget = int(budget)\n        self.dim = int(dim)\n        self.lower_bounds = np.array(lower_bounds, dtype=float)\n        self.upper_bounds = np.array(upper_bounds, dtype=float)\n\n        self.eval_count = 0\n        self.best_solution_overall = None\n        self.best_fitness_overall = float('inf')\n        self.population_size = 10 * self.dim  # Adaptive population size\n        self.F = 0.8  # Scaling factor for mutation\n        self.CR = 0.9  # Crossover rate\n        self.F_adapt = 0.5 # Initial adaptive mutation factor\n        self.density_threshold = 0.8\n\n    def optimize(self, objective_function: callable, acceptance_threshold: float = 1e-8) -> tuple:\n        self.eval_count = 0\n        self.best_solution_overall = np.random.uniform(self.lower_bounds, self.upper_bounds, self.dim)\n        self.best_fitness_overall = objective_function(np.array([self.best_solution_overall]))[0]\n        self.eval_count += 1\n\n        population = np.random.uniform(self.lower_bounds, self.upper_bounds, (self.population_size, self.dim))\n        fitness_values = objective_function(population)\n        self.eval_count += self.population_size\n\n        for i in range(self.population_size):\n            if fitness_values[i] < self.best_fitness_overall:\n                self.best_fitness_overall = fitness_values[i]\n                self.best_solution_overall = population[i, :].copy()\n\n        while self.eval_count < self.budget:\n            new_population = np.zeros_like(population)\n            for i in range(self.population_size):\n                # Adaptive Mutation Strategy\n                a, b, c = random.sample(range(self.population_size), 3)\n                while a == i or b == i or c == i:\n                    a, b, c = random.sample(range(self.population_size), 3)\n\n                mutant = population[a] + self.F_adapt * (population[b] - population[c])\n\n                # Local Search Enhancement:  Adjust mutation based on local fitness and density\n                distances = np.linalg.norm(population - population[i], axis=1)\n                density = np.sum(distances < np.mean(distances)) / self.population_size\n\n                if density > self.density_threshold:\n                    self.F_adapt = min(1.0, self.F_adapt + 0.1) #Increase Mutation\n                else:\n                    self.F_adapt = max(0.1, self.F_adapt - 0.05) #Decrease Mutation\n\n\n                #Boundary Handling\n                mutant = np.clip(mutant, self.lower_bounds, self.upper_bounds)\n\n                #Crossover\n                trial = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                #Selection\n                trial_fitness = objective_function(np.array([trial]))[0]\n                self.eval_count += 1\n                if trial_fitness < fitness_values[i]:\n                    new_population[i] = trial\n                    fitness_values[i] = trial_fitness\n                    if trial_fitness < self.best_fitness_overall:\n                        self.best_fitness_overall = trial_fitness\n                        self.best_solution_overall = trial.copy()\n\n            population = new_population\n\n\n        optimization_info = {\n            'function_evaluations_used': self.eval_count,\n            'final_best_fitness': self.best_fitness_overall\n        }\n        return self.best_solution_overall, self.best_fitness_overall, optimization_info", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEImproved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.02 with standard deviation 0.04.", "error": "", "parent_ids": ["08e7a677-9be6-46a4-8fea-90803110a680"], "operator": null, "metadata": {"aucs": [0.0, 0.0, 0.07466295449572913]}}
